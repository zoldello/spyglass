{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Spyglass", "text": "<p>Spyglass is a data analysis framework that facilitates the storage, analysis, and sharing of neuroscience data to support reproducible research. It is designed to be interoperable with the NWB format and integrates open-source tools into a coherent framework.</p>"}, {"location": "#installation", "title": "Installation", "text": "<p>To install to this project, see Installation.</p>"}, {"location": "#contributing", "title": "Contributing", "text": "<p>For contribution instructions see How to Contribute</p>"}, {"location": "#citing-spyglass", "title": "Citing Spyglass", "text": "<p>Kyu Hyun Lee, Eric Denovellis, Ryan Ly, Jeremy Magland, Jeff Soules, Alison Comrie, Jennifer Guidera, Rhino Nevers, Daniel Gramling, Philip Adenekan, Ji Hyun Bak, Emily Monroe, Andrew Tritt, Oliver R\u00fcbel, Thinh Nguyen, Dimitri Yatsenko, Joshua Chu, Caleb Kemere, Samuel Garcia, Alessio Buccino, Emily Aery Jones, Lisa Giocomo, and Loren Frank. 'Spyglass: A Data Analysis Framework for Reproducible and Shareable Neuroscience Research.' (2022) Society for Neuroscience, San Diego, CA.</p>"}, {"location": "CHANGELOG/", "title": "Change Log", "text": ""}, {"location": "CHANGELOG/#041-unreleased", "title": "0.4.1 (Unreleased)", "text": "<ul> <li>Add mkdocs automated deployment</li> </ul>"}, {"location": "CHANGELOG/#040-may-22-2023", "title": "0.4.0 (May 22, 2023)", "text": "<ul> <li>Updated call to <code>spikeinterface.preprocessing.whiten</code> to use dtype np.float16.   #446,</li> <li>Updated default spike sorting metric parameters. #447</li> <li>Updated whitening to be compatible with recent changes in spikeinterface when   using mountainsort. #449</li> <li>Moved LFP pipeline to <code>src/spyglass/lfp/v1</code> and addressed related usability   issues. #468, #478, #482, #484, #504</li> <li>Removed whiten parameter for clusterless thresholder. #454</li> <li>Added plot to plot all DIO events in a session. #457</li> <li>Added file sharing functionality through kachery_cloud. #458, #460</li> <li>Pinned numpy version to <code>numpy&lt;1.24</code></li> <li>Added scripts to add guests and collaborators as users. #463</li> <li>Cleaned up installation instructions in repo README. #467</li> <li>Added checks in decoding visualization to ensure time dimensions are the   correct length.</li> <li>Fixed artifact removed valid times. #472</li> <li>Added codespell workflow for spell checking and fixed typos. #471</li> <li>Updated LFP code to save LFP as <code>pynwb.ecephys.LFP</code> type. #475</li> <li>Added artifact detection to LFP pipeline. #473</li> <li>Replaced calls to <code>spikeinterface.sorters.get_default_params</code> with   <code>spikeinterface.sorters.get_default_sorter_params</code>. #486</li> <li>Updated position pipeline and added functionality to handle pose estimation   through DeepLabCut. #367, #505</li> <li>Updated <code>environment_position.yml</code>. #502</li> <li>Renamed <code>FirFilter</code> class to <code>FirFilterParameters</code>. #512</li> </ul>"}, {"location": "CHANGELOG/#034-march-30-2023", "title": "0.3.4 (March 30, 2023)", "text": "<ul> <li>Fixed error in spike sorting pipeline referencing the \"probe_type\" column   which is no longer accessible from the <code>Electrode</code> table. #437</li> <li>Fixed error when inserting an NWB file that does not have a probe   manufacturer. #433, #436</li> <li>Fixed error when adding a new <code>DataAcquisitionDevice</code> and a new <code>ProbeType</code>.   #436</li> <li>Fixed inconsistency between capitalized/uncapitalized versions of \"Intan\" for   DataAcquisitionAmplifier and DataAcquisitionDevice.adc_circuit. #430, #438</li> </ul>"}, {"location": "CHANGELOG/#033-march-29-2023", "title": "0.3.3 (March 29, 2023)", "text": "<ul> <li>Fixed errors from referencing the changed primary key for <code>Probe</code>. #429</li> </ul>"}, {"location": "CHANGELOG/#032-march-28-2023", "title": "0.3.2 (March 28, 2023)", "text": "<ul> <li>Fixed import of <code>common_nwbfile</code>. #424</li> </ul>"}, {"location": "CHANGELOG/#031-march-24-2023", "title": "0.3.1 (March 24, 2023)", "text": "<ul> <li>Fixed import error due to <code>sortingview.Workspace</code>. #421</li> </ul>"}, {"location": "CHANGELOG/#030-march-24-2023", "title": "0.3.0 (March 24, 2023)", "text": "<p>To be added.</p>"}, {"location": "LICENSE/", "title": "Copyright", "text": "<p>Copyright (c) 2020-present Loren Frank</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BEsq LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"}, {"location": "contribute/", "title": "Developer notes", "text": "<p>Notes on how the repo / database is organized, intended for a new developer.</p>"}, {"location": "contribute/#overall-organization", "title": "Overall organization", "text": "<ul> <li>Tables that are about similar things are grouped into a schema. Each schema is   defined in a <code>.py</code> file. Example: all the tables related to quality metrics   are part of the <code>common_metrics</code> schema and are defined in <code>common_metrics.py</code>   in <code>common</code> module.</li> <li>The <code>common</code> module only contains schema that will be useful for everyone in   the lab. If you want to add things to <code>common</code>, first check with Loren.</li> <li>For analysis that will be only useful to you, create your own schema.</li> </ul>"}, {"location": "contribute/#types-of-tables", "title": "Types of tables", "text": ""}, {"location": "contribute/#nwb-related", "title": "NWB-related", "text": "<ul> <li>Data tier: <code>dj.Imported</code></li> <li>Primary key: foreign key from <code>Session</code></li> <li>Non-primary key: <code>object_id</code></li> <li>Each NWB-related table has a corresponding data object in the NWB file. This   object can be referred by a unique hash called an object ID.</li> <li>These tables are automatically populated when an NWB file is first ingested   into the database. To enable this, include the <code>populate</code> call in the <code>make</code>   method of <code>Session</code>.</li> <li>Required methods:</li> <li><code>make</code>: must read information from an NWB file and insert it to the table.</li> <li><code>fetch_nwb</code>: retrieve the data specified by the object ID; search the repo     for examples.</li> <li>Example: <code>Raw</code>, <code>Institution</code> etc</li> </ul>"}, {"location": "contribute/#pipeline", "title": "Pipeline", "text": "<ul> <li>Each analysis pipeline defined by a schema. A typical pipeline has at least   three tables:</li> <li>Parameters table<ul> <li>Naming convention: should end with <code>Parameters</code> (e.g. <code>MetricParameters</code>)</li> <li>Data tier: <code>dj.Manual</code></li> <li>Function: holds a set of parameters for the particular analysis.</li> <li>Primary key: <code>x_params_name</code> (str); x is the name of the pipeline (e.g.   <code>metric_params_name</code>).</li> <li>Non-primary key: <code>x_params</code> (dict; use <code>blob</code> in the definition); holds   the parameters as key-value pairs.</li> <li>Required method: <code>insert_default</code> to insert a reasonable default parameter   into the table.</li> </ul> </li> <li>Selection table<ul> <li>Naming convention: should end with <code>Selection</code> (e.g. <code>MetricSelection</code>)</li> <li>Data tier: <code>dj.Manual</code></li> <li>Function: associates a set of parameters to the data to be applied. For   example, in the case of computing quality metrics, one might put extracted   waveforms and a set of metrics parameters as a single entry in this table.</li> <li>Primary key: foreign key from a table containing data and the Parameters   table (i.e. Selection tables are downstream of these two tables).</li> <li>Of course, it is possible for a Selection table to collect information     from more than one Parameter table. For example, the Selection table for     spike sorting holds information about both the interval (<code>SortInterval</code>)     and the group of electrodes (<code>SortGroup</code>) to be sorted.</li> <li>Usually no other key needs to be defined</li> </ul> </li> <li>Data table<ul> <li>Data tier: <code>dj.Computed</code></li> <li>carries out the computation specified in the Selection table when   <code>populate</code> is called.</li> <li>The primary key should be foreign key inherited from the Selection table.   The non-primary key should be <code>analysis_file_name</code> inherited from   <code>AnalysisNwbfile</code> table (i.e. name of the analysis NWB file that will hold   the output of the computation).</li> <li>Required methods:</li> <li><code>make</code>: carries out the computation and insert a new entry; must also     create an analysis NWB file and insert it to the <code>AnalysisNwbfile</code>     table. Note that this method is never called directly; it is called via     <code>populate</code>.</li> <li><code>delete</code>: extension of the <code>delete</code> method that checks user privilege     before deleting entries as a way to prevent accidental deletion of     computations that take a long time (see below).</li> <li>Example: <code>QualityMetrics</code></li> </ul> </li> <li>Why have the Parameters table? Because one might want to repeat an analysis   with different sets of parameters. This way we keep track of everything. Also   encourages sharing of parameters.</li> <li>Why have the Selection table instead of going directly from Parameter table   to Data table? one still has to manually pass in the data and the parameters   to use for the computation (e.g. as an argument to <code>populate</code>. Since this is   required, defining it first in the Selection table is no less efficient. In   addition, multiple entries in Selection table can be run in parallel when   <code>populate</code> is called with <code>reserve_jobs=True</code> option.</li> </ul>"}, {"location": "contribute/#multi-pipeline", "title": "Multi-pipeline", "text": "<ul> <li>These are tables that are part of many pipelines.</li> <li>Examples: <code>IntervalList</code> (whose entries define time interval for any   analysis), <code>AnalysisNwbfile</code> (whose entries define analysis NWB files created   for any analysis), <code>Sortings</code> (whose entries include many types of spike   sorting, such as uncurated, automatically curated, manually curated etc)</li> <li>Data tier: <code>dj.Manual</code></li> <li>Note that because these are stand-alone manual tables, they are not part of   the dependency structure. This means one should try to include enough   information such that they can be linked back to the pipelines.</li> </ul>"}, {"location": "contribute/#integration-with-nwb", "title": "Integration with NWB", "text": ""}, {"location": "contribute/#nwb-files", "title": "NWB files", "text": "<ul> <li>NWB files contain everything about the experiment and form the starting point   of all analysis</li> <li>stored in <code>/stelmo/nwb/raw</code></li> <li>A copy of the NWB file that only contains pointers to objects in original file   is made in the same directory; the name has an extra <code>_</code> at the end, e.g.   <code>beans20190718_.nwb</code>; this file is made because we want to create object IDs   to refer to parts of the NWB file, but don't want to store these object IDs in   the original file to avoid file corruption</li> <li>Listed in the <code>Nwbfile</code> table</li> </ul>"}, {"location": "contribute/#analysis-nwb-files", "title": "Analysis NWB files", "text": "<ul> <li>These are NWB files that hold the results of intermediate steps in the analysis.</li> <li>Examples of data stored: filtered recordings, spike times of putative units   after sorting, or waveform snippets.</li> <li>Stored in <code>/stelmo/nwb/analysis</code></li> <li>Listed as an entry in the <code>AnalysisNwbfile</code> table.</li> </ul> <p>Note: for both types of NWB files, the fact that a file is not listed in the table doesn't mean the file does not exist in the directory. You can 'equalize' the list of NWB files and the list of actual files on disk by running <code>cleanup</code> method (i.e. it deletes any files not listed in the table from disk).</p>"}, {"location": "contribute/#reading-and-writing-recordings", "title": "Reading and writing recordings", "text": "<ul> <li>Right now the recording starts out as an NWB file. This is opened as a   <code>NwbRecordingExtractor</code>, a class in <code>spikeinterface</code>. When using <code>sortingview</code>   for visualizing the results of spike sorting, this recording is saved again in   HDF5 format. This duplication should be resolved in the future.</li> </ul>"}, {"location": "contribute/#naming-convention", "title": "Naming convention", "text": "<p>There are a few places where a name needs to be given to objects. Follow these rules:</p> <ul> <li>Recordings: should be given unique names. As such we have decided to simply   concatenate everything that went into defining it separated by underscore,   i.e. <code>NWBFileName_IntervalName_ElectrodeGroupName_PreprocessingParamsName</code>.</li> <li>Sortings: should be unique. Simply concatenates   <code>SpikeSorter_SorterParamName</code> to the name of the recording.</li> <li>Waveforms: should be unique. Concatenates <code>WaveformParamName</code> to the name of   the sorting.</li> <li>Quality metrics: should be unique. concatenates <code>MetricParamName</code> to the   name of the waveform.</li> <li>Analysis NWB files: same as the objects, i.e. the analysis NWB file that   holds recording is named   <code>NWBFileName_IntervalName_ElectrodeGroupName_PreprocessingParamsName.nwb</code></li> <li>An alternative way to get unique names that are not as long is to generate a   UUID for each file. Currently each recording and sorting are given such IDs.</li> <li>A method that will not be explicitly called by the user should start with <code>_</code></li> </ul>"}, {"location": "contribute/#time", "title": "Time", "text": "<ul> <li>All valid intervals of any kind must be inserted into the <code>IntervalList</code> table   prior to being used.</li> <li>Store an interval as <code>[start_time, stop_time]</code>. The list can be nested for a   set of disjoint intervals.</li> <li>Some recordings have explicit timestamps associated with each sample. This is   obtained by a system called PTP. In this system, time 0 is defined as 1 Jan</li> <li>Other (typically older) recordings do not and their times must be   inferred from the TTL pulses from the camera (ask if this doesn't make sense).</li> <li>What is a valid interval? Because our experiments can be long, sometimes there   are missing samples. This can be due to many reasons, such as the commutator   connection being faulty for a few milliseconds. As a result we have 'holes' in   our data. A valid interval is a start time and an end time between which there   are no holes.</li> </ul>"}, {"location": "contribute/#misc", "title": "Misc", "text": "<ul> <li>You may want to create a development/testing environment independent of the   lab datajoint server. To do so, run your own datajoint server with Docker. See   example.</li> <li>Datajoint is unable to set delete permissions on a per-table basis. In other   words, if a user is able to delete entries in a given table, she can delete   entries in any table in the schema. Some tables that hold important data   extends the <code>delete</code> method to check if the datajoint username matches a list   of allowed users when <code>delete</code> is called. If you think this would be useful   for your table, see examples in <code>common_spikesorting.py</code>.</li> <li>In general, use <code>numpy</code> style docstring.</li> <li>Don't overload a single <code>.py</code> file. For each pipeline make a new <code>.py</code> file   and define your schema / tables.</li> <li>Some of the 'rules' above may need to change or be inappropriate for some   cases. If you want to start a discussion, talk to Loren.</li> </ul>"}, {"location": "contribute/#making-a-release", "title": "Making a release", "text": "<ol> <li>In <code>pyproject.toml</code>, under <code>[project]</code>, update the <code>version</code> key to the new    version string.</li> <li>In <code>CITATION.cff</code>, update the <code>version</code> key to the new version string.</li> <li>Make a pull request with these changes.</li> <li> <p>After merging these changes, run <code>git tag --sign -m \"spyglass ${release}\"    ${release} origin/master</code> where <code>${release}</code> is replaced with the new version    string.</p> </li> <li> <p>This step requires a     GPG signing key.</p> </li> <li> <p>Publish the new release tag. Run <code>git push origin ${release}</code>.</p> </li> <li>Generate distribution packages and upload them to PyPI following these    instructions.</li> <li>Make a new release on GitHub with the new release tag:    https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository</li> </ol>"}, {"location": "contribute/#todo", "title": "TODO", "text": "<ul> <li>Fetch nwb method is currently implemented for each table. This is unnecessary   because (1) what matters is the query, not the table the method is attached   to; and (2) you either look up the Nwbfile or the AnalysisNwbfile table for   it, so really there are only two versions. It would be better to just have two   standalone functions. Or just one that figures out which NWB file to look up.</li> </ul>"}, {"location": "api/src/spyglass/_version/", "title": "_version.py", "text": ""}, {"location": "api/src/spyglass/settings/", "title": "settings.py", "text": ""}, {"location": "api/src/spyglass/cli/cli/", "title": "cli.py", "text": ""}, {"location": "api/src/spyglass/common/common_backup/", "title": "common_backup.py", "text": ""}, {"location": "api/src/spyglass/common/common_backup/#src.spyglass.common.common_backup.SpikeSortingBackUp", "title": "<code>SpikeSortingBackUp</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_backup.py</code> <pre><code>@schema\nclass SpikeSortingBackUp(dj.Manual):\n    definition = \"\"\"\n    nwb_file_name: varchar(500)\n    sort_group_id: int\n    sort_interval_name: varchar(500)\n    filter_parameter_set_name: varchar(500)\n    sorter_name: varchar(500)\n    spikesorter_parameter_set_name: varchar(500)\n    ---\n    sorting_id: varchar(500)\n    analysis_file_name: varchar(1000)\n    time_of_sort: int   # in Unix time, to the nearest second\n    units_object_id: varchar(100)\n    \"\"\"\n\n    def insert_from_backup(self, backup_file):\n\"\"\"backup file lives in /common/backup_keys/\n\n        Parameters\n        ----------\n        backup_file : str\n            path to npy pickle file containing keys\n        \"\"\"\n        backup_keys = np.load(backup_file, allow_pickle=True)\n        self.insert(backup_keys, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_backup/#src.spyglass.common.common_backup.SpikeSortingBackUp.insert_from_backup", "title": "<code>insert_from_backup(backup_file)</code>", "text": "<p>backup file lives in /common/backup_keys/</p> <p>Parameters:</p> Name Type Description Default <code>backup_file</code> <code>str</code> <p>path to npy pickle file containing keys</p> required Source code in <code>src/spyglass/common/common_backup.py</code> <pre><code>def insert_from_backup(self, backup_file):\n\"\"\"backup file lives in /common/backup_keys/\n\n    Parameters\n    ----------\n    backup_file : str\n        path to npy pickle file containing keys\n    \"\"\"\n    backup_keys = np.load(backup_file, allow_pickle=True)\n    self.insert(backup_keys, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/", "title": "common_behav.py", "text": ""}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.IntervalList", "title": "<code>IntervalList</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@schema\nclass IntervalList(dj.Manual):\n    definition = \"\"\"\n    # Time intervals used for analysis\n    -&gt; Session\n    interval_list_name: varchar(200)  # descriptive name of this interval list\n    ---\n    valid_times: longblob  # numpy array with start and end times for each interval\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n        The interval list name for each epoch is set to the first tag for the epoch.\n        If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n        (0-indexed) of the epoch in the epochs table.\n        The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n        [start time, stop time] for each epoch.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        nwb_file_name : str\n            The file name of the NWB file, used as a primary key to the Session table.\n        \"\"\"\n        if nwbf.epochs is None:\n            print(\"No epochs found in NWB file.\")\n            return\n        epochs = nwbf.epochs.to_dataframe()\n        for epoch_index, epoch_data in epochs.iterrows():\n            epoch_dict = dict()\n            epoch_dict[\"nwb_file_name\"] = nwb_file_name\n            if epoch_data.tags[0]:\n                epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n            else:\n                epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                    epoch_index\n                )\n            epoch_dict[\"valid_times\"] = np.asarray(\n                [[epoch_data.start_time, epoch_data.stop_time]]\n            )\n            cls.insert1(epoch_dict, skip_duplicates=True)\n\n    def plot_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=figsize)\n        interval_count = 0\n        for row in interval_list.itertuples(index=False):\n            for interval in row.valid_times:\n                ax.plot(interval, [interval_count, interval_count])\n                ax.scatter(\n                    interval,\n                    [interval_count, interval_count],\n                    alpha=0.8,\n                    zorder=2,\n                )\n            interval_count += 1\n        ax.set_yticks(np.arange(interval_list.shape[0]))\n        ax.set_yticklabels(interval_list.interval_list_name)\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n\n    def plot_epoch_pos_raw_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=(30, 3))\n\n        raw_data_valid_times = interval_list.loc[\n            interval_list.interval_list_name == \"raw data valid times\"\n        ].valid_times\n        interval_y = 1\n\n        for interval in np.asarray(raw_data_valid_times)[0]:\n            ax.plot(interval, [interval_y, interval_y])\n            ax.scatter(interval, [interval_y, interval_y], alpha=0.8, zorder=2)\n\n        epoch_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n        interval_y = 2\n        for epoch, valid_times in zip(\n            epoch_valid_times.index, epoch_valid_times\n        ):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        pos_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^pos \\d+ valid times$\", axis=0)\n            .valid_times\n        ).sort_index(key=lambda index: [int(name.split()[1]) for name in index])\n        interval_y = 0\n        for epoch, valid_times in zip(pos_valid_times.index, pos_valid_times):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch.strip(\" valid times\"),\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        ax.set_ylim((-0.25, 2.25))\n        ax.set_yticks(np.arange(3))\n        ax.set_yticklabels([\"pos valid times\", \"raw data valid times\", \"epoch\"])\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_interval.IntervalList.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, *, nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add each entry in the NWB file epochs table to the IntervalList table.</p> <p>The interval list name for each epoch is set to the first tag for the epoch. If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index (0-indexed) of the epoch in the epochs table. The start time and stop time of the epoch are stored in the valid_times field as a numpy array of [start time, stop time] for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>nwb_file_name</code> <code>str</code> <p>The file name of the NWB file, used as a primary key to the Session table.</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n    The interval list name for each epoch is set to the first tag for the epoch.\n    If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n    (0-indexed) of the epoch in the epochs table.\n    The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n    [start time, stop time] for each epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    nwb_file_name : str\n        The file name of the NWB file, used as a primary key to the Session table.\n    \"\"\"\n    if nwbf.epochs is None:\n        print(\"No epochs found in NWB file.\")\n        return\n    epochs = nwbf.epochs.to_dataframe()\n    for epoch_index, epoch_data in epochs.iterrows():\n        epoch_dict = dict()\n        epoch_dict[\"nwb_file_name\"] = nwb_file_name\n        if epoch_data.tags[0]:\n            epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n        else:\n            epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                epoch_index\n            )\n        epoch_dict[\"valid_times\"] = np.asarray(\n            [[epoch_data.start_time, epoch_data.stop_time]]\n        )\n        cls.insert1(epoch_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.get_nwb_file", "title": "<code>get_nwb_file(nwb_file_path)</code>", "text": "<p>Return an NWBFile object with the given file path in read mode.    If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Path to the NWB file.</p> required <p>Returns:</p> Name Type Description <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>NWB file object for the given path opened in read mode.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_file(nwb_file_path):\n\"\"\"Return an NWBFile object with the given file path in read mode.\n       If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.\n\n    Parameters\n    ----------\n    nwb_file_path : str\n        Path to the NWB file.\n\n    Returns\n    -------\n    nwbfile : pynwb.NWBFile\n        NWB file object for the given path opened in read mode.\n    \"\"\"\n    _, nwbfile = __open_nwb_files.get(nwb_file_path, (None, None))\n    nwb_uri = None\n    nwb_raw_uri = None\n    if nwbfile is None:\n        # check to see if the file exists\n        if not os.path.exists(nwb_file_path):\n            print(\n                f\"NWB file {nwb_file_path} does not exist locally; checking kachery\"\n            )\n            # first try the analysis files\n            from ..sharing.sharing_kachery import AnalysisNwbfileKachery\n\n            # the download functions assume just the filename, so we need to get that from the path\n            if not AnalysisNwbfileKachery.download_file(\n                os.path.basename(nwb_file_path)\n            ):\n                return None\n        # now open the file\n        io = pynwb.NWBHDF5IO(\n            path=nwb_file_path, mode=\"r\", load_namespaces=True\n        )  # keep file open\n        nwbfile = io.read()\n        __open_nwb_files[nwb_file_path] = (io, nwbfile)\n\n    return nwbfile\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.PositionSource", "title": "<code>PositionSource</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass PositionSource(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    -&gt; IntervalList\n    ---\n    source: varchar(200)            # source of data; current options are \"trodes\" and \"dlc\" (deep lab cut)\n    import_file_name: varchar(2000)  # path to import file if importing position data\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwb_file_name):\n\"\"\"Given an NWB file name, get the spatial series and interval lists from the file, add the interval\n        lists to the IntervalList table, and populate the RawPosition table if possible.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        pos_dict = get_all_spatial_series(nwbf, verbose=True)\n        if pos_dict is not None:\n            for epoch in pos_dict:\n                pdict = pos_dict[epoch]\n                pos_interval_list_name = cls.get_pos_interval_name(epoch)\n\n                # create the interval list and insert it\n                interval_dict = dict()\n                interval_dict[\"nwb_file_name\"] = nwb_file_name\n                interval_dict[\"interval_list_name\"] = pos_interval_list_name\n                interval_dict[\"valid_times\"] = pdict[\"valid_times\"]\n                IntervalList().insert1(interval_dict, skip_duplicates=True)\n\n                # add this interval list to the table\n                key = dict()\n                key[\"nwb_file_name\"] = nwb_file_name\n                key[\"interval_list_name\"] = pos_interval_list_name\n                key[\"source\"] = \"trodes\"\n                key[\"import_file_name\"] = \"\"\n                cls.insert1(key)\n\n    @staticmethod\n    def get_pos_interval_name(pos_epoch_num):\n        return f\"pos {pos_epoch_num} valid times\"\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.PositionSource.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Given an NWB file name, get the spatial series and interval lists from the file, add the interval lists to the IntervalList table, and populate the RawPosition table if possible.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file.</p> required Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwb_file_name):\n\"\"\"Given an NWB file name, get the spatial series and interval lists from the file, add the interval\n    lists to the IntervalList table, and populate the RawPosition table if possible.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    nwbf = get_nwb_file(nwb_file_abspath)\n\n    pos_dict = get_all_spatial_series(nwbf, verbose=True)\n    if pos_dict is not None:\n        for epoch in pos_dict:\n            pdict = pos_dict[epoch]\n            pos_interval_list_name = cls.get_pos_interval_name(epoch)\n\n            # create the interval list and insert it\n            interval_dict = dict()\n            interval_dict[\"nwb_file_name\"] = nwb_file_name\n            interval_dict[\"interval_list_name\"] = pos_interval_list_name\n            interval_dict[\"valid_times\"] = pdict[\"valid_times\"]\n            IntervalList().insert1(interval_dict, skip_duplicates=True)\n\n            # add this interval list to the table\n            key = dict()\n            key[\"nwb_file_name\"] = nwb_file_name\n            key[\"interval_list_name\"] = pos_interval_list_name\n            key[\"source\"] = \"trodes\"\n            key[\"import_file_name\"] = \"\"\n            cls.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.Nwbfile", "title": "<code>Nwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass Nwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files.\n    nwb_file_name: varchar(255)   # name of the NWB file\n    ---\n    nwb_file_abs_path: filepath@raw\n    INDEX (nwb_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    @classmethod\n    def insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The relative path to the NWB file.\n        \"\"\"\n        nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n        assert os.path.exists(\n            nwb_file_abs_path\n        ), f\"File does not exist: {nwb_file_abs_path}\"\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n        cls.insert1(key, skip_duplicates=True)\n\n    @staticmethod\n    def get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n        Returns\n        -------\n        nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n        nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n        return str(nwb_file_abspath)\n\n    @staticmethod\n    def add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n        The NWB_LOCK_FILE environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n        \"\"\"\n        key = {\"nwb_file_name\": nwb_file_name}\n        # check to make sure the file exists\n        assert (\n            len((Nwbfile() &amp; key).fetch()) &gt; 0\n        ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n        lock_file.write(f\"{nwb_file_name}\\n\")\n        lock_file.close()\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        This does not delete the files themselves unless delete_files=True is specified\n        Run this after deleting the Nwbfile() entries themselves.\n        \"\"\"\n        schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_nwbfile.Nwbfile.insert_from_relative_file_name", "title": "<code>insert_from_relative_file_name(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Insert a new session from an existing NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The relative path to the NWB file.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The relative path to the NWB file.\n    \"\"\"\n    nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n    assert os.path.exists(\n        nwb_file_abs_path\n    ), f\"File does not exist: {nwb_file_abs_path}\"\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n    cls.insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_nwbfile.Nwbfile.get_abs_path", "title": "<code>get_abs_path(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored raw NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required <p>Returns:</p> Name Type Description <code>nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n    Returns\n    -------\n    nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n    nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n    return str(nwb_file_abspath)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_nwbfile.Nwbfile.add_to_lock", "title": "<code>add_to_lock(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Add the specified NWB file to the file with the list of NWB files to be locked.</p> <p>The NWB_LOCK_FILE environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n    The NWB_LOCK_FILE environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n    \"\"\"\n    key = {\"nwb_file_name\": nwb_file_name}\n    # check to make sure the file exists\n    assert (\n        len((Nwbfile() &amp; key).fetch()) &gt; 0\n    ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n    lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n    lock_file.write(f\"{nwb_file_name}\\n\")\n    lock_file.close()\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_nwbfile.Nwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>This does not delete the files themselves unless delete_files=True is specified Run this after deleting the Nwbfile() entries themselves.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    This does not delete the files themselves unless delete_files=True is specified\n    Run this after deleting the Nwbfile() entries themselves.\n    \"\"\"\n    schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.RawPosition", "title": "<code>RawPosition</code>", "text": "<p>         Bases: <code>dj.Imported</code></p>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.RawPosition--notes", "title": "Notes", "text": "<p>The position timestamps come from: .pos_cameraHWSync.dat. If PTP is not used, the position timestamps are inferred by finding the closest timestamps from the neural recording via the trodes time.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass RawPosition(dj.Imported):\n\"\"\"\n\n    Notes\n    -----\n    The position timestamps come from: .pos_cameraHWSync.dat.\n    If PTP is not used, the position timestamps are inferred by finding the\n    closest timestamps from the neural recording via the trodes time.\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionSource\n    ---\n    raw_position_object_id: varchar(40)    # the object id of the spatial series for this epoch in the NWB file\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        # TODO refactor this. this calculates sampling rate (unused here) and is expensive to do twice\n        pos_dict = get_all_spatial_series(nwbf)\n        for epoch in pos_dict:\n            if key[\n                \"interval_list_name\"\n            ] == PositionSource.get_pos_interval_name(epoch):\n                pdict = pos_dict[epoch]\n                key[\"raw_position_object_id\"] = pdict[\"raw_position_object_id\"]\n                self.insert1(key)\n                break\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    def fetch1_dataframe(self):\n        raw_position_nwb = self.fetch_nwb()[0][\"raw_position\"]\n        return pd.DataFrame(\n            data=raw_position_nwb.data,\n            index=pd.Index(raw_position_nwb.timestamps, name=\"time\"),\n            columns=raw_position_nwb.description.split(\", \"),\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.TaskEpoch", "title": "<code>TaskEpoch</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@schema\nclass TaskEpoch(dj.Imported):\n    # Tasks, session and time intervals\n    definition = \"\"\"\n     -&gt; Session\n     epoch: int  # the session epoch for this task and apparatus(1 based)\n     ---\n     -&gt; Task\n     -&gt; [nullable] CameraDevice\n     -&gt; IntervalList\n     task_environment = NULL: varchar(200)  # the environment the animal was in\n     camera_names : blob # list of keys corresponding to entry in CameraDevice\n     \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile().get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        camera_names = dict()\n        # the tasks refer to the camera_id which is unique for the NWB file but not for CameraDevice schema, so we\n        # need to look up the right camera\n        # map camera ID (in camera name) to camera_name\n        for device in nwbf.devices.values():\n            if isinstance(device, ndx_franklab_novela.CameraDevice):\n                # get the camera ID\n                camera_id = int(str.split(device.name)[1])\n                camera_names[camera_id] = device.camera_name\n\n        # find the task modules and for each one, add the task to the Task schema if it isn't there\n        # and then add an entry for each epoch\n        tasks_mod = nwbf.processing.get(\"tasks\")\n        if tasks_mod is None:\n            print(f\"No tasks processing module found in {nwbf}\\n\")\n            return\n\n        for task in tasks_mod.data_interfaces.values():\n            if self.check_task_table(task):\n                # check if the task is in the Task table and if not, add it\n                Task.insert_from_task_table(task)\n                key[\"task_name\"] = task.task_name[0]\n\n                # get the CameraDevice used for this task (primary key is camera name so we need\n                # to map from ID to name)\n                camera_ids = task.camera_id[0]\n                valid_camera_ids = [\n                    camera_id\n                    for camera_id in camera_ids\n                    if camera_id in camera_names.keys()\n                ]\n                if valid_camera_ids:\n                    key[\"camera_names\"] = [\n                        {\"camera_name\": camera_names[camera_id]}\n                        for camera_id in valid_camera_ids\n                    ]\n                else:\n                    print(\n                        f\"No camera device found with ID {camera_ids} in NWB file {nwbf}\\n\"\n                    )\n                # Add task environment\n                if hasattr(task, \"task_environment\"):\n                    key[\"task_environment\"] = task.task_environment[0]\n\n                # get the interval list for this task, which corresponds to the matching epoch for the raw data.\n                # Users should define more restrictive intervals as required for analyses\n                session_intervals = (\n                    IntervalList() &amp; {\"nwb_file_name\": nwb_file_name}\n                ).fetch(\"interval_list_name\")\n                for epoch in task.task_epochs[0]:\n                    # TODO in beans file, task_epochs[0] is 1x2 dset of ints, so epoch would be an int\n                    key[\"epoch\"] = epoch\n                    target_interval = str(epoch).zfill(2)\n                    for interval in session_intervals:\n                        if (\n                            target_interval in interval\n                        ):  # TODO this is not true for the beans file\n                            break\n                    # TODO case when interval is not found is not handled\n                    key[\"interval_list_name\"] = interval\n                    self.insert1(key)\n\n    @classmethod\n    def update_entries(cls, restrict={}):\n        existing_entries = (cls &amp; restrict).fetch(\"KEY\")\n        for row in existing_entries:\n            if (cls &amp; row).fetch1(\"camera_names\"):\n                continue\n            row[\"camera_names\"] = [\n                {\"camera_name\": (cls &amp; row).fetch1(\"camera_name\")}\n            ]\n            cls.update1(row=row)\n\n    @classmethod\n    def check_task_table(cls, task_table):\n\"\"\"Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.\n\n        The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name',\n        'task_description', 'camera_id', 'and 'task_epochs'.\n\n        Parameters\n        ----------\n        task_table : pynwb.core.DynamicTable\n            The table representing task metadata.\n\n        Returns\n        -------\n        bool\n            Whether the DynamicTable conforms to the expected format for loading data into the TaskEpoch table.\n        \"\"\"\n\n        # TODO this could be more strict and check data types, but really it should be schematized\n        return (\n            Task.check_task_table(task_table)\n            and hasattr(task_table, \"camera_id\")\n            and hasattr(task_table, \"task_epochs\")\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_task.TaskEpoch.check_task_table", "title": "<code>check_task_table(task_table)</code>  <code>classmethod</code>", "text": "<p>Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.</p> <p>The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name', 'task_description', 'camera_id', 'and 'task_epochs'.</p> <p>Parameters:</p> Name Type Description Default <code>task_table</code> <code>pynwb.core.DynamicTable</code> <p>The table representing task metadata.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the DynamicTable conforms to the expected format for loading data into the TaskEpoch table.</p> Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@classmethod\ndef check_task_table(cls, task_table):\n\"\"\"Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.\n\n    The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name',\n    'task_description', 'camera_id', 'and 'task_epochs'.\n\n    Parameters\n    ----------\n    task_table : pynwb.core.DynamicTable\n        The table representing task metadata.\n\n    Returns\n    -------\n    bool\n        Whether the DynamicTable conforms to the expected format for loading data into the TaskEpoch table.\n    \"\"\"\n\n    # TODO this could be more strict and check data types, but really it should be schematized\n    return (\n        Task.check_task_table(task_table)\n        and hasattr(task_table, \"camera_id\")\n        and hasattr(task_table, \"task_epochs\")\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.get_data_interface", "title": "<code>get_data_interface(nwbfile, data_interface_name, data_interface_class=None)</code>", "text": "<p>Search for a specified NWBDataInterface or DynamicTable in the processing modules of an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>The NWB file object to search in.</p> required <code>data_interface_name</code> <code>str</code> <p>The name of the NWBDataInterface or DynamicTable to search for.</p> required <code>data_interface_class</code> <code>type</code> <p>The class (or superclass) to search for. This argument helps to prevent accessing an object with the same name but the incorrect type. Default: no restriction.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If multiple NWBDataInterface and DynamicTable objects with the matching name are found.</p> <p>Returns:</p> Name Type Description <code>data_interface</code> <code>NWBDataInterface</code> <p>The data interface object with the given name, or None if not found.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_data_interface(nwbfile, data_interface_name, data_interface_class=None):\n\"\"\"Search for a specified NWBDataInterface or DynamicTable in the processing modules of an NWB file.\n\n    Parameters\n    ----------\n    nwbfile : pynwb.NWBFile\n        The NWB file object to search in.\n    data_interface_name : str\n        The name of the NWBDataInterface or DynamicTable to search for.\n    data_interface_class : type, optional\n        The class (or superclass) to search for. This argument helps to prevent accessing an object with the same\n        name but the incorrect type. Default: no restriction.\n\n    Warns\n    -----\n    UserWarning\n        If multiple NWBDataInterface and DynamicTable objects with the matching name are found.\n\n    Returns\n    -------\n    data_interface : NWBDataInterface\n        The data interface object with the given name, or None if not found.\n    \"\"\"\n    ret = []\n    for module in nwbfile.processing.values():\n        match = module.data_interfaces.get(data_interface_name, None)\n        if match is not None:\n            if data_interface_class is not None and not isinstance(\n                match, data_interface_class\n            ):\n                continue\n            ret.append(match)\n    if len(ret) &gt; 1:\n        warnings.warn(\n            f\"Multiple data interfaces with name '{data_interface_name}' \"\n            f\"found in NWBFile with identifier {nwbfile.identifier}. Using the first one found. \"\n            \"Use the data_interface_class argument to restrict the search.\"\n        )\n    if len(ret) &gt;= 1:\n        return ret[0]\n    else:\n        return None\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.StateScriptFile", "title": "<code>StateScriptFile</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass StateScriptFile(dj.Imported):\n    definition = \"\"\"\n    -&gt; TaskEpoch\n    ---\n    file_object_id: varchar(40)  # the object id of the file object\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Add a new row to the StateScriptFile table. Requires keys \"nwb_file_name\", \"file_object_id\".\"\"\"\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        associated_files = nwbf.processing.get(\n            \"associated_files\"\n        ) or nwbf.processing.get(\"associated files\")\n        if associated_files is None:\n            print(\n                f'Unable to import StateScriptFile: no processing module named \"associated_files\" '\n                f\"found in {nwb_file_name}.\"\n            )\n            return\n\n        for associated_file_obj in associated_files.data_interfaces.values():\n            if not isinstance(\n                associated_file_obj, ndx_franklab_novela.AssociatedFiles\n            ):\n                print(\n                    f'Data interface {associated_file_obj.name} within \"associated_files\" processing module is not '\n                    f\"of expected type ndx_franklab_novela.AssociatedFiles\\n\"\n                )\n                return\n            # parse the task_epochs string\n            # TODO update associated_file_obj.task_epochs to be an array of 1-based ints,\n            # not a comma-separated string of ints\n            epoch_list = associated_file_obj.task_epochs.split(\",\")\n            # only insert if this is the statescript file\n            print(associated_file_obj.description)\n            if (\n                \"statescript\".upper() in associated_file_obj.description.upper()\n                or \"state_script\".upper()\n                in associated_file_obj.description.upper()\n                or \"state script\".upper()\n                in associated_file_obj.description.upper()\n            ):\n                # find the file associated with this epoch\n                if str(key[\"epoch\"]) in epoch_list:\n                    key[\"file_object_id\"] = associated_file_obj.object_id\n                    self.insert1(key)\n            else:\n                print(\"not a statescript file\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.StateScriptFile.make", "title": "<code>make(key)</code>", "text": "<p>Add a new row to the StateScriptFile table. Requires keys \"nwb_file_name\", \"file_object_id\".</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>def make(self, key):\n\"\"\"Add a new row to the StateScriptFile table. Requires keys \"nwb_file_name\", \"file_object_id\".\"\"\"\n    nwb_file_name = key[\"nwb_file_name\"]\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    nwbf = get_nwb_file(nwb_file_abspath)\n\n    associated_files = nwbf.processing.get(\n        \"associated_files\"\n    ) or nwbf.processing.get(\"associated files\")\n    if associated_files is None:\n        print(\n            f'Unable to import StateScriptFile: no processing module named \"associated_files\" '\n            f\"found in {nwb_file_name}.\"\n        )\n        return\n\n    for associated_file_obj in associated_files.data_interfaces.values():\n        if not isinstance(\n            associated_file_obj, ndx_franklab_novela.AssociatedFiles\n        ):\n            print(\n                f'Data interface {associated_file_obj.name} within \"associated_files\" processing module is not '\n                f\"of expected type ndx_franklab_novela.AssociatedFiles\\n\"\n            )\n            return\n        # parse the task_epochs string\n        # TODO update associated_file_obj.task_epochs to be an array of 1-based ints,\n        # not a comma-separated string of ints\n        epoch_list = associated_file_obj.task_epochs.split(\",\")\n        # only insert if this is the statescript file\n        print(associated_file_obj.description)\n        if (\n            \"statescript\".upper() in associated_file_obj.description.upper()\n            or \"state_script\".upper()\n            in associated_file_obj.description.upper()\n            or \"state script\".upper()\n            in associated_file_obj.description.upper()\n        ):\n            # find the file associated with this epoch\n            if str(key[\"epoch\"]) in epoch_list:\n                key[\"file_object_id\"] = associated_file_obj.object_id\n                self.insert1(key)\n        else:\n            print(\"not a statescript file\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.VideoFile", "title": "<code>VideoFile</code>", "text": "<p>         Bases: <code>dj.Imported</code></p>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.VideoFile--notes", "title": "Notes", "text": "<p>The video timestamps come from: videoTimeStamps.cameraHWSync if PTP is used. If PTP is not used, the video timestamps come from videoTimeStamps.cameraHWFrameCount .</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass VideoFile(dj.Imported):\n\"\"\"\n\n    Notes\n    -----\n    The video timestamps come from: videoTimeStamps.cameraHWSync if PTP is used.\n    If PTP is not used, the video timestamps come from videoTimeStamps.cameraHWFrameCount .\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; TaskEpoch\n    video_file_num = 0: int\n    ---\n    camera_name: varchar(80)\n    video_file_object_id: varchar(40)  # the object id of the file object\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        videos = get_data_interface(\n            nwbf, \"video\", pynwb.behavior.BehavioralEvents\n        )\n\n        if videos is None:\n            print(f\"No video data interface found in {nwb_file_name}\\n\")\n            return\n        else:\n            videos = videos.time_series\n\n        # get the interval for the current TaskEpoch\n        interval_list_name = (TaskEpoch() &amp; key).fetch1(\"interval_list_name\")\n        valid_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n\n        is_found = False\n        for ind, video in enumerate(videos.values()):\n            if isinstance(video, pynwb.image.ImageSeries):\n                video = [video]\n            for video_obj in video:\n                # check to see if the times for this video_object are largely overlapping with the task epoch times\n                if len(\n                    interval_list_contains(valid_times, video_obj.timestamps)\n                    &gt; 0.9 * len(video_obj.timestamps)\n                ):\n                    key[\"video_file_num\"] = ind\n                    camera_name = video_obj.device.camera_name\n                    if CameraDevice &amp; {\"camera_name\": camera_name}:\n                        key[\"camera_name\"] = video_obj.device.camera_name\n                    else:\n                        raise KeyError(\n                            f\"No camera with camera_name: {camera_name} found in CameraDevice table.\"\n                        )\n                    key[\"video_file_object_id\"] = video_obj.object_id\n                    self.insert1(key)\n                    is_found = True\n\n        if not is_found:\n            print(f\"No video found corresponding to epoch {interval_list_name}\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    @classmethod\n    def update_entries(cls, restrict={}):\n        existing_entries = (cls &amp; restrict).fetch(\"KEY\")\n        for row in existing_entries:\n            if (cls &amp; row).fetch1(\"camera_name\"):\n                continue\n            video_nwb = (cls &amp; row).fetch_nwb()[0]\n            if len(video_nwb) != 1:\n                raise ValueError(\n                    f\"expecting 1 video file per entry, but {len(video_nwb)} files found\"\n                )\n            row[\"camera_name\"] = video_nwb[0][\"video_file\"].device.camera_name\n            cls.update1(row=row)\n\n    @classmethod\n    def get_abs_path(cls, key: Dict):\n\"\"\"Return the absolute path for a stored video file given a key with the nwb_file_name and epoch number\n\n        The SPYGLASS_VIDEO_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        key : dict\n            dictionary with nwb_file_name and epoch as keys\n\n        Returns\n        -------\n        nwb_video_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        video_dir = pathlib.Path(os.getenv(\"SPYGLASS_VIDEO_DIR\", None))\n        assert video_dir is not None, \"You must set SPYGLASS_VIDEO_DIR\"\n        if not video_dir.exists():\n            raise OSError(\"SPYGLASS_VIDEO_DIR does not exist\")\n        video_info = (cls &amp; key).fetch1()\n        nwb_path = Nwbfile.get_abs_path(key[\"nwb_file_name\"])\n        nwbf = get_nwb_file(nwb_path)\n        nwb_video = nwbf.objects[video_info[\"video_file_object_id\"]]\n        video_filename = nwb_video.name\n        # see if the file exists and is stored in the base analysis dir\n        nwb_video_file_abspath = pathlib.Path(\n            f\"{video_dir}/{pathlib.Path(video_filename)}\"\n        )\n        if nwb_video_file_abspath.exists():\n            return nwb_video_file_abspath.as_posix()\n        else:\n            raise FileNotFoundError(\n                f\"video file with filename: {video_filename} \"\n                f\"does not exist in {video_dir}/\"\n            )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.VideoFile.get_abs_path", "title": "<code>get_abs_path(key)</code>  <code>classmethod</code>", "text": "<p>Return the absolute path for a stored video file given a key with the nwb_file_name and epoch number</p> <p>The SPYGLASS_VIDEO_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>dictionary with nwb_file_name and epoch as keys</p> required <p>Returns:</p> Name Type Description <code>nwb_video_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@classmethod\ndef get_abs_path(cls, key: Dict):\n\"\"\"Return the absolute path for a stored video file given a key with the nwb_file_name and epoch number\n\n    The SPYGLASS_VIDEO_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    key : dict\n        dictionary with nwb_file_name and epoch as keys\n\n    Returns\n    -------\n    nwb_video_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    video_dir = pathlib.Path(os.getenv(\"SPYGLASS_VIDEO_DIR\", None))\n    assert video_dir is not None, \"You must set SPYGLASS_VIDEO_DIR\"\n    if not video_dir.exists():\n        raise OSError(\"SPYGLASS_VIDEO_DIR does not exist\")\n    video_info = (cls &amp; key).fetch1()\n    nwb_path = Nwbfile.get_abs_path(key[\"nwb_file_name\"])\n    nwbf = get_nwb_file(nwb_path)\n    nwb_video = nwbf.objects[video_info[\"video_file_object_id\"]]\n    video_filename = nwb_video.name\n    # see if the file exists and is stored in the base analysis dir\n    nwb_video_file_abspath = pathlib.Path(\n        f\"{video_dir}/{pathlib.Path(video_filename)}\"\n    )\n    if nwb_video_file_abspath.exists():\n        return nwb_video_file_abspath.as_posix()\n    else:\n        raise FileNotFoundError(\n            f\"video file with filename: {video_filename} \"\n            f\"does not exist in {video_dir}/\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.interval_list_contains", "title": "<code>interval_list_contains(interval_list, timestamps)</code>", "text": "<p>Find timestamps that are contained in an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval. Unit is seconds.</p> required <code>timestamps</code> <code>array_like</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_contains(interval_list, timestamps):\n\"\"\"Find timestamps that are contained in an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval. Unit is seconds.\n    timestamps : array_like\n    \"\"\"\n    ind = []\n    for interval in interval_list:\n        ind += np.ravel(\n            np.argwhere(\n                np.logical_and(\n                    timestamps &gt;= interval[0], timestamps &lt;= interval[1]\n                )\n            )\n        ).tolist()\n    return timestamps[ind]\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.CameraDevice", "title": "<code>CameraDevice</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass CameraDevice(dj.Manual):\n    definition = \"\"\"\n    camera_name: varchar(80)\n    ---\n    meters_per_pixel = 0: float  # height / width of pixel in meters\n    manufacturer = \"\": varchar(2000)\n    model = \"\": varchar(2000)\n    lens = \"\": varchar(2000)\n    camera_id = -1: int\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert camera devices from an NWB file\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n\n        Returns\n        -------\n        device_name_list : list\n            List of camera device object names found in the NWB file.\n        \"\"\"\n        device_name_list = list()\n        for device in nwbf.devices.values():\n            if isinstance(device, ndx_franklab_novela.CameraDevice):\n                device_dict = dict()\n                # TODO ideally the ID is not encoded in the name formatted in a particular way\n                # device.name must have the form \"[any string without a space, usually camera] [int]\"\n                device_dict[\"camera_id\"] = int(str.split(device.name)[1])\n                device_dict[\"camera_name\"] = device.camera_name\n                device_dict[\"manufacturer\"] = device.manufacturer\n                device_dict[\"model\"] = device.model\n                device_dict[\"lens\"] = device.lens\n                device_dict[\"meters_per_pixel\"] = device.meters_per_pixel\n                cls.insert1(device_dict, skip_duplicates=True)\n                device_name_list.append(device_dict[\"camera_name\"])\n        if device_name_list:\n            print(f\"Inserted camera devices {device_name_list}\")\n        else:\n            print(\"No conforming camera device metadata found.\")\n        return device_name_list\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_device.CameraDevice.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert camera devices from an NWB file</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of camera device object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert camera devices from an NWB file\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n\n    Returns\n    -------\n    device_name_list : list\n        List of camera device object names found in the NWB file.\n    \"\"\"\n    device_name_list = list()\n    for device in nwbf.devices.values():\n        if isinstance(device, ndx_franklab_novela.CameraDevice):\n            device_dict = dict()\n            # TODO ideally the ID is not encoded in the name formatted in a particular way\n            # device.name must have the form \"[any string without a space, usually camera] [int]\"\n            device_dict[\"camera_id\"] = int(str.split(device.name)[1])\n            device_dict[\"camera_name\"] = device.camera_name\n            device_dict[\"manufacturer\"] = device.manufacturer\n            device_dict[\"model\"] = device.model\n            device_dict[\"lens\"] = device.lens\n            device_dict[\"meters_per_pixel\"] = device.meters_per_pixel\n            cls.insert1(device_dict, skip_duplicates=True)\n            device_name_list.append(device_dict[\"camera_name\"])\n    if device_name_list:\n        print(f\"Inserted camera devices {device_name_list}\")\n    else:\n        print(\"No conforming camera device metadata found.\")\n    return device_name_list\n</code></pre>"}, {"location": "api/src/spyglass/common/common_behav/#src.spyglass.common.common_behav.get_all_spatial_series", "title": "<code>get_all_spatial_series(nwbf, verbose=False)</code>", "text": "<p>Given an NWBFile, get the spatial series and interval lists from the file and return a dictionary by epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>verbose</code> <code>bool</code> <p>Flag representing whether to print the sampling rate.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pos_data_dict</code> <code>dict</code> <p>Dict mapping indices to a dict with keys 'valid_times' and 'raw_position_object_id'. Returns None if there is no position data in the file. The 'raw_position_object_id' is the object ID of the SpatialSeries object.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_all_spatial_series(nwbf, verbose=False):\n\"\"\"Given an NWBFile, get the spatial series and interval lists from the file and return a dictionary by epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    verbose : bool\n        Flag representing whether to print the sampling rate.\n\n    Returns\n    -------\n    pos_data_dict : dict\n        Dict mapping indices to a dict with keys 'valid_times' and 'raw_position_object_id'. Returns None if there\n        is no position data in the file. The 'raw_position_object_id' is the object ID of the SpatialSeries object.\n    \"\"\"\n    position = get_data_interface(nwbf, \"position\", pynwb.behavior.Position)\n    if position is None:\n        return None\n\n    # for some reason the spatial_series do not necessarily come out in order, so we need to figure out the right order\n    epoch_start_time = np.zeros(len(position.spatial_series.values()))\n    for pos_epoch, spatial_series in enumerate(\n        position.spatial_series.values()\n    ):\n        epoch_start_time[pos_epoch] = spatial_series.timestamps[0]\n\n    sorted_order = np.argsort(epoch_start_time)\n    pos_data_dict = dict()\n\n    for index, orig_epoch in enumerate(sorted_order):\n        spatial_series = list(position.spatial_series.values())[orig_epoch]\n        pos_data_dict[index] = dict()\n        # get the valid intervals for the position data\n        timestamps = np.asarray(spatial_series.timestamps)\n\n        # estimate the sampling rate\n        timestamps = np.asarray(spatial_series.timestamps)\n        sampling_rate = estimate_sampling_rate(timestamps, 1.75)\n        if sampling_rate &lt; 0:\n            raise ValueError(\n                f\"Error adding position data for position epoch {index}\"\n            )\n        if verbose:\n            print(\n                \"Processing raw position data. Estimated sampling rate: {} Hz\".format(\n                    sampling_rate\n                )\n            )\n        # add the valid intervals to the Interval list\n        pos_data_dict[index][\"valid_times\"] = get_valid_intervals(\n            timestamps,\n            sampling_rate,\n            gap_proportion=2.5,\n            min_valid_len=int(sampling_rate),\n        )\n        pos_data_dict[index][\n            \"raw_position_object_id\"\n        ] = spatial_series.object_id\n\n    return pos_data_dict\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/", "title": "common_device.py", "text": ""}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.get_nwb_file", "title": "<code>get_nwb_file(nwb_file_path)</code>", "text": "<p>Return an NWBFile object with the given file path in read mode.    If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Path to the NWB file.</p> required <p>Returns:</p> Name Type Description <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>NWB file object for the given path opened in read mode.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_file(nwb_file_path):\n\"\"\"Return an NWBFile object with the given file path in read mode.\n       If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.\n\n    Parameters\n    ----------\n    nwb_file_path : str\n        Path to the NWB file.\n\n    Returns\n    -------\n    nwbfile : pynwb.NWBFile\n        NWB file object for the given path opened in read mode.\n    \"\"\"\n    _, nwbfile = __open_nwb_files.get(nwb_file_path, (None, None))\n    nwb_uri = None\n    nwb_raw_uri = None\n    if nwbfile is None:\n        # check to see if the file exists\n        if not os.path.exists(nwb_file_path):\n            print(\n                f\"NWB file {nwb_file_path} does not exist locally; checking kachery\"\n            )\n            # first try the analysis files\n            from ..sharing.sharing_kachery import AnalysisNwbfileKachery\n\n            # the download functions assume just the filename, so we need to get that from the path\n            if not AnalysisNwbfileKachery.download_file(\n                os.path.basename(nwb_file_path)\n            ):\n                return None\n        # now open the file\n        io = pynwb.NWBHDF5IO(\n            path=nwb_file_path, mode=\"r\", load_namespaces=True\n        )  # keep file open\n        nwbfile = io.read()\n        __open_nwb_files[nwb_file_path] = (io, nwbfile)\n\n    return nwbfile\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.DataAcquisitionDevice", "title": "<code>DataAcquisitionDevice</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass DataAcquisitionDevice(dj.Manual):\n    definition = \"\"\"\n    data_acquisition_device_name: varchar(80)\n    ---\n    -&gt; DataAcquisitionDeviceSystem\n    -&gt; DataAcquisitionDeviceAmplifier\n    adc_circuit = NULL: varchar(2000)\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, config):\n\"\"\"Insert data acquisition devices from an NWB file.\n\n        Note that this does not link the DataAcquisitionDevices with a Session. For that,\n        see DataAcquisitionDeviceList.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n        \"\"\"\n        _, ndx_devices, _ = cls.get_all_device_names(nwbf, config)\n\n        for device_name in ndx_devices:\n            new_device_dict = dict()\n\n            # read device properties into new_device_dict from PyNWB extension device object\n            nwb_device_obj = ndx_devices[device_name]\n\n            name = nwb_device_obj.name\n            adc_circuit = nwb_device_obj.adc_circuit\n\n            # transform system value. check if value is in DB. if not, prompt user to add an entry or cancel.\n            system = cls._add_system(nwb_device_obj.system)\n\n            # transform amplifier value. check if value is in DB. if not, prompt user to add an entry or cancel.\n            amplifier = cls._add_amplifier(nwb_device_obj.amplifier)\n\n            # standardize how Intan is represented in the database\n            if adc_circuit.title() == \"Intan\":\n                adc_circuit = \"Intan\"\n\n            new_device_dict[\"data_acquisition_device_name\"] = name\n            new_device_dict[\"data_acquisition_device_system\"] = system\n            new_device_dict[\"data_acquisition_device_amplifier\"] = amplifier\n            new_device_dict[\"adc_circuit\"] = adc_circuit\n\n            cls._add_device(new_device_dict)\n\n        if ndx_devices:\n            print(\n                f\"Inserted or referenced data acquisition device(s): {ndx_devices.keys()}\"\n            )\n        else:\n            print(\"No conforming data acquisition device metadata found.\")\n\n    @classmethod\n    def get_all_device_names(cls, nwbf, config):\n\"\"\"Get a list of all device names in the NWB file, after appending and overwriting by the config file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : list\n            List of data acquisition object names found in the NWB file.\n        \"\"\"\n        # make a dict mapping device name to PyNWB device object for all devices in the NWB file that are\n        # of type ndx_franklab_novela.DataAcqDevice and thus have the required metadata\n        ndx_devices = {\n            device_obj.name: device_obj\n            for device_obj in nwbf.devices.values()\n            if isinstance(device_obj, ndx_franklab_novela.DataAcqDevice)\n        }\n\n        # make a list of device names that are associated with this NWB file\n        if \"DataAcquisitionDevice\" in config:\n            config_devices = [\n                device_dict[\"data_acquisition_device_name\"]\n                for device_dict in config[\"DataAcquisitionDevice\"]\n            ]\n        else:\n            config_devices = list()\n\n        all_device_names = set(ndx_devices.keys()).union(set(config_devices))\n\n        return all_device_names, ndx_devices, config_devices\n\n    @classmethod\n    def _add_device(cls, new_device_dict):\n\"\"\"Check that the information in the NWB file and the database for the given device name match perfectly.\n\n        If no DataAcquisitionDevice with the given name exists in the database, check whether the user wants to add\n        a new entry instead of referencing an existing entry. If so, return. If not, raise an exception.\n\n        Parameters\n        ----------\n        new_device_dict : dict\n            Dict of new device properties\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a device to the database when prompted or if the device properties from the\n            NWB file do not match the properties of the corresponding database entry.\n        \"\"\"\n        name = new_device_dict[\"data_acquisition_device_name\"]\n        all_values = DataAcquisitionDevice.fetch(\n            \"data_acquisition_device_name\"\n        ).tolist()\n        if name not in all_values:\n            # no entry with the same name exists, prompt user about adding a new entry\n            print(\n                f\"\\nData acquisition device '{name}' was not found in the database. \"\n                f\"The current values are: {all_values}. \"\n                \"Please ensure that the device you want to add does not already \"\n                \"exist in the database under a different name or spelling. \"\n                \"If you want to use an existing device in the database, \"\n                \"please change the corresponding Device object in the NWB file. \"\n                \"Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                f\"Do you want to add data acquisition device '{name}' to the database? (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                cls.insert1(new_device_dict, skip_duplicates=True)\n                return\n            raise PopulateException(\n                f\"User chose not to add data acquisition device '{name}' to the database.\"\n            )\n\n        # effectively else (entry exists)\n        # check whether the values provided match the values stored in the database\n        db_dict = (\n            DataAcquisitionDevice &amp; {\"data_acquisition_device_name\": name}\n        ).fetch1()\n        if db_dict != new_device_dict:\n            raise PopulateException(\n                f\"Data acquisition device properties of PyNWB Device object with name '{name}': \"\n                f\"{new_device_dict} do not match properties of the corresponding database entry: {db_dict}.\"\n            )\n\n    @classmethod\n    def _add_system(cls, system):\n\"\"\"Check the system value. If it is not in the database, prompt the user to add the value to the database.\n\n        This method also renames the system value \"MCU\" to \"SpikeGadgets\".\n\n        Parameters\n        ----------\n        system : str\n            The system value to check.\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a device system value to the database when prompted.\n\n        Returns\n        -------\n        system : str\n            The system value that was added to the database.\n        \"\"\"\n        if system == \"MCU\":\n            system = \"SpikeGadgets\"\n\n        all_values = DataAcquisitionDeviceSystem.fetch(\n            \"data_acquisition_device_system\"\n        ).tolist()\n        if system not in all_values:\n            print(\n                f\"\\nData acquisition device system '{system}' was not found in the database. \"\n                f\"The current values are: {all_values}. \"\n                \"Please ensure that the system you want to add does not already \"\n                \"exist in the database under a different name or spelling. \"\n                \"If you want to use an existing system in the database, \"\n                \"please change the corresponding Device object in the NWB file. \"\n                \"Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                f\"Do you want to add data acquisition device system '{system}' to the database? (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                key = {\"data_acquisition_device_system\": system}\n                DataAcquisitionDeviceSystem.insert1(key, skip_duplicates=True)\n            else:\n                raise PopulateException(\n                    f\"User chose not to add data acquisition device system '{system}' to the database.\"\n                )\n        return system\n\n    @classmethod\n    def _add_amplifier(cls, amplifier):\n\"\"\"Check the amplifier value. If it is not in the database, prompt the user to add the value to the database.\n\n        Parameters\n        ----------\n        amplifier : str\n            The amplifier value to check.\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a device amplifier value to the database when prompted.\n\n        Returns\n        -------\n        amplifier : str\n            The amplifier value that was added to the database.\n        \"\"\"\n        # standardize how Intan is represented in the database\n        if amplifier.title() == \"Intan\":\n            amplifier = \"Intan\"\n\n        all_values = DataAcquisitionDeviceAmplifier.fetch(\n            \"data_acquisition_device_amplifier\"\n        ).tolist()\n        if amplifier not in all_values:\n            print(\n                f\"\\nData acquisition device amplifier '{amplifier}' was not found in the database. \"\n                f\"The current values are: {all_values}. \"\n                \"Please ensure that the amplifier you want to add does not already \"\n                \"exist in the database under a different name or spelling. \"\n                \"If you want to use an existing name in the database, \"\n                \"please change the corresponding Device object in the NWB file. \"\n                \"Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                f\"Do you want to add data acquisition device amplifier '{amplifier}' to the database? (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                key = {\"data_acquisition_device_amplifier\": amplifier}\n                DataAcquisitionDeviceAmplifier.insert1(\n                    key, skip_duplicates=True\n                )\n            else:\n                raise PopulateException(\n                    f\"User chose not to add data acquisition device amplifier '{amplifier}' to the database.\"\n                )\n        return amplifier\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.DataAcquisitionDevice.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Insert data acquisition devices from an NWB file.</p> <p>Note that this does not link the DataAcquisitionDevices with a Session. For that, see DataAcquisitionDeviceList.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, config):\n\"\"\"Insert data acquisition devices from an NWB file.\n\n    Note that this does not link the DataAcquisitionDevices with a Session. For that,\n    see DataAcquisitionDeviceList.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n    \"\"\"\n    _, ndx_devices, _ = cls.get_all_device_names(nwbf, config)\n\n    for device_name in ndx_devices:\n        new_device_dict = dict()\n\n        # read device properties into new_device_dict from PyNWB extension device object\n        nwb_device_obj = ndx_devices[device_name]\n\n        name = nwb_device_obj.name\n        adc_circuit = nwb_device_obj.adc_circuit\n\n        # transform system value. check if value is in DB. if not, prompt user to add an entry or cancel.\n        system = cls._add_system(nwb_device_obj.system)\n\n        # transform amplifier value. check if value is in DB. if not, prompt user to add an entry or cancel.\n        amplifier = cls._add_amplifier(nwb_device_obj.amplifier)\n\n        # standardize how Intan is represented in the database\n        if adc_circuit.title() == \"Intan\":\n            adc_circuit = \"Intan\"\n\n        new_device_dict[\"data_acquisition_device_name\"] = name\n        new_device_dict[\"data_acquisition_device_system\"] = system\n        new_device_dict[\"data_acquisition_device_amplifier\"] = amplifier\n        new_device_dict[\"adc_circuit\"] = adc_circuit\n\n        cls._add_device(new_device_dict)\n\n    if ndx_devices:\n        print(\n            f\"Inserted or referenced data acquisition device(s): {ndx_devices.keys()}\"\n        )\n    else:\n        print(\"No conforming data acquisition device metadata found.\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.DataAcquisitionDevice.get_all_device_names", "title": "<code>get_all_device_names(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Get a list of all device names in the NWB file, after appending and overwriting by the config file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of data acquisition object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef get_all_device_names(cls, nwbf, config):\n\"\"\"Get a list of all device names in the NWB file, after appending and overwriting by the config file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : list\n        List of data acquisition object names found in the NWB file.\n    \"\"\"\n    # make a dict mapping device name to PyNWB device object for all devices in the NWB file that are\n    # of type ndx_franklab_novela.DataAcqDevice and thus have the required metadata\n    ndx_devices = {\n        device_obj.name: device_obj\n        for device_obj in nwbf.devices.values()\n        if isinstance(device_obj, ndx_franklab_novela.DataAcqDevice)\n    }\n\n    # make a list of device names that are associated with this NWB file\n    if \"DataAcquisitionDevice\" in config:\n        config_devices = [\n            device_dict[\"data_acquisition_device_name\"]\n            for device_dict in config[\"DataAcquisitionDevice\"]\n        ]\n    else:\n        config_devices = list()\n\n    all_device_names = set(ndx_devices.keys()).union(set(config_devices))\n\n    return all_device_names, ndx_devices, config_devices\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.CameraDevice", "title": "<code>CameraDevice</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass CameraDevice(dj.Manual):\n    definition = \"\"\"\n    camera_name: varchar(80)\n    ---\n    meters_per_pixel = 0: float  # height / width of pixel in meters\n    manufacturer = \"\": varchar(2000)\n    model = \"\": varchar(2000)\n    lens = \"\": varchar(2000)\n    camera_id = -1: int\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert camera devices from an NWB file\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n\n        Returns\n        -------\n        device_name_list : list\n            List of camera device object names found in the NWB file.\n        \"\"\"\n        device_name_list = list()\n        for device in nwbf.devices.values():\n            if isinstance(device, ndx_franklab_novela.CameraDevice):\n                device_dict = dict()\n                # TODO ideally the ID is not encoded in the name formatted in a particular way\n                # device.name must have the form \"[any string without a space, usually camera] [int]\"\n                device_dict[\"camera_id\"] = int(str.split(device.name)[1])\n                device_dict[\"camera_name\"] = device.camera_name\n                device_dict[\"manufacturer\"] = device.manufacturer\n                device_dict[\"model\"] = device.model\n                device_dict[\"lens\"] = device.lens\n                device_dict[\"meters_per_pixel\"] = device.meters_per_pixel\n                cls.insert1(device_dict, skip_duplicates=True)\n                device_name_list.append(device_dict[\"camera_name\"])\n        if device_name_list:\n            print(f\"Inserted camera devices {device_name_list}\")\n        else:\n            print(\"No conforming camera device metadata found.\")\n        return device_name_list\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.CameraDevice.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert camera devices from an NWB file</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of camera device object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert camera devices from an NWB file\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n\n    Returns\n    -------\n    device_name_list : list\n        List of camera device object names found in the NWB file.\n    \"\"\"\n    device_name_list = list()\n    for device in nwbf.devices.values():\n        if isinstance(device, ndx_franklab_novela.CameraDevice):\n            device_dict = dict()\n            # TODO ideally the ID is not encoded in the name formatted in a particular way\n            # device.name must have the form \"[any string without a space, usually camera] [int]\"\n            device_dict[\"camera_id\"] = int(str.split(device.name)[1])\n            device_dict[\"camera_name\"] = device.camera_name\n            device_dict[\"manufacturer\"] = device.manufacturer\n            device_dict[\"model\"] = device.model\n            device_dict[\"lens\"] = device.lens\n            device_dict[\"meters_per_pixel\"] = device.meters_per_pixel\n            cls.insert1(device_dict, skip_duplicates=True)\n            device_name_list.append(device_dict[\"camera_name\"])\n    if device_name_list:\n        print(f\"Inserted camera devices {device_name_list}\")\n    else:\n        print(\"No conforming camera device metadata found.\")\n    return device_name_list\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.Probe", "title": "<code>Probe</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass Probe(dj.Manual):\n    definition = \"\"\"\n    # A configuration of a ProbeType. For most probe types, there is only one configuration, and that configuration\n    # should always be used. For Neuropixels probes, the specific channel map (which electrodes are used,\n    # where are they, and in what order) can differ between users and sessions, and each configuration should have a\n    # different ProbeType.\n    probe_id: varchar(80)     # a unique ID for this probe and dynamic configuration\n    ---\n    -&gt; ProbeType              # the type of probe, selected from a controlled list of probe types\n    -&gt; [nullable] DataAcquisitionDevice  # the data acquisition device used with this Probe\n    contact_side_numbering: enum(\"True\", \"False\")  # if True, then electrode contacts are facing you when numbering them\n    \"\"\"\n\n    class Shank(dj.Part):\n        definition = \"\"\"\n        -&gt; Probe\n        probe_shank: int              # shank number within probe. should be unique within a Probe\n        \"\"\"\n\n    class Electrode(dj.Part):\n        definition = \"\"\"\n        -&gt; Probe.Shank\n        probe_electrode: int          # electrode ID that is output from the data acquisition system\n                                      # probe_electrode should be unique within a Probe\n        ---\n        contact_size = NULL: float    # (um) contact size\n        rel_x = NULL: float           # (um) x coordinate of the electrode within the probe\n        rel_y = NULL: float           # (um) y coordinate of the electrode within the probe\n        rel_z = NULL: float           # (um) z coordinate of the electrode within the probe\n        \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, config):\n\"\"\"Insert probe devices from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : list\n            List of probe device types found in the NWB file.\n        \"\"\"\n        all_probes_types, ndx_probes, _ = cls.get_all_probe_names(nwbf, config)\n\n        for probe_type in all_probes_types:\n            new_probe_type_dict = dict()\n            new_probe_dict = dict()\n            shank_dict = dict()\n            elect_dict = dict()\n            num_shanks = 0\n\n            if probe_type in ndx_probes:\n                # read probe properties into new_probe_dict from PyNWB extension probe object\n                nwb_probe_obj = ndx_probes[probe_type]\n                cls.__read_ndx_probe_data(\n                    nwb_probe_obj,\n                    new_probe_type_dict,\n                    new_probe_dict,\n                    shank_dict,\n                    elect_dict,\n                )\n\n            # check that number of shanks is consistent\n            num_shanks = new_probe_type_dict[\"num_shanks\"]\n            assert num_shanks == 0 or num_shanks == len(\n                shank_dict\n            ), \"`num_shanks` is not equal to the number of shanks.\"\n\n            # if probe id already exists, do not overwrite anything or create new Shanks and Electrodes\n            # TODO test whether the Shanks and Electrodes in the NWB file match the ones in the database\n            query = Probe &amp; {\"probe_id\": new_probe_dict[\"probe_id\"]}\n            if len(query) &gt; 0:\n                print(\n                    f\"Probe ID '{new_probe_dict['probe_id']}' already exists in the database. Spyglass will use \"\n                    \"that and not create a new Probe, Shanks, or Electrodes.\"\n                )\n                continue\n\n            cls.insert1(new_probe_dict, skip_duplicates=True)\n\n            for shank in shank_dict.values():\n                cls.Shank.insert1(shank, skip_duplicates=True)\n            for electrode in elect_dict.values():\n                cls.Electrode.insert1(electrode, skip_duplicates=True)\n\n        if all_probes_types:\n            print(f\"Inserted probes {all_probes_types}\")\n        else:\n            print(\"No conforming probe metadata found.\")\n\n        return all_probes_types\n\n    @classmethod\n    def get_all_probe_names(cls, nwbf, config):\n\"\"\"Get a list of all device names in the NWB file, after appending and overwriting by the config file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : list\n            List of data acquisition object names found in the NWB file.\n        \"\"\"\n\n        # make a dict mapping probe type to PyNWB object for all devices in the NWB file that are\n        # of type ndx_franklab_novela.Probe and thus have the required metadata\n        ndx_probes = {\n            device_obj.probe_type: device_obj\n            for device_obj in nwbf.devices.values()\n            if isinstance(device_obj, ndx_franklab_novela.Probe)\n        }\n\n        # make a dict mapping probe type to dict of device metadata from the config YAML if exists\n        if \"Probe\" in config:\n            config_probes = [\n                probe_dict[\"probe_type\"] for probe_dict in config[\"Probe\"]\n            ]\n        else:\n            config_probes = list()\n\n        # get all the probe types from the NWB file plus the config YAML\n        all_probes_types = set(ndx_probes.keys()).union(set(config_probes))\n\n        return all_probes_types, ndx_probes, config_probes\n\n    @classmethod\n    def __read_ndx_probe_data(\n        cls,\n        nwb_probe_obj: ndx_franklab_novela.Probe,\n        new_probe_type_dict: dict,\n        new_probe_dict: dict,\n        shank_dict: dict,\n        elect_dict: dict,\n    ):\n        # construct dictionary of values to add to ProbeType\n        new_probe_type_dict[\"manufacturer\"] = (\n            getattr(nwb_probe_obj, \"manufacturer\") or \"\"\n        )\n        new_probe_type_dict[\"probe_type\"] = nwb_probe_obj.probe_type\n        new_probe_type_dict[\n            \"probe_description\"\n        ] = nwb_probe_obj.probe_description\n        new_probe_type_dict[\"num_shanks\"] = len(nwb_probe_obj.shanks)\n\n        cls._add_probe_type(new_probe_type_dict)\n\n        new_probe_dict[\"probe_id\"] = nwb_probe_obj.probe_type\n        new_probe_dict[\"probe_type\"] = nwb_probe_obj.probe_type\n        new_probe_dict[\"contact_side_numbering\"] = (\n            \"True\" if nwb_probe_obj.contact_side_numbering else \"False\"\n        )\n\n        # go through the shanks and add each one to the Shank table\n        for shank in nwb_probe_obj.shanks.values():\n            shank_dict[shank.name] = dict()\n            shank_dict[shank.name][\"probe_id\"] = new_probe_dict[\"probe_type\"]\n            shank_dict[shank.name][\"probe_shank\"] = int(shank.name)\n\n            # go through the electrodes and add each one to the Electrode table\n            for electrode in shank.shanks_electrodes.values():\n                # the next line will need to be fixed if we have different sized contacts on a shank\n                elect_dict[electrode.name] = dict()\n                elect_dict[electrode.name][\"probe_id\"] = new_probe_dict[\n                    \"probe_type\"\n                ]\n                elect_dict[electrode.name][\"probe_shank\"] = shank_dict[\n                    shank.name\n                ][\"probe_shank\"]\n                elect_dict[electrode.name][\n                    \"contact_size\"\n                ] = nwb_probe_obj.contact_size\n                elect_dict[electrode.name][\"probe_electrode\"] = int(\n                    electrode.name\n                )\n                elect_dict[electrode.name][\"rel_x\"] = electrode.rel_x\n                elect_dict[electrode.name][\"rel_y\"] = electrode.rel_y\n                elect_dict[electrode.name][\"rel_z\"] = electrode.rel_z\n\n    @classmethod\n    def _add_probe_type(cls, new_probe_type_dict):\n\"\"\"Check the probe type value against the values in the database.\n\n        Parameters\n        ----------\n        new_probe_type_dict : dict\n            Dictionary of probe type properties. See ProbeType for keys.\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a probe type to the database when prompted.\n\n        Returns\n        -------\n        probe_type : str\n            The probe type value that was added to the database.\n        \"\"\"\n        probe_type = new_probe_type_dict[\"probe_type\"]\n        all_values = ProbeType.fetch(\"probe_type\").tolist()\n        if probe_type not in all_values:\n            print(\n                f\"\\nProbe type '{probe_type}' was not found in the database. \"\n                f\"The current values are: {all_values}. \"\n                \"Please ensure that the probe type you want to add does not already \"\n                \"exist in the database under a different name or spelling. \"\n                \"If you want to use an existing name in the database, \"\n                \"please change the corresponding Probe object in the NWB file. \"\n                \"Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                f\"Do you want to add probe type '{probe_type}' to the database? (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                ProbeType.insert1(new_probe_type_dict, skip_duplicates=True)\n                return\n            raise PopulateException(\n                f\"User chose not to add probe type '{probe_type}' to the database.\"\n            )\n\n        # effectively else (entry exists)\n        # check whether the values provided match the values stored in the database\n        db_dict = (ProbeType &amp; {\"probe_type\": probe_type}).fetch1()\n        if db_dict != new_probe_type_dict:\n            raise PopulateException(\n                f\"\\nProbe type properties of PyNWB Probe object with name '{probe_type}': \"\n                f\"{new_probe_type_dict} do not match properties of the corresponding database entry: {db_dict}.\"\n            )\n        return probe_type\n\n    @classmethod\n    def create_from_nwbfile(\n        cls,\n        nwb_file_name: str,\n        nwb_device_name: str,\n        probe_id: str,\n        probe_type: str,\n        contact_side_numbering: bool,\n    ):\n\"\"\"Create a Probe entry and corresponding part table entries using the data in the NWB file.\n\n        This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices\n        (as probes) in the NWB file, but only ones that are associated with the device that matches the given\n        `nwb_device_name`.\n\n        Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe,\n        the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.\n\n        Example usage:\n        ```\n        sgc.Probe.create_from_nwbfile(\n            nwbfile=nwb_file_name,\n            nwb_device_name=\"Device\",\n            probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n            probe_type=\"Neuropixels 1.0\",\n            contact_side_numbering=True\n        )\n        ```\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the NWB file.\n        nwb_device_name : str\n            The name of the PyNWB Device object that represents the probe to read in the NWB file.\n        probe_id : str\n            A unique ID for the probe and its configuration, to be used as the primary key for the new Probe entry.\n        probe_type : str\n            The existing ProbeType entry that represents the type of probe being created. It must exist.\n        contact_side_numbering : bool\n            Whether the electrode contacts are facing you when numbering them. Stored in the new Probe entry.\n        \"\"\"\n\n        from .common_nwbfile import Nwbfile\n\n        nwb_file_path = Nwbfile.get_abs_path(nwb_file_name)\n        nwbfile = get_nwb_file(nwb_file_path)\n\n        query = ProbeType &amp; {\"probe_type\": probe_type}\n        if len(query) == 0:\n            print(\n                f\"No ProbeType found with probe_type '{probe_type}'. Aborting.\"\n            )\n            return\n\n        new_probe_dict = dict()\n        shank_dict = dict()\n        elect_dict = dict()\n\n        new_probe_dict[\"probe_id\"] = probe_id\n        new_probe_dict[\"probe_type\"] = probe_type\n        new_probe_dict[\"contact_side_numbering\"] = (\n            \"True\" if contact_side_numbering else \"False\"\n        )\n\n        # iterate through the electrodes table in the NWB file\n        # and use the group column (ElectrodeGroup) to create shanks\n        # and use the device attribute of each ElectrodeGroup to create a probe\n        created_shanks = dict()  # map device name to shank_index (int)\n        device_found = False\n        for elec_index in range(len(nwbfile.electrodes)):\n            electrode_group = nwbfile.electrodes[elec_index, \"group\"]\n            eg_device_name = electrode_group.device.name\n\n            # only look at electrodes where the associated device is the one specified\n            if eg_device_name == nwb_device_name:\n                device_found = True\n\n                # if a Shank has not yet been created from the electrode group, then create it\n                if electrode_group.name not in created_shanks:\n                    shank_index = len(created_shanks)\n                    created_shanks[electrode_group.name] = shank_index\n\n                    # build the dictionary of Probe.Shank data\n                    shank_dict[shank_index] = dict()\n                    shank_dict[shank_index][\"probe_id\"] = new_probe_dict[\n                        \"probe_id\"\n                    ]\n                    shank_dict[shank_index][\"probe_shank\"] = shank_index\n\n                # get the probe shank index associated with this Electrode\n                probe_shank = created_shanks[electrode_group.name]\n\n                # build the dictionary of Probe.Electrode data\n                elect_dict[elec_index] = dict()\n                elect_dict[elec_index][\"probe_id\"] = new_probe_dict[\"probe_id\"]\n                elect_dict[elec_index][\"probe_shank\"] = probe_shank\n                elect_dict[elec_index][\"probe_electrode\"] = elec_index\n                if \"rel_x\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_x\"] = nwbfile.electrodes[\n                        elec_index, \"rel_x\"\n                    ]\n                if \"rel_y\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_y\"] = nwbfile.electrodes[\n                        elec_index, \"rel_y\"\n                    ]\n                if \"rel_z\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_z\"] = nwbfile.electrodes[\n                        elec_index, \"rel_z\"\n                    ]\n\n        if not device_found:\n            print(\n                f\"No electrodes in the NWB file were associated with a device named '{nwb_device_name}'.\"\n            )\n            return\n\n        # insert the Probe, then the Shank parts, and then the Electrode parts\n        cls.insert1(new_probe_dict, skip_duplicates=True)\n\n        for shank in shank_dict.values():\n            cls.Shank.insert1(shank, skip_duplicates=True)\n        for electrode in elect_dict.values():\n            cls.Electrode.insert1(electrode, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.Probe.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Insert probe devices from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of probe device types found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, config):\n\"\"\"Insert probe devices from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : list\n        List of probe device types found in the NWB file.\n    \"\"\"\n    all_probes_types, ndx_probes, _ = cls.get_all_probe_names(nwbf, config)\n\n    for probe_type in all_probes_types:\n        new_probe_type_dict = dict()\n        new_probe_dict = dict()\n        shank_dict = dict()\n        elect_dict = dict()\n        num_shanks = 0\n\n        if probe_type in ndx_probes:\n            # read probe properties into new_probe_dict from PyNWB extension probe object\n            nwb_probe_obj = ndx_probes[probe_type]\n            cls.__read_ndx_probe_data(\n                nwb_probe_obj,\n                new_probe_type_dict,\n                new_probe_dict,\n                shank_dict,\n                elect_dict,\n            )\n\n        # check that number of shanks is consistent\n        num_shanks = new_probe_type_dict[\"num_shanks\"]\n        assert num_shanks == 0 or num_shanks == len(\n            shank_dict\n        ), \"`num_shanks` is not equal to the number of shanks.\"\n\n        # if probe id already exists, do not overwrite anything or create new Shanks and Electrodes\n        # TODO test whether the Shanks and Electrodes in the NWB file match the ones in the database\n        query = Probe &amp; {\"probe_id\": new_probe_dict[\"probe_id\"]}\n        if len(query) &gt; 0:\n            print(\n                f\"Probe ID '{new_probe_dict['probe_id']}' already exists in the database. Spyglass will use \"\n                \"that and not create a new Probe, Shanks, or Electrodes.\"\n            )\n            continue\n\n        cls.insert1(new_probe_dict, skip_duplicates=True)\n\n        for shank in shank_dict.values():\n            cls.Shank.insert1(shank, skip_duplicates=True)\n        for electrode in elect_dict.values():\n            cls.Electrode.insert1(electrode, skip_duplicates=True)\n\n    if all_probes_types:\n        print(f\"Inserted probes {all_probes_types}\")\n    else:\n        print(\"No conforming probe metadata found.\")\n\n    return all_probes_types\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.Probe.get_all_probe_names", "title": "<code>get_all_probe_names(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Get a list of all device names in the NWB file, after appending and overwriting by the config file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of data acquisition object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef get_all_probe_names(cls, nwbf, config):\n\"\"\"Get a list of all device names in the NWB file, after appending and overwriting by the config file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : list\n        List of data acquisition object names found in the NWB file.\n    \"\"\"\n\n    # make a dict mapping probe type to PyNWB object for all devices in the NWB file that are\n    # of type ndx_franklab_novela.Probe and thus have the required metadata\n    ndx_probes = {\n        device_obj.probe_type: device_obj\n        for device_obj in nwbf.devices.values()\n        if isinstance(device_obj, ndx_franklab_novela.Probe)\n    }\n\n    # make a dict mapping probe type to dict of device metadata from the config YAML if exists\n    if \"Probe\" in config:\n        config_probes = [\n            probe_dict[\"probe_type\"] for probe_dict in config[\"Probe\"]\n        ]\n    else:\n        config_probes = list()\n\n    # get all the probe types from the NWB file plus the config YAML\n    all_probes_types = set(ndx_probes.keys()).union(set(config_probes))\n\n    return all_probes_types, ndx_probes, config_probes\n</code></pre>"}, {"location": "api/src/spyglass/common/common_device/#src.spyglass.common.common_device.Probe.create_from_nwbfile", "title": "<code>create_from_nwbfile(nwb_file_name, nwb_device_name, probe_id, probe_type, contact_side_numbering)</code>  <code>classmethod</code>", "text": "<p>Create a Probe entry and corresponding part table entries using the data in the NWB file.</p> <p>This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices (as probes) in the NWB file, but only ones that are associated with the device that matches the given <code>nwb_device_name</code>.</p> <p>Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe, the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.</p> <p>Example usage:</p> <pre><code>sgc.Probe.create_from_nwbfile(\n    nwbfile=nwb_file_name,\n    nwb_device_name=\"Device\",\n    probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n    probe_type=\"Neuropixels 1.0\",\n    contact_side_numbering=True\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file.</p> required <code>nwb_device_name</code> <code>str</code> <p>The name of the PyNWB Device object that represents the probe to read in the NWB file.</p> required <code>probe_id</code> <code>str</code> <p>A unique ID for the probe and its configuration, to be used as the primary key for the new Probe entry.</p> required <code>probe_type</code> <code>str</code> <p>The existing ProbeType entry that represents the type of probe being created. It must exist.</p> required <code>contact_side_numbering</code> <code>bool</code> <p>Whether the electrode contacts are facing you when numbering them. Stored in the new Probe entry.</p> required Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef create_from_nwbfile(\n    cls,\n    nwb_file_name: str,\n    nwb_device_name: str,\n    probe_id: str,\n    probe_type: str,\n    contact_side_numbering: bool,\n):\n\"\"\"Create a Probe entry and corresponding part table entries using the data in the NWB file.\n\n    This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices\n    (as probes) in the NWB file, but only ones that are associated with the device that matches the given\n    `nwb_device_name`.\n\n    Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe,\n    the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.\n\n    Example usage:\n    ```\n    sgc.Probe.create_from_nwbfile(\n        nwbfile=nwb_file_name,\n        nwb_device_name=\"Device\",\n        probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n        probe_type=\"Neuropixels 1.0\",\n        contact_side_numbering=True\n    )\n    ```\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    nwb_device_name : str\n        The name of the PyNWB Device object that represents the probe to read in the NWB file.\n    probe_id : str\n        A unique ID for the probe and its configuration, to be used as the primary key for the new Probe entry.\n    probe_type : str\n        The existing ProbeType entry that represents the type of probe being created. It must exist.\n    contact_side_numbering : bool\n        Whether the electrode contacts are facing you when numbering them. Stored in the new Probe entry.\n    \"\"\"\n\n    from .common_nwbfile import Nwbfile\n\n    nwb_file_path = Nwbfile.get_abs_path(nwb_file_name)\n    nwbfile = get_nwb_file(nwb_file_path)\n\n    query = ProbeType &amp; {\"probe_type\": probe_type}\n    if len(query) == 0:\n        print(\n            f\"No ProbeType found with probe_type '{probe_type}'. Aborting.\"\n        )\n        return\n\n    new_probe_dict = dict()\n    shank_dict = dict()\n    elect_dict = dict()\n\n    new_probe_dict[\"probe_id\"] = probe_id\n    new_probe_dict[\"probe_type\"] = probe_type\n    new_probe_dict[\"contact_side_numbering\"] = (\n        \"True\" if contact_side_numbering else \"False\"\n    )\n\n    # iterate through the electrodes table in the NWB file\n    # and use the group column (ElectrodeGroup) to create shanks\n    # and use the device attribute of each ElectrodeGroup to create a probe\n    created_shanks = dict()  # map device name to shank_index (int)\n    device_found = False\n    for elec_index in range(len(nwbfile.electrodes)):\n        electrode_group = nwbfile.electrodes[elec_index, \"group\"]\n        eg_device_name = electrode_group.device.name\n\n        # only look at electrodes where the associated device is the one specified\n        if eg_device_name == nwb_device_name:\n            device_found = True\n\n            # if a Shank has not yet been created from the electrode group, then create it\n            if electrode_group.name not in created_shanks:\n                shank_index = len(created_shanks)\n                created_shanks[electrode_group.name] = shank_index\n\n                # build the dictionary of Probe.Shank data\n                shank_dict[shank_index] = dict()\n                shank_dict[shank_index][\"probe_id\"] = new_probe_dict[\n                    \"probe_id\"\n                ]\n                shank_dict[shank_index][\"probe_shank\"] = shank_index\n\n            # get the probe shank index associated with this Electrode\n            probe_shank = created_shanks[electrode_group.name]\n\n            # build the dictionary of Probe.Electrode data\n            elect_dict[elec_index] = dict()\n            elect_dict[elec_index][\"probe_id\"] = new_probe_dict[\"probe_id\"]\n            elect_dict[elec_index][\"probe_shank\"] = probe_shank\n            elect_dict[elec_index][\"probe_electrode\"] = elec_index\n            if \"rel_x\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_x\"] = nwbfile.electrodes[\n                    elec_index, \"rel_x\"\n                ]\n            if \"rel_y\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_y\"] = nwbfile.electrodes[\n                    elec_index, \"rel_y\"\n                ]\n            if \"rel_z\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_z\"] = nwbfile.electrodes[\n                    elec_index, \"rel_z\"\n                ]\n\n    if not device_found:\n        print(\n            f\"No electrodes in the NWB file were associated with a device named '{nwb_device_name}'.\"\n        )\n        return\n\n    # insert the Probe, then the Shank parts, and then the Electrode parts\n    cls.insert1(new_probe_dict, skip_duplicates=True)\n\n    for shank in shank_dict.values():\n        cls.Shank.insert1(shank, skip_duplicates=True)\n    for electrode in elect_dict.values():\n        cls.Electrode.insert1(electrode, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/", "title": "common_dio.py", "text": ""}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_dio.IntervalList", "title": "<code>IntervalList</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@schema\nclass IntervalList(dj.Manual):\n    definition = \"\"\"\n    # Time intervals used for analysis\n    -&gt; Session\n    interval_list_name: varchar(200)  # descriptive name of this interval list\n    ---\n    valid_times: longblob  # numpy array with start and end times for each interval\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n        The interval list name for each epoch is set to the first tag for the epoch.\n        If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n        (0-indexed) of the epoch in the epochs table.\n        The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n        [start time, stop time] for each epoch.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        nwb_file_name : str\n            The file name of the NWB file, used as a primary key to the Session table.\n        \"\"\"\n        if nwbf.epochs is None:\n            print(\"No epochs found in NWB file.\")\n            return\n        epochs = nwbf.epochs.to_dataframe()\n        for epoch_index, epoch_data in epochs.iterrows():\n            epoch_dict = dict()\n            epoch_dict[\"nwb_file_name\"] = nwb_file_name\n            if epoch_data.tags[0]:\n                epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n            else:\n                epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                    epoch_index\n                )\n            epoch_dict[\"valid_times\"] = np.asarray(\n                [[epoch_data.start_time, epoch_data.stop_time]]\n            )\n            cls.insert1(epoch_dict, skip_duplicates=True)\n\n    def plot_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=figsize)\n        interval_count = 0\n        for row in interval_list.itertuples(index=False):\n            for interval in row.valid_times:\n                ax.plot(interval, [interval_count, interval_count])\n                ax.scatter(\n                    interval,\n                    [interval_count, interval_count],\n                    alpha=0.8,\n                    zorder=2,\n                )\n            interval_count += 1\n        ax.set_yticks(np.arange(interval_list.shape[0]))\n        ax.set_yticklabels(interval_list.interval_list_name)\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n\n    def plot_epoch_pos_raw_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=(30, 3))\n\n        raw_data_valid_times = interval_list.loc[\n            interval_list.interval_list_name == \"raw data valid times\"\n        ].valid_times\n        interval_y = 1\n\n        for interval in np.asarray(raw_data_valid_times)[0]:\n            ax.plot(interval, [interval_y, interval_y])\n            ax.scatter(interval, [interval_y, interval_y], alpha=0.8, zorder=2)\n\n        epoch_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n        interval_y = 2\n        for epoch, valid_times in zip(\n            epoch_valid_times.index, epoch_valid_times\n        ):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        pos_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^pos \\d+ valid times$\", axis=0)\n            .valid_times\n        ).sort_index(key=lambda index: [int(name.split()[1]) for name in index])\n        interval_y = 0\n        for epoch, valid_times in zip(pos_valid_times.index, pos_valid_times):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch.strip(\" valid times\"),\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        ax.set_ylim((-0.25, 2.25))\n        ax.set_yticks(np.arange(3))\n        ax.set_yticklabels([\"pos valid times\", \"raw data valid times\", \"epoch\"])\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_interval.IntervalList.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, *, nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add each entry in the NWB file epochs table to the IntervalList table.</p> <p>The interval list name for each epoch is set to the first tag for the epoch. If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index (0-indexed) of the epoch in the epochs table. The start time and stop time of the epoch are stored in the valid_times field as a numpy array of [start time, stop time] for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>nwb_file_name</code> <code>str</code> <p>The file name of the NWB file, used as a primary key to the Session table.</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n    The interval list name for each epoch is set to the first tag for the epoch.\n    If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n    (0-indexed) of the epoch in the epochs table.\n    The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n    [start time, stop time] for each epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    nwb_file_name : str\n        The file name of the NWB file, used as a primary key to the Session table.\n    \"\"\"\n    if nwbf.epochs is None:\n        print(\"No epochs found in NWB file.\")\n        return\n    epochs = nwbf.epochs.to_dataframe()\n    for epoch_index, epoch_data in epochs.iterrows():\n        epoch_dict = dict()\n        epoch_dict[\"nwb_file_name\"] = nwb_file_name\n        if epoch_data.tags[0]:\n            epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n        else:\n            epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                epoch_index\n            )\n        epoch_dict[\"valid_times\"] = np.asarray(\n            [[epoch_data.start_time, epoch_data.stop_time]]\n        )\n        cls.insert1(epoch_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_dio.DIOEvents", "title": "<code>DIOEvents</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> Source code in <code>src/spyglass/common/common_dio.py</code> <pre><code>@schema\nclass DIOEvents(dj.Imported):\n    definition = \"\"\"\n    -&gt; Session\n    dio_event_name: varchar(80)   # the name assigned to this DIO event\n    ---\n    dio_object_id: varchar(40)    # the object id of the data in the NWB file\n    -&gt; IntervalList               # the list of intervals for this object\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        behav_events = get_data_interface(\n            nwbf, \"behavioral_events\", pynwb.behavior.BehavioralEvents\n        )\n        if behav_events is None:\n            print(\n                f\"No conforming behavioral events data interface found in {nwb_file_name}\\n\"\n            )\n            return\n\n        # the times for these events correspond to the valid times for the raw data\n        key[\"interval_list_name\"] = (\n            Raw() &amp; {\"nwb_file_name\": nwb_file_name}\n        ).fetch1(\"interval_list_name\")\n        for event_series in behav_events.time_series.values():\n            key[\"dio_event_name\"] = event_series.name\n            key[\"dio_object_id\"] = event_series.object_id\n            self.insert1(key, skip_duplicates=True)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    def plot_all_dio_events(self):\n\"\"\"Plot all DIO events in the session.\n\n        Examples\n        --------\n        &gt; (DIOEvents &amp; {'nwb_file_name': 'arthur20220314_.nwb'}).plot_all_dio_events()\n        &gt; (DIOEvents &amp; [{'nwb_file_name': \"arthur20220314_.nwb\"}, {\"nwb_file_name\": \"arthur20220316_.nwb\"}]).plot_all_dio_events()\n\n        \"\"\"\n        behavioral_events = self.fetch_nwb()\n        nwb_file_names = np.unique(\n            [event[\"nwb_file_name\"] for event in behavioral_events]\n        )\n        epoch_valid_times = (\n            pd.DataFrame(\n                IntervalList()\n                &amp; [\n                    {\"nwb_file_name\": nwb_file_name}\n                    for nwb_file_name in nwb_file_names\n                ]\n            )\n            .set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n\n        n_events = len(behavioral_events)\n\n        _, axes = plt.subplots(\n            n_events,\n            1,\n            figsize=(15, n_events * 0.3),\n            dpi=100,\n            sharex=True,\n            constrained_layout=True,\n        )\n\n        for ind, (ax, event) in enumerate(zip(axes.flat, behavioral_events)):\n            for epoch_name, epoch in epoch_valid_times.items():\n                start_time, stop_time = epoch.squeeze()\n                ax.axvspan(start_time, stop_time, alpha=0.5)\n                if ind == 0:\n                    ax.text(\n                        start_time + (stop_time - start_time) / 2,\n                        1.001,\n                        epoch_name,\n                        ha=\"center\",\n                        va=\"bottom\",\n                    )\n            ax.step(\n                np.asarray(event[\"dio\"].timestamps),\n                np.asarray(event[\"dio\"].data),\n                where=\"post\",\n                color=\"black\",\n            )\n            ax.set_ylabel(\n                event[\"dio_event_name\"], rotation=0, ha=\"right\", va=\"center\"\n            )\n            ax.set_yticks([])\n        ax.set_xlabel(\"Time\")\n\n        if len(nwb_file_names) == 1:\n            plt.suptitle(f\"DIO events in {nwb_file_names[0]}\")\n        else:\n            plt.suptitle(f\"DIO events in {', '.join(nwb_file_names)}\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_dio.DIOEvents.plot_all_dio_events", "title": "<code>plot_all_dio_events()</code>", "text": "<p>Plot all DIO events in the session.</p> <p>Examples:</p> <p>(DIOEvents &amp; {'nwb_file_name': 'arthur20220314_.nwb'}).plot_all_dio_events() (DIOEvents &amp; [{'nwb_file_name': \"arthur20220314_.nwb\"}, {\"nwb_file_name\": \"arthur20220316_.nwb\"}]).plot_all_dio_events()</p> Source code in <code>src/spyglass/common/common_dio.py</code> <pre><code>def plot_all_dio_events(self):\n\"\"\"Plot all DIO events in the session.\n\n    Examples\n    --------\n    &gt; (DIOEvents &amp; {'nwb_file_name': 'arthur20220314_.nwb'}).plot_all_dio_events()\n    &gt; (DIOEvents &amp; [{'nwb_file_name': \"arthur20220314_.nwb\"}, {\"nwb_file_name\": \"arthur20220316_.nwb\"}]).plot_all_dio_events()\n\n    \"\"\"\n    behavioral_events = self.fetch_nwb()\n    nwb_file_names = np.unique(\n        [event[\"nwb_file_name\"] for event in behavioral_events]\n    )\n    epoch_valid_times = (\n        pd.DataFrame(\n            IntervalList()\n            &amp; [\n                {\"nwb_file_name\": nwb_file_name}\n                for nwb_file_name in nwb_file_names\n            ]\n        )\n        .set_index(\"interval_list_name\")\n        .filter(regex=r\"^[0-9]\", axis=0)\n        .valid_times\n    )\n\n    n_events = len(behavioral_events)\n\n    _, axes = plt.subplots(\n        n_events,\n        1,\n        figsize=(15, n_events * 0.3),\n        dpi=100,\n        sharex=True,\n        constrained_layout=True,\n    )\n\n    for ind, (ax, event) in enumerate(zip(axes.flat, behavioral_events)):\n        for epoch_name, epoch in epoch_valid_times.items():\n            start_time, stop_time = epoch.squeeze()\n            ax.axvspan(start_time, stop_time, alpha=0.5)\n            if ind == 0:\n                ax.text(\n                    start_time + (stop_time - start_time) / 2,\n                    1.001,\n                    epoch_name,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n        ax.step(\n            np.asarray(event[\"dio\"].timestamps),\n            np.asarray(event[\"dio\"].data),\n            where=\"post\",\n            color=\"black\",\n        )\n        ax.set_ylabel(\n            event[\"dio_event_name\"], rotation=0, ha=\"right\", va=\"center\"\n        )\n        ax.set_yticks([])\n    ax.set_xlabel(\"Time\")\n\n    if len(nwb_file_names) == 1:\n        plt.suptitle(f\"DIO events in {nwb_file_names[0]}\")\n    else:\n        plt.suptitle(f\"DIO events in {', '.join(nwb_file_names)}\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_dio.get_nwb_file", "title": "<code>get_nwb_file(nwb_file_path)</code>", "text": "<p>Return an NWBFile object with the given file path in read mode.    If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Path to the NWB file.</p> required <p>Returns:</p> Name Type Description <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>NWB file object for the given path opened in read mode.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_file(nwb_file_path):\n\"\"\"Return an NWBFile object with the given file path in read mode.\n       If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.\n\n    Parameters\n    ----------\n    nwb_file_path : str\n        Path to the NWB file.\n\n    Returns\n    -------\n    nwbfile : pynwb.NWBFile\n        NWB file object for the given path opened in read mode.\n    \"\"\"\n    _, nwbfile = __open_nwb_files.get(nwb_file_path, (None, None))\n    nwb_uri = None\n    nwb_raw_uri = None\n    if nwbfile is None:\n        # check to see if the file exists\n        if not os.path.exists(nwb_file_path):\n            print(\n                f\"NWB file {nwb_file_path} does not exist locally; checking kachery\"\n            )\n            # first try the analysis files\n            from ..sharing.sharing_kachery import AnalysisNwbfileKachery\n\n            # the download functions assume just the filename, so we need to get that from the path\n            if not AnalysisNwbfileKachery.download_file(\n                os.path.basename(nwb_file_path)\n            ):\n                return None\n        # now open the file\n        io = pynwb.NWBHDF5IO(\n            path=nwb_file_path, mode=\"r\", load_namespaces=True\n        )  # keep file open\n        nwbfile = io.read()\n        __open_nwb_files[nwb_file_path] = (io, nwbfile)\n\n    return nwbfile\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_dio.Nwbfile", "title": "<code>Nwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass Nwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files.\n    nwb_file_name: varchar(255)   # name of the NWB file\n    ---\n    nwb_file_abs_path: filepath@raw\n    INDEX (nwb_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    @classmethod\n    def insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The relative path to the NWB file.\n        \"\"\"\n        nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n        assert os.path.exists(\n            nwb_file_abs_path\n        ), f\"File does not exist: {nwb_file_abs_path}\"\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n        cls.insert1(key, skip_duplicates=True)\n\n    @staticmethod\n    def get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n        Returns\n        -------\n        nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n        nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n        return str(nwb_file_abspath)\n\n    @staticmethod\n    def add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n        The NWB_LOCK_FILE environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n        \"\"\"\n        key = {\"nwb_file_name\": nwb_file_name}\n        # check to make sure the file exists\n        assert (\n            len((Nwbfile() &amp; key).fetch()) &gt; 0\n        ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n        lock_file.write(f\"{nwb_file_name}\\n\")\n        lock_file.close()\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        This does not delete the files themselves unless delete_files=True is specified\n        Run this after deleting the Nwbfile() entries themselves.\n        \"\"\"\n        schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_nwbfile.Nwbfile.insert_from_relative_file_name", "title": "<code>insert_from_relative_file_name(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Insert a new session from an existing NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The relative path to the NWB file.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The relative path to the NWB file.\n    \"\"\"\n    nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n    assert os.path.exists(\n        nwb_file_abs_path\n    ), f\"File does not exist: {nwb_file_abs_path}\"\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n    cls.insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_nwbfile.Nwbfile.get_abs_path", "title": "<code>get_abs_path(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored raw NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required <p>Returns:</p> Name Type Description <code>nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n    Returns\n    -------\n    nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n    nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n    return str(nwb_file_abspath)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_nwbfile.Nwbfile.add_to_lock", "title": "<code>add_to_lock(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Add the specified NWB file to the file with the list of NWB files to be locked.</p> <p>The NWB_LOCK_FILE environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n    The NWB_LOCK_FILE environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n    \"\"\"\n    key = {\"nwb_file_name\": nwb_file_name}\n    # check to make sure the file exists\n    assert (\n        len((Nwbfile() &amp; key).fetch()) &gt; 0\n    ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n    lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n    lock_file.write(f\"{nwb_file_name}\\n\")\n    lock_file.close()\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_nwbfile.Nwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>This does not delete the files themselves unless delete_files=True is specified Run this after deleting the Nwbfile() entries themselves.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    This does not delete the files themselves unless delete_files=True is specified\n    Run this after deleting the Nwbfile() entries themselves.\n    \"\"\"\n    schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_dio.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/common/common_dio/#src.spyglass.common.common_dio.get_data_interface", "title": "<code>get_data_interface(nwbfile, data_interface_name, data_interface_class=None)</code>", "text": "<p>Search for a specified NWBDataInterface or DynamicTable in the processing modules of an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>The NWB file object to search in.</p> required <code>data_interface_name</code> <code>str</code> <p>The name of the NWBDataInterface or DynamicTable to search for.</p> required <code>data_interface_class</code> <code>type</code> <p>The class (or superclass) to search for. This argument helps to prevent accessing an object with the same name but the incorrect type. Default: no restriction.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If multiple NWBDataInterface and DynamicTable objects with the matching name are found.</p> <p>Returns:</p> Name Type Description <code>data_interface</code> <code>NWBDataInterface</code> <p>The data interface object with the given name, or None if not found.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_data_interface(nwbfile, data_interface_name, data_interface_class=None):\n\"\"\"Search for a specified NWBDataInterface or DynamicTable in the processing modules of an NWB file.\n\n    Parameters\n    ----------\n    nwbfile : pynwb.NWBFile\n        The NWB file object to search in.\n    data_interface_name : str\n        The name of the NWBDataInterface or DynamicTable to search for.\n    data_interface_class : type, optional\n        The class (or superclass) to search for. This argument helps to prevent accessing an object with the same\n        name but the incorrect type. Default: no restriction.\n\n    Warns\n    -----\n    UserWarning\n        If multiple NWBDataInterface and DynamicTable objects with the matching name are found.\n\n    Returns\n    -------\n    data_interface : NWBDataInterface\n        The data interface object with the given name, or None if not found.\n    \"\"\"\n    ret = []\n    for module in nwbfile.processing.values():\n        match = module.data_interfaces.get(data_interface_name, None)\n        if match is not None:\n            if data_interface_class is not None and not isinstance(\n                match, data_interface_class\n            ):\n                continue\n            ret.append(match)\n    if len(ret) &gt; 1:\n        warnings.warn(\n            f\"Multiple data interfaces with name '{data_interface_name}' \"\n            f\"found in NWBFile with identifier {nwbfile.identifier}. Using the first one found. \"\n            \"Use the data_interface_class argument to restrict the search.\"\n        )\n    if len(ret) &gt;= 1:\n        return ret[0]\n    else:\n        return None\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/", "title": "common_ephys.py", "text": ""}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.BrainRegion", "title": "<code>BrainRegion</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> Source code in <code>src/spyglass/common/common_region.py</code> <pre><code>@schema\nclass BrainRegion(dj.Lookup):\n    definition = \"\"\"\n    region_id: smallint auto_increment\n    ---\n    region_name: varchar(200)             # the name of the brain region\n    subregion_name=NULL: varchar(200)     # subregion name\n    subsubregion_name=NULL: varchar(200)  # subregion within subregion\n    \"\"\"\n\n    # TODO consider making (region_name, subregion_name, subsubregion_name) a primary key\n    # subregion_name='' and subsubregion_name='' will be necessary but that seems OK\n\n    @classmethod\n    def fetch_add(\n        cls, region_name, subregion_name=None, subsubregion_name=None\n    ):\n\"\"\"Return the region ID for the given names, and if no match exists, first add it to the BrainRegion table.\n\n        The combination of (region_name, subregion_name, subsubregion_name) is effectively unique, then.\n\n        Parameters\n        ----------\n        region_name : str\n            The name of the brain region.\n        subregion_name : str, optional\n            The name of the subregion within the brain region.\n        subsubregion_name : str, optional\n            The name of the subregion within the subregion.\n\n        Returns\n        -------\n        region_id : int\n            The index of the region in the BrainRegion table.\n        \"\"\"\n        key = dict()\n        key[\"region_name\"] = region_name\n        key[\"subregion_name\"] = subregion_name\n        key[\"subsubregion_name\"] = subsubregion_name\n        query = BrainRegion &amp; key\n        if not query:\n            cls.insert1(key)\n            query = BrainRegion &amp; key\n        return query.fetch1(\"region_id\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_region.BrainRegion.fetch_add", "title": "<code>fetch_add(region_name, subregion_name=None, subsubregion_name=None)</code>  <code>classmethod</code>", "text": "<p>Return the region ID for the given names, and if no match exists, first add it to the BrainRegion table.</p> <p>The combination of (region_name, subregion_name, subsubregion_name) is effectively unique, then.</p> <p>Parameters:</p> Name Type Description Default <code>region_name</code> <code>str</code> <p>The name of the brain region.</p> required <code>subregion_name</code> <code>str</code> <p>The name of the subregion within the brain region.</p> <code>None</code> <code>subsubregion_name</code> <code>str</code> <p>The name of the subregion within the subregion.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>region_id</code> <code>int</code> <p>The index of the region in the BrainRegion table.</p> Source code in <code>src/spyglass/common/common_region.py</code> <pre><code>@classmethod\ndef fetch_add(\n    cls, region_name, subregion_name=None, subsubregion_name=None\n):\n\"\"\"Return the region ID for the given names, and if no match exists, first add it to the BrainRegion table.\n\n    The combination of (region_name, subregion_name, subsubregion_name) is effectively unique, then.\n\n    Parameters\n    ----------\n    region_name : str\n        The name of the brain region.\n    subregion_name : str, optional\n        The name of the subregion within the brain region.\n    subsubregion_name : str, optional\n        The name of the subregion within the subregion.\n\n    Returns\n    -------\n    region_id : int\n        The index of the region in the BrainRegion table.\n    \"\"\"\n    key = dict()\n    key[\"region_name\"] = region_name\n    key[\"subregion_name\"] = subregion_name\n    key[\"subsubregion_name\"] = subsubregion_name\n    query = BrainRegion &amp; key\n    if not query:\n        cls.insert1(key)\n        query = BrainRegion &amp; key\n    return query.fetch1(\"region_id\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.IntervalList", "title": "<code>IntervalList</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@schema\nclass IntervalList(dj.Manual):\n    definition = \"\"\"\n    # Time intervals used for analysis\n    -&gt; Session\n    interval_list_name: varchar(200)  # descriptive name of this interval list\n    ---\n    valid_times: longblob  # numpy array with start and end times for each interval\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n        The interval list name for each epoch is set to the first tag for the epoch.\n        If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n        (0-indexed) of the epoch in the epochs table.\n        The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n        [start time, stop time] for each epoch.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        nwb_file_name : str\n            The file name of the NWB file, used as a primary key to the Session table.\n        \"\"\"\n        if nwbf.epochs is None:\n            print(\"No epochs found in NWB file.\")\n            return\n        epochs = nwbf.epochs.to_dataframe()\n        for epoch_index, epoch_data in epochs.iterrows():\n            epoch_dict = dict()\n            epoch_dict[\"nwb_file_name\"] = nwb_file_name\n            if epoch_data.tags[0]:\n                epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n            else:\n                epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                    epoch_index\n                )\n            epoch_dict[\"valid_times\"] = np.asarray(\n                [[epoch_data.start_time, epoch_data.stop_time]]\n            )\n            cls.insert1(epoch_dict, skip_duplicates=True)\n\n    def plot_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=figsize)\n        interval_count = 0\n        for row in interval_list.itertuples(index=False):\n            for interval in row.valid_times:\n                ax.plot(interval, [interval_count, interval_count])\n                ax.scatter(\n                    interval,\n                    [interval_count, interval_count],\n                    alpha=0.8,\n                    zorder=2,\n                )\n            interval_count += 1\n        ax.set_yticks(np.arange(interval_list.shape[0]))\n        ax.set_yticklabels(interval_list.interval_list_name)\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n\n    def plot_epoch_pos_raw_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=(30, 3))\n\n        raw_data_valid_times = interval_list.loc[\n            interval_list.interval_list_name == \"raw data valid times\"\n        ].valid_times\n        interval_y = 1\n\n        for interval in np.asarray(raw_data_valid_times)[0]:\n            ax.plot(interval, [interval_y, interval_y])\n            ax.scatter(interval, [interval_y, interval_y], alpha=0.8, zorder=2)\n\n        epoch_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n        interval_y = 2\n        for epoch, valid_times in zip(\n            epoch_valid_times.index, epoch_valid_times\n        ):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        pos_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^pos \\d+ valid times$\", axis=0)\n            .valid_times\n        ).sort_index(key=lambda index: [int(name.split()[1]) for name in index])\n        interval_y = 0\n        for epoch, valid_times in zip(pos_valid_times.index, pos_valid_times):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch.strip(\" valid times\"),\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        ax.set_ylim((-0.25, 2.25))\n        ax.set_yticks(np.arange(3))\n        ax.set_yticklabels([\"pos valid times\", \"raw data valid times\", \"epoch\"])\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_interval.IntervalList.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, *, nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add each entry in the NWB file epochs table to the IntervalList table.</p> <p>The interval list name for each epoch is set to the first tag for the epoch. If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index (0-indexed) of the epoch in the epochs table. The start time and stop time of the epoch are stored in the valid_times field as a numpy array of [start time, stop time] for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>nwb_file_name</code> <code>str</code> <p>The file name of the NWB file, used as a primary key to the Session table.</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n    The interval list name for each epoch is set to the first tag for the epoch.\n    If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n    (0-indexed) of the epoch in the epochs table.\n    The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n    [start time, stop time] for each epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    nwb_file_name : str\n        The file name of the NWB file, used as a primary key to the Session table.\n    \"\"\"\n    if nwbf.epochs is None:\n        print(\"No epochs found in NWB file.\")\n        return\n    epochs = nwbf.epochs.to_dataframe()\n    for epoch_index, epoch_data in epochs.iterrows():\n        epoch_dict = dict()\n        epoch_dict[\"nwb_file_name\"] = nwb_file_name\n        if epoch_data.tags[0]:\n            epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n        else:\n            epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                epoch_index\n            )\n        epoch_dict[\"valid_times\"] = np.asarray(\n            [[epoch_data.start_time, epoch_data.stop_time]]\n        )\n        cls.insert1(epoch_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.get_nwb_file", "title": "<code>get_nwb_file(nwb_file_path)</code>", "text": "<p>Return an NWBFile object with the given file path in read mode.    If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Path to the NWB file.</p> required <p>Returns:</p> Name Type Description <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>NWB file object for the given path opened in read mode.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_file(nwb_file_path):\n\"\"\"Return an NWBFile object with the given file path in read mode.\n       If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.\n\n    Parameters\n    ----------\n    nwb_file_path : str\n        Path to the NWB file.\n\n    Returns\n    -------\n    nwbfile : pynwb.NWBFile\n        NWB file object for the given path opened in read mode.\n    \"\"\"\n    _, nwbfile = __open_nwb_files.get(nwb_file_path, (None, None))\n    nwb_uri = None\n    nwb_raw_uri = None\n    if nwbfile is None:\n        # check to see if the file exists\n        if not os.path.exists(nwb_file_path):\n            print(\n                f\"NWB file {nwb_file_path} does not exist locally; checking kachery\"\n            )\n            # first try the analysis files\n            from ..sharing.sharing_kachery import AnalysisNwbfileKachery\n\n            # the download functions assume just the filename, so we need to get that from the path\n            if not AnalysisNwbfileKachery.download_file(\n                os.path.basename(nwb_file_path)\n            ):\n                return None\n        # now open the file\n        io = pynwb.NWBHDF5IO(\n            path=nwb_file_path, mode=\"r\", load_namespaces=True\n        )  # keep file open\n        nwbfile = io.read()\n        __open_nwb_files[nwb_file_path] = (io, nwbfile)\n\n    return nwbfile\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.FirFilterParameters", "title": "<code>FirFilterParameters</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>@schema\nclass FirFilterParameters(dj.Manual):\n    definition = \"\"\"\n    filter_name: varchar(80)           # descriptive name of this filter\n    filter_sampling_rate: int          # sampling rate for this filter\n    ---\n    filter_type: enum(\"lowpass\", \"highpass\", \"bandpass\")\n    filter_low_stop = 0: float         # lowest frequency for stop band for low frequency side of filter\n    filter_low_pass = 0: float         # lowest frequency for pass band of low frequency side of filter\n    filter_high_pass = 0: float        # highest frequency for pass band for high frequency side of filter\n    filter_high_stop = 0: float        # highest frequency for stop band of high frequency side of filter\n    filter_comments: varchar(2000)     # comments about the filter\n    filter_band_edges: blob            # numpy array containing the filter bands (redundant with individual parameters)\n    filter_coeff: longblob             # numpy array containing the filter coefficients\n    \"\"\"\n\n    def add_filter(self, filter_name, fs, filter_type, band_edges, comments=\"\"):\n        gsp = _import_ghostipy()\n\n        # add an FIR bandpass filter of the specified type ('lowpass', 'highpass', or 'bandpass').\n        # band_edges should be as follows:\n        #   low pass filter: [high_pass high_stop]\n        #   high pass filter: [low stop low pass]\n        #   band pass filter: [low_stop low_pass high_pass high_stop].\n        if filter_type not in [\"lowpass\", \"highpass\", \"bandpass\"]:\n            print(\n                \"Error in Filter.add_filter: filter type {} is not \"\n                \"lowpass\"\n                \", \"\n                \"highpass\"\n                \" or \"\n\"\"\"bandpass\"\"\".format(filter_type)\n            )\n            return None\n\n        p = 2  # transition spline will be quadratic\n        if filter_type == \"lowpass\" or filter_type == \"highpass\":\n            # check that two frequencies were passed in and that they are in the right order\n            if len(band_edges) != 2:\n                print(\n                    \"Error in Filter.add_filter: lowpass and highpass filter requires two band_frequencies\"\n                )\n                return None\n            tw = band_edges[1] - band_edges[0]\n\n        elif filter_type == \"bandpass\":\n            if len(band_edges) != 4:\n                print(\n                    \"Error in Filter.add_filter: bandpass filter requires four band_frequencies.\"\n                )\n                return None\n            # the transition width is the mean of the widths of left and right transition regions\n            tw = (\n                (band_edges[1] - band_edges[0])\n                + (band_edges[3] - band_edges[2])\n            ) / 2.0\n\n        else:\n            raise Exception(f\"Unexpected filter type: {filter_type}\")\n\n        numtaps = gsp.estimate_taps(fs, tw)\n        filterdict = dict()\n        filterdict[\"filter_name\"] = filter_name\n        filterdict[\"filter_sampling_rate\"] = fs\n        filterdict[\"filter_comments\"] = comments\n\n        # set the desired frequency response\n        if filter_type == \"lowpass\":\n            desired = [1, 0]\n            filterdict[\"filter_low_stop\"] = 0\n            filterdict[\"filter_low_pass\"] = 0\n            filterdict[\"filter_high_pass\"] = band_edges[0]\n            filterdict[\"filter_high_stop\"] = band_edges[1]\n        elif filter_type == \"highpass\":\n            desired = [0, 1]\n            filterdict[\"filter_low_stop\"] = band_edges[0]\n            filterdict[\"filter_low_pass\"] = band_edges[1]\n            filterdict[\"filter_high_pass\"] = 0\n            filterdict[\"filter_high_stop\"] = 0\n        else:\n            desired = [0, 1, 1, 0]\n            filterdict[\"filter_low_stop\"] = band_edges[0]\n            filterdict[\"filter_low_pass\"] = band_edges[1]\n            filterdict[\"filter_high_pass\"] = band_edges[2]\n            filterdict[\"filter_high_stop\"] = band_edges[3]\n        filterdict[\"filter_type\"] = filter_type\n        filterdict[\"filter_band_edges\"] = np.asarray(band_edges)\n        # create 1d array for coefficients\n        filterdict[\"filter_coeff\"] = np.array(\n            gsp.firdesign(numtaps, band_edges, desired, fs=fs, p=p), ndmin=1\n        )\n        # add this filter to the table\n        self.insert1(filterdict, skip_duplicates=True)\n\n    def plot_magnitude(self, filter_name, fs):\n        filter = (\n            self &amp; {\"filter_name\": filter_name} &amp; {\"filter_sampling_rate\": fs}\n        ).fetch(as_dict=True)\n        f = filter[0]\n        plt.figure()\n        w, h = signal.freqz(filter[0][\"filter_coeff\"], worN=65536)\n        magnitude = 20 * np.log10(np.abs(h))\n        plt.plot(w / np.pi * fs / 2, magnitude)\n        plt.xlabel(\"Frequency (Hz)\")\n        plt.ylabel(\"Magnitude\")\n        plt.title(\"Frequency Response\")\n        plt.xlim(0, np.max(f[\"filter_coeffand_edges\"] * 2))\n        plt.ylim(np.min(magnitude), -1 * np.min(magnitude) * 0.1)\n        plt.grid(True)\n\n    def plot_fir_filter(self, filter_name, fs):\n        filter = (\n            self &amp; {\"filter_name\": filter_name} &amp; {\"filter_sampling_rate\": fs}\n        ).fetch(as_dict=True)\n        f = filter[0]\n        plt.figure()\n        plt.clf()\n        plt.plot(f[\"filter_coeff\"], \"k\")\n        plt.xlabel(\"Coefficient\")\n        plt.ylabel(\"Magnitude\")\n        plt.title(\"Filter Taps\")\n        plt.grid(True)\n\n    def filter_delay(self, filter_name, fs):\n        # return the filter delay\n        filter = (\n            self &amp; {\"filter_name\": filter_name} &amp; {\"filter_sampling_rate\": fs}\n        ).fetch(as_dict=True)\n        return self.calc_filter_delay(filter[\"filter_coeff\"])\n\n    def filter_data_nwb(\n        self,\n        analysis_file_abs_path,\n        eseries,\n        filter_coeff,\n        valid_times,\n        electrode_ids,\n        decimation,\n        description: str = \"filtered data\",\n        type: Union[None, str] = None,\n    ):\n\"\"\"\n        :param analysis_nwb_file_name: str   full path to previously created analysis nwb file where filtered data\n        should be stored. This also has the name of the original NWB file where the data will be taken from\n        :param eseries: electrical series with data to be filtered\n        :param filter_coeff: numpy array with filter coefficients for FIR filter\n        :param valid_times: 2D numpy array with start and stop times of intervals to be filtered\n        :param electrode_ids: list of electrode_ids to filter\n        :param decimation: int decimation factor\n        :return: The NWB object id of the filtered data (str), list containing first and last timestamp\n\n        This function takes data and timestamps from an NWB electrical series and filters them using the ghostipy\n        package, saving the result as a new electricalseries in the nwb_file_name, which should have previously been\n        created and linked to the original NWB file using common_session.AnalysisNwbfile.create()\n        \"\"\"\n        gsp = _import_ghostipy()\n\n        data_on_disk = eseries.data\n        timestamps_on_disk = eseries.timestamps\n        n_dim = len(data_on_disk.shape)\n        n_samples = len(timestamps_on_disk)\n        # find the\n        time_axis = 0 if data_on_disk.shape[0] == n_samples else 1\n        electrode_axis = 1 - time_axis\n        n_electrodes = data_on_disk.shape[electrode_axis]\n        input_dim_restrictions = [None] * n_dim\n\n        # to get the input dimension restrictions we need to look at the electrode table for the eseries and get\n        # the indices from that\n        input_dim_restrictions[electrode_axis] = np.s_[\n            get_electrode_indices(eseries, electrode_ids)\n        ]\n\n        indices = []\n        output_shape_list = [0] * n_dim\n        output_shape_list[electrode_axis] = len(electrode_ids)\n        output_offsets = [0]\n\n        timestamp_size = timestamps_on_disk[0].itemsize\n        timestamp_dtype = timestamps_on_disk[0].dtype\n        data_size = data_on_disk[0][0].itemsize\n        data_dtype = data_on_disk[0][0].dtype\n\n        filter_delay = self.calc_filter_delay(filter_coeff)\n        for a_start, a_stop in valid_times:\n            if a_start &lt; timestamps_on_disk[0]:\n                warnings.warn(\n                    f\"Interval start time {a_start} is smaller than first timestamp {timestamps_on_disk[0]}, \"\n                    \"using first timestamp instead\"\n                )\n                a_start = timestamps_on_disk[0]\n            if a_stop &gt; timestamps_on_disk[-1]:\n                warnings.warn(\n                    f\"Interval stop time {a_stop} is larger than last timestamp {timestamps_on_disk[-1]}, \"\n                    \"using last timestamp instead\"\n                )\n                a_stop = timestamps_on_disk[-1]\n            frm, to = np.searchsorted(timestamps_on_disk, (a_start, a_stop))\n            if to &gt; n_samples:\n                to = n_samples\n            indices.append((frm, to))\n            shape, dtype = gsp.filter_data_fir(\n                data_on_disk,\n                filter_coeff,\n                axis=time_axis,\n                input_index_bounds=[frm, to - 1],\n                output_index_bounds=[filter_delay, filter_delay + to - frm],\n                describe_dims=True,\n                ds=decimation,\n                input_dim_restrictions=input_dim_restrictions,\n            )\n            output_offsets.append(output_offsets[-1] + shape[time_axis])\n            output_shape_list[time_axis] += shape[time_axis]\n\n        # open the nwb file to create the dynamic table region and electrode series, then write and close the file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"a\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the indices of the electrodes in the electrode table\n            elect_ind = get_electrode_indices(nwbf, electrode_ids)\n\n            electrode_table_region = nwbf.create_electrode_table_region(\n                elect_ind, \"filtered electrode table\"\n            )\n            eseries_name = \"filtered data\"\n            es = pynwb.ecephys.ElectricalSeries(\n                name=eseries_name,\n                data=np.empty(tuple(output_shape_list), dtype=data_dtype),\n                electrodes=electrode_table_region,\n                timestamps=np.empty(output_shape_list[time_axis]),\n                description=description,\n            )\n            if type == \"LFP\":\n                lfp = pynwb.ecephys.LFP(electrical_series=es)\n                ecephys_module = nwbf.create_processing_module(\n                    name=\"ecephys\", description=description\n                )\n                ecephys_module.add(lfp)\n            else:\n                nwbf.add_scratch(es)\n            io.write(nwbf)\n\n            # reload the NWB file to get the h5py objects for the data and the timestamps\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"a\", load_namespaces=True\n            ) as io:\n                nwbf = io.read()\n                es = nwbf.objects[es.object_id]\n                filtered_data = es.data\n                new_timestamps = es.timestamps\n                indices = np.array(indices, ndmin=2)\n                # Filter and write the output dataset\n                ts_offset = 0\n\n                print(\"Filtering data\")\n                for ii, (start, stop) in enumerate(indices):\n                    # calculate the size of the timestamps and the data and determine whether they\n                    # can be loaded into &lt; 90% of available RAM\n                    mem = psutil.virtual_memory()\n                    interval_samples = stop - start\n                    if (\n                        interval_samples\n                        * (timestamp_size + n_electrodes * data_size)\n                        &lt; 0.9 * mem.available\n                    ):\n                        print(f\"Interval {ii}: loading data into memory\")\n                        timestamps = np.asarray(\n                            timestamps_on_disk[start:stop],\n                            dtype=timestamp_dtype,\n                        )\n                        if time_axis == 0:\n                            data = np.asarray(\n                                data_on_disk[start:stop, :], dtype=data_dtype\n                            )\n                        else:\n                            data = np.asarray(\n                                data_on_disk[:, start:stop], dtype=data_dtype\n                            )\n                        extracted_ts = timestamps[0::decimation]\n                        new_timestamps[\n                            ts_offset : ts_offset + len(extracted_ts)\n                        ] = extracted_ts\n                        ts_offset += len(extracted_ts)\n                        # filter the data\n                        gsp.filter_data_fir(\n                            data,\n                            filter_coeff,\n                            axis=time_axis,\n                            input_index_bounds=[0, interval_samples - 1],\n                            output_index_bounds=[\n                                filter_delay,\n                                filter_delay + stop - start,\n                            ],\n                            ds=decimation,\n                            input_dim_restrictions=input_dim_restrictions,\n                            outarray=filtered_data,\n                            output_offset=output_offsets[ii],\n                        )\n                    else:\n                        print(f\"Interval {ii}: leaving data on disk\")\n                        data = data_on_disk\n                        timestamps = timestamps_on_disk\n                        extracted_ts = timestamps[start:stop:decimation]\n                        new_timestamps[\n                            ts_offset : ts_offset + len(extracted_ts)\n                        ] = extracted_ts\n                        ts_offset += len(extracted_ts)\n                        # filter the data\n                        gsp.filter_data_fir(\n                            data,\n                            filter_coeff,\n                            axis=time_axis,\n                            input_index_bounds=[start, stop],\n                            output_index_bounds=[\n                                filter_delay,\n                                filter_delay + stop - start,\n                            ],\n                            ds=decimation,\n                            input_dim_restrictions=input_dim_restrictions,\n                            outarray=filtered_data,\n                            output_offset=output_offsets[ii],\n                        )\n\n                start_end = [new_timestamps[0], new_timestamps[-1]]\n\n                io.write(nwbf)\n\n        return es.object_id, start_end\n\n    def filter_data(\n        self,\n        timestamps,\n        data,\n        filter_coeff,\n        valid_times,\n        electrodes,\n        decimation,\n    ):\n\"\"\"\n        :param timestamps: numpy array with list of timestamps for data\n        :param data: original data array\n        :param filter_coeff: numpy array with filter coefficients for FIR filter\n        :param valid_times: 2D numpy array with start and stop times of intervals to be filtered\n        :param electrodes: list of electrodes to filter\n        :param decimation: decimation factor\n        :return: filtered_data, timestamps\n        \"\"\"\n        gsp = _import_ghostipy()\n\n        n_dim = len(data.shape)\n        n_samples = len(timestamps)\n        time_axis = 0 if data.shape[0] == n_samples else 1\n        electrode_axis = 1 - time_axis\n        input_dim_restrictions = [None] * n_dim\n        input_dim_restrictions[electrode_axis] = np.s_[electrodes]\n\n        indices = []\n        output_shape_list = [0] * n_dim\n        output_shape_list[electrode_axis] = len(electrodes)\n        output_offsets = [0]\n\n        filter_delay = self.calc_filter_delay(filter_coeff)\n        for a_start, a_stop in valid_times:\n            if a_start &lt; timestamps[0]:\n                print(\n                    f\"Interval start time {a_start} is smaller than first timestamp \"\n                    f\"{timestamps[0]}, using first timestamp instead\"\n                )\n                a_start = timestamps[0]\n            if a_stop &gt; timestamps[-1]:\n                print(\n                    f\"Interval stop time {a_stop} is larger than last timestamp \"\n                    f\"{timestamps[-1]}, using last timestamp instead\"\n                )\n                a_stop = timestamps[-1]\n            frm, to = np.searchsorted(timestamps, (a_start, a_stop))\n            if to &gt; n_samples:\n                to = n_samples\n\n            indices.append((frm, to))\n            shape, dtype = gsp.filter_data_fir(\n                data,\n                filter_coeff,\n                axis=time_axis,\n                input_index_bounds=[frm, to],\n                output_index_bounds=[filter_delay, filter_delay + to - frm],\n                describe_dims=True,\n                ds=decimation,\n                input_dim_restrictions=input_dim_restrictions,\n            )\n            output_offsets.append(output_offsets[-1] + shape[time_axis])\n            output_shape_list[time_axis] += shape[time_axis]\n\n        # create the dataset and the timestamps array\n        filtered_data = np.empty(tuple(output_shape_list), dtype=data.dtype)\n\n        new_timestamps = np.empty(\n            (output_shape_list[time_axis],), timestamps.dtype\n        )\n\n        indices = np.array(indices, ndmin=2)\n\n        # Filter  the output dataset\n        ts_offset = 0\n\n        for ii, (start, stop) in enumerate(indices):\n            extracted_ts = timestamps[start:stop:decimation]\n\n            # print(f\"Diffs {np.diff(extracted_ts)}\")\n            new_timestamps[\n                ts_offset : ts_offset + len(extracted_ts)\n            ] = extracted_ts\n            ts_offset += len(extracted_ts)\n\n            # finally ready to filter data!\n            gsp.filter_data_fir(\n                data,\n                filter_coeff,\n                axis=time_axis,\n                input_index_bounds=[start, stop],\n                output_index_bounds=[filter_delay, filter_delay + stop - start],\n                ds=decimation,\n                input_dim_restrictions=input_dim_restrictions,\n                outarray=filtered_data,\n                output_offset=output_offsets[ii],\n            )\n\n        return filtered_data, new_timestamps\n\n    def calc_filter_delay(self, filter_coeff):\n\"\"\"\n        :param filter_coeff:\n        :return: filter delay\n        \"\"\"\n        return (len(filter_coeff) - 1) // 2\n\n    def create_standard_filters(self):\n\"\"\"Add standard filters to the Filter table including\n        0-400 Hz low pass for continuous raw data -&gt; LFP\n        \"\"\"\n        self.add_filter(\n            \"LFP 0-400 Hz\",\n            20000,\n            \"lowpass\",\n            [400, 425],\n            \"standard LFP filter for 20 KHz data\",\n        )\n        self.add_filter(\n            \"LFP 0-400 Hz\",\n            30000,\n            \"lowpass\",\n            [400, 425],\n            \"standard LFP filter for 30 KHz data\",\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_filter.FirFilterParameters.filter_data_nwb", "title": "<code>filter_data_nwb(analysis_file_abs_path, eseries, filter_coeff, valid_times, electrode_ids, decimation, description='filtered data', type=None)</code>", "text": "<p>:param analysis_nwb_file_name: str   full path to previously created analysis nwb file where filtered data should be stored. This also has the name of the original NWB file where the data will be taken from :param eseries: electrical series with data to be filtered :param filter_coeff: numpy array with filter coefficients for FIR filter :param valid_times: 2D numpy array with start and stop times of intervals to be filtered :param electrode_ids: list of electrode_ids to filter :param decimation: int decimation factor :return: The NWB object id of the filtered data (str), list containing first and last timestamp</p> <p>This function takes data and timestamps from an NWB electrical series and filters them using the ghostipy package, saving the result as a new electricalseries in the nwb_file_name, which should have previously been created and linked to the original NWB file using common_session.AnalysisNwbfile.create()</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def filter_data_nwb(\n    self,\n    analysis_file_abs_path,\n    eseries,\n    filter_coeff,\n    valid_times,\n    electrode_ids,\n    decimation,\n    description: str = \"filtered data\",\n    type: Union[None, str] = None,\n):\n\"\"\"\n    :param analysis_nwb_file_name: str   full path to previously created analysis nwb file where filtered data\n    should be stored. This also has the name of the original NWB file where the data will be taken from\n    :param eseries: electrical series with data to be filtered\n    :param filter_coeff: numpy array with filter coefficients for FIR filter\n    :param valid_times: 2D numpy array with start and stop times of intervals to be filtered\n    :param electrode_ids: list of electrode_ids to filter\n    :param decimation: int decimation factor\n    :return: The NWB object id of the filtered data (str), list containing first and last timestamp\n\n    This function takes data and timestamps from an NWB electrical series and filters them using the ghostipy\n    package, saving the result as a new electricalseries in the nwb_file_name, which should have previously been\n    created and linked to the original NWB file using common_session.AnalysisNwbfile.create()\n    \"\"\"\n    gsp = _import_ghostipy()\n\n    data_on_disk = eseries.data\n    timestamps_on_disk = eseries.timestamps\n    n_dim = len(data_on_disk.shape)\n    n_samples = len(timestamps_on_disk)\n    # find the\n    time_axis = 0 if data_on_disk.shape[0] == n_samples else 1\n    electrode_axis = 1 - time_axis\n    n_electrodes = data_on_disk.shape[electrode_axis]\n    input_dim_restrictions = [None] * n_dim\n\n    # to get the input dimension restrictions we need to look at the electrode table for the eseries and get\n    # the indices from that\n    input_dim_restrictions[electrode_axis] = np.s_[\n        get_electrode_indices(eseries, electrode_ids)\n    ]\n\n    indices = []\n    output_shape_list = [0] * n_dim\n    output_shape_list[electrode_axis] = len(electrode_ids)\n    output_offsets = [0]\n\n    timestamp_size = timestamps_on_disk[0].itemsize\n    timestamp_dtype = timestamps_on_disk[0].dtype\n    data_size = data_on_disk[0][0].itemsize\n    data_dtype = data_on_disk[0][0].dtype\n\n    filter_delay = self.calc_filter_delay(filter_coeff)\n    for a_start, a_stop in valid_times:\n        if a_start &lt; timestamps_on_disk[0]:\n            warnings.warn(\n                f\"Interval start time {a_start} is smaller than first timestamp {timestamps_on_disk[0]}, \"\n                \"using first timestamp instead\"\n            )\n            a_start = timestamps_on_disk[0]\n        if a_stop &gt; timestamps_on_disk[-1]:\n            warnings.warn(\n                f\"Interval stop time {a_stop} is larger than last timestamp {timestamps_on_disk[-1]}, \"\n                \"using last timestamp instead\"\n            )\n            a_stop = timestamps_on_disk[-1]\n        frm, to = np.searchsorted(timestamps_on_disk, (a_start, a_stop))\n        if to &gt; n_samples:\n            to = n_samples\n        indices.append((frm, to))\n        shape, dtype = gsp.filter_data_fir(\n            data_on_disk,\n            filter_coeff,\n            axis=time_axis,\n            input_index_bounds=[frm, to - 1],\n            output_index_bounds=[filter_delay, filter_delay + to - frm],\n            describe_dims=True,\n            ds=decimation,\n            input_dim_restrictions=input_dim_restrictions,\n        )\n        output_offsets.append(output_offsets[-1] + shape[time_axis])\n        output_shape_list[time_axis] += shape[time_axis]\n\n    # open the nwb file to create the dynamic table region and electrode series, then write and close the file\n    with pynwb.NWBHDF5IO(\n        path=analysis_file_abs_path, mode=\"a\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the indices of the electrodes in the electrode table\n        elect_ind = get_electrode_indices(nwbf, electrode_ids)\n\n        electrode_table_region = nwbf.create_electrode_table_region(\n            elect_ind, \"filtered electrode table\"\n        )\n        eseries_name = \"filtered data\"\n        es = pynwb.ecephys.ElectricalSeries(\n            name=eseries_name,\n            data=np.empty(tuple(output_shape_list), dtype=data_dtype),\n            electrodes=electrode_table_region,\n            timestamps=np.empty(output_shape_list[time_axis]),\n            description=description,\n        )\n        if type == \"LFP\":\n            lfp = pynwb.ecephys.LFP(electrical_series=es)\n            ecephys_module = nwbf.create_processing_module(\n                name=\"ecephys\", description=description\n            )\n            ecephys_module.add(lfp)\n        else:\n            nwbf.add_scratch(es)\n        io.write(nwbf)\n\n        # reload the NWB file to get the h5py objects for the data and the timestamps\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"a\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            es = nwbf.objects[es.object_id]\n            filtered_data = es.data\n            new_timestamps = es.timestamps\n            indices = np.array(indices, ndmin=2)\n            # Filter and write the output dataset\n            ts_offset = 0\n\n            print(\"Filtering data\")\n            for ii, (start, stop) in enumerate(indices):\n                # calculate the size of the timestamps and the data and determine whether they\n                # can be loaded into &lt; 90% of available RAM\n                mem = psutil.virtual_memory()\n                interval_samples = stop - start\n                if (\n                    interval_samples\n                    * (timestamp_size + n_electrodes * data_size)\n                    &lt; 0.9 * mem.available\n                ):\n                    print(f\"Interval {ii}: loading data into memory\")\n                    timestamps = np.asarray(\n                        timestamps_on_disk[start:stop],\n                        dtype=timestamp_dtype,\n                    )\n                    if time_axis == 0:\n                        data = np.asarray(\n                            data_on_disk[start:stop, :], dtype=data_dtype\n                        )\n                    else:\n                        data = np.asarray(\n                            data_on_disk[:, start:stop], dtype=data_dtype\n                        )\n                    extracted_ts = timestamps[0::decimation]\n                    new_timestamps[\n                        ts_offset : ts_offset + len(extracted_ts)\n                    ] = extracted_ts\n                    ts_offset += len(extracted_ts)\n                    # filter the data\n                    gsp.filter_data_fir(\n                        data,\n                        filter_coeff,\n                        axis=time_axis,\n                        input_index_bounds=[0, interval_samples - 1],\n                        output_index_bounds=[\n                            filter_delay,\n                            filter_delay + stop - start,\n                        ],\n                        ds=decimation,\n                        input_dim_restrictions=input_dim_restrictions,\n                        outarray=filtered_data,\n                        output_offset=output_offsets[ii],\n                    )\n                else:\n                    print(f\"Interval {ii}: leaving data on disk\")\n                    data = data_on_disk\n                    timestamps = timestamps_on_disk\n                    extracted_ts = timestamps[start:stop:decimation]\n                    new_timestamps[\n                        ts_offset : ts_offset + len(extracted_ts)\n                    ] = extracted_ts\n                    ts_offset += len(extracted_ts)\n                    # filter the data\n                    gsp.filter_data_fir(\n                        data,\n                        filter_coeff,\n                        axis=time_axis,\n                        input_index_bounds=[start, stop],\n                        output_index_bounds=[\n                            filter_delay,\n                            filter_delay + stop - start,\n                        ],\n                        ds=decimation,\n                        input_dim_restrictions=input_dim_restrictions,\n                        outarray=filtered_data,\n                        output_offset=output_offsets[ii],\n                    )\n\n            start_end = [new_timestamps[0], new_timestamps[-1]]\n\n            io.write(nwbf)\n\n    return es.object_id, start_end\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_filter.FirFilterParameters.filter_data", "title": "<code>filter_data(timestamps, data, filter_coeff, valid_times, electrodes, decimation)</code>", "text": "<p>:param timestamps: numpy array with list of timestamps for data :param data: original data array :param filter_coeff: numpy array with filter coefficients for FIR filter :param valid_times: 2D numpy array with start and stop times of intervals to be filtered :param electrodes: list of electrodes to filter :param decimation: decimation factor :return: filtered_data, timestamps</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def filter_data(\n    self,\n    timestamps,\n    data,\n    filter_coeff,\n    valid_times,\n    electrodes,\n    decimation,\n):\n\"\"\"\n    :param timestamps: numpy array with list of timestamps for data\n    :param data: original data array\n    :param filter_coeff: numpy array with filter coefficients for FIR filter\n    :param valid_times: 2D numpy array with start and stop times of intervals to be filtered\n    :param electrodes: list of electrodes to filter\n    :param decimation: decimation factor\n    :return: filtered_data, timestamps\n    \"\"\"\n    gsp = _import_ghostipy()\n\n    n_dim = len(data.shape)\n    n_samples = len(timestamps)\n    time_axis = 0 if data.shape[0] == n_samples else 1\n    electrode_axis = 1 - time_axis\n    input_dim_restrictions = [None] * n_dim\n    input_dim_restrictions[electrode_axis] = np.s_[electrodes]\n\n    indices = []\n    output_shape_list = [0] * n_dim\n    output_shape_list[electrode_axis] = len(electrodes)\n    output_offsets = [0]\n\n    filter_delay = self.calc_filter_delay(filter_coeff)\n    for a_start, a_stop in valid_times:\n        if a_start &lt; timestamps[0]:\n            print(\n                f\"Interval start time {a_start} is smaller than first timestamp \"\n                f\"{timestamps[0]}, using first timestamp instead\"\n            )\n            a_start = timestamps[0]\n        if a_stop &gt; timestamps[-1]:\n            print(\n                f\"Interval stop time {a_stop} is larger than last timestamp \"\n                f\"{timestamps[-1]}, using last timestamp instead\"\n            )\n            a_stop = timestamps[-1]\n        frm, to = np.searchsorted(timestamps, (a_start, a_stop))\n        if to &gt; n_samples:\n            to = n_samples\n\n        indices.append((frm, to))\n        shape, dtype = gsp.filter_data_fir(\n            data,\n            filter_coeff,\n            axis=time_axis,\n            input_index_bounds=[frm, to],\n            output_index_bounds=[filter_delay, filter_delay + to - frm],\n            describe_dims=True,\n            ds=decimation,\n            input_dim_restrictions=input_dim_restrictions,\n        )\n        output_offsets.append(output_offsets[-1] + shape[time_axis])\n        output_shape_list[time_axis] += shape[time_axis]\n\n    # create the dataset and the timestamps array\n    filtered_data = np.empty(tuple(output_shape_list), dtype=data.dtype)\n\n    new_timestamps = np.empty(\n        (output_shape_list[time_axis],), timestamps.dtype\n    )\n\n    indices = np.array(indices, ndmin=2)\n\n    # Filter  the output dataset\n    ts_offset = 0\n\n    for ii, (start, stop) in enumerate(indices):\n        extracted_ts = timestamps[start:stop:decimation]\n\n        # print(f\"Diffs {np.diff(extracted_ts)}\")\n        new_timestamps[\n            ts_offset : ts_offset + len(extracted_ts)\n        ] = extracted_ts\n        ts_offset += len(extracted_ts)\n\n        # finally ready to filter data!\n        gsp.filter_data_fir(\n            data,\n            filter_coeff,\n            axis=time_axis,\n            input_index_bounds=[start, stop],\n            output_index_bounds=[filter_delay, filter_delay + stop - start],\n            ds=decimation,\n            input_dim_restrictions=input_dim_restrictions,\n            outarray=filtered_data,\n            output_offset=output_offsets[ii],\n        )\n\n    return filtered_data, new_timestamps\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_filter.FirFilterParameters.calc_filter_delay", "title": "<code>calc_filter_delay(filter_coeff)</code>", "text": "<p>:param filter_coeff: :return: filter delay</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def calc_filter_delay(self, filter_coeff):\n\"\"\"\n    :param filter_coeff:\n    :return: filter delay\n    \"\"\"\n    return (len(filter_coeff) - 1) // 2\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_filter.FirFilterParameters.create_standard_filters", "title": "<code>create_standard_filters()</code>", "text": "<p>Add standard filters to the Filter table including 0-400 Hz low pass for continuous raw data -&gt; LFP</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def create_standard_filters(self):\n\"\"\"Add standard filters to the Filter table including\n    0-400 Hz low pass for continuous raw data -&gt; LFP\n    \"\"\"\n    self.add_filter(\n        \"LFP 0-400 Hz\",\n        20000,\n        \"lowpass\",\n        [400, 425],\n        \"standard LFP filter for 20 KHz data\",\n    )\n    self.add_filter(\n        \"LFP 0-400 Hz\",\n        30000,\n        \"lowpass\",\n        [400, 425],\n        \"standard LFP filter for 30 KHz data\",\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.Nwbfile", "title": "<code>Nwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass Nwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files.\n    nwb_file_name: varchar(255)   # name of the NWB file\n    ---\n    nwb_file_abs_path: filepath@raw\n    INDEX (nwb_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    @classmethod\n    def insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The relative path to the NWB file.\n        \"\"\"\n        nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n        assert os.path.exists(\n            nwb_file_abs_path\n        ), f\"File does not exist: {nwb_file_abs_path}\"\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n        cls.insert1(key, skip_duplicates=True)\n\n    @staticmethod\n    def get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n        Returns\n        -------\n        nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n        nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n        return str(nwb_file_abspath)\n\n    @staticmethod\n    def add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n        The NWB_LOCK_FILE environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n        \"\"\"\n        key = {\"nwb_file_name\": nwb_file_name}\n        # check to make sure the file exists\n        assert (\n            len((Nwbfile() &amp; key).fetch()) &gt; 0\n        ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n        lock_file.write(f\"{nwb_file_name}\\n\")\n        lock_file.close()\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        This does not delete the files themselves unless delete_files=True is specified\n        Run this after deleting the Nwbfile() entries themselves.\n        \"\"\"\n        schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.Nwbfile.insert_from_relative_file_name", "title": "<code>insert_from_relative_file_name(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Insert a new session from an existing NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The relative path to the NWB file.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The relative path to the NWB file.\n    \"\"\"\n    nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n    assert os.path.exists(\n        nwb_file_abs_path\n    ), f\"File does not exist: {nwb_file_abs_path}\"\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n    cls.insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.Nwbfile.get_abs_path", "title": "<code>get_abs_path(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored raw NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required <p>Returns:</p> Name Type Description <code>nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n    Returns\n    -------\n    nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n    nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n    return str(nwb_file_abspath)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.Nwbfile.add_to_lock", "title": "<code>add_to_lock(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Add the specified NWB file to the file with the list of NWB files to be locked.</p> <p>The NWB_LOCK_FILE environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n    The NWB_LOCK_FILE environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n    \"\"\"\n    key = {\"nwb_file_name\": nwb_file_name}\n    # check to make sure the file exists\n    assert (\n        len((Nwbfile() &amp; key).fetch()) &gt; 0\n    ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n    lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n    lock_file.write(f\"{nwb_file_name}\\n\")\n    lock_file.close()\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.Nwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>This does not delete the files themselves unless delete_files=True is specified Run this after deleting the Nwbfile() entries themselves.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    This does not delete the files themselves unless delete_files=True is specified\n    Run this after deleting the Nwbfile() entries themselves.\n    \"\"\"\n    schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.get_config", "title": "<code>get_config(nwb_file_path)</code>", "text": "<p>Return a dictionary of config settings for the given NWB file. If the file does not exist, return an empty dict.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Absolute path to the NWB file.</p> required <code>Returns</code> required <code>d</code> <code>dict</code> <p>Dictionary of configuration settings loaded from the corresponding YAML file</p> required Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_config(nwb_file_path):\n\"\"\"Return a dictionary of config settings for the given NWB file.\n    If the file does not exist, return an empty dict.\n    Parameters\n    ----------\n    nwb_file_path : str\n        Absolute path to the NWB file.\n    Returns\n    -------\n    d : dict\n        Dictionary of configuration settings loaded from the corresponding YAML file\n    \"\"\"\n    if nwb_file_path in __configs:  # load from cache if exists\n        return __configs[nwb_file_path]\n\n    p = Path(nwb_file_path)\n    # NOTE use p.stem[:-1] to remove the underscore that was added to the file\n    config_path = p.parent / (p.stem[:-1] + \"_spyglass_config.yaml\")\n    if not os.path.exists(config_path):\n        print(f\"No config found at file path {config_path}\")\n        return dict()\n    with open(config_path, \"r\") as stream:\n        d = yaml.safe_load(stream)\n\n    # TODO write a JSON schema for the yaml file and validate the yaml file\n    __configs[nwb_file_path] = d  # store in cache\n    return d\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.Electrode", "title": "<code>Electrode</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>@schema\nclass Electrode(dj.Imported):\n    definition = \"\"\"\n    -&gt; ElectrodeGroup\n    electrode_id: int                      # the unique number for this electrode\n    ---\n    -&gt; [nullable] Probe.Electrode\n    -&gt; BrainRegion\n    name = \"\": varchar(200)                 # unique label for each contact\n    original_reference_electrode = -1: int  # the configured reference electrode for this electrode\n    x = NULL: float                         # the x coordinate of the electrode position in the brain\n    y = NULL: float                         # the y coordinate of the electrode position in the brain\n    z = NULL: float                         # the z coordinate of the electrode position in the brain\n    filtering: varchar(2000)                # description of the signal filtering\n    impedance = NULL: float                 # electrode impedance\n    bad_channel = \"False\": enum(\"True\", \"False\")  # if electrode is \"good\" or \"bad\" as observed during recording\n    x_warped = NULL: float                  # x coordinate of electrode position warped to common template brain\n    y_warped = NULL: float                  # y coordinate of electrode position warped to common template brain\n    z_warped = NULL: float                  # z coordinate of electrode position warped to common template brain\n    contacts: varchar(200)                  # label of electrode contacts used for a bipolar signal - current workaround\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        config = get_config(nwb_file_abspath)\n\n        if \"Electrode\" in config:\n            electrode_config_dicts = {\n                electrode_dict[\"electrode_id\"]: electrode_dict\n                for electrode_dict in config[\"Electrode\"]\n            }\n        else:\n            electrode_config_dicts = dict()\n\n        electrodes = nwbf.electrodes.to_dataframe()\n        for elect_id, elect_data in electrodes.iterrows():\n            key[\"electrode_id\"] = elect_id\n            key[\"name\"] = str(elect_id)\n            key[\"electrode_group_name\"] = elect_data.group_name\n            key[\"region_id\"] = BrainRegion.fetch_add(\n                region_name=elect_data.group.location\n            )\n            key[\"x\"] = elect_data.x\n            key[\"y\"] = elect_data.y\n            key[\"z\"] = elect_data.z\n            key[\"x_warped\"] = 0\n            key[\"y_warped\"] = 0\n            key[\"z_warped\"] = 0\n            key[\"contacts\"] = \"\"\n            key[\"filtering\"] = elect_data.filtering\n            key[\"impedance\"] = elect_data.get(\"imp\")\n\n            # rough check of whether the electrodes table was created by rec_to_nwb and has\n            # the appropriate custom columns used by rec_to_nwb\n            # TODO this could be better resolved by making an extension for the electrodes table\n            if (\n                isinstance(elect_data.group.device, ndx_franklab_novela.Probe)\n                and \"probe_shank\" in elect_data\n                and \"probe_electrode\" in elect_data\n                and \"bad_channel\" in elect_data\n                and \"ref_elect_id\" in elect_data\n            ):\n                key[\"probe_id\"] = elect_data.group.device.probe_type\n                key[\"probe_shank\"] = elect_data.probe_shank\n                key[\"probe_electrode\"] = elect_data.probe_electrode\n                key[\"bad_channel\"] = (\n                    \"True\" if elect_data.bad_channel else \"False\"\n                )\n                key[\"original_reference_electrode\"] = elect_data.ref_elect_id\n\n            # override with information from the config YAML based on primary key (electrode id)\n            if elect_id in electrode_config_dicts:\n                # check whether the Probe.Electrode being referenced exists\n                query = Probe.Electrode &amp; electrode_config_dicts[elect_id]\n                if len(query) == 0:\n                    warnings.warn(\n                        f\"No Probe.Electrode exists that matches the data: {electrode_config_dicts[elect_id]}. \"\n                        f\"The config YAML for Electrode with electrode_id {elect_id} will be ignored.\"\n                    )\n                else:\n                    key.update(electrode_config_dicts[elect_id])\n\n            self.insert1(key, skip_duplicates=True)\n\n    @classmethod\n    def create_from_config(cls, nwb_file_name: str):\n\"\"\"Create or update Electrode entries from what is specified in the config YAML file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        config = get_config(nwb_file_abspath)\n        if \"Electrode\" not in config:\n            return\n\n        # map electrode id to dictionary of electrode information from config YAML\n        electrode_dicts = {\n            electrode_dict[\"electrode_id\"]: electrode_dict\n            for electrode_dict in config[\"Electrode\"]\n        }\n\n        electrodes = nwbf.electrodes.to_dataframe()\n        for nwbfile_elect_id, elect_data in electrodes.iterrows():\n            if nwbfile_elect_id in electrode_dicts:\n                # use the information in the electrodes table to start and then add (or overwrite) values from the\n                # config YAML\n                key = dict()\n                key[\"nwb_file_name\"] = nwb_file_name\n                key[\"name\"] = str(nwbfile_elect_id)\n                key[\"electrode_group_name\"] = elect_data.group_name\n                key[\"region_id\"] = BrainRegion.fetch_add(\n                    region_name=elect_data.group.location\n                )\n                key[\"x\"] = elect_data.x\n                key[\"y\"] = elect_data.y\n                key[\"z\"] = elect_data.z\n                key[\"x_warped\"] = 0\n                key[\"y_warped\"] = 0\n                key[\"z_warped\"] = 0\n                key[\"contacts\"] = \"\"\n                key[\"filtering\"] = elect_data.filtering\n                key[\"impedance\"] = elect_data.get(\"imp\")\n                key.update(electrode_dicts[nwbfile_elect_id])\n                query = Electrode &amp; {\"electrode_id\": nwbfile_elect_id}\n                if len(query):\n                    cls.update1(key)\n                    print(f\"Updated Electrode with ID {nwbfile_elect_id}.\")\n                else:\n                    cls.insert1(\n                        key, skip_duplicates=True, allow_direct_insert=True\n                    )\n                    print(f\"Inserted Electrode with ID {nwbfile_elect_id}.\")\n            else:\n                warnings.warn(\n                    f\"Electrode ID {nwbfile_elect_id} exists in the NWB file but has no corresponding \"\n                    \"config YAML entry.\"\n                )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.Electrode.create_from_config", "title": "<code>create_from_config(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Create or update Electrode entries from what is specified in the config YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file.</p> required Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>@classmethod\ndef create_from_config(cls, nwb_file_name: str):\n\"\"\"Create or update Electrode entries from what is specified in the config YAML file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    nwbf = get_nwb_file(nwb_file_abspath)\n    config = get_config(nwb_file_abspath)\n    if \"Electrode\" not in config:\n        return\n\n    # map electrode id to dictionary of electrode information from config YAML\n    electrode_dicts = {\n        electrode_dict[\"electrode_id\"]: electrode_dict\n        for electrode_dict in config[\"Electrode\"]\n    }\n\n    electrodes = nwbf.electrodes.to_dataframe()\n    for nwbfile_elect_id, elect_data in electrodes.iterrows():\n        if nwbfile_elect_id in electrode_dicts:\n            # use the information in the electrodes table to start and then add (or overwrite) values from the\n            # config YAML\n            key = dict()\n            key[\"nwb_file_name\"] = nwb_file_name\n            key[\"name\"] = str(nwbfile_elect_id)\n            key[\"electrode_group_name\"] = elect_data.group_name\n            key[\"region_id\"] = BrainRegion.fetch_add(\n                region_name=elect_data.group.location\n            )\n            key[\"x\"] = elect_data.x\n            key[\"y\"] = elect_data.y\n            key[\"z\"] = elect_data.z\n            key[\"x_warped\"] = 0\n            key[\"y_warped\"] = 0\n            key[\"z_warped\"] = 0\n            key[\"contacts\"] = \"\"\n            key[\"filtering\"] = elect_data.filtering\n            key[\"impedance\"] = elect_data.get(\"imp\")\n            key.update(electrode_dicts[nwbfile_elect_id])\n            query = Electrode &amp; {\"electrode_id\": nwbfile_elect_id}\n            if len(query):\n                cls.update1(key)\n                print(f\"Updated Electrode with ID {nwbfile_elect_id}.\")\n            else:\n                cls.insert1(\n                    key, skip_duplicates=True, allow_direct_insert=True\n                )\n                print(f\"Inserted Electrode with ID {nwbfile_elect_id}.\")\n        else:\n            warnings.warn(\n                f\"Electrode ID {nwbfile_elect_id} exists in the NWB file but has no corresponding \"\n                \"config YAML entry.\"\n            )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.get_data_interface", "title": "<code>get_data_interface(nwbfile, data_interface_name, data_interface_class=None)</code>", "text": "<p>Search for a specified NWBDataInterface or DynamicTable in the processing modules of an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>The NWB file object to search in.</p> required <code>data_interface_name</code> <code>str</code> <p>The name of the NWBDataInterface or DynamicTable to search for.</p> required <code>data_interface_class</code> <code>type</code> <p>The class (or superclass) to search for. This argument helps to prevent accessing an object with the same name but the incorrect type. Default: no restriction.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If multiple NWBDataInterface and DynamicTable objects with the matching name are found.</p> <p>Returns:</p> Name Type Description <code>data_interface</code> <code>NWBDataInterface</code> <p>The data interface object with the given name, or None if not found.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_data_interface(nwbfile, data_interface_name, data_interface_class=None):\n\"\"\"Search for a specified NWBDataInterface or DynamicTable in the processing modules of an NWB file.\n\n    Parameters\n    ----------\n    nwbfile : pynwb.NWBFile\n        The NWB file object to search in.\n    data_interface_name : str\n        The name of the NWBDataInterface or DynamicTable to search for.\n    data_interface_class : type, optional\n        The class (or superclass) to search for. This argument helps to prevent accessing an object with the same\n        name but the incorrect type. Default: no restriction.\n\n    Warns\n    -----\n    UserWarning\n        If multiple NWBDataInterface and DynamicTable objects with the matching name are found.\n\n    Returns\n    -------\n    data_interface : NWBDataInterface\n        The data interface object with the given name, or None if not found.\n    \"\"\"\n    ret = []\n    for module in nwbfile.processing.values():\n        match = module.data_interfaces.get(data_interface_name, None)\n        if match is not None:\n            if data_interface_class is not None and not isinstance(\n                match, data_interface_class\n            ):\n                continue\n            ret.append(match)\n    if len(ret) &gt; 1:\n        warnings.warn(\n            f\"Multiple data interfaces with name '{data_interface_name}' \"\n            f\"found in NWBFile with identifier {nwbfile.identifier}. Using the first one found. \"\n            \"Use the data_interface_class argument to restrict the search.\"\n        )\n    if len(ret) &gt;= 1:\n        return ret[0]\n    else:\n        return None\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.interval_list_contains_ind", "title": "<code>interval_list_contains_ind(interval_list, timestamps)</code>", "text": "<p>Find indices of a list of timestamps that are contained in an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval. Unit is seconds.</p> required <code>timestamps</code> <code>array_like</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_contains_ind(interval_list, timestamps):\n\"\"\"Find indices of a list of timestamps that are contained in an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval. Unit is seconds.\n    timestamps : array_like\n    \"\"\"\n    ind = []\n    for interval in interval_list:\n        ind += np.ravel(\n            np.argwhere(\n                np.logical_and(\n                    timestamps &gt;= interval[0], timestamps &lt;= interval[1]\n                )\n            )\n        ).tolist()\n    return np.asarray(ind)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.estimate_sampling_rate", "title": "<code>estimate_sampling_rate(timestamps, multiplier)</code>", "text": "<p>Estimate the sampling rate given a list of timestamps.</p> <p>Assumes that the most common temporal differences between timestamps approximate the sampling rate. Note that this can fail for very high sampling rates and irregular timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>numpy.ndarray</code> <p>1D numpy array of timestamp values.</p> required <code>multiplier</code> <code>float or int</code> required <p>Returns:</p> Name Type Description <code>estimated_rate</code> <code>float</code> <p>The estimated sampling rate.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def estimate_sampling_rate(timestamps, multiplier):\n\"\"\"Estimate the sampling rate given a list of timestamps.\n\n    Assumes that the most common temporal differences between timestamps approximate the sampling rate. Note that this\n    can fail for very high sampling rates and irregular timestamps.\n\n    Parameters\n    ----------\n    timestamps : numpy.ndarray\n        1D numpy array of timestamp values.\n    multiplier : float or int\n\n    Returns\n    -------\n    estimated_rate : float\n        The estimated sampling rate.\n    \"\"\"\n\n    # approach:\n    # 1. use a box car smoother and a histogram to get the modal value\n    # 2. identify adjacent samples as those that have a time difference &lt; the multiplier * the modal value\n    # 3. average the time differences between adjacent samples\n    sample_diff = np.diff(timestamps[~np.isnan(timestamps)])\n    if len(sample_diff) &lt; 10:\n        raise ValueError(\n            f\"Only {len(sample_diff)} timestamps are valid. Check the data.\"\n        )\n    nsmooth = 10\n    smoother = np.ones(nsmooth) / nsmooth\n    smooth_diff = np.convolve(sample_diff, smoother, mode=\"same\")\n\n    # we histogram with 100 bins out to 3 * mean, which should be fine for any reasonable number of samples\n    hist, bins = np.histogram(\n        smooth_diff, bins=100, range=[0, 3 * np.mean(smooth_diff)]\n    )\n    mode = bins[np.where(hist == np.max(hist))]\n\n    adjacent = sample_diff &lt; mode[0] * multiplier\n    return np.round(1.0 / np.mean(sample_diff[adjacent]))\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.get_valid_intervals", "title": "<code>get_valid_intervals(timestamps, sampling_rate, gap_proportion, min_valid_len)</code>", "text": "<p>Finds the set of all valid intervals in a list of timestamps. Valid interval: (start time, stop time) during which there are no gaps (i.e. missing samples).</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>numpy.ndarray</code> <p>1D numpy array of timestamp values.</p> required <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data.</p> required <code>gap_proportion</code> <code>float, greater than 1; unit: samples</code> <p>Threshold for detecting a gap; i.e. if the difference (in samples) between consecutive timestamps exceeds gap_proportion, it is considered a gap</p> required <code>min_valid_len</code> <code>float</code> <p>Length of smallest valid interval.</p> required <p>Returns:</p> Name Type Description <code>valid_times</code> <code>np.ndarray</code> <p>Array of start and stop times of shape (N, 2) for valid data.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_valid_intervals(\n    timestamps, sampling_rate, gap_proportion, min_valid_len\n):\n\"\"\"Finds the set of all valid intervals in a list of timestamps.\n    Valid interval: (start time, stop time) during which there are\n    no gaps (i.e. missing samples).\n\n    Parameters\n    ----------\n    timestamps : numpy.ndarray\n        1D numpy array of timestamp values.\n    sampling_rate : float\n        Sampling rate of the data.\n    gap_proportion : float, greater than 1; unit: samples\n        Threshold for detecting a gap;\n        i.e. if the difference (in samples) between\n        consecutive timestamps exceeds gap_proportion,\n        it is considered a gap\n    min_valid_len : float\n        Length of smallest valid interval.\n\n    Returns\n    -------\n    valid_times : np.ndarray\n        Array of start and stop times of shape (N, 2) for valid data.\n    \"\"\"\n\n    eps = 0.0000001\n\n    # get rid of NaN elements\n    timestamps = timestamps[~np.isnan(timestamps)]\n    # find gaps\n    gap = np.diff(timestamps) &gt; 1.0 / sampling_rate * gap_proportion\n\n    # all true entries of gap represent gaps. Get the times bounding these intervals.\n    gapind = np.asarray(np.where(gap))\n    # The end of each valid interval are the indices of the gaps and the final value\n    valid_end = np.append(gapind, np.asarray(len(timestamps) - 1))\n\n    # the beginning of the gaps are the first element and gapind+1\n    valid_start = np.insert(gapind + 1, 0, 0)\n\n    valid_indices = np.vstack([valid_start, valid_end]).transpose()\n\n    valid_times = timestamps[valid_indices]\n    # adjust the times to deal with single valid samples\n    valid_times[:, 0] = valid_times[:, 0] - eps\n    valid_times[:, 1] = valid_times[:, 1] + eps\n\n    valid_intervals = (valid_times[:, 1] - valid_times[:, 0]) &gt; min_valid_len\n\n    return valid_times[valid_intervals, :]\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.interval_list_intersect", "title": "<code>interval_list_intersect(interval_list1, interval_list2, min_length=0)</code>", "text": "<p>Finds the intersections between two interval lists</p> <p>Parameters:</p> Name Type Description Default <code>interval_list1</code> <code>np.array, (N,2) where N = number of intervals</code> required <code>interval_list2</code> <code>np.array, (N,2) where N = number of intervals</code> required <code>min_length</code> <code>float, optional.</code> <p>Minimum length of intervals to include, default 0</p> <code>0</code> <p>Each interval is (start time, stop time)</p> <p>Returns:</p> Name Type Description <code>interval_list</code> <code>np.array, N, 2</code> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_intersect(interval_list1, interval_list2, min_length=0):\n\"\"\"Finds the intersections between two interval lists\n\n    Parameters\n    ----------\n    interval_list1 : np.array, (N,2) where N = number of intervals\n    interval_list2 : np.array, (N,2) where N = number of intervals\n    min_length : float, optional.\n        Minimum length of intervals to include, default 0\n\n    Each interval is (start time, stop time)\n\n    Returns\n    -------\n    interval_list: np.array, (N,2)\n    \"\"\"\n\n    # first, consolidate interval lists to disjoint intervals by sorting and applying union\n    if interval_list1.ndim == 1:\n        interval_list1 = np.expand_dims(interval_list1, 0)\n    else:\n        interval_list1 = interval_list1[np.argsort(interval_list1[:, 0])]\n        interval_list1 = reduce(_union_concat, interval_list1)\n        # the following check is needed in the case where the interval list is a single element (behavior of reduce)\n        if interval_list1.ndim == 1:\n            interval_list1 = np.expand_dims(interval_list1, 0)\n\n    if interval_list2.ndim == 1:\n        interval_list2 = np.expand_dims(interval_list2, 0)\n    else:\n        interval_list2 = interval_list2[np.argsort(interval_list2[:, 0])]\n        interval_list2 = reduce(_union_concat, interval_list2)\n        # the following check is needed in the case where the interval list is a single element (behavior of reduce)\n        if interval_list2.ndim == 1:\n            interval_list2 = np.expand_dims(interval_list2, 0)\n\n    # then do pairwise comparison and collect intersections\n    intersecting_intervals = []\n    for interval2 in interval_list2:\n        for interval1 in interval_list1:\n            if _intersection(interval2, interval1) is not None:\n                intersecting_intervals.append(\n                    _intersection(interval1, interval2)\n                )\n\n    # if no intersection, then return an empty list\n    if not intersecting_intervals:\n        return []\n    else:\n        intersecting_intervals = np.asarray(intersecting_intervals)\n        intersecting_intervals = intersecting_intervals[\n            np.argsort(intersecting_intervals[:, 0])\n        ]\n\n        return intervals_by_length(\n            intersecting_intervals, min_length=min_length\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.get_electrode_indices", "title": "<code>get_electrode_indices(nwb_object, electrode_ids)</code>", "text": "<p>Given an NWB file or electrical series object, return the indices of the specified electrode_ids.</p> <p>If an ElectricalSeries is given, then the indices returned are relative to the selected rows in ElectricalSeries.electrodes. For example, if electricalseries.electrodes = [5], and row index 5 of nwbfile.electrodes has ID 10, then calling get_electrode_indices(electricalseries, 10) will return 0, the index of the matching electrode in electricalseries.electrodes.</p> <p>Indices for electrode_ids that are not in the electrical series are returned as np.nan</p> <p>If an NWBFile is given, then the row indices with the matching IDs in the file's electrodes table are returned.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_object</code> <code>pynwb.NWBFile or pynwb.ecephys.ElectricalSeries</code> <p>The NWB file object or NWB electrical series object.</p> required <code>electrode_ids</code> <code>np.ndarray or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>list</code> <p>Array of indices of the specified electrode IDs.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_electrode_indices(nwb_object, electrode_ids):\n\"\"\"Given an NWB file or electrical series object, return the indices of the specified electrode_ids.\n\n    If an ElectricalSeries is given, then the indices returned are relative to the selected rows in\n    ElectricalSeries.electrodes. For example, if electricalseries.electrodes = [5], and row index 5 of\n    nwbfile.electrodes has ID 10, then calling get_electrode_indices(electricalseries, 10) will return 0, the\n    index of the matching electrode in electricalseries.electrodes.\n\n    Indices for electrode_ids that are not in the electrical series are returned as np.nan\n\n    If an NWBFile is given, then the row indices with the matching IDs in the file's electrodes table are returned.\n\n    Parameters\n    ----------\n    nwb_object : pynwb.NWBFile or pynwb.ecephys.ElectricalSeries\n        The NWB file object or NWB electrical series object.\n    electrode_ids : np.ndarray or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : list\n        Array of indices of the specified electrode IDs.\n    \"\"\"\n    if isinstance(nwb_object, pynwb.ecephys.ElectricalSeries):\n        # electrodes is a DynamicTableRegion which may contain a subset of the rows in NWBFile.electrodes\n        # match against only the subset of electrodes referenced by this ElectricalSeries\n        electrode_table_indices = nwb_object.electrodes.data[:]\n        selected_elect_ids = [\n            nwb_object.electrodes.table.id[x] for x in electrode_table_indices\n        ]\n    elif isinstance(nwb_object, pynwb.NWBFile):\n        # electrodes is a DynamicTable that contains all electrodes\n        selected_elect_ids = list(nwb_object.electrodes.id[:])\n    else:\n        raise ValueError(\n            \"nwb_object must be of type ElectricalSeries or NWBFile\"\n        )\n\n    # for each electrode_id, find its index in selected_elect_ids and return that if it's there and invalid_electrode_index if not.\n    return [\n        selected_elect_ids.index(elect_id)\n        if elect_id in selected_elect_ids\n        else invalid_electrode_index\n        for elect_id in electrode_ids\n    ]\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.LFPSelection", "title": "<code>LFPSelection</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>@schema\nclass LFPSelection(dj.Manual):\n    definition = \"\"\"\n     -&gt; Session\n     \"\"\"\n\n    class LFPElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; LFPSelection\n        -&gt; Electrode\n        \"\"\"\n\n    def set_lfp_electrodes(self, nwb_file_name, electrode_list):\n\"\"\"Removes all electrodes for the specified nwb file and then adds back the electrodes in the list\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the nwb file for the desired session\n        electrode_list : list\n            list of electrodes to be used for LFP\n\n        \"\"\"\n        # remove the session and then recreate the session and Electrode list\n        (LFPSelection() &amp; {\"nwb_file_name\": nwb_file_name}).delete()\n        # check to see if the user allowed the deletion\n        if (\n            len((LFPSelection() &amp; {\"nwb_file_name\": nwb_file_name}).fetch())\n            == 0\n        ):\n            LFPSelection().insert1({\"nwb_file_name\": nwb_file_name})\n\n            # TODO: do this in a better way\n            all_electrodes = (\n                Electrode() &amp; {\"nwb_file_name\": nwb_file_name}\n            ).fetch(as_dict=True)\n            primary_key = Electrode.primary_key\n            for e in all_electrodes:\n                # create a dictionary so we can insert new elects\n                if e[\"electrode_id\"] in electrode_list:\n                    lfpelectdict = {\n                        k: v for k, v in e.items() if k in primary_key\n                    }\n                    LFPSelection().LFPElectrode.insert1(\n                        lfpelectdict, replace=True\n                    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.LFPSelection.set_lfp_electrodes", "title": "<code>set_lfp_electrodes(nwb_file_name, electrode_list)</code>", "text": "<p>Removes all electrodes for the specified nwb file and then adds back the electrodes in the list</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the nwb file for the desired session</p> required <code>electrode_list</code> <code>list</code> <p>list of electrodes to be used for LFP</p> required Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>def set_lfp_electrodes(self, nwb_file_name, electrode_list):\n\"\"\"Removes all electrodes for the specified nwb file and then adds back the electrodes in the list\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the nwb file for the desired session\n    electrode_list : list\n        list of electrodes to be used for LFP\n\n    \"\"\"\n    # remove the session and then recreate the session and Electrode list\n    (LFPSelection() &amp; {\"nwb_file_name\": nwb_file_name}).delete()\n    # check to see if the user allowed the deletion\n    if (\n        len((LFPSelection() &amp; {\"nwb_file_name\": nwb_file_name}).fetch())\n        == 0\n    ):\n        LFPSelection().insert1({\"nwb_file_name\": nwb_file_name})\n\n        # TODO: do this in a better way\n        all_electrodes = (\n            Electrode() &amp; {\"nwb_file_name\": nwb_file_name}\n        ).fetch(as_dict=True)\n        primary_key = Electrode.primary_key\n        for e in all_electrodes:\n            # create a dictionary so we can insert new elects\n            if e[\"electrode_id\"] in electrode_list:\n                lfpelectdict = {\n                    k: v for k, v in e.items() if k in primary_key\n                }\n                LFPSelection().LFPElectrode.insert1(\n                    lfpelectdict, replace=True\n                )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.Probe", "title": "<code>Probe</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass Probe(dj.Manual):\n    definition = \"\"\"\n    # A configuration of a ProbeType. For most probe types, there is only one configuration, and that configuration\n    # should always be used. For Neuropixels probes, the specific channel map (which electrodes are used,\n    # where are they, and in what order) can differ between users and sessions, and each configuration should have a\n    # different ProbeType.\n    probe_id: varchar(80)     # a unique ID for this probe and dynamic configuration\n    ---\n    -&gt; ProbeType              # the type of probe, selected from a controlled list of probe types\n    -&gt; [nullable] DataAcquisitionDevice  # the data acquisition device used with this Probe\n    contact_side_numbering: enum(\"True\", \"False\")  # if True, then electrode contacts are facing you when numbering them\n    \"\"\"\n\n    class Shank(dj.Part):\n        definition = \"\"\"\n        -&gt; Probe\n        probe_shank: int              # shank number within probe. should be unique within a Probe\n        \"\"\"\n\n    class Electrode(dj.Part):\n        definition = \"\"\"\n        -&gt; Probe.Shank\n        probe_electrode: int          # electrode ID that is output from the data acquisition system\n                                      # probe_electrode should be unique within a Probe\n        ---\n        contact_size = NULL: float    # (um) contact size\n        rel_x = NULL: float           # (um) x coordinate of the electrode within the probe\n        rel_y = NULL: float           # (um) y coordinate of the electrode within the probe\n        rel_z = NULL: float           # (um) z coordinate of the electrode within the probe\n        \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, config):\n\"\"\"Insert probe devices from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : list\n            List of probe device types found in the NWB file.\n        \"\"\"\n        all_probes_types, ndx_probes, _ = cls.get_all_probe_names(nwbf, config)\n\n        for probe_type in all_probes_types:\n            new_probe_type_dict = dict()\n            new_probe_dict = dict()\n            shank_dict = dict()\n            elect_dict = dict()\n            num_shanks = 0\n\n            if probe_type in ndx_probes:\n                # read probe properties into new_probe_dict from PyNWB extension probe object\n                nwb_probe_obj = ndx_probes[probe_type]\n                cls.__read_ndx_probe_data(\n                    nwb_probe_obj,\n                    new_probe_type_dict,\n                    new_probe_dict,\n                    shank_dict,\n                    elect_dict,\n                )\n\n            # check that number of shanks is consistent\n            num_shanks = new_probe_type_dict[\"num_shanks\"]\n            assert num_shanks == 0 or num_shanks == len(\n                shank_dict\n            ), \"`num_shanks` is not equal to the number of shanks.\"\n\n            # if probe id already exists, do not overwrite anything or create new Shanks and Electrodes\n            # TODO test whether the Shanks and Electrodes in the NWB file match the ones in the database\n            query = Probe &amp; {\"probe_id\": new_probe_dict[\"probe_id\"]}\n            if len(query) &gt; 0:\n                print(\n                    f\"Probe ID '{new_probe_dict['probe_id']}' already exists in the database. Spyglass will use \"\n                    \"that and not create a new Probe, Shanks, or Electrodes.\"\n                )\n                continue\n\n            cls.insert1(new_probe_dict, skip_duplicates=True)\n\n            for shank in shank_dict.values():\n                cls.Shank.insert1(shank, skip_duplicates=True)\n            for electrode in elect_dict.values():\n                cls.Electrode.insert1(electrode, skip_duplicates=True)\n\n        if all_probes_types:\n            print(f\"Inserted probes {all_probes_types}\")\n        else:\n            print(\"No conforming probe metadata found.\")\n\n        return all_probes_types\n\n    @classmethod\n    def get_all_probe_names(cls, nwbf, config):\n\"\"\"Get a list of all device names in the NWB file, after appending and overwriting by the config file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : list\n            List of data acquisition object names found in the NWB file.\n        \"\"\"\n\n        # make a dict mapping probe type to PyNWB object for all devices in the NWB file that are\n        # of type ndx_franklab_novela.Probe and thus have the required metadata\n        ndx_probes = {\n            device_obj.probe_type: device_obj\n            for device_obj in nwbf.devices.values()\n            if isinstance(device_obj, ndx_franklab_novela.Probe)\n        }\n\n        # make a dict mapping probe type to dict of device metadata from the config YAML if exists\n        if \"Probe\" in config:\n            config_probes = [\n                probe_dict[\"probe_type\"] for probe_dict in config[\"Probe\"]\n            ]\n        else:\n            config_probes = list()\n\n        # get all the probe types from the NWB file plus the config YAML\n        all_probes_types = set(ndx_probes.keys()).union(set(config_probes))\n\n        return all_probes_types, ndx_probes, config_probes\n\n    @classmethod\n    def __read_ndx_probe_data(\n        cls,\n        nwb_probe_obj: ndx_franklab_novela.Probe,\n        new_probe_type_dict: dict,\n        new_probe_dict: dict,\n        shank_dict: dict,\n        elect_dict: dict,\n    ):\n        # construct dictionary of values to add to ProbeType\n        new_probe_type_dict[\"manufacturer\"] = (\n            getattr(nwb_probe_obj, \"manufacturer\") or \"\"\n        )\n        new_probe_type_dict[\"probe_type\"] = nwb_probe_obj.probe_type\n        new_probe_type_dict[\n            \"probe_description\"\n        ] = nwb_probe_obj.probe_description\n        new_probe_type_dict[\"num_shanks\"] = len(nwb_probe_obj.shanks)\n\n        cls._add_probe_type(new_probe_type_dict)\n\n        new_probe_dict[\"probe_id\"] = nwb_probe_obj.probe_type\n        new_probe_dict[\"probe_type\"] = nwb_probe_obj.probe_type\n        new_probe_dict[\"contact_side_numbering\"] = (\n            \"True\" if nwb_probe_obj.contact_side_numbering else \"False\"\n        )\n\n        # go through the shanks and add each one to the Shank table\n        for shank in nwb_probe_obj.shanks.values():\n            shank_dict[shank.name] = dict()\n            shank_dict[shank.name][\"probe_id\"] = new_probe_dict[\"probe_type\"]\n            shank_dict[shank.name][\"probe_shank\"] = int(shank.name)\n\n            # go through the electrodes and add each one to the Electrode table\n            for electrode in shank.shanks_electrodes.values():\n                # the next line will need to be fixed if we have different sized contacts on a shank\n                elect_dict[electrode.name] = dict()\n                elect_dict[electrode.name][\"probe_id\"] = new_probe_dict[\n                    \"probe_type\"\n                ]\n                elect_dict[electrode.name][\"probe_shank\"] = shank_dict[\n                    shank.name\n                ][\"probe_shank\"]\n                elect_dict[electrode.name][\n                    \"contact_size\"\n                ] = nwb_probe_obj.contact_size\n                elect_dict[electrode.name][\"probe_electrode\"] = int(\n                    electrode.name\n                )\n                elect_dict[electrode.name][\"rel_x\"] = electrode.rel_x\n                elect_dict[electrode.name][\"rel_y\"] = electrode.rel_y\n                elect_dict[electrode.name][\"rel_z\"] = electrode.rel_z\n\n    @classmethod\n    def _add_probe_type(cls, new_probe_type_dict):\n\"\"\"Check the probe type value against the values in the database.\n\n        Parameters\n        ----------\n        new_probe_type_dict : dict\n            Dictionary of probe type properties. See ProbeType for keys.\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a probe type to the database when prompted.\n\n        Returns\n        -------\n        probe_type : str\n            The probe type value that was added to the database.\n        \"\"\"\n        probe_type = new_probe_type_dict[\"probe_type\"]\n        all_values = ProbeType.fetch(\"probe_type\").tolist()\n        if probe_type not in all_values:\n            print(\n                f\"\\nProbe type '{probe_type}' was not found in the database. \"\n                f\"The current values are: {all_values}. \"\n                \"Please ensure that the probe type you want to add does not already \"\n                \"exist in the database under a different name or spelling. \"\n                \"If you want to use an existing name in the database, \"\n                \"please change the corresponding Probe object in the NWB file. \"\n                \"Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                f\"Do you want to add probe type '{probe_type}' to the database? (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                ProbeType.insert1(new_probe_type_dict, skip_duplicates=True)\n                return\n            raise PopulateException(\n                f\"User chose not to add probe type '{probe_type}' to the database.\"\n            )\n\n        # effectively else (entry exists)\n        # check whether the values provided match the values stored in the database\n        db_dict = (ProbeType &amp; {\"probe_type\": probe_type}).fetch1()\n        if db_dict != new_probe_type_dict:\n            raise PopulateException(\n                f\"\\nProbe type properties of PyNWB Probe object with name '{probe_type}': \"\n                f\"{new_probe_type_dict} do not match properties of the corresponding database entry: {db_dict}.\"\n            )\n        return probe_type\n\n    @classmethod\n    def create_from_nwbfile(\n        cls,\n        nwb_file_name: str,\n        nwb_device_name: str,\n        probe_id: str,\n        probe_type: str,\n        contact_side_numbering: bool,\n    ):\n\"\"\"Create a Probe entry and corresponding part table entries using the data in the NWB file.\n\n        This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices\n        (as probes) in the NWB file, but only ones that are associated with the device that matches the given\n        `nwb_device_name`.\n\n        Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe,\n        the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.\n\n        Example usage:\n        ```\n        sgc.Probe.create_from_nwbfile(\n            nwbfile=nwb_file_name,\n            nwb_device_name=\"Device\",\n            probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n            probe_type=\"Neuropixels 1.0\",\n            contact_side_numbering=True\n        )\n        ```\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the NWB file.\n        nwb_device_name : str\n            The name of the PyNWB Device object that represents the probe to read in the NWB file.\n        probe_id : str\n            A unique ID for the probe and its configuration, to be used as the primary key for the new Probe entry.\n        probe_type : str\n            The existing ProbeType entry that represents the type of probe being created. It must exist.\n        contact_side_numbering : bool\n            Whether the electrode contacts are facing you when numbering them. Stored in the new Probe entry.\n        \"\"\"\n\n        from .common_nwbfile import Nwbfile\n\n        nwb_file_path = Nwbfile.get_abs_path(nwb_file_name)\n        nwbfile = get_nwb_file(nwb_file_path)\n\n        query = ProbeType &amp; {\"probe_type\": probe_type}\n        if len(query) == 0:\n            print(\n                f\"No ProbeType found with probe_type '{probe_type}'. Aborting.\"\n            )\n            return\n\n        new_probe_dict = dict()\n        shank_dict = dict()\n        elect_dict = dict()\n\n        new_probe_dict[\"probe_id\"] = probe_id\n        new_probe_dict[\"probe_type\"] = probe_type\n        new_probe_dict[\"contact_side_numbering\"] = (\n            \"True\" if contact_side_numbering else \"False\"\n        )\n\n        # iterate through the electrodes table in the NWB file\n        # and use the group column (ElectrodeGroup) to create shanks\n        # and use the device attribute of each ElectrodeGroup to create a probe\n        created_shanks = dict()  # map device name to shank_index (int)\n        device_found = False\n        for elec_index in range(len(nwbfile.electrodes)):\n            electrode_group = nwbfile.electrodes[elec_index, \"group\"]\n            eg_device_name = electrode_group.device.name\n\n            # only look at electrodes where the associated device is the one specified\n            if eg_device_name == nwb_device_name:\n                device_found = True\n\n                # if a Shank has not yet been created from the electrode group, then create it\n                if electrode_group.name not in created_shanks:\n                    shank_index = len(created_shanks)\n                    created_shanks[electrode_group.name] = shank_index\n\n                    # build the dictionary of Probe.Shank data\n                    shank_dict[shank_index] = dict()\n                    shank_dict[shank_index][\"probe_id\"] = new_probe_dict[\n                        \"probe_id\"\n                    ]\n                    shank_dict[shank_index][\"probe_shank\"] = shank_index\n\n                # get the probe shank index associated with this Electrode\n                probe_shank = created_shanks[electrode_group.name]\n\n                # build the dictionary of Probe.Electrode data\n                elect_dict[elec_index] = dict()\n                elect_dict[elec_index][\"probe_id\"] = new_probe_dict[\"probe_id\"]\n                elect_dict[elec_index][\"probe_shank\"] = probe_shank\n                elect_dict[elec_index][\"probe_electrode\"] = elec_index\n                if \"rel_x\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_x\"] = nwbfile.electrodes[\n                        elec_index, \"rel_x\"\n                    ]\n                if \"rel_y\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_y\"] = nwbfile.electrodes[\n                        elec_index, \"rel_y\"\n                    ]\n                if \"rel_z\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_z\"] = nwbfile.electrodes[\n                        elec_index, \"rel_z\"\n                    ]\n\n        if not device_found:\n            print(\n                f\"No electrodes in the NWB file were associated with a device named '{nwb_device_name}'.\"\n            )\n            return\n\n        # insert the Probe, then the Shank parts, and then the Electrode parts\n        cls.insert1(new_probe_dict, skip_duplicates=True)\n\n        for shank in shank_dict.values():\n            cls.Shank.insert1(shank, skip_duplicates=True)\n        for electrode in elect_dict.values():\n            cls.Electrode.insert1(electrode, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_device.Probe.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Insert probe devices from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of probe device types found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, config):\n\"\"\"Insert probe devices from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : list\n        List of probe device types found in the NWB file.\n    \"\"\"\n    all_probes_types, ndx_probes, _ = cls.get_all_probe_names(nwbf, config)\n\n    for probe_type in all_probes_types:\n        new_probe_type_dict = dict()\n        new_probe_dict = dict()\n        shank_dict = dict()\n        elect_dict = dict()\n        num_shanks = 0\n\n        if probe_type in ndx_probes:\n            # read probe properties into new_probe_dict from PyNWB extension probe object\n            nwb_probe_obj = ndx_probes[probe_type]\n            cls.__read_ndx_probe_data(\n                nwb_probe_obj,\n                new_probe_type_dict,\n                new_probe_dict,\n                shank_dict,\n                elect_dict,\n            )\n\n        # check that number of shanks is consistent\n        num_shanks = new_probe_type_dict[\"num_shanks\"]\n        assert num_shanks == 0 or num_shanks == len(\n            shank_dict\n        ), \"`num_shanks` is not equal to the number of shanks.\"\n\n        # if probe id already exists, do not overwrite anything or create new Shanks and Electrodes\n        # TODO test whether the Shanks and Electrodes in the NWB file match the ones in the database\n        query = Probe &amp; {\"probe_id\": new_probe_dict[\"probe_id\"]}\n        if len(query) &gt; 0:\n            print(\n                f\"Probe ID '{new_probe_dict['probe_id']}' already exists in the database. Spyglass will use \"\n                \"that and not create a new Probe, Shanks, or Electrodes.\"\n            )\n            continue\n\n        cls.insert1(new_probe_dict, skip_duplicates=True)\n\n        for shank in shank_dict.values():\n            cls.Shank.insert1(shank, skip_duplicates=True)\n        for electrode in elect_dict.values():\n            cls.Electrode.insert1(electrode, skip_duplicates=True)\n\n    if all_probes_types:\n        print(f\"Inserted probes {all_probes_types}\")\n    else:\n        print(\"No conforming probe metadata found.\")\n\n    return all_probes_types\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_device.Probe.get_all_probe_names", "title": "<code>get_all_probe_names(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Get a list of all device names in the NWB file, after appending and overwriting by the config file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of data acquisition object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef get_all_probe_names(cls, nwbf, config):\n\"\"\"Get a list of all device names in the NWB file, after appending and overwriting by the config file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : list\n        List of data acquisition object names found in the NWB file.\n    \"\"\"\n\n    # make a dict mapping probe type to PyNWB object for all devices in the NWB file that are\n    # of type ndx_franklab_novela.Probe and thus have the required metadata\n    ndx_probes = {\n        device_obj.probe_type: device_obj\n        for device_obj in nwbf.devices.values()\n        if isinstance(device_obj, ndx_franklab_novela.Probe)\n    }\n\n    # make a dict mapping probe type to dict of device metadata from the config YAML if exists\n    if \"Probe\" in config:\n        config_probes = [\n            probe_dict[\"probe_type\"] for probe_dict in config[\"Probe\"]\n        ]\n    else:\n        config_probes = list()\n\n    # get all the probe types from the NWB file plus the config YAML\n    all_probes_types = set(ndx_probes.keys()).union(set(config_probes))\n\n    return all_probes_types, ndx_probes, config_probes\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_device.Probe.create_from_nwbfile", "title": "<code>create_from_nwbfile(nwb_file_name, nwb_device_name, probe_id, probe_type, contact_side_numbering)</code>  <code>classmethod</code>", "text": "<p>Create a Probe entry and corresponding part table entries using the data in the NWB file.</p> <p>This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices (as probes) in the NWB file, but only ones that are associated with the device that matches the given <code>nwb_device_name</code>.</p> <p>Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe, the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.</p> <p>Example usage:</p> <pre><code>sgc.Probe.create_from_nwbfile(\n    nwbfile=nwb_file_name,\n    nwb_device_name=\"Device\",\n    probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n    probe_type=\"Neuropixels 1.0\",\n    contact_side_numbering=True\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file.</p> required <code>nwb_device_name</code> <code>str</code> <p>The name of the PyNWB Device object that represents the probe to read in the NWB file.</p> required <code>probe_id</code> <code>str</code> <p>A unique ID for the probe and its configuration, to be used as the primary key for the new Probe entry.</p> required <code>probe_type</code> <code>str</code> <p>The existing ProbeType entry that represents the type of probe being created. It must exist.</p> required <code>contact_side_numbering</code> <code>bool</code> <p>Whether the electrode contacts are facing you when numbering them. Stored in the new Probe entry.</p> required Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef create_from_nwbfile(\n    cls,\n    nwb_file_name: str,\n    nwb_device_name: str,\n    probe_id: str,\n    probe_type: str,\n    contact_side_numbering: bool,\n):\n\"\"\"Create a Probe entry and corresponding part table entries using the data in the NWB file.\n\n    This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices\n    (as probes) in the NWB file, but only ones that are associated with the device that matches the given\n    `nwb_device_name`.\n\n    Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe,\n    the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.\n\n    Example usage:\n    ```\n    sgc.Probe.create_from_nwbfile(\n        nwbfile=nwb_file_name,\n        nwb_device_name=\"Device\",\n        probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n        probe_type=\"Neuropixels 1.0\",\n        contact_side_numbering=True\n    )\n    ```\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    nwb_device_name : str\n        The name of the PyNWB Device object that represents the probe to read in the NWB file.\n    probe_id : str\n        A unique ID for the probe and its configuration, to be used as the primary key for the new Probe entry.\n    probe_type : str\n        The existing ProbeType entry that represents the type of probe being created. It must exist.\n    contact_side_numbering : bool\n        Whether the electrode contacts are facing you when numbering them. Stored in the new Probe entry.\n    \"\"\"\n\n    from .common_nwbfile import Nwbfile\n\n    nwb_file_path = Nwbfile.get_abs_path(nwb_file_name)\n    nwbfile = get_nwb_file(nwb_file_path)\n\n    query = ProbeType &amp; {\"probe_type\": probe_type}\n    if len(query) == 0:\n        print(\n            f\"No ProbeType found with probe_type '{probe_type}'. Aborting.\"\n        )\n        return\n\n    new_probe_dict = dict()\n    shank_dict = dict()\n    elect_dict = dict()\n\n    new_probe_dict[\"probe_id\"] = probe_id\n    new_probe_dict[\"probe_type\"] = probe_type\n    new_probe_dict[\"contact_side_numbering\"] = (\n        \"True\" if contact_side_numbering else \"False\"\n    )\n\n    # iterate through the electrodes table in the NWB file\n    # and use the group column (ElectrodeGroup) to create shanks\n    # and use the device attribute of each ElectrodeGroup to create a probe\n    created_shanks = dict()  # map device name to shank_index (int)\n    device_found = False\n    for elec_index in range(len(nwbfile.electrodes)):\n        electrode_group = nwbfile.electrodes[elec_index, \"group\"]\n        eg_device_name = electrode_group.device.name\n\n        # only look at electrodes where the associated device is the one specified\n        if eg_device_name == nwb_device_name:\n            device_found = True\n\n            # if a Shank has not yet been created from the electrode group, then create it\n            if electrode_group.name not in created_shanks:\n                shank_index = len(created_shanks)\n                created_shanks[electrode_group.name] = shank_index\n\n                # build the dictionary of Probe.Shank data\n                shank_dict[shank_index] = dict()\n                shank_dict[shank_index][\"probe_id\"] = new_probe_dict[\n                    \"probe_id\"\n                ]\n                shank_dict[shank_index][\"probe_shank\"] = shank_index\n\n            # get the probe shank index associated with this Electrode\n            probe_shank = created_shanks[electrode_group.name]\n\n            # build the dictionary of Probe.Electrode data\n            elect_dict[elec_index] = dict()\n            elect_dict[elec_index][\"probe_id\"] = new_probe_dict[\"probe_id\"]\n            elect_dict[elec_index][\"probe_shank\"] = probe_shank\n            elect_dict[elec_index][\"probe_electrode\"] = elec_index\n            if \"rel_x\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_x\"] = nwbfile.electrodes[\n                    elec_index, \"rel_x\"\n                ]\n            if \"rel_y\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_y\"] = nwbfile.electrodes[\n                    elec_index, \"rel_y\"\n                ]\n            if \"rel_z\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_z\"] = nwbfile.electrodes[\n                    elec_index, \"rel_z\"\n                ]\n\n    if not device_found:\n        print(\n            f\"No electrodes in the NWB file were associated with a device named '{nwb_device_name}'.\"\n        )\n        return\n\n    # insert the Probe, then the Shank parts, and then the Electrode parts\n    cls.insert1(new_probe_dict, skip_duplicates=True)\n\n    for shank in shank_dict.values():\n        cls.Shank.insert1(shank, skip_duplicates=True)\n    for electrode in elect_dict.values():\n        cls.Electrode.insert1(electrode, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.interval_list_censor", "title": "<code>interval_list_censor(interval_list, timestamps)</code>", "text": "<p>returns a new interval list that starts and ends at the first and last timestamp</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>numpy array of intervals [start, stop]</code> <p>interval list from IntervalList valid times</p> required <code>timestamps</code> <code>numpy array or list</code> required <p>Returns:</p> Type Description <code>interval_list (numpy array of intervals [start, stop])</code> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_censor(interval_list, timestamps):\n\"\"\"returns a new interval list that starts and ends at the first and last timestamp\n\n    Parameters\n    ----------\n    interval_list : numpy array of intervals [start, stop]\n        interval list from IntervalList valid times\n    timestamps : numpy array or list\n\n    Returns\n    -------\n    interval_list (numpy array of intervals [start, stop])\n    \"\"\"\n    # check that all timestamps are in the interval list\n    assert len(interval_list_contains_ind(interval_list, timestamps)) == len(\n        timestamps\n    ), \"interval_list must contain all timestamps\"\n\n    timestamps_interval = np.asarray([[timestamps[0], timestamps[-1]]])\n    return interval_list_intersect(interval_list, timestamps_interval)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.LFPBandSelection", "title": "<code>LFPBandSelection</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>@schema\nclass LFPBandSelection(dj.Manual):\n    definition = \"\"\"\n    -&gt; LFP\n    -&gt; FirFilterParameters                   # the filter to use for the data\n    -&gt; IntervalList.proj(target_interval_list_name='interval_list_name')  # the original set of times to be filtered\n    lfp_band_sampling_rate: int    # the sampling rate for this band\n    ---\n    min_interval_len = 1: float  # the minimum length of a valid interval to filter\n    \"\"\"\n\n    class LFPBandElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; LFPBandSelection\n        -&gt; LFPSelection.LFPElectrode  # the LFP electrode to be filtered\n        reference_elect_id = -1: int  # the reference electrode to use; -1 for no reference\n        ---\n        \"\"\"\n\n    def set_lfp_band_electrodes(\n        self,\n        nwb_file_name,\n        electrode_list,\n        filter_name,\n        interval_list_name,\n        reference_electrode_list,\n        lfp_band_sampling_rate,\n    ):\n\"\"\"\n        Adds an entry for each electrode in the electrode_list with the specified filter, interval_list, and\n        reference electrode.\n        Also removes any entries that have the same filter, interval list and reference electrode but are not\n        in the electrode_list.\n        :param nwb_file_name: string - the name of the nwb file for the desired session\n        :param electrode_list: list of LFP electrodes to be filtered\n        :param filter_name: the name of the filter (from the FirFilterParameters schema)\n        :param interval_name: the name of the interval list (from the IntervalList schema)\n        :param reference_electrode_list: A single electrode id corresponding to the reference to use for all\n        electrodes or a list with one element per entry in the electrode_list\n        :param lfp_band_sampling_rate: The output sampling rate to be used for the filtered data; must be an\n        integer divisor of the LFP sampling rate\n        :return: none\n        \"\"\"\n        # Error checks on parameters\n        # electrode_list\n        query = LFPSelection().LFPElectrode() &amp; {\"nwb_file_name\": nwb_file_name}\n        available_electrodes = query.fetch(\"electrode_id\")\n        if not np.all(np.isin(electrode_list, available_electrodes)):\n            raise ValueError(\n                \"All elements in electrode_list must be valid electrode_ids in the LFPSelection table\"\n            )\n        # sampling rate\n        lfp_sampling_rate = (LFP() &amp; {\"nwb_file_name\": nwb_file_name}).fetch1(\n            \"lfp_sampling_rate\"\n        )\n        decimation = lfp_sampling_rate // lfp_band_sampling_rate\n        if lfp_sampling_rate // decimation != lfp_band_sampling_rate:\n            raise ValueError(\n                f\"lfp_band_sampling rate {lfp_band_sampling_rate} is not an integer divisor of lfp \"\n                f\"samping rate {lfp_sampling_rate}\"\n            )\n        # filter\n        query = FirFilterParameters() &amp; {\n            \"filter_name\": filter_name,\n            \"filter_sampling_rate\": lfp_sampling_rate,\n        }\n        if not query:\n            raise ValueError(\n                f\"filter {filter_name}, sampling rate {lfp_sampling_rate} is not in the FirFilterParameters table\"\n            )\n        # interval_list\n        query = IntervalList() &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_name\": interval_list_name,\n        }\n        if not query:\n            raise ValueError(\n                f\"interval list {interval_list_name} is not in the IntervalList table; the list must be \"\n                \"added before this function is called\"\n            )\n        # reference_electrode_list\n        if len(reference_electrode_list) != 1 and len(\n            reference_electrode_list\n        ) != len(electrode_list):\n            raise ValueError(\n                \"reference_electrode_list must contain either 1 or len(electrode_list) elements\"\n            )\n        # add a -1 element to the list to allow for the no reference option\n        available_electrodes = np.append(available_electrodes, [-1])\n        if not np.all(np.isin(reference_electrode_list, available_electrodes)):\n            raise ValueError(\n                \"All elements in reference_electrode_list must be valid electrode_ids in the LFPSelection \"\n                \"table\"\n            )\n\n        # make a list of all the references\n        ref_list = np.zeros((len(electrode_list),))\n        ref_list[:] = reference_electrode_list\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"filter_name\"] = filter_name\n        key[\"filter_sampling_rate\"] = lfp_sampling_rate\n        key[\"target_interval_list_name\"] = interval_list_name\n        key[\"lfp_band_sampling_rate\"] = lfp_sampling_rate // decimation\n        # insert an entry into the main LFPBandSelectionTable\n        self.insert1(key, skip_duplicates=True)\n\n        # get all of the current entries and delete any that are not in the list\n        elect_id, ref_id = (self.LFPBandElectrode() &amp; key).fetch(\n            \"electrode_id\", \"reference_elect_id\"\n        )\n        for e, r in zip(elect_id, ref_id):\n            if not len(np.where((electrode_list == e) &amp; (ref_list == r))[0]):\n                key[\"electrode_id\"] = e\n                key[\"reference_elect_id\"] = r\n                (self.LFPBandElectrode() &amp; key).delete()\n\n        # iterate through all of the new elements and add them\n        for e, r in zip(electrode_list, ref_list):\n            key[\"electrode_id\"] = e\n            query = Electrode &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"electrode_id\": e,\n            }\n            key[\"electrode_group_name\"] = query.fetch1(\"electrode_group_name\")\n            key[\"reference_elect_id\"] = r\n            self.LFPBandElectrode().insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ephys/#src.spyglass.common.common_ephys.LFPBandSelection.set_lfp_band_electrodes", "title": "<code>set_lfp_band_electrodes(nwb_file_name, electrode_list, filter_name, interval_list_name, reference_electrode_list, lfp_band_sampling_rate)</code>", "text": "<p>Adds an entry for each electrode in the electrode_list with the specified filter, interval_list, and reference electrode. Also removes any entries that have the same filter, interval list and reference electrode but are not in the electrode_list. :param nwb_file_name: string - the name of the nwb file for the desired session :param electrode_list: list of LFP electrodes to be filtered :param filter_name: the name of the filter (from the FirFilterParameters schema) :param interval_name: the name of the interval list (from the IntervalList schema) :param reference_electrode_list: A single electrode id corresponding to the reference to use for all electrodes or a list with one element per entry in the electrode_list :param lfp_band_sampling_rate: The output sampling rate to be used for the filtered data; must be an integer divisor of the LFP sampling rate :return: none</p> Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>def set_lfp_band_electrodes(\n    self,\n    nwb_file_name,\n    electrode_list,\n    filter_name,\n    interval_list_name,\n    reference_electrode_list,\n    lfp_band_sampling_rate,\n):\n\"\"\"\n    Adds an entry for each electrode in the electrode_list with the specified filter, interval_list, and\n    reference electrode.\n    Also removes any entries that have the same filter, interval list and reference electrode but are not\n    in the electrode_list.\n    :param nwb_file_name: string - the name of the nwb file for the desired session\n    :param electrode_list: list of LFP electrodes to be filtered\n    :param filter_name: the name of the filter (from the FirFilterParameters schema)\n    :param interval_name: the name of the interval list (from the IntervalList schema)\n    :param reference_electrode_list: A single electrode id corresponding to the reference to use for all\n    electrodes or a list with one element per entry in the electrode_list\n    :param lfp_band_sampling_rate: The output sampling rate to be used for the filtered data; must be an\n    integer divisor of the LFP sampling rate\n    :return: none\n    \"\"\"\n    # Error checks on parameters\n    # electrode_list\n    query = LFPSelection().LFPElectrode() &amp; {\"nwb_file_name\": nwb_file_name}\n    available_electrodes = query.fetch(\"electrode_id\")\n    if not np.all(np.isin(electrode_list, available_electrodes)):\n        raise ValueError(\n            \"All elements in electrode_list must be valid electrode_ids in the LFPSelection table\"\n        )\n    # sampling rate\n    lfp_sampling_rate = (LFP() &amp; {\"nwb_file_name\": nwb_file_name}).fetch1(\n        \"lfp_sampling_rate\"\n    )\n    decimation = lfp_sampling_rate // lfp_band_sampling_rate\n    if lfp_sampling_rate // decimation != lfp_band_sampling_rate:\n        raise ValueError(\n            f\"lfp_band_sampling rate {lfp_band_sampling_rate} is not an integer divisor of lfp \"\n            f\"samping rate {lfp_sampling_rate}\"\n        )\n    # filter\n    query = FirFilterParameters() &amp; {\n        \"filter_name\": filter_name,\n        \"filter_sampling_rate\": lfp_sampling_rate,\n    }\n    if not query:\n        raise ValueError(\n            f\"filter {filter_name}, sampling rate {lfp_sampling_rate} is not in the FirFilterParameters table\"\n        )\n    # interval_list\n    query = IntervalList() &amp; {\n        \"nwb_file_name\": nwb_file_name,\n        \"interval_name\": interval_list_name,\n    }\n    if not query:\n        raise ValueError(\n            f\"interval list {interval_list_name} is not in the IntervalList table; the list must be \"\n            \"added before this function is called\"\n        )\n    # reference_electrode_list\n    if len(reference_electrode_list) != 1 and len(\n        reference_electrode_list\n    ) != len(electrode_list):\n        raise ValueError(\n            \"reference_electrode_list must contain either 1 or len(electrode_list) elements\"\n        )\n    # add a -1 element to the list to allow for the no reference option\n    available_electrodes = np.append(available_electrodes, [-1])\n    if not np.all(np.isin(reference_electrode_list, available_electrodes)):\n        raise ValueError(\n            \"All elements in reference_electrode_list must be valid electrode_ids in the LFPSelection \"\n            \"table\"\n        )\n\n    # make a list of all the references\n    ref_list = np.zeros((len(electrode_list),))\n    ref_list[:] = reference_electrode_list\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"filter_name\"] = filter_name\n    key[\"filter_sampling_rate\"] = lfp_sampling_rate\n    key[\"target_interval_list_name\"] = interval_list_name\n    key[\"lfp_band_sampling_rate\"] = lfp_sampling_rate // decimation\n    # insert an entry into the main LFPBandSelectionTable\n    self.insert1(key, skip_duplicates=True)\n\n    # get all of the current entries and delete any that are not in the list\n    elect_id, ref_id = (self.LFPBandElectrode() &amp; key).fetch(\n        \"electrode_id\", \"reference_elect_id\"\n    )\n    for e, r in zip(elect_id, ref_id):\n        if not len(np.where((electrode_list == e) &amp; (ref_list == r))[0]):\n            key[\"electrode_id\"] = e\n            key[\"reference_elect_id\"] = r\n            (self.LFPBandElectrode() &amp; key).delete()\n\n    # iterate through all of the new elements and add them\n    for e, r in zip(electrode_list, ref_list):\n        key[\"electrode_id\"] = e\n        query = Electrode &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"electrode_id\": e,\n        }\n        key[\"electrode_group_name\"] = query.fetch1(\"electrode_group_name\")\n        key[\"reference_elect_id\"] = r\n        self.LFPBandElectrode().insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_filter/", "title": "common_filter.py", "text": ""}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters", "title": "<code>FirFilterParameters</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>@schema\nclass FirFilterParameters(dj.Manual):\n    definition = \"\"\"\n    filter_name: varchar(80)           # descriptive name of this filter\n    filter_sampling_rate: int          # sampling rate for this filter\n    ---\n    filter_type: enum(\"lowpass\", \"highpass\", \"bandpass\")\n    filter_low_stop = 0: float         # lowest frequency for stop band for low frequency side of filter\n    filter_low_pass = 0: float         # lowest frequency for pass band of low frequency side of filter\n    filter_high_pass = 0: float        # highest frequency for pass band for high frequency side of filter\n    filter_high_stop = 0: float        # highest frequency for stop band of high frequency side of filter\n    filter_comments: varchar(2000)     # comments about the filter\n    filter_band_edges: blob            # numpy array containing the filter bands (redundant with individual parameters)\n    filter_coeff: longblob             # numpy array containing the filter coefficients\n    \"\"\"\n\n    def add_filter(self, filter_name, fs, filter_type, band_edges, comments=\"\"):\n        gsp = _import_ghostipy()\n\n        # add an FIR bandpass filter of the specified type ('lowpass', 'highpass', or 'bandpass').\n        # band_edges should be as follows:\n        #   low pass filter: [high_pass high_stop]\n        #   high pass filter: [low stop low pass]\n        #   band pass filter: [low_stop low_pass high_pass high_stop].\n        if filter_type not in [\"lowpass\", \"highpass\", \"bandpass\"]:\n            print(\n                \"Error in Filter.add_filter: filter type {} is not \"\n                \"lowpass\"\n                \", \"\n                \"highpass\"\n                \" or \"\n\"\"\"bandpass\"\"\".format(filter_type)\n            )\n            return None\n\n        p = 2  # transition spline will be quadratic\n        if filter_type == \"lowpass\" or filter_type == \"highpass\":\n            # check that two frequencies were passed in and that they are in the right order\n            if len(band_edges) != 2:\n                print(\n                    \"Error in Filter.add_filter: lowpass and highpass filter requires two band_frequencies\"\n                )\n                return None\n            tw = band_edges[1] - band_edges[0]\n\n        elif filter_type == \"bandpass\":\n            if len(band_edges) != 4:\n                print(\n                    \"Error in Filter.add_filter: bandpass filter requires four band_frequencies.\"\n                )\n                return None\n            # the transition width is the mean of the widths of left and right transition regions\n            tw = (\n                (band_edges[1] - band_edges[0])\n                + (band_edges[3] - band_edges[2])\n            ) / 2.0\n\n        else:\n            raise Exception(f\"Unexpected filter type: {filter_type}\")\n\n        numtaps = gsp.estimate_taps(fs, tw)\n        filterdict = dict()\n        filterdict[\"filter_name\"] = filter_name\n        filterdict[\"filter_sampling_rate\"] = fs\n        filterdict[\"filter_comments\"] = comments\n\n        # set the desired frequency response\n        if filter_type == \"lowpass\":\n            desired = [1, 0]\n            filterdict[\"filter_low_stop\"] = 0\n            filterdict[\"filter_low_pass\"] = 0\n            filterdict[\"filter_high_pass\"] = band_edges[0]\n            filterdict[\"filter_high_stop\"] = band_edges[1]\n        elif filter_type == \"highpass\":\n            desired = [0, 1]\n            filterdict[\"filter_low_stop\"] = band_edges[0]\n            filterdict[\"filter_low_pass\"] = band_edges[1]\n            filterdict[\"filter_high_pass\"] = 0\n            filterdict[\"filter_high_stop\"] = 0\n        else:\n            desired = [0, 1, 1, 0]\n            filterdict[\"filter_low_stop\"] = band_edges[0]\n            filterdict[\"filter_low_pass\"] = band_edges[1]\n            filterdict[\"filter_high_pass\"] = band_edges[2]\n            filterdict[\"filter_high_stop\"] = band_edges[3]\n        filterdict[\"filter_type\"] = filter_type\n        filterdict[\"filter_band_edges\"] = np.asarray(band_edges)\n        # create 1d array for coefficients\n        filterdict[\"filter_coeff\"] = np.array(\n            gsp.firdesign(numtaps, band_edges, desired, fs=fs, p=p), ndmin=1\n        )\n        # add this filter to the table\n        self.insert1(filterdict, skip_duplicates=True)\n\n    def plot_magnitude(self, filter_name, fs):\n        filter = (\n            self &amp; {\"filter_name\": filter_name} &amp; {\"filter_sampling_rate\": fs}\n        ).fetch(as_dict=True)\n        f = filter[0]\n        plt.figure()\n        w, h = signal.freqz(filter[0][\"filter_coeff\"], worN=65536)\n        magnitude = 20 * np.log10(np.abs(h))\n        plt.plot(w / np.pi * fs / 2, magnitude)\n        plt.xlabel(\"Frequency (Hz)\")\n        plt.ylabel(\"Magnitude\")\n        plt.title(\"Frequency Response\")\n        plt.xlim(0, np.max(f[\"filter_coeffand_edges\"] * 2))\n        plt.ylim(np.min(magnitude), -1 * np.min(magnitude) * 0.1)\n        plt.grid(True)\n\n    def plot_fir_filter(self, filter_name, fs):\n        filter = (\n            self &amp; {\"filter_name\": filter_name} &amp; {\"filter_sampling_rate\": fs}\n        ).fetch(as_dict=True)\n        f = filter[0]\n        plt.figure()\n        plt.clf()\n        plt.plot(f[\"filter_coeff\"], \"k\")\n        plt.xlabel(\"Coefficient\")\n        plt.ylabel(\"Magnitude\")\n        plt.title(\"Filter Taps\")\n        plt.grid(True)\n\n    def filter_delay(self, filter_name, fs):\n        # return the filter delay\n        filter = (\n            self &amp; {\"filter_name\": filter_name} &amp; {\"filter_sampling_rate\": fs}\n        ).fetch(as_dict=True)\n        return self.calc_filter_delay(filter[\"filter_coeff\"])\n\n    def filter_data_nwb(\n        self,\n        analysis_file_abs_path,\n        eseries,\n        filter_coeff,\n        valid_times,\n        electrode_ids,\n        decimation,\n        description: str = \"filtered data\",\n        type: Union[None, str] = None,\n    ):\n\"\"\"\n        :param analysis_nwb_file_name: str   full path to previously created analysis nwb file where filtered data\n        should be stored. This also has the name of the original NWB file where the data will be taken from\n        :param eseries: electrical series with data to be filtered\n        :param filter_coeff: numpy array with filter coefficients for FIR filter\n        :param valid_times: 2D numpy array with start and stop times of intervals to be filtered\n        :param electrode_ids: list of electrode_ids to filter\n        :param decimation: int decimation factor\n        :return: The NWB object id of the filtered data (str), list containing first and last timestamp\n\n        This function takes data and timestamps from an NWB electrical series and filters them using the ghostipy\n        package, saving the result as a new electricalseries in the nwb_file_name, which should have previously been\n        created and linked to the original NWB file using common_session.AnalysisNwbfile.create()\n        \"\"\"\n        gsp = _import_ghostipy()\n\n        data_on_disk = eseries.data\n        timestamps_on_disk = eseries.timestamps\n        n_dim = len(data_on_disk.shape)\n        n_samples = len(timestamps_on_disk)\n        # find the\n        time_axis = 0 if data_on_disk.shape[0] == n_samples else 1\n        electrode_axis = 1 - time_axis\n        n_electrodes = data_on_disk.shape[electrode_axis]\n        input_dim_restrictions = [None] * n_dim\n\n        # to get the input dimension restrictions we need to look at the electrode table for the eseries and get\n        # the indices from that\n        input_dim_restrictions[electrode_axis] = np.s_[\n            get_electrode_indices(eseries, electrode_ids)\n        ]\n\n        indices = []\n        output_shape_list = [0] * n_dim\n        output_shape_list[electrode_axis] = len(electrode_ids)\n        output_offsets = [0]\n\n        timestamp_size = timestamps_on_disk[0].itemsize\n        timestamp_dtype = timestamps_on_disk[0].dtype\n        data_size = data_on_disk[0][0].itemsize\n        data_dtype = data_on_disk[0][0].dtype\n\n        filter_delay = self.calc_filter_delay(filter_coeff)\n        for a_start, a_stop in valid_times:\n            if a_start &lt; timestamps_on_disk[0]:\n                warnings.warn(\n                    f\"Interval start time {a_start} is smaller than first timestamp {timestamps_on_disk[0]}, \"\n                    \"using first timestamp instead\"\n                )\n                a_start = timestamps_on_disk[0]\n            if a_stop &gt; timestamps_on_disk[-1]:\n                warnings.warn(\n                    f\"Interval stop time {a_stop} is larger than last timestamp {timestamps_on_disk[-1]}, \"\n                    \"using last timestamp instead\"\n                )\n                a_stop = timestamps_on_disk[-1]\n            frm, to = np.searchsorted(timestamps_on_disk, (a_start, a_stop))\n            if to &gt; n_samples:\n                to = n_samples\n            indices.append((frm, to))\n            shape, dtype = gsp.filter_data_fir(\n                data_on_disk,\n                filter_coeff,\n                axis=time_axis,\n                input_index_bounds=[frm, to - 1],\n                output_index_bounds=[filter_delay, filter_delay + to - frm],\n                describe_dims=True,\n                ds=decimation,\n                input_dim_restrictions=input_dim_restrictions,\n            )\n            output_offsets.append(output_offsets[-1] + shape[time_axis])\n            output_shape_list[time_axis] += shape[time_axis]\n\n        # open the nwb file to create the dynamic table region and electrode series, then write and close the file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"a\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the indices of the electrodes in the electrode table\n            elect_ind = get_electrode_indices(nwbf, electrode_ids)\n\n            electrode_table_region = nwbf.create_electrode_table_region(\n                elect_ind, \"filtered electrode table\"\n            )\n            eseries_name = \"filtered data\"\n            es = pynwb.ecephys.ElectricalSeries(\n                name=eseries_name,\n                data=np.empty(tuple(output_shape_list), dtype=data_dtype),\n                electrodes=electrode_table_region,\n                timestamps=np.empty(output_shape_list[time_axis]),\n                description=description,\n            )\n            if type == \"LFP\":\n                lfp = pynwb.ecephys.LFP(electrical_series=es)\n                ecephys_module = nwbf.create_processing_module(\n                    name=\"ecephys\", description=description\n                )\n                ecephys_module.add(lfp)\n            else:\n                nwbf.add_scratch(es)\n            io.write(nwbf)\n\n            # reload the NWB file to get the h5py objects for the data and the timestamps\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"a\", load_namespaces=True\n            ) as io:\n                nwbf = io.read()\n                es = nwbf.objects[es.object_id]\n                filtered_data = es.data\n                new_timestamps = es.timestamps\n                indices = np.array(indices, ndmin=2)\n                # Filter and write the output dataset\n                ts_offset = 0\n\n                print(\"Filtering data\")\n                for ii, (start, stop) in enumerate(indices):\n                    # calculate the size of the timestamps and the data and determine whether they\n                    # can be loaded into &lt; 90% of available RAM\n                    mem = psutil.virtual_memory()\n                    interval_samples = stop - start\n                    if (\n                        interval_samples\n                        * (timestamp_size + n_electrodes * data_size)\n                        &lt; 0.9 * mem.available\n                    ):\n                        print(f\"Interval {ii}: loading data into memory\")\n                        timestamps = np.asarray(\n                            timestamps_on_disk[start:stop],\n                            dtype=timestamp_dtype,\n                        )\n                        if time_axis == 0:\n                            data = np.asarray(\n                                data_on_disk[start:stop, :], dtype=data_dtype\n                            )\n                        else:\n                            data = np.asarray(\n                                data_on_disk[:, start:stop], dtype=data_dtype\n                            )\n                        extracted_ts = timestamps[0::decimation]\n                        new_timestamps[\n                            ts_offset : ts_offset + len(extracted_ts)\n                        ] = extracted_ts\n                        ts_offset += len(extracted_ts)\n                        # filter the data\n                        gsp.filter_data_fir(\n                            data,\n                            filter_coeff,\n                            axis=time_axis,\n                            input_index_bounds=[0, interval_samples - 1],\n                            output_index_bounds=[\n                                filter_delay,\n                                filter_delay + stop - start,\n                            ],\n                            ds=decimation,\n                            input_dim_restrictions=input_dim_restrictions,\n                            outarray=filtered_data,\n                            output_offset=output_offsets[ii],\n                        )\n                    else:\n                        print(f\"Interval {ii}: leaving data on disk\")\n                        data = data_on_disk\n                        timestamps = timestamps_on_disk\n                        extracted_ts = timestamps[start:stop:decimation]\n                        new_timestamps[\n                            ts_offset : ts_offset + len(extracted_ts)\n                        ] = extracted_ts\n                        ts_offset += len(extracted_ts)\n                        # filter the data\n                        gsp.filter_data_fir(\n                            data,\n                            filter_coeff,\n                            axis=time_axis,\n                            input_index_bounds=[start, stop],\n                            output_index_bounds=[\n                                filter_delay,\n                                filter_delay + stop - start,\n                            ],\n                            ds=decimation,\n                            input_dim_restrictions=input_dim_restrictions,\n                            outarray=filtered_data,\n                            output_offset=output_offsets[ii],\n                        )\n\n                start_end = [new_timestamps[0], new_timestamps[-1]]\n\n                io.write(nwbf)\n\n        return es.object_id, start_end\n\n    def filter_data(\n        self,\n        timestamps,\n        data,\n        filter_coeff,\n        valid_times,\n        electrodes,\n        decimation,\n    ):\n\"\"\"\n        :param timestamps: numpy array with list of timestamps for data\n        :param data: original data array\n        :param filter_coeff: numpy array with filter coefficients for FIR filter\n        :param valid_times: 2D numpy array with start and stop times of intervals to be filtered\n        :param electrodes: list of electrodes to filter\n        :param decimation: decimation factor\n        :return: filtered_data, timestamps\n        \"\"\"\n        gsp = _import_ghostipy()\n\n        n_dim = len(data.shape)\n        n_samples = len(timestamps)\n        time_axis = 0 if data.shape[0] == n_samples else 1\n        electrode_axis = 1 - time_axis\n        input_dim_restrictions = [None] * n_dim\n        input_dim_restrictions[electrode_axis] = np.s_[electrodes]\n\n        indices = []\n        output_shape_list = [0] * n_dim\n        output_shape_list[electrode_axis] = len(electrodes)\n        output_offsets = [0]\n\n        filter_delay = self.calc_filter_delay(filter_coeff)\n        for a_start, a_stop in valid_times:\n            if a_start &lt; timestamps[0]:\n                print(\n                    f\"Interval start time {a_start} is smaller than first timestamp \"\n                    f\"{timestamps[0]}, using first timestamp instead\"\n                )\n                a_start = timestamps[0]\n            if a_stop &gt; timestamps[-1]:\n                print(\n                    f\"Interval stop time {a_stop} is larger than last timestamp \"\n                    f\"{timestamps[-1]}, using last timestamp instead\"\n                )\n                a_stop = timestamps[-1]\n            frm, to = np.searchsorted(timestamps, (a_start, a_stop))\n            if to &gt; n_samples:\n                to = n_samples\n\n            indices.append((frm, to))\n            shape, dtype = gsp.filter_data_fir(\n                data,\n                filter_coeff,\n                axis=time_axis,\n                input_index_bounds=[frm, to],\n                output_index_bounds=[filter_delay, filter_delay + to - frm],\n                describe_dims=True,\n                ds=decimation,\n                input_dim_restrictions=input_dim_restrictions,\n            )\n            output_offsets.append(output_offsets[-1] + shape[time_axis])\n            output_shape_list[time_axis] += shape[time_axis]\n\n        # create the dataset and the timestamps array\n        filtered_data = np.empty(tuple(output_shape_list), dtype=data.dtype)\n\n        new_timestamps = np.empty(\n            (output_shape_list[time_axis],), timestamps.dtype\n        )\n\n        indices = np.array(indices, ndmin=2)\n\n        # Filter  the output dataset\n        ts_offset = 0\n\n        for ii, (start, stop) in enumerate(indices):\n            extracted_ts = timestamps[start:stop:decimation]\n\n            # print(f\"Diffs {np.diff(extracted_ts)}\")\n            new_timestamps[\n                ts_offset : ts_offset + len(extracted_ts)\n            ] = extracted_ts\n            ts_offset += len(extracted_ts)\n\n            # finally ready to filter data!\n            gsp.filter_data_fir(\n                data,\n                filter_coeff,\n                axis=time_axis,\n                input_index_bounds=[start, stop],\n                output_index_bounds=[filter_delay, filter_delay + stop - start],\n                ds=decimation,\n                input_dim_restrictions=input_dim_restrictions,\n                outarray=filtered_data,\n                output_offset=output_offsets[ii],\n            )\n\n        return filtered_data, new_timestamps\n\n    def calc_filter_delay(self, filter_coeff):\n\"\"\"\n        :param filter_coeff:\n        :return: filter delay\n        \"\"\"\n        return (len(filter_coeff) - 1) // 2\n\n    def create_standard_filters(self):\n\"\"\"Add standard filters to the Filter table including\n        0-400 Hz low pass for continuous raw data -&gt; LFP\n        \"\"\"\n        self.add_filter(\n            \"LFP 0-400 Hz\",\n            20000,\n            \"lowpass\",\n            [400, 425],\n            \"standard LFP filter for 20 KHz data\",\n        )\n        self.add_filter(\n            \"LFP 0-400 Hz\",\n            30000,\n            \"lowpass\",\n            [400, 425],\n            \"standard LFP filter for 30 KHz data\",\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters.filter_data_nwb", "title": "<code>filter_data_nwb(analysis_file_abs_path, eseries, filter_coeff, valid_times, electrode_ids, decimation, description='filtered data', type=None)</code>", "text": "<p>:param analysis_nwb_file_name: str   full path to previously created analysis nwb file where filtered data should be stored. This also has the name of the original NWB file where the data will be taken from :param eseries: electrical series with data to be filtered :param filter_coeff: numpy array with filter coefficients for FIR filter :param valid_times: 2D numpy array with start and stop times of intervals to be filtered :param electrode_ids: list of electrode_ids to filter :param decimation: int decimation factor :return: The NWB object id of the filtered data (str), list containing first and last timestamp</p> <p>This function takes data and timestamps from an NWB electrical series and filters them using the ghostipy package, saving the result as a new electricalseries in the nwb_file_name, which should have previously been created and linked to the original NWB file using common_session.AnalysisNwbfile.create()</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def filter_data_nwb(\n    self,\n    analysis_file_abs_path,\n    eseries,\n    filter_coeff,\n    valid_times,\n    electrode_ids,\n    decimation,\n    description: str = \"filtered data\",\n    type: Union[None, str] = None,\n):\n\"\"\"\n    :param analysis_nwb_file_name: str   full path to previously created analysis nwb file where filtered data\n    should be stored. This also has the name of the original NWB file where the data will be taken from\n    :param eseries: electrical series with data to be filtered\n    :param filter_coeff: numpy array with filter coefficients for FIR filter\n    :param valid_times: 2D numpy array with start and stop times of intervals to be filtered\n    :param electrode_ids: list of electrode_ids to filter\n    :param decimation: int decimation factor\n    :return: The NWB object id of the filtered data (str), list containing first and last timestamp\n\n    This function takes data and timestamps from an NWB electrical series and filters them using the ghostipy\n    package, saving the result as a new electricalseries in the nwb_file_name, which should have previously been\n    created and linked to the original NWB file using common_session.AnalysisNwbfile.create()\n    \"\"\"\n    gsp = _import_ghostipy()\n\n    data_on_disk = eseries.data\n    timestamps_on_disk = eseries.timestamps\n    n_dim = len(data_on_disk.shape)\n    n_samples = len(timestamps_on_disk)\n    # find the\n    time_axis = 0 if data_on_disk.shape[0] == n_samples else 1\n    electrode_axis = 1 - time_axis\n    n_electrodes = data_on_disk.shape[electrode_axis]\n    input_dim_restrictions = [None] * n_dim\n\n    # to get the input dimension restrictions we need to look at the electrode table for the eseries and get\n    # the indices from that\n    input_dim_restrictions[electrode_axis] = np.s_[\n        get_electrode_indices(eseries, electrode_ids)\n    ]\n\n    indices = []\n    output_shape_list = [0] * n_dim\n    output_shape_list[electrode_axis] = len(electrode_ids)\n    output_offsets = [0]\n\n    timestamp_size = timestamps_on_disk[0].itemsize\n    timestamp_dtype = timestamps_on_disk[0].dtype\n    data_size = data_on_disk[0][0].itemsize\n    data_dtype = data_on_disk[0][0].dtype\n\n    filter_delay = self.calc_filter_delay(filter_coeff)\n    for a_start, a_stop in valid_times:\n        if a_start &lt; timestamps_on_disk[0]:\n            warnings.warn(\n                f\"Interval start time {a_start} is smaller than first timestamp {timestamps_on_disk[0]}, \"\n                \"using first timestamp instead\"\n            )\n            a_start = timestamps_on_disk[0]\n        if a_stop &gt; timestamps_on_disk[-1]:\n            warnings.warn(\n                f\"Interval stop time {a_stop} is larger than last timestamp {timestamps_on_disk[-1]}, \"\n                \"using last timestamp instead\"\n            )\n            a_stop = timestamps_on_disk[-1]\n        frm, to = np.searchsorted(timestamps_on_disk, (a_start, a_stop))\n        if to &gt; n_samples:\n            to = n_samples\n        indices.append((frm, to))\n        shape, dtype = gsp.filter_data_fir(\n            data_on_disk,\n            filter_coeff,\n            axis=time_axis,\n            input_index_bounds=[frm, to - 1],\n            output_index_bounds=[filter_delay, filter_delay + to - frm],\n            describe_dims=True,\n            ds=decimation,\n            input_dim_restrictions=input_dim_restrictions,\n        )\n        output_offsets.append(output_offsets[-1] + shape[time_axis])\n        output_shape_list[time_axis] += shape[time_axis]\n\n    # open the nwb file to create the dynamic table region and electrode series, then write and close the file\n    with pynwb.NWBHDF5IO(\n        path=analysis_file_abs_path, mode=\"a\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the indices of the electrodes in the electrode table\n        elect_ind = get_electrode_indices(nwbf, electrode_ids)\n\n        electrode_table_region = nwbf.create_electrode_table_region(\n            elect_ind, \"filtered electrode table\"\n        )\n        eseries_name = \"filtered data\"\n        es = pynwb.ecephys.ElectricalSeries(\n            name=eseries_name,\n            data=np.empty(tuple(output_shape_list), dtype=data_dtype),\n            electrodes=electrode_table_region,\n            timestamps=np.empty(output_shape_list[time_axis]),\n            description=description,\n        )\n        if type == \"LFP\":\n            lfp = pynwb.ecephys.LFP(electrical_series=es)\n            ecephys_module = nwbf.create_processing_module(\n                name=\"ecephys\", description=description\n            )\n            ecephys_module.add(lfp)\n        else:\n            nwbf.add_scratch(es)\n        io.write(nwbf)\n\n        # reload the NWB file to get the h5py objects for the data and the timestamps\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"a\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            es = nwbf.objects[es.object_id]\n            filtered_data = es.data\n            new_timestamps = es.timestamps\n            indices = np.array(indices, ndmin=2)\n            # Filter and write the output dataset\n            ts_offset = 0\n\n            print(\"Filtering data\")\n            for ii, (start, stop) in enumerate(indices):\n                # calculate the size of the timestamps and the data and determine whether they\n                # can be loaded into &lt; 90% of available RAM\n                mem = psutil.virtual_memory()\n                interval_samples = stop - start\n                if (\n                    interval_samples\n                    * (timestamp_size + n_electrodes * data_size)\n                    &lt; 0.9 * mem.available\n                ):\n                    print(f\"Interval {ii}: loading data into memory\")\n                    timestamps = np.asarray(\n                        timestamps_on_disk[start:stop],\n                        dtype=timestamp_dtype,\n                    )\n                    if time_axis == 0:\n                        data = np.asarray(\n                            data_on_disk[start:stop, :], dtype=data_dtype\n                        )\n                    else:\n                        data = np.asarray(\n                            data_on_disk[:, start:stop], dtype=data_dtype\n                        )\n                    extracted_ts = timestamps[0::decimation]\n                    new_timestamps[\n                        ts_offset : ts_offset + len(extracted_ts)\n                    ] = extracted_ts\n                    ts_offset += len(extracted_ts)\n                    # filter the data\n                    gsp.filter_data_fir(\n                        data,\n                        filter_coeff,\n                        axis=time_axis,\n                        input_index_bounds=[0, interval_samples - 1],\n                        output_index_bounds=[\n                            filter_delay,\n                            filter_delay + stop - start,\n                        ],\n                        ds=decimation,\n                        input_dim_restrictions=input_dim_restrictions,\n                        outarray=filtered_data,\n                        output_offset=output_offsets[ii],\n                    )\n                else:\n                    print(f\"Interval {ii}: leaving data on disk\")\n                    data = data_on_disk\n                    timestamps = timestamps_on_disk\n                    extracted_ts = timestamps[start:stop:decimation]\n                    new_timestamps[\n                        ts_offset : ts_offset + len(extracted_ts)\n                    ] = extracted_ts\n                    ts_offset += len(extracted_ts)\n                    # filter the data\n                    gsp.filter_data_fir(\n                        data,\n                        filter_coeff,\n                        axis=time_axis,\n                        input_index_bounds=[start, stop],\n                        output_index_bounds=[\n                            filter_delay,\n                            filter_delay + stop - start,\n                        ],\n                        ds=decimation,\n                        input_dim_restrictions=input_dim_restrictions,\n                        outarray=filtered_data,\n                        output_offset=output_offsets[ii],\n                    )\n\n            start_end = [new_timestamps[0], new_timestamps[-1]]\n\n            io.write(nwbf)\n\n    return es.object_id, start_end\n</code></pre>"}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters.filter_data", "title": "<code>filter_data(timestamps, data, filter_coeff, valid_times, electrodes, decimation)</code>", "text": "<p>:param timestamps: numpy array with list of timestamps for data :param data: original data array :param filter_coeff: numpy array with filter coefficients for FIR filter :param valid_times: 2D numpy array with start and stop times of intervals to be filtered :param electrodes: list of electrodes to filter :param decimation: decimation factor :return: filtered_data, timestamps</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def filter_data(\n    self,\n    timestamps,\n    data,\n    filter_coeff,\n    valid_times,\n    electrodes,\n    decimation,\n):\n\"\"\"\n    :param timestamps: numpy array with list of timestamps for data\n    :param data: original data array\n    :param filter_coeff: numpy array with filter coefficients for FIR filter\n    :param valid_times: 2D numpy array with start and stop times of intervals to be filtered\n    :param electrodes: list of electrodes to filter\n    :param decimation: decimation factor\n    :return: filtered_data, timestamps\n    \"\"\"\n    gsp = _import_ghostipy()\n\n    n_dim = len(data.shape)\n    n_samples = len(timestamps)\n    time_axis = 0 if data.shape[0] == n_samples else 1\n    electrode_axis = 1 - time_axis\n    input_dim_restrictions = [None] * n_dim\n    input_dim_restrictions[electrode_axis] = np.s_[electrodes]\n\n    indices = []\n    output_shape_list = [0] * n_dim\n    output_shape_list[electrode_axis] = len(electrodes)\n    output_offsets = [0]\n\n    filter_delay = self.calc_filter_delay(filter_coeff)\n    for a_start, a_stop in valid_times:\n        if a_start &lt; timestamps[0]:\n            print(\n                f\"Interval start time {a_start} is smaller than first timestamp \"\n                f\"{timestamps[0]}, using first timestamp instead\"\n            )\n            a_start = timestamps[0]\n        if a_stop &gt; timestamps[-1]:\n            print(\n                f\"Interval stop time {a_stop} is larger than last timestamp \"\n                f\"{timestamps[-1]}, using last timestamp instead\"\n            )\n            a_stop = timestamps[-1]\n        frm, to = np.searchsorted(timestamps, (a_start, a_stop))\n        if to &gt; n_samples:\n            to = n_samples\n\n        indices.append((frm, to))\n        shape, dtype = gsp.filter_data_fir(\n            data,\n            filter_coeff,\n            axis=time_axis,\n            input_index_bounds=[frm, to],\n            output_index_bounds=[filter_delay, filter_delay + to - frm],\n            describe_dims=True,\n            ds=decimation,\n            input_dim_restrictions=input_dim_restrictions,\n        )\n        output_offsets.append(output_offsets[-1] + shape[time_axis])\n        output_shape_list[time_axis] += shape[time_axis]\n\n    # create the dataset and the timestamps array\n    filtered_data = np.empty(tuple(output_shape_list), dtype=data.dtype)\n\n    new_timestamps = np.empty(\n        (output_shape_list[time_axis],), timestamps.dtype\n    )\n\n    indices = np.array(indices, ndmin=2)\n\n    # Filter  the output dataset\n    ts_offset = 0\n\n    for ii, (start, stop) in enumerate(indices):\n        extracted_ts = timestamps[start:stop:decimation]\n\n        # print(f\"Diffs {np.diff(extracted_ts)}\")\n        new_timestamps[\n            ts_offset : ts_offset + len(extracted_ts)\n        ] = extracted_ts\n        ts_offset += len(extracted_ts)\n\n        # finally ready to filter data!\n        gsp.filter_data_fir(\n            data,\n            filter_coeff,\n            axis=time_axis,\n            input_index_bounds=[start, stop],\n            output_index_bounds=[filter_delay, filter_delay + stop - start],\n            ds=decimation,\n            input_dim_restrictions=input_dim_restrictions,\n            outarray=filtered_data,\n            output_offset=output_offsets[ii],\n        )\n\n    return filtered_data, new_timestamps\n</code></pre>"}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters.calc_filter_delay", "title": "<code>calc_filter_delay(filter_coeff)</code>", "text": "<p>:param filter_coeff: :return: filter delay</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def calc_filter_delay(self, filter_coeff):\n\"\"\"\n    :param filter_coeff:\n    :return: filter delay\n    \"\"\"\n    return (len(filter_coeff) - 1) // 2\n</code></pre>"}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.FirFilterParameters.create_standard_filters", "title": "<code>create_standard_filters()</code>", "text": "<p>Add standard filters to the Filter table including 0-400 Hz low pass for continuous raw data -&gt; LFP</p> Source code in <code>src/spyglass/common/common_filter.py</code> <pre><code>def create_standard_filters(self):\n\"\"\"Add standard filters to the Filter table including\n    0-400 Hz low pass for continuous raw data -&gt; LFP\n    \"\"\"\n    self.add_filter(\n        \"LFP 0-400 Hz\",\n        20000,\n        \"lowpass\",\n        [400, 425],\n        \"standard LFP filter for 20 KHz data\",\n    )\n    self.add_filter(\n        \"LFP 0-400 Hz\",\n        30000,\n        \"lowpass\",\n        [400, 425],\n        \"standard LFP filter for 30 KHz data\",\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_filter/#src.spyglass.common.common_filter.get_electrode_indices", "title": "<code>get_electrode_indices(nwb_object, electrode_ids)</code>", "text": "<p>Given an NWB file or electrical series object, return the indices of the specified electrode_ids.</p> <p>If an ElectricalSeries is given, then the indices returned are relative to the selected rows in ElectricalSeries.electrodes. For example, if electricalseries.electrodes = [5], and row index 5 of nwbfile.electrodes has ID 10, then calling get_electrode_indices(electricalseries, 10) will return 0, the index of the matching electrode in electricalseries.electrodes.</p> <p>Indices for electrode_ids that are not in the electrical series are returned as np.nan</p> <p>If an NWBFile is given, then the row indices with the matching IDs in the file's electrodes table are returned.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_object</code> <code>pynwb.NWBFile or pynwb.ecephys.ElectricalSeries</code> <p>The NWB file object or NWB electrical series object.</p> required <code>electrode_ids</code> <code>np.ndarray or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>list</code> <p>Array of indices of the specified electrode IDs.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_electrode_indices(nwb_object, electrode_ids):\n\"\"\"Given an NWB file or electrical series object, return the indices of the specified electrode_ids.\n\n    If an ElectricalSeries is given, then the indices returned are relative to the selected rows in\n    ElectricalSeries.electrodes. For example, if electricalseries.electrodes = [5], and row index 5 of\n    nwbfile.electrodes has ID 10, then calling get_electrode_indices(electricalseries, 10) will return 0, the\n    index of the matching electrode in electricalseries.electrodes.\n\n    Indices for electrode_ids that are not in the electrical series are returned as np.nan\n\n    If an NWBFile is given, then the row indices with the matching IDs in the file's electrodes table are returned.\n\n    Parameters\n    ----------\n    nwb_object : pynwb.NWBFile or pynwb.ecephys.ElectricalSeries\n        The NWB file object or NWB electrical series object.\n    electrode_ids : np.ndarray or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : list\n        Array of indices of the specified electrode IDs.\n    \"\"\"\n    if isinstance(nwb_object, pynwb.ecephys.ElectricalSeries):\n        # electrodes is a DynamicTableRegion which may contain a subset of the rows in NWBFile.electrodes\n        # match against only the subset of electrodes referenced by this ElectricalSeries\n        electrode_table_indices = nwb_object.electrodes.data[:]\n        selected_elect_ids = [\n            nwb_object.electrodes.table.id[x] for x in electrode_table_indices\n        ]\n    elif isinstance(nwb_object, pynwb.NWBFile):\n        # electrodes is a DynamicTable that contains all electrodes\n        selected_elect_ids = list(nwb_object.electrodes.id[:])\n    else:\n        raise ValueError(\n            \"nwb_object must be of type ElectricalSeries or NWBFile\"\n        )\n\n    # for each electrode_id, find its index in selected_elect_ids and return that if it's there and invalid_electrode_index if not.\n    return [\n        selected_elect_ids.index(elect_id)\n        if elect_id in selected_elect_ids\n        else invalid_electrode_index\n        for elect_id in electrode_ids\n    ]\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/", "title": "common_interval.py", "text": ""}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.IntervalList", "title": "<code>IntervalList</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@schema\nclass IntervalList(dj.Manual):\n    definition = \"\"\"\n    # Time intervals used for analysis\n    -&gt; Session\n    interval_list_name: varchar(200)  # descriptive name of this interval list\n    ---\n    valid_times: longblob  # numpy array with start and end times for each interval\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n        The interval list name for each epoch is set to the first tag for the epoch.\n        If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n        (0-indexed) of the epoch in the epochs table.\n        The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n        [start time, stop time] for each epoch.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        nwb_file_name : str\n            The file name of the NWB file, used as a primary key to the Session table.\n        \"\"\"\n        if nwbf.epochs is None:\n            print(\"No epochs found in NWB file.\")\n            return\n        epochs = nwbf.epochs.to_dataframe()\n        for epoch_index, epoch_data in epochs.iterrows():\n            epoch_dict = dict()\n            epoch_dict[\"nwb_file_name\"] = nwb_file_name\n            if epoch_data.tags[0]:\n                epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n            else:\n                epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                    epoch_index\n                )\n            epoch_dict[\"valid_times\"] = np.asarray(\n                [[epoch_data.start_time, epoch_data.stop_time]]\n            )\n            cls.insert1(epoch_dict, skip_duplicates=True)\n\n    def plot_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=figsize)\n        interval_count = 0\n        for row in interval_list.itertuples(index=False):\n            for interval in row.valid_times:\n                ax.plot(interval, [interval_count, interval_count])\n                ax.scatter(\n                    interval,\n                    [interval_count, interval_count],\n                    alpha=0.8,\n                    zorder=2,\n                )\n            interval_count += 1\n        ax.set_yticks(np.arange(interval_list.shape[0]))\n        ax.set_yticklabels(interval_list.interval_list_name)\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n\n    def plot_epoch_pos_raw_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=(30, 3))\n\n        raw_data_valid_times = interval_list.loc[\n            interval_list.interval_list_name == \"raw data valid times\"\n        ].valid_times\n        interval_y = 1\n\n        for interval in np.asarray(raw_data_valid_times)[0]:\n            ax.plot(interval, [interval_y, interval_y])\n            ax.scatter(interval, [interval_y, interval_y], alpha=0.8, zorder=2)\n\n        epoch_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n        interval_y = 2\n        for epoch, valid_times in zip(\n            epoch_valid_times.index, epoch_valid_times\n        ):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        pos_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^pos \\d+ valid times$\", axis=0)\n            .valid_times\n        ).sort_index(key=lambda index: [int(name.split()[1]) for name in index])\n        interval_y = 0\n        for epoch, valid_times in zip(pos_valid_times.index, pos_valid_times):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch.strip(\" valid times\"),\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        ax.set_ylim((-0.25, 2.25))\n        ax.set_yticks(np.arange(3))\n        ax.set_yticklabels([\"pos valid times\", \"raw data valid times\", \"epoch\"])\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.IntervalList.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, *, nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add each entry in the NWB file epochs table to the IntervalList table.</p> <p>The interval list name for each epoch is set to the first tag for the epoch. If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index (0-indexed) of the epoch in the epochs table. The start time and stop time of the epoch are stored in the valid_times field as a numpy array of [start time, stop time] for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>nwb_file_name</code> <code>str</code> <p>The file name of the NWB file, used as a primary key to the Session table.</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n    The interval list name for each epoch is set to the first tag for the epoch.\n    If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n    (0-indexed) of the epoch in the epochs table.\n    The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n    [start time, stop time] for each epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    nwb_file_name : str\n        The file name of the NWB file, used as a primary key to the Session table.\n    \"\"\"\n    if nwbf.epochs is None:\n        print(\"No epochs found in NWB file.\")\n        return\n    epochs = nwbf.epochs.to_dataframe()\n    for epoch_index, epoch_data in epochs.iterrows():\n        epoch_dict = dict()\n        epoch_dict[\"nwb_file_name\"] = nwb_file_name\n        if epoch_data.tags[0]:\n            epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n        else:\n            epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                epoch_index\n            )\n        epoch_dict[\"valid_times\"] = np.asarray(\n            [[epoch_data.start_time, epoch_data.stop_time]]\n        )\n        cls.insert1(epoch_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.intervals_by_length", "title": "<code>intervals_by_length(interval_list, min_length=0.0, max_length=10000000000.0)</code>", "text": "<p>Select intervals of certain lengths from an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval. Unit is seconds.</p> required <code>min_length</code> <code>float</code> <p>Minimum interval length in seconds. Defaults to 0.0.</p> <code>0.0</code> <code>max_length</code> <code>float</code> <p>Maximum interval length in seconds. Defaults to 1e10.</p> <code>10000000000.0</code> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def intervals_by_length(interval_list, min_length=0.0, max_length=1e10):\n\"\"\"Select intervals of certain lengths from an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval. Unit is seconds.\n    min_length : float, optional\n        Minimum interval length in seconds. Defaults to 0.0.\n    max_length : float, optional\n        Maximum interval length in seconds. Defaults to 1e10.\n    \"\"\"\n    lengths = np.ravel(np.diff(interval_list))\n    return interval_list[\n        np.logical_and(lengths &gt; min_length, lengths &lt; max_length)\n    ]\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_contains_ind", "title": "<code>interval_list_contains_ind(interval_list, timestamps)</code>", "text": "<p>Find indices of a list of timestamps that are contained in an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval. Unit is seconds.</p> required <code>timestamps</code> <code>array_like</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_contains_ind(interval_list, timestamps):\n\"\"\"Find indices of a list of timestamps that are contained in an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval. Unit is seconds.\n    timestamps : array_like\n    \"\"\"\n    ind = []\n    for interval in interval_list:\n        ind += np.ravel(\n            np.argwhere(\n                np.logical_and(\n                    timestamps &gt;= interval[0], timestamps &lt;= interval[1]\n                )\n            )\n        ).tolist()\n    return np.asarray(ind)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_contains", "title": "<code>interval_list_contains(interval_list, timestamps)</code>", "text": "<p>Find timestamps that are contained in an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval. Unit is seconds.</p> required <code>timestamps</code> <code>array_like</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_contains(interval_list, timestamps):\n\"\"\"Find timestamps that are contained in an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval. Unit is seconds.\n    timestamps : array_like\n    \"\"\"\n    ind = []\n    for interval in interval_list:\n        ind += np.ravel(\n            np.argwhere(\n                np.logical_and(\n                    timestamps &gt;= interval[0], timestamps &lt;= interval[1]\n                )\n            )\n        ).tolist()\n    return timestamps[ind]\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_excludes_ind", "title": "<code>interval_list_excludes_ind(interval_list, timestamps)</code>", "text": "<p>Find indices of a list of timestamps that are not contained in an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval. Unit is seconds.</p> required <code>timestamps</code> <code>array_like</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_excludes_ind(interval_list, timestamps):\n\"\"\"Find indices of a list of timestamps that are not contained in an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval. Unit is seconds.\n    timestamps : array_like\n    \"\"\"\n\n    contained_inds = interval_list_contains_ind(interval_list, timestamps)\n    return np.setdiff1d(np.arange(len(timestamps)), contained_inds)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_excludes", "title": "<code>interval_list_excludes(interval_list, timestamps)</code>", "text": "<p>Find timestamps that are not contained in an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval. Unit is seconds.</p> required <code>timestamps</code> <code>array_like</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_excludes(interval_list, timestamps):\n\"\"\"Find timestamps that are not contained in an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval. Unit is seconds.\n    timestamps : array_like\n    \"\"\"\n    contained_times = interval_list_contains(interval_list, timestamps)\n    return np.setdiff1d(timestamps, contained_times)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_intersect", "title": "<code>interval_list_intersect(interval_list1, interval_list2, min_length=0)</code>", "text": "<p>Finds the intersections between two interval lists</p> <p>Parameters:</p> Name Type Description Default <code>interval_list1</code> <code>np.array, (N,2) where N = number of intervals</code> required <code>interval_list2</code> <code>np.array, (N,2) where N = number of intervals</code> required <code>min_length</code> <code>float, optional.</code> <p>Minimum length of intervals to include, default 0</p> <code>0</code> <p>Each interval is (start time, stop time)</p> <p>Returns:</p> Name Type Description <code>interval_list</code> <code>np.array, N, 2</code> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_intersect(interval_list1, interval_list2, min_length=0):\n\"\"\"Finds the intersections between two interval lists\n\n    Parameters\n    ----------\n    interval_list1 : np.array, (N,2) where N = number of intervals\n    interval_list2 : np.array, (N,2) where N = number of intervals\n    min_length : float, optional.\n        Minimum length of intervals to include, default 0\n\n    Each interval is (start time, stop time)\n\n    Returns\n    -------\n    interval_list: np.array, (N,2)\n    \"\"\"\n\n    # first, consolidate interval lists to disjoint intervals by sorting and applying union\n    if interval_list1.ndim == 1:\n        interval_list1 = np.expand_dims(interval_list1, 0)\n    else:\n        interval_list1 = interval_list1[np.argsort(interval_list1[:, 0])]\n        interval_list1 = reduce(_union_concat, interval_list1)\n        # the following check is needed in the case where the interval list is a single element (behavior of reduce)\n        if interval_list1.ndim == 1:\n            interval_list1 = np.expand_dims(interval_list1, 0)\n\n    if interval_list2.ndim == 1:\n        interval_list2 = np.expand_dims(interval_list2, 0)\n    else:\n        interval_list2 = interval_list2[np.argsort(interval_list2[:, 0])]\n        interval_list2 = reduce(_union_concat, interval_list2)\n        # the following check is needed in the case where the interval list is a single element (behavior of reduce)\n        if interval_list2.ndim == 1:\n            interval_list2 = np.expand_dims(interval_list2, 0)\n\n    # then do pairwise comparison and collect intersections\n    intersecting_intervals = []\n    for interval2 in interval_list2:\n        for interval1 in interval_list1:\n            if _intersection(interval2, interval1) is not None:\n                intersecting_intervals.append(\n                    _intersection(interval1, interval2)\n                )\n\n    # if no intersection, then return an empty list\n    if not intersecting_intervals:\n        return []\n    else:\n        intersecting_intervals = np.asarray(intersecting_intervals)\n        intersecting_intervals = intersecting_intervals[\n            np.argsort(intersecting_intervals[:, 0])\n        ]\n\n        return intervals_by_length(\n            intersecting_intervals, min_length=min_length\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.union_adjacent_index", "title": "<code>union_adjacent_index(interval1, interval2)</code>", "text": "<p>unions two intervals that are adjacent in index e.g. [a,b] and [b+1, c] is converted to [a,c] if not adjacent, just concatenates interval2 at the end of interval1</p> <p>Parameters:</p> Name Type Description Default <code>interval1</code> <code>np.array</code> <p>[description]</p> required <code>interval2</code> <code>np.array</code> <p>[description]</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def union_adjacent_index(interval1, interval2):\n\"\"\"unions two intervals that are adjacent in index\n    e.g. [a,b] and [b+1, c] is converted to [a,c]\n    if not adjacent, just concatenates interval2 at the end of interval1\n\n    Parameters\n    ----------\n    interval1 : np.array\n        [description]\n    interval2 : np.array\n        [description]\n    \"\"\"\n    if interval1.ndim == 1:\n        interval1 = np.expand_dims(interval1, 0)\n    if interval2.ndim == 1:\n        interval2 = np.expand_dims(interval2, 0)\n\n    if (\n        interval1[-1][1] + 1 == interval2[0][0]\n        or interval2[0][1] + 1 == interval1[-1][0]\n    ):\n        x = np.array(\n            [\n                [\n                    np.min([interval1[-1][0], interval2[0][0]]),\n                    np.max([interval1[-1][1], interval2[0][1]]),\n                ]\n            ]\n        )\n        return np.concatenate((interval1[:-1], x), axis=0)\n    else:\n        return np.concatenate((interval1, interval2), axis=0)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_union", "title": "<code>interval_list_union(interval_list1, interval_list2, min_length=0.0, max_length=10000000000.0)</code>", "text": "<p>Finds the union (all times in one or both) for two interval lists</p> <p>:param interval_list1: The first interval list :type interval_list1: numpy array of intervals [start, stop] :param interval_list2: The second interval list :type interval_list2: numpy array of intervals [start, stop] :param min_length: optional minimum length of interval for inclusion in output, default 0.0 :type min_length: float :param max_length: optional maximum length of interval for inclusion in output, default 1e10 :type max_length: float :return: interval_list :rtype:  numpy array of intervals [start, stop]</p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_union(\n    interval_list1, interval_list2, min_length=0.0, max_length=1e10\n):\n\"\"\"Finds the union (all times in one or both) for two interval lists\n\n    :param interval_list1: The first interval list\n    :type interval_list1: numpy array of intervals [start, stop]\n    :param interval_list2: The second interval list\n    :type interval_list2: numpy array of intervals [start, stop]\n    :param min_length: optional minimum length of interval for inclusion in output, default 0.0\n    :type min_length: float\n    :param max_length: optional maximum length of interval for inclusion in output, default 1e10\n    :type max_length: float\n    :return: interval_list\n    :rtype:  numpy array of intervals [start, stop]\n    \"\"\"\n    # return np.array([min(interval_list1[0],interval_list2[0]),\n    #                  max(interval_list1[1],interval_list2[1])])\n    interval_list1 = np.ravel(interval_list1)\n    # create a parallel list where 1 indicates the start and -1 the end of an interval\n    interval_list1_start_end = np.ones(interval_list1.shape)\n    interval_list1_start_end[1::2] = -1\n\n    interval_list2 = np.ravel(interval_list2)\n    # create a parallel list for the second interval where 1 indicates the start and -1 the end of an interval\n    interval_list2_start_end = np.ones(interval_list2.shape)\n    interval_list2_start_end[1::2] = -1\n\n    # concatenate the two lists so we can resort the intervals and apply the same sorting to the start-end arrays\n    combined_intervals = np.concatenate((interval_list1, interval_list2))\n    ss = np.concatenate((interval_list1_start_end, interval_list2_start_end))\n    sort_ind = np.argsort(combined_intervals)\n    combined_intervals = combined_intervals[sort_ind]\n    # a cumulative sum of 1 indicates the beginning of a joint interval; a cumulative sum of 0 indicates the end\n    union_starts = np.ravel(np.array(np.where(np.cumsum(ss[sort_ind]) == 1)))\n    union_stops = np.ravel(np.array(np.where(np.cumsum(ss[sort_ind]) == 0)))\n    union = []\n    for start, stop in zip(union_starts, union_stops):\n        union.append([combined_intervals[start], combined_intervals[stop]])\n    return np.asarray(union)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_list_censor", "title": "<code>interval_list_censor(interval_list, timestamps)</code>", "text": "<p>returns a new interval list that starts and ends at the first and last timestamp</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>numpy array of intervals [start, stop]</code> <p>interval list from IntervalList valid times</p> required <code>timestamps</code> <code>numpy array or list</code> required <p>Returns:</p> Type Description <code>interval_list (numpy array of intervals [start, stop])</code> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_censor(interval_list, timestamps):\n\"\"\"returns a new interval list that starts and ends at the first and last timestamp\n\n    Parameters\n    ----------\n    interval_list : numpy array of intervals [start, stop]\n        interval list from IntervalList valid times\n    timestamps : numpy array or list\n\n    Returns\n    -------\n    interval_list (numpy array of intervals [start, stop])\n    \"\"\"\n    # check that all timestamps are in the interval list\n    assert len(interval_list_contains_ind(interval_list, timestamps)) == len(\n        timestamps\n    ), \"interval_list must contain all timestamps\"\n\n    timestamps_interval = np.asarray([[timestamps[0], timestamps[-1]]])\n    return interval_list_intersect(interval_list, timestamps_interval)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_from_inds", "title": "<code>interval_from_inds(list_frames)</code>", "text": "<p>Converts a list of indices to a list of intervals. e.g. [2,3,4,6,7,8,9,10] -&gt; [[2,4],[6,10]]</p> <p>Parameters:</p> Name Type Description Default <code>list_frames</code> <code>array_like of int</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_from_inds(list_frames):\n\"\"\"Converts a list of indices to a list of intervals.\n    e.g. [2,3,4,6,7,8,9,10] -&gt; [[2,4],[6,10]]\n\n    Parameters\n    ----------\n    list_frames : array_like of int\n    \"\"\"\n    list_frames = np.unique(list_frames)\n    interval_list = []\n    for key, group in itertools.groupby(\n        enumerate(list_frames), lambda t: t[1] - t[0]\n    ):\n        group = list(group)\n        interval_list.append([group[0][1], group[-1][1]])\n    return np.asarray(interval_list)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_interval/#src.spyglass.common.common_interval.interval_set_difference_inds", "title": "<code>interval_set_difference_inds(intervals1, intervals2)</code>", "text": "<p>e.g. intervals1 = [(0, 5), (8, 10)] intervals2 = [(1, 2), (3, 4), (6, 9)]</p> <p>result = [(0, 1), (4, 5), (9, 10)]</p> <p>Parameters:</p> Name Type Description Default <code>intervals1</code> <code>_type_</code> <p>description</p> required <code>intervals2</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Type Description <code>_type_</code> <p>description</p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_set_difference_inds(intervals1, intervals2):\n\"\"\"\n    e.g.\n    intervals1 = [(0, 5), (8, 10)]\n    intervals2 = [(1, 2), (3, 4), (6, 9)]\n\n    result = [(0, 1), (4, 5), (9, 10)]\n\n    Parameters\n    ----------\n    intervals1 : _type_\n        _description_\n    intervals2 : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    result = []\n    i = j = 0\n    while i &lt; len(intervals1) and j &lt; len(intervals2):\n        if intervals1[i][1] &lt;= intervals2[j][0]:\n            result.append(intervals1[i])\n            i += 1\n        elif intervals2[j][1] &lt;= intervals1[i][0]:\n            j += 1\n        else:\n            if intervals1[i][0] &lt; intervals2[j][0]:\n                result.append((intervals1[i][0], intervals2[j][0]))\n            if intervals1[i][1] &gt; intervals2[j][1]:\n                intervals1[i] = (intervals2[j][1], intervals1[i][1])\n                j += 1\n            else:\n                i += 1\n    result += intervals1[i:]\n    return result\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/", "title": "common_lab.py", "text": "<p>Schema for institution name, lab team, lab name, and lab members (independent of a session).</p>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.LabMember", "title": "<code>LabMember</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass LabMember(dj.Manual):\n    definition = \"\"\"\n    lab_member_name: varchar(80)\n    ---\n    first_name: varchar(200)\n    last_name: varchar(200)\n    \"\"\"\n\n    # NOTE that names must be unique here. If there are two neuroscientists named Jack Black that have data in this\n    # database, this will create an incorrect linkage. NWB does not yet provide unique IDs for names.\n\n    class LabMemberInfo(dj.Part):\n        definition = \"\"\"\n        # Information about lab member in the context of Frank lab network\n        -&gt; LabMember\n        ---\n        google_user_name: varchar(200)              # used for permission to curate\n        datajoint_user_name = \"\": varchar(200)      # used for permission to delete entries\n        \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab member information from an NWB file.\n\n        Parameters\n        ----------\n        nwbf: pynwb.NWBFile\n            The NWB file with experimenter information.\n        \"\"\"\n        if nwbf.experimenter is None:\n            print(\"No experimenter metadata found.\\n\")\n            return\n        for experimenter in nwbf.experimenter:\n            cls.insert_from_name(experimenter)\n            # each person is by default the member of their own LabTeam (same as their name)\n            LabTeam.create_new_team(\n                team_name=experimenter, team_members=[experimenter]\n            )\n\n    @classmethod\n    def insert_from_name(cls, full_name):\n\"\"\"Insert a lab member by name.\n\n        The first name is the part of the name that precedes the last space, and the last name is the part of the\n        name that follows the last space.\n\n        Parameters\n        ----------\n        full_name : str\n            The name to be added.\n        \"\"\"\n        labmember_dict = dict()\n        labmember_dict[\"lab_member_name\"] = full_name\n        full_name_split = str.split(full_name)\n        labmember_dict[\"first_name\"] = \" \".join(full_name_split[:-1])\n        labmember_dict[\"last_name\"] = full_name_split[-1]\n        cls.insert1(labmember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.LabMember.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert lab member information from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <p>The NWB file with experimenter information.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab member information from an NWB file.\n\n    Parameters\n    ----------\n    nwbf: pynwb.NWBFile\n        The NWB file with experimenter information.\n    \"\"\"\n    if nwbf.experimenter is None:\n        print(\"No experimenter metadata found.\\n\")\n        return\n    for experimenter in nwbf.experimenter:\n        cls.insert_from_name(experimenter)\n        # each person is by default the member of their own LabTeam (same as their name)\n        LabTeam.create_new_team(\n            team_name=experimenter, team_members=[experimenter]\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.LabMember.insert_from_name", "title": "<code>insert_from_name(full_name)</code>  <code>classmethod</code>", "text": "<p>Insert a lab member by name.</p> <p>The first name is the part of the name that precedes the last space, and the last name is the part of the name that follows the last space.</p> <p>Parameters:</p> Name Type Description Default <code>full_name</code> <code>str</code> <p>The name to be added.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_name(cls, full_name):\n\"\"\"Insert a lab member by name.\n\n    The first name is the part of the name that precedes the last space, and the last name is the part of the\n    name that follows the last space.\n\n    Parameters\n    ----------\n    full_name : str\n        The name to be added.\n    \"\"\"\n    labmember_dict = dict()\n    labmember_dict[\"lab_member_name\"] = full_name\n    full_name_split = str.split(full_name)\n    labmember_dict[\"first_name\"] = \" \".join(full_name_split[:-1])\n    labmember_dict[\"last_name\"] = full_name_split[-1]\n    cls.insert1(labmember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.LabTeam", "title": "<code>LabTeam</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass LabTeam(dj.Manual):\n    definition = \"\"\"\n    team_name: varchar(80)\n    ---\n    team_description = \"\": varchar(2000)\n    \"\"\"\n\n    class LabTeamMember(dj.Part):\n        definition = \"\"\"\n        -&gt; LabTeam\n        -&gt; LabMember\n        \"\"\"\n\n    @classmethod\n    def create_new_team(\n        cls, team_name: str, team_members: list, team_description: str = \"\"\n    ):\n\"\"\"Create a new team with a list of team members.\n\n        If the lab member does not exist in the database, they will be added.\n\n        Parameters\n        ----------\n        team_name : str\n            The name of the team.\n        team_members : str\n            The full names of the lab members that are part of the team.\n        team_description: str\n            The description of the team.\n        \"\"\"\n        labteam_dict = dict()\n        labteam_dict[\"team_name\"] = team_name\n        labteam_dict[\"team_description\"] = team_description\n        cls.insert1(labteam_dict, skip_duplicates=True)\n\n        for team_member in team_members:\n            LabMember.insert_from_name(team_member)\n            query = (\n                LabMember.LabMemberInfo() &amp; {\"lab_member_name\": team_member}\n            ).fetch(\"google_user_name\")\n            if not len(query):\n                print(\n                    f\"Please add the Google user ID for {team_member} in the LabMember.LabMemberInfo table \"\n                    \"if you want to give them permission to manually curate sorting by this team.\"\n                )\n            labteammember_dict = dict()\n            labteammember_dict[\"team_name\"] = team_name\n            labteammember_dict[\"lab_member_name\"] = team_member\n            cls.LabTeamMember.insert1(labteammember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.LabTeam.create_new_team", "title": "<code>create_new_team(team_name, team_members, team_description='')</code>  <code>classmethod</code>", "text": "<p>Create a new team with a list of team members.</p> <p>If the lab member does not exist in the database, they will be added.</p> <p>Parameters:</p> Name Type Description Default <code>team_name</code> <code>str</code> <p>The name of the team.</p> required <code>team_members</code> <code>str</code> <p>The full names of the lab members that are part of the team.</p> required <code>team_description</code> <code>str</code> <p>The description of the team.</p> <code>''</code> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef create_new_team(\n    cls, team_name: str, team_members: list, team_description: str = \"\"\n):\n\"\"\"Create a new team with a list of team members.\n\n    If the lab member does not exist in the database, they will be added.\n\n    Parameters\n    ----------\n    team_name : str\n        The name of the team.\n    team_members : str\n        The full names of the lab members that are part of the team.\n    team_description: str\n        The description of the team.\n    \"\"\"\n    labteam_dict = dict()\n    labteam_dict[\"team_name\"] = team_name\n    labteam_dict[\"team_description\"] = team_description\n    cls.insert1(labteam_dict, skip_duplicates=True)\n\n    for team_member in team_members:\n        LabMember.insert_from_name(team_member)\n        query = (\n            LabMember.LabMemberInfo() &amp; {\"lab_member_name\": team_member}\n        ).fetch(\"google_user_name\")\n        if not len(query):\n            print(\n                f\"Please add the Google user ID for {team_member} in the LabMember.LabMemberInfo table \"\n                \"if you want to give them permission to manually curate sorting by this team.\"\n            )\n        labteammember_dict = dict()\n        labteammember_dict[\"team_name\"] = team_name\n        labteammember_dict[\"lab_member_name\"] = team_member\n        cls.LabTeamMember.insert1(labteammember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.Institution", "title": "<code>Institution</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass Institution(dj.Manual):\n    definition = \"\"\"\n    institution_name: varchar(80)\n    ---\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert institution information from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The NWB file with institution information.\n        \"\"\"\n        if nwbf.institution is None:\n            print(\"No institution metadata found.\\n\")\n            return\n        cls.insert1(\n            dict(institution_name=nwbf.institution), skip_duplicates=True\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.Institution.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert institution information from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The NWB file with institution information.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert institution information from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The NWB file with institution information.\n    \"\"\"\n    if nwbf.institution is None:\n        print(\"No institution metadata found.\\n\")\n        return\n    cls.insert1(\n        dict(institution_name=nwbf.institution), skip_duplicates=True\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.Lab", "title": "<code>Lab</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass Lab(dj.Manual):\n    definition = \"\"\"\n    lab_name: varchar(80)\n    ---\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab name information from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The NWB file with lab name information.\n        \"\"\"\n        if nwbf.lab is None:\n            print(\"No lab metadata found.\\n\")\n            return\n        cls.insert1(dict(lab_name=nwbf.lab), skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_lab/#src.spyglass.common.common_lab.Lab.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert lab name information from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The NWB file with lab name information.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab name information from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The NWB file with lab name information.\n    \"\"\"\n    if nwbf.lab is None:\n        print(\"No lab metadata found.\\n\")\n        return\n    cls.insert1(dict(lab_name=nwbf.lab), skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/", "title": "common_nwbfile.py", "text": ""}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.get_nwb_file", "title": "<code>get_nwb_file(nwb_file_path)</code>", "text": "<p>Return an NWBFile object with the given file path in read mode.    If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Path to the NWB file.</p> required <p>Returns:</p> Name Type Description <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>NWB file object for the given path opened in read mode.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_file(nwb_file_path):\n\"\"\"Return an NWBFile object with the given file path in read mode.\n       If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.\n\n    Parameters\n    ----------\n    nwb_file_path : str\n        Path to the NWB file.\n\n    Returns\n    -------\n    nwbfile : pynwb.NWBFile\n        NWB file object for the given path opened in read mode.\n    \"\"\"\n    _, nwbfile = __open_nwb_files.get(nwb_file_path, (None, None))\n    nwb_uri = None\n    nwb_raw_uri = None\n    if nwbfile is None:\n        # check to see if the file exists\n        if not os.path.exists(nwb_file_path):\n            print(\n                f\"NWB file {nwb_file_path} does not exist locally; checking kachery\"\n            )\n            # first try the analysis files\n            from ..sharing.sharing_kachery import AnalysisNwbfileKachery\n\n            # the download functions assume just the filename, so we need to get that from the path\n            if not AnalysisNwbfileKachery.download_file(\n                os.path.basename(nwb_file_path)\n            ):\n                return None\n        # now open the file\n        io = pynwb.NWBHDF5IO(\n            path=nwb_file_path, mode=\"r\", load_namespaces=True\n        )  # keep file open\n        nwbfile = io.read()\n        __open_nwb_files[nwb_file_path] = (io, nwbfile)\n\n    return nwbfile\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.Nwbfile", "title": "<code>Nwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass Nwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files.\n    nwb_file_name: varchar(255)   # name of the NWB file\n    ---\n    nwb_file_abs_path: filepath@raw\n    INDEX (nwb_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    @classmethod\n    def insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The relative path to the NWB file.\n        \"\"\"\n        nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n        assert os.path.exists(\n            nwb_file_abs_path\n        ), f\"File does not exist: {nwb_file_abs_path}\"\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n        cls.insert1(key, skip_duplicates=True)\n\n    @staticmethod\n    def get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n        Returns\n        -------\n        nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n        nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n        return str(nwb_file_abspath)\n\n    @staticmethod\n    def add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n        The NWB_LOCK_FILE environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n        \"\"\"\n        key = {\"nwb_file_name\": nwb_file_name}\n        # check to make sure the file exists\n        assert (\n            len((Nwbfile() &amp; key).fetch()) &gt; 0\n        ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n        lock_file.write(f\"{nwb_file_name}\\n\")\n        lock_file.close()\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        This does not delete the files themselves unless delete_files=True is specified\n        Run this after deleting the Nwbfile() entries themselves.\n        \"\"\"\n        schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.Nwbfile.insert_from_relative_file_name", "title": "<code>insert_from_relative_file_name(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Insert a new session from an existing NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The relative path to the NWB file.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The relative path to the NWB file.\n    \"\"\"\n    nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n    assert os.path.exists(\n        nwb_file_abs_path\n    ), f\"File does not exist: {nwb_file_abs_path}\"\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n    cls.insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.Nwbfile.get_abs_path", "title": "<code>get_abs_path(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored raw NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required <p>Returns:</p> Name Type Description <code>nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n    Returns\n    -------\n    nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n    nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n    return str(nwb_file_abspath)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.Nwbfile.add_to_lock", "title": "<code>add_to_lock(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Add the specified NWB file to the file with the list of NWB files to be locked.</p> <p>The NWB_LOCK_FILE environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n    The NWB_LOCK_FILE environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n    \"\"\"\n    key = {\"nwb_file_name\": nwb_file_name}\n    # check to make sure the file exists\n    assert (\n        len((Nwbfile() &amp; key).fetch()) &gt; 0\n    ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n    lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n    lock_file.write(f\"{nwb_file_name}\\n\")\n    lock_file.close()\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.Nwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>This does not delete the files themselves unless delete_files=True is specified Run this after deleting the Nwbfile() entries themselves.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    This does not delete the files themselves unless delete_files=True is specified\n    Run this after deleting the Nwbfile() entries themselves.\n    \"\"\"\n    schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_nwbfile/#src.spyglass.common.common_nwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(nwb_object, electrode_ids)</code>", "text": "<p>Given an NWB file or electrical series object, return the indices of the specified electrode_ids.</p> <p>If an ElectricalSeries is given, then the indices returned are relative to the selected rows in ElectricalSeries.electrodes. For example, if electricalseries.electrodes = [5], and row index 5 of nwbfile.electrodes has ID 10, then calling get_electrode_indices(electricalseries, 10) will return 0, the index of the matching electrode in electricalseries.electrodes.</p> <p>Indices for electrode_ids that are not in the electrical series are returned as np.nan</p> <p>If an NWBFile is given, then the row indices with the matching IDs in the file's electrodes table are returned.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_object</code> <code>pynwb.NWBFile or pynwb.ecephys.ElectricalSeries</code> <p>The NWB file object or NWB electrical series object.</p> required <code>electrode_ids</code> <code>np.ndarray or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>list</code> <p>Array of indices of the specified electrode IDs.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_electrode_indices(nwb_object, electrode_ids):\n\"\"\"Given an NWB file or electrical series object, return the indices of the specified electrode_ids.\n\n    If an ElectricalSeries is given, then the indices returned are relative to the selected rows in\n    ElectricalSeries.electrodes. For example, if electricalseries.electrodes = [5], and row index 5 of\n    nwbfile.electrodes has ID 10, then calling get_electrode_indices(electricalseries, 10) will return 0, the\n    index of the matching electrode in electricalseries.electrodes.\n\n    Indices for electrode_ids that are not in the electrical series are returned as np.nan\n\n    If an NWBFile is given, then the row indices with the matching IDs in the file's electrodes table are returned.\n\n    Parameters\n    ----------\n    nwb_object : pynwb.NWBFile or pynwb.ecephys.ElectricalSeries\n        The NWB file object or NWB electrical series object.\n    electrode_ids : np.ndarray or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : list\n        Array of indices of the specified electrode IDs.\n    \"\"\"\n    if isinstance(nwb_object, pynwb.ecephys.ElectricalSeries):\n        # electrodes is a DynamicTableRegion which may contain a subset of the rows in NWBFile.electrodes\n        # match against only the subset of electrodes referenced by this ElectricalSeries\n        electrode_table_indices = nwb_object.electrodes.data[:]\n        selected_elect_ids = [\n            nwb_object.electrodes.table.id[x] for x in electrode_table_indices\n        ]\n    elif isinstance(nwb_object, pynwb.NWBFile):\n        # electrodes is a DynamicTable that contains all electrodes\n        selected_elect_ids = list(nwb_object.electrodes.id[:])\n    else:\n        raise ValueError(\n            \"nwb_object must be of type ElectricalSeries or NWBFile\"\n        )\n\n    # for each electrode_id, find its index in selected_elect_ids and return that if it's there and invalid_electrode_index if not.\n    return [\n        selected_elect_ids.index(elect_id)\n        if elect_id in selected_elect_ids\n        else invalid_electrode_index\n        for elect_id in electrode_ids\n    ]\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/", "title": "common_position.py", "text": ""}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.IntervalList", "title": "<code>IntervalList</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@schema\nclass IntervalList(dj.Manual):\n    definition = \"\"\"\n    # Time intervals used for analysis\n    -&gt; Session\n    interval_list_name: varchar(200)  # descriptive name of this interval list\n    ---\n    valid_times: longblob  # numpy array with start and end times for each interval\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n        The interval list name for each epoch is set to the first tag for the epoch.\n        If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n        (0-indexed) of the epoch in the epochs table.\n        The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n        [start time, stop time] for each epoch.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        nwb_file_name : str\n            The file name of the NWB file, used as a primary key to the Session table.\n        \"\"\"\n        if nwbf.epochs is None:\n            print(\"No epochs found in NWB file.\")\n            return\n        epochs = nwbf.epochs.to_dataframe()\n        for epoch_index, epoch_data in epochs.iterrows():\n            epoch_dict = dict()\n            epoch_dict[\"nwb_file_name\"] = nwb_file_name\n            if epoch_data.tags[0]:\n                epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n            else:\n                epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                    epoch_index\n                )\n            epoch_dict[\"valid_times\"] = np.asarray(\n                [[epoch_data.start_time, epoch_data.stop_time]]\n            )\n            cls.insert1(epoch_dict, skip_duplicates=True)\n\n    def plot_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=figsize)\n        interval_count = 0\n        for row in interval_list.itertuples(index=False):\n            for interval in row.valid_times:\n                ax.plot(interval, [interval_count, interval_count])\n                ax.scatter(\n                    interval,\n                    [interval_count, interval_count],\n                    alpha=0.8,\n                    zorder=2,\n                )\n            interval_count += 1\n        ax.set_yticks(np.arange(interval_list.shape[0]))\n        ax.set_yticklabels(interval_list.interval_list_name)\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n\n    def plot_epoch_pos_raw_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=(30, 3))\n\n        raw_data_valid_times = interval_list.loc[\n            interval_list.interval_list_name == \"raw data valid times\"\n        ].valid_times\n        interval_y = 1\n\n        for interval in np.asarray(raw_data_valid_times)[0]:\n            ax.plot(interval, [interval_y, interval_y])\n            ax.scatter(interval, [interval_y, interval_y], alpha=0.8, zorder=2)\n\n        epoch_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n        interval_y = 2\n        for epoch, valid_times in zip(\n            epoch_valid_times.index, epoch_valid_times\n        ):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        pos_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^pos \\d+ valid times$\", axis=0)\n            .valid_times\n        ).sort_index(key=lambda index: [int(name.split()[1]) for name in index])\n        interval_y = 0\n        for epoch, valid_times in zip(pos_valid_times.index, pos_valid_times):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch.strip(\" valid times\"),\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        ax.set_ylim((-0.25, 2.25))\n        ax.set_yticks(np.arange(3))\n        ax.set_yticklabels([\"pos valid times\", \"raw data valid times\", \"epoch\"])\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_interval.IntervalList.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, *, nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add each entry in the NWB file epochs table to the IntervalList table.</p> <p>The interval list name for each epoch is set to the first tag for the epoch. If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index (0-indexed) of the epoch in the epochs table. The start time and stop time of the epoch are stored in the valid_times field as a numpy array of [start time, stop time] for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>nwb_file_name</code> <code>str</code> <p>The file name of the NWB file, used as a primary key to the Session table.</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n    The interval list name for each epoch is set to the first tag for the epoch.\n    If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n    (0-indexed) of the epoch in the epochs table.\n    The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n    [start time, stop time] for each epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    nwb_file_name : str\n        The file name of the NWB file, used as a primary key to the Session table.\n    \"\"\"\n    if nwbf.epochs is None:\n        print(\"No epochs found in NWB file.\")\n        return\n    epochs = nwbf.epochs.to_dataframe()\n    for epoch_index, epoch_data in epochs.iterrows():\n        epoch_dict = dict()\n        epoch_dict[\"nwb_file_name\"] = nwb_file_name\n        if epoch_data.tags[0]:\n            epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n        else:\n            epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                epoch_index\n            )\n        epoch_dict[\"valid_times\"] = np.asarray(\n            [[epoch_data.start_time, epoch_data.stop_time]]\n        )\n        cls.insert1(epoch_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.PositionInfoParameters", "title": "<code>PositionInfoParameters</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameters for extracting the smoothed head position, oriention and velocity.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass PositionInfoParameters(dj.Lookup):\n\"\"\"Parameters for extracting the smoothed head position, oriention and\n    velocity.\"\"\"\n\n    definition = \"\"\"\n    position_info_param_name : varchar(80) # name for this set of parameters\n    ---\n    max_separation = 9.0  : float   # max distance (in cm) between head LEDs\n    max_speed = 300.0     : float   # max speed (in cm / s) of animal\n    position_smoothing_duration = 0.125 : float # size of moving window (in seconds)\n    speed_smoothing_std_dev = 0.100 : float # smoothing standard deviation (in seconds)\n    head_orient_smoothing_std_dev = 0.001 : float # smoothing std deviation (in seconds)\n    led1_is_front = 1 : int # first LED is front LED and second is back LED, else first LED is back\n    is_upsampled = 0 : int # upsample the position to higher sampling rate\n    upsampling_sampling_rate = NULL : float # The rate to be upsampled to\n    upsampling_interpolation_method = linear : varchar(80) # see pandas.DataFrame.interpolation for list of methods\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.IntervalPositionInfoSelection", "title": "<code>IntervalPositionInfoSelection</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Combines the parameters for position extraction and a time interval to extract the smoothed position on.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass IntervalPositionInfoSelection(dj.Lookup):\n\"\"\"Combines the parameters for position extraction and a time interval to\n    extract the smoothed position on.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionInfoParameters\n    -&gt; IntervalList\n    ---\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.IntervalPositionInfo", "title": "<code>IntervalPositionInfo</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Computes the smoothed head position, orientation and velocity for a given interval.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass IntervalPositionInfo(dj.Computed):\n\"\"\"Computes the smoothed head position, orientation and velocity for a given\n    interval.\"\"\"\n\n    definition = \"\"\"\n    -&gt; IntervalPositionInfoSelection\n    ---\n    -&gt; AnalysisNwbfile\n    head_position_object_id : varchar(40)\n    head_orientation_object_id : varchar(40)\n    head_velocity_object_id : varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        print(f\"Computing position for: {key}\")\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        raw_position = (\n            RawPosition()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n            }\n        ).fetch_nwb()[0]\n        position_info_parameters = (\n            PositionInfoParameters()\n            &amp; {\"position_info_param_name\": key[\"position_info_param_name\"]}\n        ).fetch1()\n\n        head_position = pynwb.behavior.Position()\n        head_orientation = pynwb.behavior.CompassDirection()\n        head_velocity = pynwb.behavior.BehavioralTimeSeries()\n\n        METERS_PER_CM = 0.01\n\n        try:\n            # calculate the processed position\n            spatial_series = raw_position[\"raw_position\"]\n            position_info = self.calculate_position_info_from_spatial_series(\n                spatial_series,\n                position_info_parameters[\"max_separation\"],\n                position_info_parameters[\"max_speed\"],\n                position_info_parameters[\"speed_smoothing_std_dev\"],\n                position_info_parameters[\"position_smoothing_duration\"],\n                position_info_parameters[\"head_orient_smoothing_std_dev\"],\n                position_info_parameters[\"led1_is_front\"],\n                position_info_parameters[\"is_upsampled\"],\n                position_info_parameters[\"upsampling_sampling_rate\"],\n                position_info_parameters[\"upsampling_interpolation_method\"],\n            )\n\n            # create nwb objects for insertion into analysis nwb file\n            head_position.create_spatial_series(\n                name=\"head_position\",\n                timestamps=position_info[\"time\"],\n                conversion=METERS_PER_CM,\n                data=position_info[\"head_position\"],\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"head_x_position, head_y_position\",\n            )\n\n            head_orientation.create_spatial_series(\n                name=\"head_orientation\",\n                timestamps=position_info[\"time\"],\n                conversion=1.0,\n                data=position_info[\"head_orientation\"],\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"head_orientation\",\n            )\n\n            head_velocity.create_timeseries(\n                name=\"head_velocity\",\n                timestamps=position_info[\"time\"],\n                conversion=METERS_PER_CM,\n                unit=\"m/s\",\n                data=np.concatenate(\n                    (\n                        position_info[\"velocity\"],\n                        position_info[\"speed\"][:, np.newaxis],\n                    ),\n                    axis=1,\n                ),\n                comments=spatial_series.comments,\n                description=\"head_x_velocity, head_y_velocity, head_speed\",\n            )\n        except ValueError as e:\n            print(e)\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n\n        key[\"head_position_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], head_position\n        )\n        key[\"head_orientation_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], head_orientation\n        )\n        key[\"head_velocity_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], head_velocity\n        )\n\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], key[\"analysis_file_name\"])\n\n        self.insert1(key)\n\n    @staticmethod\n    def calculate_position_info_from_spatial_series(\n        spatial_series,\n        max_LED_separation,\n        max_plausible_speed,\n        speed_smoothing_std_dev,\n        position_smoothing_duration,\n        head_orient_smoothing_std_dev,\n        led1_is_front,\n        is_upsampled,\n        upsampling_sampling_rate,\n        upsampling_interpolation_method,\n    ):\n        CM_TO_METERS = 100\n\n        # Get spatial series properties\n        time = np.asarray(spatial_series.timestamps)  # seconds\n        position = np.asarray(\n            pd.DataFrame(\n                spatial_series.data,\n                columns=spatial_series.description.split(\", \"),\n            ).loc[:, [\"xloc\", \"yloc\", \"xloc2\", \"yloc2\"]]\n        )  # meters\n\n        # remove NaN times\n        is_nan_time = np.isnan(time)\n        position = position[~is_nan_time]\n        time = time[~is_nan_time]\n\n        dt = np.median(np.diff(time))\n        sampling_rate = 1 / dt\n        meters_to_pixels = spatial_series.conversion\n\n        # Define LEDs\n        if led1_is_front:\n            front_LED = position[:, [0, 1]].astype(float)\n            back_LED = position[:, [2, 3]].astype(float)\n        else:\n            back_LED = position[:, [0, 1]].astype(float)\n            front_LED = position[:, [2, 3]].astype(float)\n\n        # Convert to cm\n        back_LED *= meters_to_pixels * CM_TO_METERS\n        front_LED *= meters_to_pixels * CM_TO_METERS\n\n        # Set points to NaN where the front and back LEDs are too separated\n        dist_between_LEDs = get_distance(back_LED, front_LED)\n        is_too_separated = dist_between_LEDs &gt;= max_LED_separation\n\n        back_LED[is_too_separated] = np.nan\n        front_LED[is_too_separated] = np.nan\n\n        # Calculate speed\n        front_LED_speed = get_speed(\n            front_LED,\n            time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )\n        back_LED_speed = get_speed(\n            back_LED,\n            time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )\n\n        # Set to points to NaN where the speed is too fast\n        is_too_fast = (front_LED_speed &gt; max_plausible_speed) | (\n            back_LED_speed &gt; max_plausible_speed\n        )\n        back_LED[is_too_fast] = np.nan\n        front_LED[is_too_fast] = np.nan\n\n        # Interpolate the NaN points\n        back_LED = interpolate_nan(back_LED)\n        front_LED = interpolate_nan(front_LED)\n\n        # Smooth\n        moving_average_window = int(position_smoothing_duration * sampling_rate)\n        back_LED = bottleneck.move_mean(\n            back_LED, window=moving_average_window, axis=0, min_count=1\n        )\n        front_LED = bottleneck.move_mean(\n            front_LED, window=moving_average_window, axis=0, min_count=1\n        )\n\n        if is_upsampled:\n            position_df = pd.DataFrame(\n                {\n                    \"time\": time,\n                    \"back_LED_x\": back_LED[:, 0],\n                    \"back_LED_y\": back_LED[:, 1],\n                    \"front_LED_x\": front_LED[:, 0],\n                    \"front_LED_y\": front_LED[:, 1],\n                }\n            ).set_index(\"time\")\n\n            upsampling_start_time = time[0]\n            upsampling_end_time = time[-1]\n\n            n_samples = (\n                int(\n                    np.ceil(\n                        (upsampling_end_time - upsampling_start_time)\n                        * upsampling_sampling_rate\n                    )\n                )\n                + 1\n            )\n            new_time = np.linspace(\n                upsampling_start_time, upsampling_end_time, n_samples\n            )\n            new_index = pd.Index(\n                np.unique(np.concatenate((position_df.index, new_time))),\n                name=\"time\",\n            )\n            position_df = (\n                position_df.reindex(index=new_index)\n                .interpolate(method=upsampling_interpolation_method)\n                .reindex(index=new_time)\n            )\n\n            time = np.asarray(position_df.index)\n            back_LED = np.asarray(\n                position_df.loc[:, [\"back_LED_x\", \"back_LED_y\"]]\n            )\n            front_LED = np.asarray(\n                position_df.loc[:, [\"front_LED_x\", \"front_LED_y\"]]\n            )\n\n            sampling_rate = upsampling_sampling_rate\n\n        # Calculate head position, head orientation, velocity, speed\n        head_position = get_centriod(back_LED, front_LED)  # cm\n\n        head_orientation = get_angle(back_LED, front_LED)  # radians\n        is_nan = np.isnan(head_orientation)\n\n        # Unwrap orientation before smoothing\n        head_orientation[~is_nan] = np.unwrap(head_orientation[~is_nan])\n        head_orientation[~is_nan] = gaussian_smooth(\n            head_orientation[~is_nan],\n            head_orient_smoothing_std_dev,\n            sampling_rate,\n            axis=0,\n            truncate=8,\n        )\n        # convert back to between -pi and pi\n        head_orientation[~is_nan] = np.angle(\n            np.exp(1j * head_orientation[~is_nan])\n        )\n\n        velocity = get_velocity(\n            head_position,\n            time=time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )  # cm/s\n        speed = np.sqrt(np.sum(velocity**2, axis=1))  # cm/s\n\n        return {\n            \"time\": time,\n            \"head_position\": head_position,\n            \"head_orientation\": head_orientation,\n            \"velocity\": velocity,\n            \"speed\": speed,\n        }\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(\n                nwb_data[\"head_position\"].get_spatial_series().timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"head_position_x\",\n            \"head_position_y\",\n            \"head_orientation\",\n            \"head_velocity_x\",\n            \"head_velocity_y\",\n            \"head_speed\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"head_position\"].get_spatial_series().data\n                    ),\n                    np.asarray(\n                        nwb_data[\"head_orientation\"].get_spatial_series().data\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"head_velocity\"]\n                        .time_series[\"head_velocity\"]\n                        .data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.RawPosition", "title": "<code>RawPosition</code>", "text": "<p>         Bases: <code>dj.Imported</code></p>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.RawPosition--notes", "title": "Notes", "text": "<p>The position timestamps come from: .pos_cameraHWSync.dat. If PTP is not used, the position timestamps are inferred by finding the closest timestamps from the neural recording via the trodes time.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass RawPosition(dj.Imported):\n\"\"\"\n\n    Notes\n    -----\n    The position timestamps come from: .pos_cameraHWSync.dat.\n    If PTP is not used, the position timestamps are inferred by finding the\n    closest timestamps from the neural recording via the trodes time.\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionSource\n    ---\n    raw_position_object_id: varchar(40)    # the object id of the spatial series for this epoch in the NWB file\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        # TODO refactor this. this calculates sampling rate (unused here) and is expensive to do twice\n        pos_dict = get_all_spatial_series(nwbf)\n        for epoch in pos_dict:\n            if key[\n                \"interval_list_name\"\n            ] == PositionSource.get_pos_interval_name(epoch):\n                pdict = pos_dict[epoch]\n                key[\"raw_position_object_id\"] = pdict[\"raw_position_object_id\"]\n                self.insert1(key)\n                break\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    def fetch1_dataframe(self):\n        raw_position_nwb = self.fetch_nwb()[0][\"raw_position\"]\n        return pd.DataFrame(\n            data=raw_position_nwb.data,\n            index=pd.Index(raw_position_nwb.timestamps, name=\"time\"),\n            columns=raw_position_nwb.description.split(\", \"),\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.VideoFile", "title": "<code>VideoFile</code>", "text": "<p>         Bases: <code>dj.Imported</code></p>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.VideoFile--notes", "title": "Notes", "text": "<p>The video timestamps come from: videoTimeStamps.cameraHWSync if PTP is used. If PTP is not used, the video timestamps come from videoTimeStamps.cameraHWFrameCount .</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass VideoFile(dj.Imported):\n\"\"\"\n\n    Notes\n    -----\n    The video timestamps come from: videoTimeStamps.cameraHWSync if PTP is used.\n    If PTP is not used, the video timestamps come from videoTimeStamps.cameraHWFrameCount .\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; TaskEpoch\n    video_file_num = 0: int\n    ---\n    camera_name: varchar(80)\n    video_file_object_id: varchar(40)  # the object id of the file object\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        videos = get_data_interface(\n            nwbf, \"video\", pynwb.behavior.BehavioralEvents\n        )\n\n        if videos is None:\n            print(f\"No video data interface found in {nwb_file_name}\\n\")\n            return\n        else:\n            videos = videos.time_series\n\n        # get the interval for the current TaskEpoch\n        interval_list_name = (TaskEpoch() &amp; key).fetch1(\"interval_list_name\")\n        valid_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n\n        is_found = False\n        for ind, video in enumerate(videos.values()):\n            if isinstance(video, pynwb.image.ImageSeries):\n                video = [video]\n            for video_obj in video:\n                # check to see if the times for this video_object are largely overlapping with the task epoch times\n                if len(\n                    interval_list_contains(valid_times, video_obj.timestamps)\n                    &gt; 0.9 * len(video_obj.timestamps)\n                ):\n                    key[\"video_file_num\"] = ind\n                    camera_name = video_obj.device.camera_name\n                    if CameraDevice &amp; {\"camera_name\": camera_name}:\n                        key[\"camera_name\"] = video_obj.device.camera_name\n                    else:\n                        raise KeyError(\n                            f\"No camera with camera_name: {camera_name} found in CameraDevice table.\"\n                        )\n                    key[\"video_file_object_id\"] = video_obj.object_id\n                    self.insert1(key)\n                    is_found = True\n\n        if not is_found:\n            print(f\"No video found corresponding to epoch {interval_list_name}\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    @classmethod\n    def update_entries(cls, restrict={}):\n        existing_entries = (cls &amp; restrict).fetch(\"KEY\")\n        for row in existing_entries:\n            if (cls &amp; row).fetch1(\"camera_name\"):\n                continue\n            video_nwb = (cls &amp; row).fetch_nwb()[0]\n            if len(video_nwb) != 1:\n                raise ValueError(\n                    f\"expecting 1 video file per entry, but {len(video_nwb)} files found\"\n                )\n            row[\"camera_name\"] = video_nwb[0][\"video_file\"].device.camera_name\n            cls.update1(row=row)\n\n    @classmethod\n    def get_abs_path(cls, key: Dict):\n\"\"\"Return the absolute path for a stored video file given a key with the nwb_file_name and epoch number\n\n        The SPYGLASS_VIDEO_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        key : dict\n            dictionary with nwb_file_name and epoch as keys\n\n        Returns\n        -------\n        nwb_video_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        video_dir = pathlib.Path(os.getenv(\"SPYGLASS_VIDEO_DIR\", None))\n        assert video_dir is not None, \"You must set SPYGLASS_VIDEO_DIR\"\n        if not video_dir.exists():\n            raise OSError(\"SPYGLASS_VIDEO_DIR does not exist\")\n        video_info = (cls &amp; key).fetch1()\n        nwb_path = Nwbfile.get_abs_path(key[\"nwb_file_name\"])\n        nwbf = get_nwb_file(nwb_path)\n        nwb_video = nwbf.objects[video_info[\"video_file_object_id\"]]\n        video_filename = nwb_video.name\n        # see if the file exists and is stored in the base analysis dir\n        nwb_video_file_abspath = pathlib.Path(\n            f\"{video_dir}/{pathlib.Path(video_filename)}\"\n        )\n        if nwb_video_file_abspath.exists():\n            return nwb_video_file_abspath.as_posix()\n        else:\n            raise FileNotFoundError(\n                f\"video file with filename: {video_filename} \"\n                f\"does not exist in {video_dir}/\"\n            )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_behav.VideoFile.get_abs_path", "title": "<code>get_abs_path(key)</code>  <code>classmethod</code>", "text": "<p>Return the absolute path for a stored video file given a key with the nwb_file_name and epoch number</p> <p>The SPYGLASS_VIDEO_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>dictionary with nwb_file_name and epoch as keys</p> required <p>Returns:</p> Name Type Description <code>nwb_video_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@classmethod\ndef get_abs_path(cls, key: Dict):\n\"\"\"Return the absolute path for a stored video file given a key with the nwb_file_name and epoch number\n\n    The SPYGLASS_VIDEO_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    key : dict\n        dictionary with nwb_file_name and epoch as keys\n\n    Returns\n    -------\n    nwb_video_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    video_dir = pathlib.Path(os.getenv(\"SPYGLASS_VIDEO_DIR\", None))\n    assert video_dir is not None, \"You must set SPYGLASS_VIDEO_DIR\"\n    if not video_dir.exists():\n        raise OSError(\"SPYGLASS_VIDEO_DIR does not exist\")\n    video_info = (cls &amp; key).fetch1()\n    nwb_path = Nwbfile.get_abs_path(key[\"nwb_file_name\"])\n    nwbf = get_nwb_file(nwb_path)\n    nwb_video = nwbf.objects[video_info[\"video_file_object_id\"]]\n    video_filename = nwb_video.name\n    # see if the file exists and is stored in the base analysis dir\n    nwb_video_file_abspath = pathlib.Path(\n        f\"{video_dir}/{pathlib.Path(video_filename)}\"\n    )\n    if nwb_video_file_abspath.exists():\n        return nwb_video_file_abspath.as_posix()\n    else:\n        raise FileNotFoundError(\n            f\"video file with filename: {video_filename} \"\n            f\"does not exist in {video_dir}/\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.LinearizationParameters", "title": "<code>LinearizationParameters</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Choose whether to use an HMM to linearize position. This can help when the eucledian distances between separate arms are too close and the previous position has some information about which arm the animal is on.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass LinearizationParameters(dj.Lookup):\n\"\"\"Choose whether to use an HMM to linearize position. This can help when\n    the eucledian distances between separate arms are too close and the previous\n    position has some information about which arm the animal is on.\"\"\"\n\n    definition = \"\"\"\n    linearization_param_name : varchar(80)   # name for this set of parameters\n    ---\n    use_hmm = 0 : int   # use HMM to determine linearization\n    # How much to prefer route distances between successive time points that are closer to the euclidean distance. Smaller numbers mean the route distance is more likely to be close to the euclidean distance.\n    route_euclidean_distance_scaling = 1.0 : float\n    sensor_std_dev = 5.0 : float   # Uncertainty of position sensor (in cm).\n    # Biases the transition matrix to prefer the current track segment.\n    diagonal_bias = 0.5 : float\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.TrackGraph", "title": "<code>TrackGraph</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Graph representation of track representing the spatial environment. Used for linearizing position.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass TrackGraph(dj.Manual):\n\"\"\"Graph representation of track representing the spatial environment.\n    Used for linearizing position.\"\"\"\n\n    definition = \"\"\"\n    track_graph_name : varchar(80)\n    ----\n    environment : varchar(80)    # Type of Environment\n    node_positions : blob  # 2D position of track_graph nodes, shape (n_nodes, 2)\n    edges: blob                  # shape (n_edges, 2)\n    linear_edge_order : blob  # order of track graph edges in the linear space, shape (n_edges, 2)\n    linear_edge_spacing : blob  # amount of space between edges in the linear space, shape (n_edges,)\n    \"\"\"\n\n    def get_networkx_track_graph(self, track_graph_parameters=None):\n        if track_graph_parameters is None:\n            track_graph_parameters = self.fetch1()\n        return make_track_graph(\n            node_positions=track_graph_parameters[\"node_positions\"],\n            edges=track_graph_parameters[\"edges\"],\n        )\n\n    def plot_track_graph(self, ax=None, draw_edge_labels=False, **kwds):\n\"\"\"Plot the track graph in 2D position space.\"\"\"\n        track_graph = self.get_networkx_track_graph()\n        plot_track_graph(\n            track_graph, ax=ax, draw_edge_labels=draw_edge_labels, **kwds\n        )\n\n    def plot_track_graph_as_1D(\n        self,\n        ax=None,\n        axis=\"x\",\n        other_axis_start=0.0,\n        draw_edge_labels=False,\n        node_size=300,\n        node_color=\"#1f77b4\",\n    ):\n\"\"\"Plot the track graph in 1D to see how the linearization is set up.\"\"\"\n        track_graph_parameters = self.fetch1()\n        track_graph = self.get_networkx_track_graph(\n            track_graph_parameters=track_graph_parameters\n        )\n        plot_graph_as_1D(\n            track_graph,\n            edge_order=track_graph_parameters[\"linear_edge_order\"],\n            edge_spacing=track_graph_parameters[\"linear_edge_spacing\"],\n            ax=ax,\n            axis=axis,\n            other_axis_start=other_axis_start,\n            draw_edge_labels=draw_edge_labels,\n            node_size=node_size,\n            node_color=node_color,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.TrackGraph.plot_track_graph", "title": "<code>plot_track_graph(ax=None, draw_edge_labels=False, **kwds)</code>", "text": "<p>Plot the track graph in 2D position space.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>def plot_track_graph(self, ax=None, draw_edge_labels=False, **kwds):\n\"\"\"Plot the track graph in 2D position space.\"\"\"\n    track_graph = self.get_networkx_track_graph()\n    plot_track_graph(\n        track_graph, ax=ax, draw_edge_labels=draw_edge_labels, **kwds\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.TrackGraph.plot_track_graph_as_1D", "title": "<code>plot_track_graph_as_1D(ax=None, axis='x', other_axis_start=0.0, draw_edge_labels=False, node_size=300, node_color='#1f77b4')</code>", "text": "<p>Plot the track graph in 1D to see how the linearization is set up.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>def plot_track_graph_as_1D(\n    self,\n    ax=None,\n    axis=\"x\",\n    other_axis_start=0.0,\n    draw_edge_labels=False,\n    node_size=300,\n    node_color=\"#1f77b4\",\n):\n\"\"\"Plot the track graph in 1D to see how the linearization is set up.\"\"\"\n    track_graph_parameters = self.fetch1()\n    track_graph = self.get_networkx_track_graph(\n        track_graph_parameters=track_graph_parameters\n    )\n    plot_graph_as_1D(\n        track_graph,\n        edge_order=track_graph_parameters[\"linear_edge_order\"],\n        edge_spacing=track_graph_parameters[\"linear_edge_spacing\"],\n        ax=ax,\n        axis=axis,\n        other_axis_start=other_axis_start,\n        draw_edge_labels=draw_edge_labels,\n        node_size=node_size,\n        node_color=node_color,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.IntervalLinearizedPosition", "title": "<code>IntervalLinearizedPosition</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Linearized position for a given interval</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass IntervalLinearizedPosition(dj.Computed):\n\"\"\"Linearized position for a given interval\"\"\"\n\n    definition = \"\"\"\n    -&gt; IntervalLinearizationSelection\n    ---\n    -&gt; AnalysisNwbfile\n    linearized_position_object_id : varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        print(f\"Computing linear position for: {key}\")\n\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n\n        position_nwb = (\n            IntervalPositionInfo\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n                \"position_info_param_name\": key[\"position_info_param_name\"],\n            }\n        ).fetch_nwb()[0]\n\n        position = np.asarray(\n            position_nwb[\"head_position\"].get_spatial_series().data\n        )\n        time = np.asarray(\n            position_nwb[\"head_position\"].get_spatial_series().timestamps\n        )\n\n        linearization_parameters = (\n            LinearizationParameters()\n            &amp; {\"linearization_param_name\": key[\"linearization_param_name\"]}\n        ).fetch1()\n        track_graph_info = (\n            TrackGraph() &amp; {\"track_graph_name\": key[\"track_graph_name\"]}\n        ).fetch1()\n\n        track_graph = make_track_graph(\n            node_positions=track_graph_info[\"node_positions\"],\n            edges=track_graph_info[\"edges\"],\n        )\n\n        linear_position_df = get_linearized_position(\n            position=position,\n            track_graph=track_graph,\n            edge_spacing=track_graph_info[\"linear_edge_spacing\"],\n            edge_order=track_graph_info[\"linear_edge_order\"],\n            use_HMM=linearization_parameters[\"use_hmm\"],\n            route_euclidean_distance_scaling=linearization_parameters[\n                \"route_euclidean_distance_scaling\"\n            ],\n            sensor_std_dev=linearization_parameters[\"sensor_std_dev\"],\n            diagonal_bias=linearization_parameters[\"diagonal_bias\"],\n        )\n\n        linear_position_df[\"time\"] = time\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n\n        key[\"linearized_position_object_id\"] = nwb_analysis_file.add_nwb_object(\n            analysis_file_name=key[\"analysis_file_name\"],\n            nwb_object=linear_position_df,\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        return self.fetch_nwb()[0][\"linearized_position\"].set_index(\"time\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.NodePicker", "title": "<code>NodePicker</code>", "text": "<p>Interactive creation of track graph by looking at video frames.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>class NodePicker:\n\"\"\"Interactive creation of track graph by looking at video frames.\"\"\"\n\n    def __init__(\n        self, ax=None, video_filename=None, node_color=\"#1f78b4\", node_size=100\n    ):\n        if ax is None:\n            ax = plt.gca()\n        self.ax = ax\n        self.canvas = ax.get_figure().canvas\n        self.cid = None\n        self._nodes = []\n        self.node_color = node_color\n        self._nodes_plot = ax.scatter(\n            [], [], zorder=5, s=node_size, color=node_color\n        )\n        self.edges = [[]]\n        self.video_filename = video_filename\n\n        if video_filename is not None:\n            self.video = cv2.VideoCapture(video_filename)\n            frame = self.get_video_frame()\n            ax.imshow(frame, picker=True)\n            ax.set_title(\n                \"Left click to place node.\\nRight click to remove node.\"\n                \"\\nShift+Left click to clear nodes.\\nCntrl+Left click two nodes to place an edge\"\n            )\n\n        self.connect()\n\n    @property\n    def node_positions(self):\n        return np.asarray(self._nodes)\n\n    def connect(self):\n        if self.cid is None:\n            self.cid = self.canvas.mpl_connect(\n                \"button_press_event\", self.click_event\n            )\n\n    def disconnect(self):\n        if self.cid is not None:\n            self.canvas.mpl_disconnect(self.cid)\n            self.cid = None\n\n    def click_event(self, event):\n        if not event.inaxes:\n            return\n        if (event.key not in [\"control\", \"shift\"]) &amp; (\n            event.button == 1\n        ):  # left click\n            self._nodes.append((event.xdata, event.ydata))\n        if (event.key not in [\"control\", \"shift\"]) &amp; (\n            event.button == 3\n        ):  # right click\n            self.remove_point((event.xdata, event.ydata))\n        if (event.key == \"shift\") &amp; (event.button == 1):\n            self.clear()\n        if (event.key == \"control\") &amp; (event.button == 1):\n            point = (event.xdata, event.ydata)\n            distance_to_nodes = np.linalg.norm(\n                self.node_positions - point, axis=1\n            )\n            closest_node_ind = np.argmin(distance_to_nodes)\n            if len(self.edges[-1]) &lt; 2:\n                self.edges[-1].append(closest_node_ind)\n            else:\n                self.edges.append([closest_node_ind])\n\n        self.redraw()\n\n    def redraw(self):\n        # Draw Node Circles\n        if len(self.node_positions) &gt; 0:\n            self._nodes_plot.set_offsets(self.node_positions)\n        else:\n            self._nodes_plot.set_offsets([])\n\n        # Draw Node Numbers\n        self.ax.texts = []\n        for ind, (x, y) in enumerate(self.node_positions):\n            self.ax.text(\n                x,\n                y,\n                ind,\n                zorder=6,\n                fontsize=12,\n                horizontalalignment=\"center\",\n                verticalalignment=\"center\",\n                clip_on=True,\n                bbox=None,\n                transform=self.ax.transData,\n            )\n        # Draw Edges\n        self.ax.lines = []  # clears the existing lines\n        for edge in self.edges:\n            if len(edge) &gt; 1:\n                x1, y1 = self.node_positions[edge[0]]\n                x2, y2 = self.node_positions[edge[1]]\n                self.ax.plot(\n                    [x1, x2], [y1, y2], color=self.node_color, linewidth=2\n                )\n\n        self.canvas.draw_idle()\n\n    def remove_point(self, point):\n        if len(self._nodes) &gt; 0:\n            distance_to_nodes = np.linalg.norm(\n                self.node_positions - point, axis=1\n            )\n            closest_node_ind = np.argmin(distance_to_nodes)\n            self._nodes.pop(closest_node_ind)\n\n    def clear(self):\n        self._nodes = []\n        self.edges = [[]]\n        self.redraw()\n\n    def get_video_frame(self):\n        is_grabbed, frame = self.video.read()\n        if is_grabbed:\n            return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.PositionVideo", "title": "<code>PositionVideo</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Creates a video of the computed head position and orientation as well as the original LED positions overlaid on the video of the animal.</p> <p>Use for debugging the effect of position extraction parameters.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass PositionVideo(dj.Computed):\n\"\"\"Creates a video of the computed head position and orientation as well as\n    the original LED positions overlaid on the video of the animal.\n\n    Use for debugging the effect of position extraction parameters.\"\"\"\n\n    definition = \"\"\"\n    -&gt; IntervalPositionInfo\n    ---\n    \"\"\"\n\n    def make(self, key):\n        M_TO_CM = 100\n\n        print(\"Loading position data...\")\n        raw_position_df = (\n            RawPosition()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n            }\n        ).fetch1_dataframe()\n        position_info_df = (\n            IntervalPositionInfo()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n                \"position_info_param_name\": key[\"position_info_param_name\"],\n            }\n        ).fetch1_dataframe()\n\n        print(\"Loading video data...\")\n        epoch = (\n            int(\n                key[\"interval_list_name\"]\n                .replace(\"pos \", \"\")\n                .replace(\" valid times\", \"\")\n            )\n            + 1\n        )\n        video_info = (\n            VideoFile()\n            &amp; {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": epoch}\n        ).fetch1()\n        io = pynwb.NWBHDF5IO(\n            \"/stelmo/nwb/raw/\" + video_info[\"nwb_file_name\"], \"r\"\n        )\n        nwb_file = io.read()\n        nwb_video = nwb_file.objects[video_info[\"video_file_object_id\"]]\n        video_filename = nwb_video.external_file.value[0]\n\n        nwb_base_filename = key[\"nwb_file_name\"].replace(\".nwb\", \"\")\n        output_video_filename = (\n            f\"{nwb_base_filename}_{epoch:02d}_\"\n            f'{key[\"position_info_param_name\"]}.mp4'\n        )\n\n        centroids = {\n            \"red\": np.asarray(raw_position_df[[\"xloc\", \"yloc\"]]),\n            \"green\": np.asarray(raw_position_df[[\"xloc2\", \"yloc2\"]]),\n        }\n        head_position_mean = np.asarray(\n            position_info_df[[\"head_position_x\", \"head_position_y\"]]\n        )\n        head_orientation_mean = np.asarray(\n            position_info_df[[\"head_orientation\"]]\n        )\n        video_time = np.asarray(nwb_video.timestamps)\n        position_time = np.asarray(position_info_df.index)\n        cm_per_pixel = nwb_video.device.meters_per_pixel * M_TO_CM\n\n        print(\"Making video...\")\n        self.make_video(\n            video_filename,\n            centroids,\n            head_position_mean,\n            head_orientation_mean,\n            video_time,\n            position_time,\n            output_video_filename=output_video_filename,\n            cm_to_pixels=cm_per_pixel,\n            disable_progressbar=False,\n        )\n\n    @staticmethod\n    def convert_to_pixels(data, frame_size, cm_to_pixels=1.0):\n\"\"\"Converts from cm to pixels and flips the y-axis.\n        Parameters\n        ----------\n        data : ndarray, shape (n_time, 2)\n        frame_size : array_like, shape (2,)\n        cm_to_pixels : float\n\n        Returns\n        -------\n        converted_data : ndarray, shape (n_time, 2)\n        \"\"\"\n        return data / cm_to_pixels\n\n    @staticmethod\n    def fill_nan(variable, video_time, variable_time):\n        video_ind = np.digitize(variable_time, video_time[1:])\n\n        n_video_time = len(video_time)\n        try:\n            n_variable_dims = variable.shape[1]\n            filled_variable = np.full((n_video_time, n_variable_dims), np.nan)\n        except IndexError:\n            filled_variable = np.full((n_video_time,), np.nan)\n        filled_variable[video_ind] = variable\n\n        return filled_variable\n\n    def make_video(\n        self,\n        video_filename,\n        centroids,\n        head_position_mean,\n        head_orientation_mean,\n        video_time,\n        position_time,\n        output_video_filename=\"output.mp4\",\n        cm_to_pixels=1.0,\n        disable_progressbar=False,\n        arrow_radius=15,\n        circle_radius=8,\n    ):\n        RGB_PINK = (234, 82, 111)\n        RGB_YELLOW = (253, 231, 76)\n        RGB_WHITE = (255, 255, 255)\n\n        video = cv2.VideoCapture(video_filename)\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n        frame_size = (int(video.get(3)), int(video.get(4)))\n        frame_rate = video.get(5)\n        n_frames = int(head_orientation_mean.shape[0])\n\n        out = cv2.VideoWriter(\n            output_video_filename, fourcc, frame_rate, frame_size, True\n        )\n\n        centroids = {\n            color: self.fill_nan(data, video_time, position_time)\n            for color, data in centroids.items()\n        }\n        head_position_mean = self.fill_nan(\n            head_position_mean, video_time, position_time\n        )\n        head_orientation_mean = self.fill_nan(\n            head_orientation_mean, video_time, position_time\n        )\n\n        for time_ind in tqdm(\n            range(n_frames - 1), desc=\"frames\", disable=disable_progressbar\n        ):\n            is_grabbed, frame = video.read()\n            if is_grabbed:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n                red_centroid = centroids[\"red\"][time_ind]\n                green_centroid = centroids[\"green\"][time_ind]\n\n                head_position = head_position_mean[time_ind]\n                head_position = self.convert_to_pixels(\n                    head_position, frame_size, cm_to_pixels\n                )\n                head_orientation = head_orientation_mean[time_ind]\n\n                if np.all(~np.isnan(red_centroid)):\n                    cv2.circle(\n                        img=frame,\n                        center=tuple(red_centroid.astype(int)),\n                        radius=circle_radius,\n                        color=RGB_YELLOW,\n                        thickness=-1,\n                        shift=cv2.CV_8U,\n                    )\n\n                if np.all(~np.isnan(green_centroid)):\n                    cv2.circle(\n                        img=frame,\n                        center=tuple(green_centroid.astype(int)),\n                        radius=circle_radius,\n                        color=RGB_PINK,\n                        thickness=-1,\n                        shift=cv2.CV_8U,\n                    )\n\n                if np.all(~np.isnan(head_position)) &amp; np.all(\n                    ~np.isnan(head_orientation)\n                ):\n                    arrow_tip = (\n                        int(\n                            head_position[0]\n                            + arrow_radius * np.cos(head_orientation)\n                        ),\n                        int(\n                            head_position[1]\n                            + arrow_radius * np.sin(head_orientation)\n                        ),\n                    )\n                    cv2.arrowedLine(\n                        img=frame,\n                        pt1=tuple(head_position.astype(int)),\n                        pt2=arrow_tip,\n                        color=RGB_WHITE,\n                        thickness=4,\n                        line_type=8,\n                        shift=cv2.CV_8U,\n                        tipLength=0.25,\n                    )\n\n                if np.all(~np.isnan(head_position)):\n                    cv2.circle(\n                        img=frame,\n                        center=tuple(head_position.astype(int)),\n                        radius=circle_radius,\n                        color=RGB_WHITE,\n                        thickness=-1,\n                        shift=cv2.CV_8U,\n                    )\n\n                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n                out.write(frame)\n            else:\n                break\n\n        video.release()\n        out.release()\n        cv2.destroyAllWindows()\n</code></pre>"}, {"location": "api/src/spyglass/common/common_position/#src.spyglass.common.common_position.PositionVideo.convert_to_pixels", "title": "<code>convert_to_pixels(data, frame_size, cm_to_pixels=1.0)</code>  <code>staticmethod</code>", "text": "<p>Converts from cm to pixels and flips the y-axis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray, shape(n_time, 2)</code> required <code>frame_size</code> <code>array_like, shape(2)</code> required <code>cm_to_pixels</code> <code>float</code> <code>1.0</code> <p>Returns:</p> Name Type Description <code>converted_data</code> <code>ndarray, shape(n_time, 2)</code> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@staticmethod\ndef convert_to_pixels(data, frame_size, cm_to_pixels=1.0):\n\"\"\"Converts from cm to pixels and flips the y-axis.\n    Parameters\n    ----------\n    data : ndarray, shape (n_time, 2)\n    frame_size : array_like, shape (2,)\n    cm_to_pixels : float\n\n    Returns\n    -------\n    converted_data : ndarray, shape (n_time, 2)\n    \"\"\"\n    return data / cm_to_pixels\n</code></pre>"}, {"location": "api/src/spyglass/common/common_region/", "title": "common_region.py", "text": ""}, {"location": "api/src/spyglass/common/common_region/#src.spyglass.common.common_region.BrainRegion", "title": "<code>BrainRegion</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> Source code in <code>src/spyglass/common/common_region.py</code> <pre><code>@schema\nclass BrainRegion(dj.Lookup):\n    definition = \"\"\"\n    region_id: smallint auto_increment\n    ---\n    region_name: varchar(200)             # the name of the brain region\n    subregion_name=NULL: varchar(200)     # subregion name\n    subsubregion_name=NULL: varchar(200)  # subregion within subregion\n    \"\"\"\n\n    # TODO consider making (region_name, subregion_name, subsubregion_name) a primary key\n    # subregion_name='' and subsubregion_name='' will be necessary but that seems OK\n\n    @classmethod\n    def fetch_add(\n        cls, region_name, subregion_name=None, subsubregion_name=None\n    ):\n\"\"\"Return the region ID for the given names, and if no match exists, first add it to the BrainRegion table.\n\n        The combination of (region_name, subregion_name, subsubregion_name) is effectively unique, then.\n\n        Parameters\n        ----------\n        region_name : str\n            The name of the brain region.\n        subregion_name : str, optional\n            The name of the subregion within the brain region.\n        subsubregion_name : str, optional\n            The name of the subregion within the subregion.\n\n        Returns\n        -------\n        region_id : int\n            The index of the region in the BrainRegion table.\n        \"\"\"\n        key = dict()\n        key[\"region_name\"] = region_name\n        key[\"subregion_name\"] = subregion_name\n        key[\"subsubregion_name\"] = subsubregion_name\n        query = BrainRegion &amp; key\n        if not query:\n            cls.insert1(key)\n            query = BrainRegion &amp; key\n        return query.fetch1(\"region_id\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_region/#src.spyglass.common.common_region.BrainRegion.fetch_add", "title": "<code>fetch_add(region_name, subregion_name=None, subsubregion_name=None)</code>  <code>classmethod</code>", "text": "<p>Return the region ID for the given names, and if no match exists, first add it to the BrainRegion table.</p> <p>The combination of (region_name, subregion_name, subsubregion_name) is effectively unique, then.</p> <p>Parameters:</p> Name Type Description Default <code>region_name</code> <code>str</code> <p>The name of the brain region.</p> required <code>subregion_name</code> <code>str</code> <p>The name of the subregion within the brain region.</p> <code>None</code> <code>subsubregion_name</code> <code>str</code> <p>The name of the subregion within the subregion.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>region_id</code> <code>int</code> <p>The index of the region in the BrainRegion table.</p> Source code in <code>src/spyglass/common/common_region.py</code> <pre><code>@classmethod\ndef fetch_add(\n    cls, region_name, subregion_name=None, subsubregion_name=None\n):\n\"\"\"Return the region ID for the given names, and if no match exists, first add it to the BrainRegion table.\n\n    The combination of (region_name, subregion_name, subsubregion_name) is effectively unique, then.\n\n    Parameters\n    ----------\n    region_name : str\n        The name of the brain region.\n    subregion_name : str, optional\n        The name of the subregion within the brain region.\n    subsubregion_name : str, optional\n        The name of the subregion within the subregion.\n\n    Returns\n    -------\n    region_id : int\n        The index of the region in the BrainRegion table.\n    \"\"\"\n    key = dict()\n    key[\"region_name\"] = region_name\n    key[\"subregion_name\"] = subregion_name\n    key[\"subsubregion_name\"] = subsubregion_name\n    query = BrainRegion &amp; key\n    if not query:\n        cls.insert1(key)\n        query = BrainRegion &amp; key\n    return query.fetch1(\"region_id\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ripple/", "title": "common_ripple.py", "text": ""}, {"location": "api/src/spyglass/common/common_ripple/#src.spyglass.common.common_ripple.RippleLFPSelection", "title": "<code>RippleLFPSelection</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_ripple.py</code> <pre><code>@schema\nclass RippleLFPSelection(dj.Manual):\n    definition = \"\"\"\n     -&gt; LFPBand\n     group_name = 'CA1' : varchar(80)\n     \"\"\"\n\n    class RippleLFPElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; RippleLFPSelection\n        -&gt; LFPBandSelection.LFPBandElectrode\n        \"\"\"\n\n    def insert1(self, key, **kwargs):\n        filter_name = (LFPBand &amp; key).fetch1(\"filter_name\")\n        if \"ripple\" not in filter_name.lower():\n            raise UserWarning(\"Please use a ripple filter\")\n        super().insert1(key, **kwargs)\n\n    @staticmethod\n    def set_lfp_electrodes(\n        key,\n        electrode_list=None,\n        group_name=\"CA1\",\n        **kwargs,\n    ):\n\"\"\"Removes all electrodes for the specified nwb file and then adds back the electrodes in the list\n\n        Parameters\n        ----------\n        key : dict\n            dictionary corresponding to the LFPBand entry to use for ripple detection\n        electrode_list : list\n            list of electrodes from LFPBandSelection.LFPBandElectrode\n            to be used as the ripple LFP during detection\n        group_name : str, optional\n            description of the electrode group, by default \"CA1\"\n        \"\"\"\n\n        RippleLFPSelection().insert1(\n            {**key, \"group_name\": group_name},\n            skip_duplicates=True,\n            **kwargs,\n        )\n        if not electrode_list:\n            electrode_list = (\n                (LFPBandSelection.LFPBandElectrode() &amp; key)\n                .fetch(\"electrode_id\")\n                .tolist()\n            )\n        electrode_list.sort()\n        electrode_keys = (\n            pd.DataFrame(LFPBandSelection.LFPBandElectrode() &amp; key)\n            .set_index(\"electrode_id\")\n            .loc[electrode_list]\n            .reset_index()\n            .loc[:, LFPBandSelection.LFPBandElectrode.primary_key]\n        )\n        electrode_keys[\"group_name\"] = group_name\n        electrode_keys = electrode_keys.sort_values(by=[\"electrode_id\"])\n        RippleLFPSelection().RippleLFPElectrode.insert(\n            electrode_keys.to_dict(orient=\"records\"),\n            replace=True,\n            **kwargs,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ripple/#src.spyglass.common.common_ripple.RippleLFPSelection.set_lfp_electrodes", "title": "<code>set_lfp_electrodes(key, electrode_list=None, group_name='CA1', **kwargs)</code>  <code>staticmethod</code>", "text": "<p>Removes all electrodes for the specified nwb file and then adds back the electrodes in the list</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>dictionary corresponding to the LFPBand entry to use for ripple detection</p> required <code>electrode_list</code> <code>list</code> <p>list of electrodes from LFPBandSelection.LFPBandElectrode to be used as the ripple LFP during detection</p> <code>None</code> <code>group_name</code> <code>str</code> <p>description of the electrode group, by default \"CA1\"</p> <code>'CA1'</code> Source code in <code>src/spyglass/common/common_ripple.py</code> <pre><code>@staticmethod\ndef set_lfp_electrodes(\n    key,\n    electrode_list=None,\n    group_name=\"CA1\",\n    **kwargs,\n):\n\"\"\"Removes all electrodes for the specified nwb file and then adds back the electrodes in the list\n\n    Parameters\n    ----------\n    key : dict\n        dictionary corresponding to the LFPBand entry to use for ripple detection\n    electrode_list : list\n        list of electrodes from LFPBandSelection.LFPBandElectrode\n        to be used as the ripple LFP during detection\n    group_name : str, optional\n        description of the electrode group, by default \"CA1\"\n    \"\"\"\n\n    RippleLFPSelection().insert1(\n        {**key, \"group_name\": group_name},\n        skip_duplicates=True,\n        **kwargs,\n    )\n    if not electrode_list:\n        electrode_list = (\n            (LFPBandSelection.LFPBandElectrode() &amp; key)\n            .fetch(\"electrode_id\")\n            .tolist()\n        )\n    electrode_list.sort()\n    electrode_keys = (\n        pd.DataFrame(LFPBandSelection.LFPBandElectrode() &amp; key)\n        .set_index(\"electrode_id\")\n        .loc[electrode_list]\n        .reset_index()\n        .loc[:, LFPBandSelection.LFPBandElectrode.primary_key]\n    )\n    electrode_keys[\"group_name\"] = group_name\n    electrode_keys = electrode_keys.sort_values(by=[\"electrode_id\"])\n    RippleLFPSelection().RippleLFPElectrode.insert(\n        electrode_keys.to_dict(orient=\"records\"),\n        replace=True,\n        **kwargs,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ripple/#src.spyglass.common.common_ripple.RippleParameters", "title": "<code>RippleParameters</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> Source code in <code>src/spyglass/common/common_ripple.py</code> <pre><code>@schema\nclass RippleParameters(dj.Lookup):\n    definition = \"\"\"\n    ripple_param_name : varchar(80) # a name for this set of parameters\n    ----\n    ripple_param_dict : BLOB    # dictionary of parameters\n    \"\"\"\n\n    def insert_default(self):\n\"\"\"Insert the default parameter set\"\"\"\n        default_dict = {\n            \"speed_name\": \"head_speed\",\n            \"ripple_detection_algorithm\": \"Kay_ripple_detector\",\n            \"ripple_detection_params\": dict(\n                speed_threshold=4.0,  # cm/s\n                minimum_duration=0.015,  # sec\n                zscore_threshold=2.0,  # std\n                smoothing_sigma=0.004,  # sec\n                close_ripple_threshold=0.0,  # sec\n            ),\n        }\n        self.insert1(\n            {\"ripple_param_name\": \"default\", \"ripple_param_dict\": default_dict},\n            skip_duplicates=True,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ripple/#src.spyglass.common.common_ripple.RippleParameters.insert_default", "title": "<code>insert_default()</code>", "text": "<p>Insert the default parameter set</p> Source code in <code>src/spyglass/common/common_ripple.py</code> <pre><code>def insert_default(self):\n\"\"\"Insert the default parameter set\"\"\"\n    default_dict = {\n        \"speed_name\": \"head_speed\",\n        \"ripple_detection_algorithm\": \"Kay_ripple_detector\",\n        \"ripple_detection_params\": dict(\n            speed_threshold=4.0,  # cm/s\n            minimum_duration=0.015,  # sec\n            zscore_threshold=2.0,  # std\n            smoothing_sigma=0.004,  # sec\n            close_ripple_threshold=0.0,  # sec\n        ),\n    }\n    self.insert1(\n        {\"ripple_param_name\": \"default\", \"ripple_param_dict\": default_dict},\n        skip_duplicates=True,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ripple/#src.spyglass.common.common_ripple.RippleTimes", "title": "<code>RippleTimes</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/common/common_ripple.py</code> <pre><code>@schema\nclass RippleTimes(dj.Computed):\n    definition = \"\"\"\n    -&gt; RippleParameters\n    -&gt; RippleLFPSelection\n    -&gt; IntervalPositionInfo\n    ---\n    -&gt; AnalysisNwbfile\n    ripple_times_object_id : varchar(40)\n     \"\"\"\n\n    def make(self, key):\n        print(f\"Computing ripple times for: {key}\")\n        ripple_params = (\n            RippleParameters &amp; {\"ripple_param_name\": key[\"ripple_param_name\"]}\n        ).fetch1(\"ripple_param_dict\")\n\n        ripple_detection_algorithm = ripple_params[\"ripple_detection_algorithm\"]\n        ripple_detection_params = ripple_params[\"ripple_detection_params\"]\n\n        (\n            speed,\n            interval_ripple_lfps,\n            sampling_frequency,\n        ) = self.get_ripple_lfps_and_position_info(key)\n\n        ripple_times = RIPPLE_DETECTION_ALGORITHMS[ripple_detection_algorithm](\n            time=np.asarray(interval_ripple_lfps.index),\n            filtered_lfps=np.asarray(interval_ripple_lfps),\n            speed=np.asarray(speed),\n            sampling_frequency=sampling_frequency,\n            **ripple_detection_params,\n        )\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"analysis_file_name\"] = nwb_analysis_file.create(\n            key[\"nwb_file_name\"]\n        )\n        key[\"ripple_times_object_id\"] = nwb_analysis_file.add_nwb_object(\n            analysis_file_name=key[\"analysis_file_name\"],\n            nwb_object=ripple_times,\n        )\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n\"\"\"Convenience function for returning the marks in a readable format\"\"\"\n        return self.fetch_dataframe()[0]\n\n    def fetch_dataframe(self):\n        return [data[\"ripple_times\"] for data in self.fetch_nwb()]\n\n    @staticmethod\n    def get_ripple_lfps_and_position_info(key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        interval_list_name = key[\"target_interval_list_name\"]\n        position_info_param_name = key[\"position_info_param_name\"]\n        ripple_params = (\n            RippleParameters &amp; {\"ripple_param_name\": key[\"ripple_param_name\"]}\n        ).fetch1(\"ripple_param_dict\")\n\n        speed_name = ripple_params[\"speed_name\"]\n\n        electrode_keys = (RippleLFPSelection.RippleLFPElectrode() &amp; key).fetch(\n            \"electrode_id\"\n        )\n\n        # warn/validate that there is only one wire per electrode\n        lfp_key = key.copy()\n        del lfp_key[\"interval_list_name\"]\n        ripple_lfp_nwb = (LFPBand &amp; lfp_key).fetch_nwb()[0]\n        ripple_lfp_electrodes = ripple_lfp_nwb[\"filtered_data\"].electrodes.data[\n            :\n        ]\n        elec_mask = np.full_like(ripple_lfp_electrodes, 0, dtype=bool)\n        elec_mask[\n            [\n                ind\n                for ind, elec in enumerate(ripple_lfp_electrodes)\n                if elec in electrode_keys\n            ]\n        ] = True\n        ripple_lfp = pd.DataFrame(\n            ripple_lfp_nwb[\"filtered_data\"].data,\n            index=pd.Index(\n                ripple_lfp_nwb[\"filtered_data\"].timestamps, name=\"time\"\n            ),\n        )\n        sampling_frequency = ripple_lfp_nwb[\"lfp_band_sampling_rate\"]\n\n        ripple_lfp = ripple_lfp.loc[:, elec_mask]\n\n        position_valid_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n\n        position_info = (\n            IntervalPositionInfo()\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"interval_list_name\": interval_list_name,\n                \"position_info_param_name\": position_info_param_name,\n            }\n        ).fetch1_dataframe()\n\n        position_info = pd.concat(\n            [\n                position_info.loc[slice(valid_time[0], valid_time[1])]\n                for valid_time in position_valid_times\n            ],\n            axis=1,\n        )\n        interval_ripple_lfps = pd.concat(\n            [\n                ripple_lfp.loc[slice(valid_time[0], valid_time[1])]\n                for valid_time in position_valid_times\n            ],\n            axis=1,\n        )\n\n        position_info = interpolate_to_new_time(\n            valid_position_info, interval_ripple_lfps.index\n        )\n\n        return (\n            position_info[speed_name],\n            interval_ripple_lfps,\n            sampling_frequency,\n        )\n\n    @staticmethod\n    def get_Kay_ripple_consensus_trace(\n        ripple_filtered_lfps, sampling_frequency, smoothing_sigma=0.004\n    ):\n        ripple_consensus_trace = np.full_like(ripple_filtered_lfps, np.nan)\n        not_null = np.all(pd.notnull(ripple_filtered_lfps), axis=1)\n\n        ripple_consensus_trace[not_null] = get_envelope(\n            np.asarray(ripple_filtered_lfps)[not_null]\n        )\n        ripple_consensus_trace = np.sum(ripple_consensus_trace**2, axis=1)\n        ripple_consensus_trace[not_null] = gaussian_smooth(\n            ripple_consensus_trace[not_null],\n            smoothing_sigma,\n            sampling_frequency,\n        )\n        return pd.DataFrame(\n            np.sqrt(ripple_consensus_trace), index=ripple_filtered_lfps.index\n        )\n\n    @staticmethod\n    def plot_ripple_consensus_trace(\n        ripple_consensus_trace,\n        ripple_times,\n        ripple_label=1,\n        offset=0.100,\n        relative=True,\n        ax=None,\n    ):\n        ripple_start = ripple_times.loc[ripple_label].start_time\n        ripple_end = ripple_times.loc[ripple_label].end_time\n        time_slice = slice(ripple_start - offset, ripple_end + offset)\n\n        start_offset = ripple_start if relative else 0\n        if ax is None:\n            fig, ax = plt.subplots(1, 1, figsize=(12, 1))\n        ax.plot(\n            ripple_consensus_trace.loc[time_slice].index - start_offset,\n            ripple_consensus_trace.loc[time_slice],\n        )\n        ax.axvspan(\n            ripple_start - start_offset,\n            ripple_end - start_offset,\n            zorder=-1,\n            alpha=0.5,\n            color=\"lightgrey\",\n        )\n        ax.set_xlabel(\"Time [s]\")\n        ax.set_xlim(\n            (time_slice.start - start_offset, time_slice.stop - start_offset)\n        )\n\n    @staticmethod\n    def plot_ripple(\n        lfps, ripple_times, ripple_label=1, offset=0.100, relative=True, ax=None\n    ):\n        lfp_labels = lfps.columns\n        n_lfps = len(lfp_labels)\n        ripple_start = ripple_times.loc[ripple_label].start_time\n        ripple_end = ripple_times.loc[ripple_label].end_time\n        time_slice = slice(ripple_start - offset, ripple_end + offset)\n        if ax is None:\n            fig, ax = plt.subplots(1, 1, figsize=(12, n_lfps * 0.20))\n\n        start_offset = ripple_start if relative else 0\n\n        for lfp_ind, lfp_label in enumerate(lfp_labels):\n            lfp = lfps.loc[time_slice, lfp_label]\n            ax.plot(\n                lfp.index - start_offset,\n                lfp_ind + (lfp - lfp.mean()) / (lfp.max() - lfp.min()),\n                color=\"black\",\n            )\n\n        ax.axvspan(\n            ripple_start - start_offset,\n            ripple_end - start_offset,\n            zorder=-1,\n            alpha=0.5,\n            color=\"lightgrey\",\n        )\n        ax.set_ylim((-1, n_lfps))\n        ax.set_xlim(\n            (time_slice.start - start_offset, time_slice.stop - start_offset)\n        )\n        ax.set_ylabel(\"LFPs\")\n        ax.set_xlabel(\"Time [s]\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_ripple/#src.spyglass.common.common_ripple.RippleTimes.fetch1_dataframe", "title": "<code>fetch1_dataframe()</code>", "text": "<p>Convenience function for returning the marks in a readable format</p> Source code in <code>src/spyglass/common/common_ripple.py</code> <pre><code>def fetch1_dataframe(self):\n\"\"\"Convenience function for returning the marks in a readable format\"\"\"\n    return self.fetch_dataframe()[0]\n</code></pre>"}, {"location": "api/src/spyglass/common/common_sensors/", "title": "common_sensors.py", "text": "<p>Schema for headstage or other environmental sensors.</p>"}, {"location": "api/src/spyglass/common/common_sensors/#src.spyglass.common.common_sensors.IntervalList", "title": "<code>IntervalList</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@schema\nclass IntervalList(dj.Manual):\n    definition = \"\"\"\n    # Time intervals used for analysis\n    -&gt; Session\n    interval_list_name: varchar(200)  # descriptive name of this interval list\n    ---\n    valid_times: longblob  # numpy array with start and end times for each interval\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n        The interval list name for each epoch is set to the first tag for the epoch.\n        If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n        (0-indexed) of the epoch in the epochs table.\n        The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n        [start time, stop time] for each epoch.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        nwb_file_name : str\n            The file name of the NWB file, used as a primary key to the Session table.\n        \"\"\"\n        if nwbf.epochs is None:\n            print(\"No epochs found in NWB file.\")\n            return\n        epochs = nwbf.epochs.to_dataframe()\n        for epoch_index, epoch_data in epochs.iterrows():\n            epoch_dict = dict()\n            epoch_dict[\"nwb_file_name\"] = nwb_file_name\n            if epoch_data.tags[0]:\n                epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n            else:\n                epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                    epoch_index\n                )\n            epoch_dict[\"valid_times\"] = np.asarray(\n                [[epoch_data.start_time, epoch_data.stop_time]]\n            )\n            cls.insert1(epoch_dict, skip_duplicates=True)\n\n    def plot_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=figsize)\n        interval_count = 0\n        for row in interval_list.itertuples(index=False):\n            for interval in row.valid_times:\n                ax.plot(interval, [interval_count, interval_count])\n                ax.scatter(\n                    interval,\n                    [interval_count, interval_count],\n                    alpha=0.8,\n                    zorder=2,\n                )\n            interval_count += 1\n        ax.set_yticks(np.arange(interval_list.shape[0]))\n        ax.set_yticklabels(interval_list.interval_list_name)\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n\n    def plot_epoch_pos_raw_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=(30, 3))\n\n        raw_data_valid_times = interval_list.loc[\n            interval_list.interval_list_name == \"raw data valid times\"\n        ].valid_times\n        interval_y = 1\n\n        for interval in np.asarray(raw_data_valid_times)[0]:\n            ax.plot(interval, [interval_y, interval_y])\n            ax.scatter(interval, [interval_y, interval_y], alpha=0.8, zorder=2)\n\n        epoch_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n        interval_y = 2\n        for epoch, valid_times in zip(\n            epoch_valid_times.index, epoch_valid_times\n        ):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        pos_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^pos \\d+ valid times$\", axis=0)\n            .valid_times\n        ).sort_index(key=lambda index: [int(name.split()[1]) for name in index])\n        interval_y = 0\n        for epoch, valid_times in zip(pos_valid_times.index, pos_valid_times):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch.strip(\" valid times\"),\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        ax.set_ylim((-0.25, 2.25))\n        ax.set_yticks(np.arange(3))\n        ax.set_yticklabels([\"pos valid times\", \"raw data valid times\", \"epoch\"])\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_sensors/#src.spyglass.common.common_interval.IntervalList.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, *, nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add each entry in the NWB file epochs table to the IntervalList table.</p> <p>The interval list name for each epoch is set to the first tag for the epoch. If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index (0-indexed) of the epoch in the epochs table. The start time and stop time of the epoch are stored in the valid_times field as a numpy array of [start time, stop time] for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>nwb_file_name</code> <code>str</code> <p>The file name of the NWB file, used as a primary key to the Session table.</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n    The interval list name for each epoch is set to the first tag for the epoch.\n    If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n    (0-indexed) of the epoch in the epochs table.\n    The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n    [start time, stop time] for each epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    nwb_file_name : str\n        The file name of the NWB file, used as a primary key to the Session table.\n    \"\"\"\n    if nwbf.epochs is None:\n        print(\"No epochs found in NWB file.\")\n        return\n    epochs = nwbf.epochs.to_dataframe()\n    for epoch_index, epoch_data in epochs.iterrows():\n        epoch_dict = dict()\n        epoch_dict[\"nwb_file_name\"] = nwb_file_name\n        if epoch_data.tags[0]:\n            epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n        else:\n            epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                epoch_index\n            )\n        epoch_dict[\"valid_times\"] = np.asarray(\n            [[epoch_data.start_time, epoch_data.stop_time]]\n        )\n        cls.insert1(epoch_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_sensors/#src.spyglass.common.common_sensors.get_nwb_file", "title": "<code>get_nwb_file(nwb_file_path)</code>", "text": "<p>Return an NWBFile object with the given file path in read mode.    If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Path to the NWB file.</p> required <p>Returns:</p> Name Type Description <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>NWB file object for the given path opened in read mode.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_file(nwb_file_path):\n\"\"\"Return an NWBFile object with the given file path in read mode.\n       If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.\n\n    Parameters\n    ----------\n    nwb_file_path : str\n        Path to the NWB file.\n\n    Returns\n    -------\n    nwbfile : pynwb.NWBFile\n        NWB file object for the given path opened in read mode.\n    \"\"\"\n    _, nwbfile = __open_nwb_files.get(nwb_file_path, (None, None))\n    nwb_uri = None\n    nwb_raw_uri = None\n    if nwbfile is None:\n        # check to see if the file exists\n        if not os.path.exists(nwb_file_path):\n            print(\n                f\"NWB file {nwb_file_path} does not exist locally; checking kachery\"\n            )\n            # first try the analysis files\n            from ..sharing.sharing_kachery import AnalysisNwbfileKachery\n\n            # the download functions assume just the filename, so we need to get that from the path\n            if not AnalysisNwbfileKachery.download_file(\n                os.path.basename(nwb_file_path)\n            ):\n                return None\n        # now open the file\n        io = pynwb.NWBHDF5IO(\n            path=nwb_file_path, mode=\"r\", load_namespaces=True\n        )  # keep file open\n        nwbfile = io.read()\n        __open_nwb_files[nwb_file_path] = (io, nwbfile)\n\n    return nwbfile\n</code></pre>"}, {"location": "api/src/spyglass/common/common_sensors/#src.spyglass.common.common_sensors.Nwbfile", "title": "<code>Nwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass Nwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files.\n    nwb_file_name: varchar(255)   # name of the NWB file\n    ---\n    nwb_file_abs_path: filepath@raw\n    INDEX (nwb_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    @classmethod\n    def insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The relative path to the NWB file.\n        \"\"\"\n        nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n        assert os.path.exists(\n            nwb_file_abs_path\n        ), f\"File does not exist: {nwb_file_abs_path}\"\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n        cls.insert1(key, skip_duplicates=True)\n\n    @staticmethod\n    def get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n        Returns\n        -------\n        nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n        nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n        return str(nwb_file_abspath)\n\n    @staticmethod\n    def add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n        The NWB_LOCK_FILE environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n        \"\"\"\n        key = {\"nwb_file_name\": nwb_file_name}\n        # check to make sure the file exists\n        assert (\n            len((Nwbfile() &amp; key).fetch()) &gt; 0\n        ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n        lock_file.write(f\"{nwb_file_name}\\n\")\n        lock_file.close()\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        This does not delete the files themselves unless delete_files=True is specified\n        Run this after deleting the Nwbfile() entries themselves.\n        \"\"\"\n        schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_sensors/#src.spyglass.common.common_nwbfile.Nwbfile.insert_from_relative_file_name", "title": "<code>insert_from_relative_file_name(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Insert a new session from an existing NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The relative path to the NWB file.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The relative path to the NWB file.\n    \"\"\"\n    nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n    assert os.path.exists(\n        nwb_file_abs_path\n    ), f\"File does not exist: {nwb_file_abs_path}\"\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n    cls.insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_sensors/#src.spyglass.common.common_nwbfile.Nwbfile.get_abs_path", "title": "<code>get_abs_path(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored raw NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required <p>Returns:</p> Name Type Description <code>nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n    Returns\n    -------\n    nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n    nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n    return str(nwb_file_abspath)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_sensors/#src.spyglass.common.common_nwbfile.Nwbfile.add_to_lock", "title": "<code>add_to_lock(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Add the specified NWB file to the file with the list of NWB files to be locked.</p> <p>The NWB_LOCK_FILE environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n    The NWB_LOCK_FILE environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n    \"\"\"\n    key = {\"nwb_file_name\": nwb_file_name}\n    # check to make sure the file exists\n    assert (\n        len((Nwbfile() &amp; key).fetch()) &gt; 0\n    ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n    lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n    lock_file.write(f\"{nwb_file_name}\\n\")\n    lock_file.close()\n</code></pre>"}, {"location": "api/src/spyglass/common/common_sensors/#src.spyglass.common.common_nwbfile.Nwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>This does not delete the files themselves unless delete_files=True is specified Run this after deleting the Nwbfile() entries themselves.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    This does not delete the files themselves unless delete_files=True is specified\n    Run this after deleting the Nwbfile() entries themselves.\n    \"\"\"\n    schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_sensors/#src.spyglass.common.common_sensors.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/common/common_sensors/#src.spyglass.common.common_sensors.get_data_interface", "title": "<code>get_data_interface(nwbfile, data_interface_name, data_interface_class=None)</code>", "text": "<p>Search for a specified NWBDataInterface or DynamicTable in the processing modules of an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>The NWB file object to search in.</p> required <code>data_interface_name</code> <code>str</code> <p>The name of the NWBDataInterface or DynamicTable to search for.</p> required <code>data_interface_class</code> <code>type</code> <p>The class (or superclass) to search for. This argument helps to prevent accessing an object with the same name but the incorrect type. Default: no restriction.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If multiple NWBDataInterface and DynamicTable objects with the matching name are found.</p> <p>Returns:</p> Name Type Description <code>data_interface</code> <code>NWBDataInterface</code> <p>The data interface object with the given name, or None if not found.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_data_interface(nwbfile, data_interface_name, data_interface_class=None):\n\"\"\"Search for a specified NWBDataInterface or DynamicTable in the processing modules of an NWB file.\n\n    Parameters\n    ----------\n    nwbfile : pynwb.NWBFile\n        The NWB file object to search in.\n    data_interface_name : str\n        The name of the NWBDataInterface or DynamicTable to search for.\n    data_interface_class : type, optional\n        The class (or superclass) to search for. This argument helps to prevent accessing an object with the same\n        name but the incorrect type. Default: no restriction.\n\n    Warns\n    -----\n    UserWarning\n        If multiple NWBDataInterface and DynamicTable objects with the matching name are found.\n\n    Returns\n    -------\n    data_interface : NWBDataInterface\n        The data interface object with the given name, or None if not found.\n    \"\"\"\n    ret = []\n    for module in nwbfile.processing.values():\n        match = module.data_interfaces.get(data_interface_name, None)\n        if match is not None:\n            if data_interface_class is not None and not isinstance(\n                match, data_interface_class\n            ):\n                continue\n            ret.append(match)\n    if len(ret) &gt; 1:\n        warnings.warn(\n            f\"Multiple data interfaces with name '{data_interface_name}' \"\n            f\"found in NWBFile with identifier {nwbfile.identifier}. Using the first one found. \"\n            \"Use the data_interface_class argument to restrict the search.\"\n        )\n    if len(ret) &gt;= 1:\n        return ret[0]\n    else:\n        return None\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/", "title": "common_session.py", "text": ""}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_session.Subject", "title": "<code>Subject</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_subject.py</code> <pre><code>@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id: varchar(80)\n    ---\n    age = NULL: varchar(200)\n    description = NULL: varchar(2000)\n    genotype = NULL: varchar(2000)\n    sex = \"U\": enum(\"M\", \"F\", \"U\")\n    species = NULL: varchar(200)\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Get the subject information from the NWBFile and insert it into the Subject table.\"\"\"\n        sub = nwbf.subject\n        if sub is None:\n            print(\"No subject metadata found.\\n\")\n            return\n        subject_dict = dict()\n        subject_dict[\"subject_id\"] = sub.subject_id\n        subject_dict[\"age\"] = sub.age\n        subject_dict[\"description\"] = sub.description\n        subject_dict[\"genotype\"] = sub.genotype\n        if sub.sex in (\"Male\", \"male\", \"M\", \"m\"):\n            sex = \"M\"\n        elif sub.sex in (\"Female\", \"female\", \"F\", \"f\"):\n            sex = \"F\"\n        else:\n            sex = \"U\"\n        subject_dict[\"sex\"] = sex\n        subject_dict[\"species\"] = sub.species\n        cls.insert1(subject_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_subject.Subject.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Get the subject information from the NWBFile and insert it into the Subject table.</p> Source code in <code>src/spyglass/common/common_subject.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Get the subject information from the NWBFile and insert it into the Subject table.\"\"\"\n    sub = nwbf.subject\n    if sub is None:\n        print(\"No subject metadata found.\\n\")\n        return\n    subject_dict = dict()\n    subject_dict[\"subject_id\"] = sub.subject_id\n    subject_dict[\"age\"] = sub.age\n    subject_dict[\"description\"] = sub.description\n    subject_dict[\"genotype\"] = sub.genotype\n    if sub.sex in (\"Male\", \"male\", \"M\", \"m\"):\n        sex = \"M\"\n    elif sub.sex in (\"Female\", \"female\", \"F\", \"f\"):\n        sex = \"F\"\n    else:\n        sex = \"U\"\n    subject_dict[\"sex\"] = sex\n    subject_dict[\"species\"] = sub.species\n    cls.insert1(subject_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_session.LabMember", "title": "<code>LabMember</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass LabMember(dj.Manual):\n    definition = \"\"\"\n    lab_member_name: varchar(80)\n    ---\n    first_name: varchar(200)\n    last_name: varchar(200)\n    \"\"\"\n\n    # NOTE that names must be unique here. If there are two neuroscientists named Jack Black that have data in this\n    # database, this will create an incorrect linkage. NWB does not yet provide unique IDs for names.\n\n    class LabMemberInfo(dj.Part):\n        definition = \"\"\"\n        # Information about lab member in the context of Frank lab network\n        -&gt; LabMember\n        ---\n        google_user_name: varchar(200)              # used for permission to curate\n        datajoint_user_name = \"\": varchar(200)      # used for permission to delete entries\n        \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab member information from an NWB file.\n\n        Parameters\n        ----------\n        nwbf: pynwb.NWBFile\n            The NWB file with experimenter information.\n        \"\"\"\n        if nwbf.experimenter is None:\n            print(\"No experimenter metadata found.\\n\")\n            return\n        for experimenter in nwbf.experimenter:\n            cls.insert_from_name(experimenter)\n            # each person is by default the member of their own LabTeam (same as their name)\n            LabTeam.create_new_team(\n                team_name=experimenter, team_members=[experimenter]\n            )\n\n    @classmethod\n    def insert_from_name(cls, full_name):\n\"\"\"Insert a lab member by name.\n\n        The first name is the part of the name that precedes the last space, and the last name is the part of the\n        name that follows the last space.\n\n        Parameters\n        ----------\n        full_name : str\n            The name to be added.\n        \"\"\"\n        labmember_dict = dict()\n        labmember_dict[\"lab_member_name\"] = full_name\n        full_name_split = str.split(full_name)\n        labmember_dict[\"first_name\"] = \" \".join(full_name_split[:-1])\n        labmember_dict[\"last_name\"] = full_name_split[-1]\n        cls.insert1(labmember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_lab.LabMember.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert lab member information from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <p>The NWB file with experimenter information.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab member information from an NWB file.\n\n    Parameters\n    ----------\n    nwbf: pynwb.NWBFile\n        The NWB file with experimenter information.\n    \"\"\"\n    if nwbf.experimenter is None:\n        print(\"No experimenter metadata found.\\n\")\n        return\n    for experimenter in nwbf.experimenter:\n        cls.insert_from_name(experimenter)\n        # each person is by default the member of their own LabTeam (same as their name)\n        LabTeam.create_new_team(\n            team_name=experimenter, team_members=[experimenter]\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_lab.LabMember.insert_from_name", "title": "<code>insert_from_name(full_name)</code>  <code>classmethod</code>", "text": "<p>Insert a lab member by name.</p> <p>The first name is the part of the name that precedes the last space, and the last name is the part of the name that follows the last space.</p> <p>Parameters:</p> Name Type Description Default <code>full_name</code> <code>str</code> <p>The name to be added.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_name(cls, full_name):\n\"\"\"Insert a lab member by name.\n\n    The first name is the part of the name that precedes the last space, and the last name is the part of the\n    name that follows the last space.\n\n    Parameters\n    ----------\n    full_name : str\n        The name to be added.\n    \"\"\"\n    labmember_dict = dict()\n    labmember_dict[\"lab_member_name\"] = full_name\n    full_name_split = str.split(full_name)\n    labmember_dict[\"first_name\"] = \" \".join(full_name_split[:-1])\n    labmember_dict[\"last_name\"] = full_name_split[-1]\n    cls.insert1(labmember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_session.get_nwb_file", "title": "<code>get_nwb_file(nwb_file_path)</code>", "text": "<p>Return an NWBFile object with the given file path in read mode.    If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Path to the NWB file.</p> required <p>Returns:</p> Name Type Description <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>NWB file object for the given path opened in read mode.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_file(nwb_file_path):\n\"\"\"Return an NWBFile object with the given file path in read mode.\n       If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.\n\n    Parameters\n    ----------\n    nwb_file_path : str\n        Path to the NWB file.\n\n    Returns\n    -------\n    nwbfile : pynwb.NWBFile\n        NWB file object for the given path opened in read mode.\n    \"\"\"\n    _, nwbfile = __open_nwb_files.get(nwb_file_path, (None, None))\n    nwb_uri = None\n    nwb_raw_uri = None\n    if nwbfile is None:\n        # check to see if the file exists\n        if not os.path.exists(nwb_file_path):\n            print(\n                f\"NWB file {nwb_file_path} does not exist locally; checking kachery\"\n            )\n            # first try the analysis files\n            from ..sharing.sharing_kachery import AnalysisNwbfileKachery\n\n            # the download functions assume just the filename, so we need to get that from the path\n            if not AnalysisNwbfileKachery.download_file(\n                os.path.basename(nwb_file_path)\n            ):\n                return None\n        # now open the file\n        io = pynwb.NWBHDF5IO(\n            path=nwb_file_path, mode=\"r\", load_namespaces=True\n        )  # keep file open\n        nwbfile = io.read()\n        __open_nwb_files[nwb_file_path] = (io, nwbfile)\n\n    return nwbfile\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_session.DataAcquisitionDevice", "title": "<code>DataAcquisitionDevice</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass DataAcquisitionDevice(dj.Manual):\n    definition = \"\"\"\n    data_acquisition_device_name: varchar(80)\n    ---\n    -&gt; DataAcquisitionDeviceSystem\n    -&gt; DataAcquisitionDeviceAmplifier\n    adc_circuit = NULL: varchar(2000)\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, config):\n\"\"\"Insert data acquisition devices from an NWB file.\n\n        Note that this does not link the DataAcquisitionDevices with a Session. For that,\n        see DataAcquisitionDeviceList.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n        \"\"\"\n        _, ndx_devices, _ = cls.get_all_device_names(nwbf, config)\n\n        for device_name in ndx_devices:\n            new_device_dict = dict()\n\n            # read device properties into new_device_dict from PyNWB extension device object\n            nwb_device_obj = ndx_devices[device_name]\n\n            name = nwb_device_obj.name\n            adc_circuit = nwb_device_obj.adc_circuit\n\n            # transform system value. check if value is in DB. if not, prompt user to add an entry or cancel.\n            system = cls._add_system(nwb_device_obj.system)\n\n            # transform amplifier value. check if value is in DB. if not, prompt user to add an entry or cancel.\n            amplifier = cls._add_amplifier(nwb_device_obj.amplifier)\n\n            # standardize how Intan is represented in the database\n            if adc_circuit.title() == \"Intan\":\n                adc_circuit = \"Intan\"\n\n            new_device_dict[\"data_acquisition_device_name\"] = name\n            new_device_dict[\"data_acquisition_device_system\"] = system\n            new_device_dict[\"data_acquisition_device_amplifier\"] = amplifier\n            new_device_dict[\"adc_circuit\"] = adc_circuit\n\n            cls._add_device(new_device_dict)\n\n        if ndx_devices:\n            print(\n                f\"Inserted or referenced data acquisition device(s): {ndx_devices.keys()}\"\n            )\n        else:\n            print(\"No conforming data acquisition device metadata found.\")\n\n    @classmethod\n    def get_all_device_names(cls, nwbf, config):\n\"\"\"Get a list of all device names in the NWB file, after appending and overwriting by the config file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : list\n            List of data acquisition object names found in the NWB file.\n        \"\"\"\n        # make a dict mapping device name to PyNWB device object for all devices in the NWB file that are\n        # of type ndx_franklab_novela.DataAcqDevice and thus have the required metadata\n        ndx_devices = {\n            device_obj.name: device_obj\n            for device_obj in nwbf.devices.values()\n            if isinstance(device_obj, ndx_franklab_novela.DataAcqDevice)\n        }\n\n        # make a list of device names that are associated with this NWB file\n        if \"DataAcquisitionDevice\" in config:\n            config_devices = [\n                device_dict[\"data_acquisition_device_name\"]\n                for device_dict in config[\"DataAcquisitionDevice\"]\n            ]\n        else:\n            config_devices = list()\n\n        all_device_names = set(ndx_devices.keys()).union(set(config_devices))\n\n        return all_device_names, ndx_devices, config_devices\n\n    @classmethod\n    def _add_device(cls, new_device_dict):\n\"\"\"Check that the information in the NWB file and the database for the given device name match perfectly.\n\n        If no DataAcquisitionDevice with the given name exists in the database, check whether the user wants to add\n        a new entry instead of referencing an existing entry. If so, return. If not, raise an exception.\n\n        Parameters\n        ----------\n        new_device_dict : dict\n            Dict of new device properties\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a device to the database when prompted or if the device properties from the\n            NWB file do not match the properties of the corresponding database entry.\n        \"\"\"\n        name = new_device_dict[\"data_acquisition_device_name\"]\n        all_values = DataAcquisitionDevice.fetch(\n            \"data_acquisition_device_name\"\n        ).tolist()\n        if name not in all_values:\n            # no entry with the same name exists, prompt user about adding a new entry\n            print(\n                f\"\\nData acquisition device '{name}' was not found in the database. \"\n                f\"The current values are: {all_values}. \"\n                \"Please ensure that the device you want to add does not already \"\n                \"exist in the database under a different name or spelling. \"\n                \"If you want to use an existing device in the database, \"\n                \"please change the corresponding Device object in the NWB file. \"\n                \"Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                f\"Do you want to add data acquisition device '{name}' to the database? (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                cls.insert1(new_device_dict, skip_duplicates=True)\n                return\n            raise PopulateException(\n                f\"User chose not to add data acquisition device '{name}' to the database.\"\n            )\n\n        # effectively else (entry exists)\n        # check whether the values provided match the values stored in the database\n        db_dict = (\n            DataAcquisitionDevice &amp; {\"data_acquisition_device_name\": name}\n        ).fetch1()\n        if db_dict != new_device_dict:\n            raise PopulateException(\n                f\"Data acquisition device properties of PyNWB Device object with name '{name}': \"\n                f\"{new_device_dict} do not match properties of the corresponding database entry: {db_dict}.\"\n            )\n\n    @classmethod\n    def _add_system(cls, system):\n\"\"\"Check the system value. If it is not in the database, prompt the user to add the value to the database.\n\n        This method also renames the system value \"MCU\" to \"SpikeGadgets\".\n\n        Parameters\n        ----------\n        system : str\n            The system value to check.\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a device system value to the database when prompted.\n\n        Returns\n        -------\n        system : str\n            The system value that was added to the database.\n        \"\"\"\n        if system == \"MCU\":\n            system = \"SpikeGadgets\"\n\n        all_values = DataAcquisitionDeviceSystem.fetch(\n            \"data_acquisition_device_system\"\n        ).tolist()\n        if system not in all_values:\n            print(\n                f\"\\nData acquisition device system '{system}' was not found in the database. \"\n                f\"The current values are: {all_values}. \"\n                \"Please ensure that the system you want to add does not already \"\n                \"exist in the database under a different name or spelling. \"\n                \"If you want to use an existing system in the database, \"\n                \"please change the corresponding Device object in the NWB file. \"\n                \"Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                f\"Do you want to add data acquisition device system '{system}' to the database? (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                key = {\"data_acquisition_device_system\": system}\n                DataAcquisitionDeviceSystem.insert1(key, skip_duplicates=True)\n            else:\n                raise PopulateException(\n                    f\"User chose not to add data acquisition device system '{system}' to the database.\"\n                )\n        return system\n\n    @classmethod\n    def _add_amplifier(cls, amplifier):\n\"\"\"Check the amplifier value. If it is not in the database, prompt the user to add the value to the database.\n\n        Parameters\n        ----------\n        amplifier : str\n            The amplifier value to check.\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a device amplifier value to the database when prompted.\n\n        Returns\n        -------\n        amplifier : str\n            The amplifier value that was added to the database.\n        \"\"\"\n        # standardize how Intan is represented in the database\n        if amplifier.title() == \"Intan\":\n            amplifier = \"Intan\"\n\n        all_values = DataAcquisitionDeviceAmplifier.fetch(\n            \"data_acquisition_device_amplifier\"\n        ).tolist()\n        if amplifier not in all_values:\n            print(\n                f\"\\nData acquisition device amplifier '{amplifier}' was not found in the database. \"\n                f\"The current values are: {all_values}. \"\n                \"Please ensure that the amplifier you want to add does not already \"\n                \"exist in the database under a different name or spelling. \"\n                \"If you want to use an existing name in the database, \"\n                \"please change the corresponding Device object in the NWB file. \"\n                \"Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                f\"Do you want to add data acquisition device amplifier '{amplifier}' to the database? (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                key = {\"data_acquisition_device_amplifier\": amplifier}\n                DataAcquisitionDeviceAmplifier.insert1(\n                    key, skip_duplicates=True\n                )\n            else:\n                raise PopulateException(\n                    f\"User chose not to add data acquisition device amplifier '{amplifier}' to the database.\"\n                )\n        return amplifier\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_device.DataAcquisitionDevice.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Insert data acquisition devices from an NWB file.</p> <p>Note that this does not link the DataAcquisitionDevices with a Session. For that, see DataAcquisitionDeviceList.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, config):\n\"\"\"Insert data acquisition devices from an NWB file.\n\n    Note that this does not link the DataAcquisitionDevices with a Session. For that,\n    see DataAcquisitionDeviceList.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n    \"\"\"\n    _, ndx_devices, _ = cls.get_all_device_names(nwbf, config)\n\n    for device_name in ndx_devices:\n        new_device_dict = dict()\n\n        # read device properties into new_device_dict from PyNWB extension device object\n        nwb_device_obj = ndx_devices[device_name]\n\n        name = nwb_device_obj.name\n        adc_circuit = nwb_device_obj.adc_circuit\n\n        # transform system value. check if value is in DB. if not, prompt user to add an entry or cancel.\n        system = cls._add_system(nwb_device_obj.system)\n\n        # transform amplifier value. check if value is in DB. if not, prompt user to add an entry or cancel.\n        amplifier = cls._add_amplifier(nwb_device_obj.amplifier)\n\n        # standardize how Intan is represented in the database\n        if adc_circuit.title() == \"Intan\":\n            adc_circuit = \"Intan\"\n\n        new_device_dict[\"data_acquisition_device_name\"] = name\n        new_device_dict[\"data_acquisition_device_system\"] = system\n        new_device_dict[\"data_acquisition_device_amplifier\"] = amplifier\n        new_device_dict[\"adc_circuit\"] = adc_circuit\n\n        cls._add_device(new_device_dict)\n\n    if ndx_devices:\n        print(\n            f\"Inserted or referenced data acquisition device(s): {ndx_devices.keys()}\"\n        )\n    else:\n        print(\"No conforming data acquisition device metadata found.\")\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_device.DataAcquisitionDevice.get_all_device_names", "title": "<code>get_all_device_names(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Get a list of all device names in the NWB file, after appending and overwriting by the config file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of data acquisition object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef get_all_device_names(cls, nwbf, config):\n\"\"\"Get a list of all device names in the NWB file, after appending and overwriting by the config file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : list\n        List of data acquisition object names found in the NWB file.\n    \"\"\"\n    # make a dict mapping device name to PyNWB device object for all devices in the NWB file that are\n    # of type ndx_franklab_novela.DataAcqDevice and thus have the required metadata\n    ndx_devices = {\n        device_obj.name: device_obj\n        for device_obj in nwbf.devices.values()\n        if isinstance(device_obj, ndx_franklab_novela.DataAcqDevice)\n    }\n\n    # make a list of device names that are associated with this NWB file\n    if \"DataAcquisitionDevice\" in config:\n        config_devices = [\n            device_dict[\"data_acquisition_device_name\"]\n            for device_dict in config[\"DataAcquisitionDevice\"]\n        ]\n    else:\n        config_devices = list()\n\n    all_device_names = set(ndx_devices.keys()).union(set(config_devices))\n\n    return all_device_names, ndx_devices, config_devices\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_session.Nwbfile", "title": "<code>Nwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass Nwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files.\n    nwb_file_name: varchar(255)   # name of the NWB file\n    ---\n    nwb_file_abs_path: filepath@raw\n    INDEX (nwb_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    @classmethod\n    def insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The relative path to the NWB file.\n        \"\"\"\n        nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n        assert os.path.exists(\n            nwb_file_abs_path\n        ), f\"File does not exist: {nwb_file_abs_path}\"\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n        cls.insert1(key, skip_duplicates=True)\n\n    @staticmethod\n    def get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n        Returns\n        -------\n        nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n        nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n        return str(nwb_file_abspath)\n\n    @staticmethod\n    def add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n        The NWB_LOCK_FILE environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n        \"\"\"\n        key = {\"nwb_file_name\": nwb_file_name}\n        # check to make sure the file exists\n        assert (\n            len((Nwbfile() &amp; key).fetch()) &gt; 0\n        ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n        lock_file.write(f\"{nwb_file_name}\\n\")\n        lock_file.close()\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        This does not delete the files themselves unless delete_files=True is specified\n        Run this after deleting the Nwbfile() entries themselves.\n        \"\"\"\n        schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_nwbfile.Nwbfile.insert_from_relative_file_name", "title": "<code>insert_from_relative_file_name(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Insert a new session from an existing NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The relative path to the NWB file.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The relative path to the NWB file.\n    \"\"\"\n    nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n    assert os.path.exists(\n        nwb_file_abs_path\n    ), f\"File does not exist: {nwb_file_abs_path}\"\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n    cls.insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_nwbfile.Nwbfile.get_abs_path", "title": "<code>get_abs_path(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored raw NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required <p>Returns:</p> Name Type Description <code>nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n    Returns\n    -------\n    nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n    nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n    return str(nwb_file_abspath)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_nwbfile.Nwbfile.add_to_lock", "title": "<code>add_to_lock(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Add the specified NWB file to the file with the list of NWB files to be locked.</p> <p>The NWB_LOCK_FILE environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n    The NWB_LOCK_FILE environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n    \"\"\"\n    key = {\"nwb_file_name\": nwb_file_name}\n    # check to make sure the file exists\n    assert (\n        len((Nwbfile() &amp; key).fetch()) &gt; 0\n    ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n    lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n    lock_file.write(f\"{nwb_file_name}\\n\")\n    lock_file.close()\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_nwbfile.Nwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>This does not delete the files themselves unless delete_files=True is specified Run this after deleting the Nwbfile() entries themselves.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    This does not delete the files themselves unless delete_files=True is specified\n    Run this after deleting the Nwbfile() entries themselves.\n    \"\"\"\n    schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_session.get_config", "title": "<code>get_config(nwb_file_path)</code>", "text": "<p>Return a dictionary of config settings for the given NWB file. If the file does not exist, return an empty dict.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Absolute path to the NWB file.</p> required <code>Returns</code> required <code>d</code> <code>dict</code> <p>Dictionary of configuration settings loaded from the corresponding YAML file</p> required Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_config(nwb_file_path):\n\"\"\"Return a dictionary of config settings for the given NWB file.\n    If the file does not exist, return an empty dict.\n    Parameters\n    ----------\n    nwb_file_path : str\n        Absolute path to the NWB file.\n    Returns\n    -------\n    d : dict\n        Dictionary of configuration settings loaded from the corresponding YAML file\n    \"\"\"\n    if nwb_file_path in __configs:  # load from cache if exists\n        return __configs[nwb_file_path]\n\n    p = Path(nwb_file_path)\n    # NOTE use p.stem[:-1] to remove the underscore that was added to the file\n    config_path = p.parent / (p.stem[:-1] + \"_spyglass_config.yaml\")\n    if not os.path.exists(config_path):\n        print(f\"No config found at file path {config_path}\")\n        return dict()\n    with open(config_path, \"r\") as stream:\n        d = yaml.safe_load(stream)\n\n    # TODO write a JSON schema for the yaml file and validate the yaml file\n    __configs[nwb_file_path] = d  # store in cache\n    return d\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_session.Institution", "title": "<code>Institution</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass Institution(dj.Manual):\n    definition = \"\"\"\n    institution_name: varchar(80)\n    ---\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert institution information from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The NWB file with institution information.\n        \"\"\"\n        if nwbf.institution is None:\n            print(\"No institution metadata found.\\n\")\n            return\n        cls.insert1(\n            dict(institution_name=nwbf.institution), skip_duplicates=True\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_lab.Institution.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert institution information from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The NWB file with institution information.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert institution information from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The NWB file with institution information.\n    \"\"\"\n    if nwbf.institution is None:\n        print(\"No institution metadata found.\\n\")\n        return\n    cls.insert1(\n        dict(institution_name=nwbf.institution), skip_duplicates=True\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_session.Lab", "title": "<code>Lab</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass Lab(dj.Manual):\n    definition = \"\"\"\n    lab_name: varchar(80)\n    ---\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab name information from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The NWB file with lab name information.\n        \"\"\"\n        if nwbf.lab is None:\n            print(\"No lab metadata found.\\n\")\n            return\n        cls.insert1(dict(lab_name=nwbf.lab), skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_lab.Lab.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert lab name information from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The NWB file with lab name information.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab name information from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The NWB file with lab name information.\n    \"\"\"\n    if nwbf.lab is None:\n        print(\"No lab metadata found.\\n\")\n        return\n    cls.insert1(dict(lab_name=nwbf.lab), skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_session.CameraDevice", "title": "<code>CameraDevice</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass CameraDevice(dj.Manual):\n    definition = \"\"\"\n    camera_name: varchar(80)\n    ---\n    meters_per_pixel = 0: float  # height / width of pixel in meters\n    manufacturer = \"\": varchar(2000)\n    model = \"\": varchar(2000)\n    lens = \"\": varchar(2000)\n    camera_id = -1: int\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert camera devices from an NWB file\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n\n        Returns\n        -------\n        device_name_list : list\n            List of camera device object names found in the NWB file.\n        \"\"\"\n        device_name_list = list()\n        for device in nwbf.devices.values():\n            if isinstance(device, ndx_franklab_novela.CameraDevice):\n                device_dict = dict()\n                # TODO ideally the ID is not encoded in the name formatted in a particular way\n                # device.name must have the form \"[any string without a space, usually camera] [int]\"\n                device_dict[\"camera_id\"] = int(str.split(device.name)[1])\n                device_dict[\"camera_name\"] = device.camera_name\n                device_dict[\"manufacturer\"] = device.manufacturer\n                device_dict[\"model\"] = device.model\n                device_dict[\"lens\"] = device.lens\n                device_dict[\"meters_per_pixel\"] = device.meters_per_pixel\n                cls.insert1(device_dict, skip_duplicates=True)\n                device_name_list.append(device_dict[\"camera_name\"])\n        if device_name_list:\n            print(f\"Inserted camera devices {device_name_list}\")\n        else:\n            print(\"No conforming camera device metadata found.\")\n        return device_name_list\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_device.CameraDevice.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert camera devices from an NWB file</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of camera device object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert camera devices from an NWB file\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n\n    Returns\n    -------\n    device_name_list : list\n        List of camera device object names found in the NWB file.\n    \"\"\"\n    device_name_list = list()\n    for device in nwbf.devices.values():\n        if isinstance(device, ndx_franklab_novela.CameraDevice):\n            device_dict = dict()\n            # TODO ideally the ID is not encoded in the name formatted in a particular way\n            # device.name must have the form \"[any string without a space, usually camera] [int]\"\n            device_dict[\"camera_id\"] = int(str.split(device.name)[1])\n            device_dict[\"camera_name\"] = device.camera_name\n            device_dict[\"manufacturer\"] = device.manufacturer\n            device_dict[\"model\"] = device.model\n            device_dict[\"lens\"] = device.lens\n            device_dict[\"meters_per_pixel\"] = device.meters_per_pixel\n            cls.insert1(device_dict, skip_duplicates=True)\n            device_name_list.append(device_dict[\"camera_name\"])\n    if device_name_list:\n        print(f\"Inserted camera devices {device_name_list}\")\n    else:\n        print(\"No conforming camera device metadata found.\")\n    return device_name_list\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_session.Probe", "title": "<code>Probe</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass Probe(dj.Manual):\n    definition = \"\"\"\n    # A configuration of a ProbeType. For most probe types, there is only one configuration, and that configuration\n    # should always be used. For Neuropixels probes, the specific channel map (which electrodes are used,\n    # where are they, and in what order) can differ between users and sessions, and each configuration should have a\n    # different ProbeType.\n    probe_id: varchar(80)     # a unique ID for this probe and dynamic configuration\n    ---\n    -&gt; ProbeType              # the type of probe, selected from a controlled list of probe types\n    -&gt; [nullable] DataAcquisitionDevice  # the data acquisition device used with this Probe\n    contact_side_numbering: enum(\"True\", \"False\")  # if True, then electrode contacts are facing you when numbering them\n    \"\"\"\n\n    class Shank(dj.Part):\n        definition = \"\"\"\n        -&gt; Probe\n        probe_shank: int              # shank number within probe. should be unique within a Probe\n        \"\"\"\n\n    class Electrode(dj.Part):\n        definition = \"\"\"\n        -&gt; Probe.Shank\n        probe_electrode: int          # electrode ID that is output from the data acquisition system\n                                      # probe_electrode should be unique within a Probe\n        ---\n        contact_size = NULL: float    # (um) contact size\n        rel_x = NULL: float           # (um) x coordinate of the electrode within the probe\n        rel_y = NULL: float           # (um) y coordinate of the electrode within the probe\n        rel_z = NULL: float           # (um) z coordinate of the electrode within the probe\n        \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, config):\n\"\"\"Insert probe devices from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : list\n            List of probe device types found in the NWB file.\n        \"\"\"\n        all_probes_types, ndx_probes, _ = cls.get_all_probe_names(nwbf, config)\n\n        for probe_type in all_probes_types:\n            new_probe_type_dict = dict()\n            new_probe_dict = dict()\n            shank_dict = dict()\n            elect_dict = dict()\n            num_shanks = 0\n\n            if probe_type in ndx_probes:\n                # read probe properties into new_probe_dict from PyNWB extension probe object\n                nwb_probe_obj = ndx_probes[probe_type]\n                cls.__read_ndx_probe_data(\n                    nwb_probe_obj,\n                    new_probe_type_dict,\n                    new_probe_dict,\n                    shank_dict,\n                    elect_dict,\n                )\n\n            # check that number of shanks is consistent\n            num_shanks = new_probe_type_dict[\"num_shanks\"]\n            assert num_shanks == 0 or num_shanks == len(\n                shank_dict\n            ), \"`num_shanks` is not equal to the number of shanks.\"\n\n            # if probe id already exists, do not overwrite anything or create new Shanks and Electrodes\n            # TODO test whether the Shanks and Electrodes in the NWB file match the ones in the database\n            query = Probe &amp; {\"probe_id\": new_probe_dict[\"probe_id\"]}\n            if len(query) &gt; 0:\n                print(\n                    f\"Probe ID '{new_probe_dict['probe_id']}' already exists in the database. Spyglass will use \"\n                    \"that and not create a new Probe, Shanks, or Electrodes.\"\n                )\n                continue\n\n            cls.insert1(new_probe_dict, skip_duplicates=True)\n\n            for shank in shank_dict.values():\n                cls.Shank.insert1(shank, skip_duplicates=True)\n            for electrode in elect_dict.values():\n                cls.Electrode.insert1(electrode, skip_duplicates=True)\n\n        if all_probes_types:\n            print(f\"Inserted probes {all_probes_types}\")\n        else:\n            print(\"No conforming probe metadata found.\")\n\n        return all_probes_types\n\n    @classmethod\n    def get_all_probe_names(cls, nwbf, config):\n\"\"\"Get a list of all device names in the NWB file, after appending and overwriting by the config file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : list\n            List of data acquisition object names found in the NWB file.\n        \"\"\"\n\n        # make a dict mapping probe type to PyNWB object for all devices in the NWB file that are\n        # of type ndx_franklab_novela.Probe and thus have the required metadata\n        ndx_probes = {\n            device_obj.probe_type: device_obj\n            for device_obj in nwbf.devices.values()\n            if isinstance(device_obj, ndx_franklab_novela.Probe)\n        }\n\n        # make a dict mapping probe type to dict of device metadata from the config YAML if exists\n        if \"Probe\" in config:\n            config_probes = [\n                probe_dict[\"probe_type\"] for probe_dict in config[\"Probe\"]\n            ]\n        else:\n            config_probes = list()\n\n        # get all the probe types from the NWB file plus the config YAML\n        all_probes_types = set(ndx_probes.keys()).union(set(config_probes))\n\n        return all_probes_types, ndx_probes, config_probes\n\n    @classmethod\n    def __read_ndx_probe_data(\n        cls,\n        nwb_probe_obj: ndx_franklab_novela.Probe,\n        new_probe_type_dict: dict,\n        new_probe_dict: dict,\n        shank_dict: dict,\n        elect_dict: dict,\n    ):\n        # construct dictionary of values to add to ProbeType\n        new_probe_type_dict[\"manufacturer\"] = (\n            getattr(nwb_probe_obj, \"manufacturer\") or \"\"\n        )\n        new_probe_type_dict[\"probe_type\"] = nwb_probe_obj.probe_type\n        new_probe_type_dict[\n            \"probe_description\"\n        ] = nwb_probe_obj.probe_description\n        new_probe_type_dict[\"num_shanks\"] = len(nwb_probe_obj.shanks)\n\n        cls._add_probe_type(new_probe_type_dict)\n\n        new_probe_dict[\"probe_id\"] = nwb_probe_obj.probe_type\n        new_probe_dict[\"probe_type\"] = nwb_probe_obj.probe_type\n        new_probe_dict[\"contact_side_numbering\"] = (\n            \"True\" if nwb_probe_obj.contact_side_numbering else \"False\"\n        )\n\n        # go through the shanks and add each one to the Shank table\n        for shank in nwb_probe_obj.shanks.values():\n            shank_dict[shank.name] = dict()\n            shank_dict[shank.name][\"probe_id\"] = new_probe_dict[\"probe_type\"]\n            shank_dict[shank.name][\"probe_shank\"] = int(shank.name)\n\n            # go through the electrodes and add each one to the Electrode table\n            for electrode in shank.shanks_electrodes.values():\n                # the next line will need to be fixed if we have different sized contacts on a shank\n                elect_dict[electrode.name] = dict()\n                elect_dict[electrode.name][\"probe_id\"] = new_probe_dict[\n                    \"probe_type\"\n                ]\n                elect_dict[electrode.name][\"probe_shank\"] = shank_dict[\n                    shank.name\n                ][\"probe_shank\"]\n                elect_dict[electrode.name][\n                    \"contact_size\"\n                ] = nwb_probe_obj.contact_size\n                elect_dict[electrode.name][\"probe_electrode\"] = int(\n                    electrode.name\n                )\n                elect_dict[electrode.name][\"rel_x\"] = electrode.rel_x\n                elect_dict[electrode.name][\"rel_y\"] = electrode.rel_y\n                elect_dict[electrode.name][\"rel_z\"] = electrode.rel_z\n\n    @classmethod\n    def _add_probe_type(cls, new_probe_type_dict):\n\"\"\"Check the probe type value against the values in the database.\n\n        Parameters\n        ----------\n        new_probe_type_dict : dict\n            Dictionary of probe type properties. See ProbeType for keys.\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a probe type to the database when prompted.\n\n        Returns\n        -------\n        probe_type : str\n            The probe type value that was added to the database.\n        \"\"\"\n        probe_type = new_probe_type_dict[\"probe_type\"]\n        all_values = ProbeType.fetch(\"probe_type\").tolist()\n        if probe_type not in all_values:\n            print(\n                f\"\\nProbe type '{probe_type}' was not found in the database. \"\n                f\"The current values are: {all_values}. \"\n                \"Please ensure that the probe type you want to add does not already \"\n                \"exist in the database under a different name or spelling. \"\n                \"If you want to use an existing name in the database, \"\n                \"please change the corresponding Probe object in the NWB file. \"\n                \"Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                f\"Do you want to add probe type '{probe_type}' to the database? (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                ProbeType.insert1(new_probe_type_dict, skip_duplicates=True)\n                return\n            raise PopulateException(\n                f\"User chose not to add probe type '{probe_type}' to the database.\"\n            )\n\n        # effectively else (entry exists)\n        # check whether the values provided match the values stored in the database\n        db_dict = (ProbeType &amp; {\"probe_type\": probe_type}).fetch1()\n        if db_dict != new_probe_type_dict:\n            raise PopulateException(\n                f\"\\nProbe type properties of PyNWB Probe object with name '{probe_type}': \"\n                f\"{new_probe_type_dict} do not match properties of the corresponding database entry: {db_dict}.\"\n            )\n        return probe_type\n\n    @classmethod\n    def create_from_nwbfile(\n        cls,\n        nwb_file_name: str,\n        nwb_device_name: str,\n        probe_id: str,\n        probe_type: str,\n        contact_side_numbering: bool,\n    ):\n\"\"\"Create a Probe entry and corresponding part table entries using the data in the NWB file.\n\n        This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices\n        (as probes) in the NWB file, but only ones that are associated with the device that matches the given\n        `nwb_device_name`.\n\n        Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe,\n        the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.\n\n        Example usage:\n        ```\n        sgc.Probe.create_from_nwbfile(\n            nwbfile=nwb_file_name,\n            nwb_device_name=\"Device\",\n            probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n            probe_type=\"Neuropixels 1.0\",\n            contact_side_numbering=True\n        )\n        ```\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the NWB file.\n        nwb_device_name : str\n            The name of the PyNWB Device object that represents the probe to read in the NWB file.\n        probe_id : str\n            A unique ID for the probe and its configuration, to be used as the primary key for the new Probe entry.\n        probe_type : str\n            The existing ProbeType entry that represents the type of probe being created. It must exist.\n        contact_side_numbering : bool\n            Whether the electrode contacts are facing you when numbering them. Stored in the new Probe entry.\n        \"\"\"\n\n        from .common_nwbfile import Nwbfile\n\n        nwb_file_path = Nwbfile.get_abs_path(nwb_file_name)\n        nwbfile = get_nwb_file(nwb_file_path)\n\n        query = ProbeType &amp; {\"probe_type\": probe_type}\n        if len(query) == 0:\n            print(\n                f\"No ProbeType found with probe_type '{probe_type}'. Aborting.\"\n            )\n            return\n\n        new_probe_dict = dict()\n        shank_dict = dict()\n        elect_dict = dict()\n\n        new_probe_dict[\"probe_id\"] = probe_id\n        new_probe_dict[\"probe_type\"] = probe_type\n        new_probe_dict[\"contact_side_numbering\"] = (\n            \"True\" if contact_side_numbering else \"False\"\n        )\n\n        # iterate through the electrodes table in the NWB file\n        # and use the group column (ElectrodeGroup) to create shanks\n        # and use the device attribute of each ElectrodeGroup to create a probe\n        created_shanks = dict()  # map device name to shank_index (int)\n        device_found = False\n        for elec_index in range(len(nwbfile.electrodes)):\n            electrode_group = nwbfile.electrodes[elec_index, \"group\"]\n            eg_device_name = electrode_group.device.name\n\n            # only look at electrodes where the associated device is the one specified\n            if eg_device_name == nwb_device_name:\n                device_found = True\n\n                # if a Shank has not yet been created from the electrode group, then create it\n                if electrode_group.name not in created_shanks:\n                    shank_index = len(created_shanks)\n                    created_shanks[electrode_group.name] = shank_index\n\n                    # build the dictionary of Probe.Shank data\n                    shank_dict[shank_index] = dict()\n                    shank_dict[shank_index][\"probe_id\"] = new_probe_dict[\n                        \"probe_id\"\n                    ]\n                    shank_dict[shank_index][\"probe_shank\"] = shank_index\n\n                # get the probe shank index associated with this Electrode\n                probe_shank = created_shanks[electrode_group.name]\n\n                # build the dictionary of Probe.Electrode data\n                elect_dict[elec_index] = dict()\n                elect_dict[elec_index][\"probe_id\"] = new_probe_dict[\"probe_id\"]\n                elect_dict[elec_index][\"probe_shank\"] = probe_shank\n                elect_dict[elec_index][\"probe_electrode\"] = elec_index\n                if \"rel_x\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_x\"] = nwbfile.electrodes[\n                        elec_index, \"rel_x\"\n                    ]\n                if \"rel_y\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_y\"] = nwbfile.electrodes[\n                        elec_index, \"rel_y\"\n                    ]\n                if \"rel_z\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_z\"] = nwbfile.electrodes[\n                        elec_index, \"rel_z\"\n                    ]\n\n        if not device_found:\n            print(\n                f\"No electrodes in the NWB file were associated with a device named '{nwb_device_name}'.\"\n            )\n            return\n\n        # insert the Probe, then the Shank parts, and then the Electrode parts\n        cls.insert1(new_probe_dict, skip_duplicates=True)\n\n        for shank in shank_dict.values():\n            cls.Shank.insert1(shank, skip_duplicates=True)\n        for electrode in elect_dict.values():\n            cls.Electrode.insert1(electrode, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_device.Probe.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Insert probe devices from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of probe device types found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, config):\n\"\"\"Insert probe devices from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : list\n        List of probe device types found in the NWB file.\n    \"\"\"\n    all_probes_types, ndx_probes, _ = cls.get_all_probe_names(nwbf, config)\n\n    for probe_type in all_probes_types:\n        new_probe_type_dict = dict()\n        new_probe_dict = dict()\n        shank_dict = dict()\n        elect_dict = dict()\n        num_shanks = 0\n\n        if probe_type in ndx_probes:\n            # read probe properties into new_probe_dict from PyNWB extension probe object\n            nwb_probe_obj = ndx_probes[probe_type]\n            cls.__read_ndx_probe_data(\n                nwb_probe_obj,\n                new_probe_type_dict,\n                new_probe_dict,\n                shank_dict,\n                elect_dict,\n            )\n\n        # check that number of shanks is consistent\n        num_shanks = new_probe_type_dict[\"num_shanks\"]\n        assert num_shanks == 0 or num_shanks == len(\n            shank_dict\n        ), \"`num_shanks` is not equal to the number of shanks.\"\n\n        # if probe id already exists, do not overwrite anything or create new Shanks and Electrodes\n        # TODO test whether the Shanks and Electrodes in the NWB file match the ones in the database\n        query = Probe &amp; {\"probe_id\": new_probe_dict[\"probe_id\"]}\n        if len(query) &gt; 0:\n            print(\n                f\"Probe ID '{new_probe_dict['probe_id']}' already exists in the database. Spyglass will use \"\n                \"that and not create a new Probe, Shanks, or Electrodes.\"\n            )\n            continue\n\n        cls.insert1(new_probe_dict, skip_duplicates=True)\n\n        for shank in shank_dict.values():\n            cls.Shank.insert1(shank, skip_duplicates=True)\n        for electrode in elect_dict.values():\n            cls.Electrode.insert1(electrode, skip_duplicates=True)\n\n    if all_probes_types:\n        print(f\"Inserted probes {all_probes_types}\")\n    else:\n        print(\"No conforming probe metadata found.\")\n\n    return all_probes_types\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_device.Probe.get_all_probe_names", "title": "<code>get_all_probe_names(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Get a list of all device names in the NWB file, after appending and overwriting by the config file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of data acquisition object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef get_all_probe_names(cls, nwbf, config):\n\"\"\"Get a list of all device names in the NWB file, after appending and overwriting by the config file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : list\n        List of data acquisition object names found in the NWB file.\n    \"\"\"\n\n    # make a dict mapping probe type to PyNWB object for all devices in the NWB file that are\n    # of type ndx_franklab_novela.Probe and thus have the required metadata\n    ndx_probes = {\n        device_obj.probe_type: device_obj\n        for device_obj in nwbf.devices.values()\n        if isinstance(device_obj, ndx_franklab_novela.Probe)\n    }\n\n    # make a dict mapping probe type to dict of device metadata from the config YAML if exists\n    if \"Probe\" in config:\n        config_probes = [\n            probe_dict[\"probe_type\"] for probe_dict in config[\"Probe\"]\n        ]\n    else:\n        config_probes = list()\n\n    # get all the probe types from the NWB file plus the config YAML\n    all_probes_types = set(ndx_probes.keys()).union(set(config_probes))\n\n    return all_probes_types, ndx_probes, config_probes\n</code></pre>"}, {"location": "api/src/spyglass/common/common_session/#src.spyglass.common.common_device.Probe.create_from_nwbfile", "title": "<code>create_from_nwbfile(nwb_file_name, nwb_device_name, probe_id, probe_type, contact_side_numbering)</code>  <code>classmethod</code>", "text": "<p>Create a Probe entry and corresponding part table entries using the data in the NWB file.</p> <p>This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices (as probes) in the NWB file, but only ones that are associated with the device that matches the given <code>nwb_device_name</code>.</p> <p>Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe, the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.</p> <p>Example usage:</p> <pre><code>sgc.Probe.create_from_nwbfile(\n    nwbfile=nwb_file_name,\n    nwb_device_name=\"Device\",\n    probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n    probe_type=\"Neuropixels 1.0\",\n    contact_side_numbering=True\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file.</p> required <code>nwb_device_name</code> <code>str</code> <p>The name of the PyNWB Device object that represents the probe to read in the NWB file.</p> required <code>probe_id</code> <code>str</code> <p>A unique ID for the probe and its configuration, to be used as the primary key for the new Probe entry.</p> required <code>probe_type</code> <code>str</code> <p>The existing ProbeType entry that represents the type of probe being created. It must exist.</p> required <code>contact_side_numbering</code> <code>bool</code> <p>Whether the electrode contacts are facing you when numbering them. Stored in the new Probe entry.</p> required Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef create_from_nwbfile(\n    cls,\n    nwb_file_name: str,\n    nwb_device_name: str,\n    probe_id: str,\n    probe_type: str,\n    contact_side_numbering: bool,\n):\n\"\"\"Create a Probe entry and corresponding part table entries using the data in the NWB file.\n\n    This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices\n    (as probes) in the NWB file, but only ones that are associated with the device that matches the given\n    `nwb_device_name`.\n\n    Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe,\n    the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.\n\n    Example usage:\n    ```\n    sgc.Probe.create_from_nwbfile(\n        nwbfile=nwb_file_name,\n        nwb_device_name=\"Device\",\n        probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n        probe_type=\"Neuropixels 1.0\",\n        contact_side_numbering=True\n    )\n    ```\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    nwb_device_name : str\n        The name of the PyNWB Device object that represents the probe to read in the NWB file.\n    probe_id : str\n        A unique ID for the probe and its configuration, to be used as the primary key for the new Probe entry.\n    probe_type : str\n        The existing ProbeType entry that represents the type of probe being created. It must exist.\n    contact_side_numbering : bool\n        Whether the electrode contacts are facing you when numbering them. Stored in the new Probe entry.\n    \"\"\"\n\n    from .common_nwbfile import Nwbfile\n\n    nwb_file_path = Nwbfile.get_abs_path(nwb_file_name)\n    nwbfile = get_nwb_file(nwb_file_path)\n\n    query = ProbeType &amp; {\"probe_type\": probe_type}\n    if len(query) == 0:\n        print(\n            f\"No ProbeType found with probe_type '{probe_type}'. Aborting.\"\n        )\n        return\n\n    new_probe_dict = dict()\n    shank_dict = dict()\n    elect_dict = dict()\n\n    new_probe_dict[\"probe_id\"] = probe_id\n    new_probe_dict[\"probe_type\"] = probe_type\n    new_probe_dict[\"contact_side_numbering\"] = (\n        \"True\" if contact_side_numbering else \"False\"\n    )\n\n    # iterate through the electrodes table in the NWB file\n    # and use the group column (ElectrodeGroup) to create shanks\n    # and use the device attribute of each ElectrodeGroup to create a probe\n    created_shanks = dict()  # map device name to shank_index (int)\n    device_found = False\n    for elec_index in range(len(nwbfile.electrodes)):\n        electrode_group = nwbfile.electrodes[elec_index, \"group\"]\n        eg_device_name = electrode_group.device.name\n\n        # only look at electrodes where the associated device is the one specified\n        if eg_device_name == nwb_device_name:\n            device_found = True\n\n            # if a Shank has not yet been created from the electrode group, then create it\n            if electrode_group.name not in created_shanks:\n                shank_index = len(created_shanks)\n                created_shanks[electrode_group.name] = shank_index\n\n                # build the dictionary of Probe.Shank data\n                shank_dict[shank_index] = dict()\n                shank_dict[shank_index][\"probe_id\"] = new_probe_dict[\n                    \"probe_id\"\n                ]\n                shank_dict[shank_index][\"probe_shank\"] = shank_index\n\n            # get the probe shank index associated with this Electrode\n            probe_shank = created_shanks[electrode_group.name]\n\n            # build the dictionary of Probe.Electrode data\n            elect_dict[elec_index] = dict()\n            elect_dict[elec_index][\"probe_id\"] = new_probe_dict[\"probe_id\"]\n            elect_dict[elec_index][\"probe_shank\"] = probe_shank\n            elect_dict[elec_index][\"probe_electrode\"] = elec_index\n            if \"rel_x\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_x\"] = nwbfile.electrodes[\n                    elec_index, \"rel_x\"\n                ]\n            if \"rel_y\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_y\"] = nwbfile.electrodes[\n                    elec_index, \"rel_y\"\n                ]\n            if \"rel_z\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_z\"] = nwbfile.electrodes[\n                    elec_index, \"rel_z\"\n                ]\n\n    if not device_found:\n        print(\n            f\"No electrodes in the NWB file were associated with a device named '{nwb_device_name}'.\"\n        )\n        return\n\n    # insert the Probe, then the Shank parts, and then the Electrode parts\n    cls.insert1(new_probe_dict, skip_duplicates=True)\n\n    for shank in shank_dict.values():\n        cls.Shank.insert1(shank, skip_duplicates=True)\n    for electrode in elect_dict.values():\n        cls.Electrode.insert1(electrode, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_subject/", "title": "common_subject.py", "text": ""}, {"location": "api/src/spyglass/common/common_subject/#src.spyglass.common.common_subject.Subject", "title": "<code>Subject</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_subject.py</code> <pre><code>@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id: varchar(80)\n    ---\n    age = NULL: varchar(200)\n    description = NULL: varchar(2000)\n    genotype = NULL: varchar(2000)\n    sex = \"U\": enum(\"M\", \"F\", \"U\")\n    species = NULL: varchar(200)\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Get the subject information from the NWBFile and insert it into the Subject table.\"\"\"\n        sub = nwbf.subject\n        if sub is None:\n            print(\"No subject metadata found.\\n\")\n            return\n        subject_dict = dict()\n        subject_dict[\"subject_id\"] = sub.subject_id\n        subject_dict[\"age\"] = sub.age\n        subject_dict[\"description\"] = sub.description\n        subject_dict[\"genotype\"] = sub.genotype\n        if sub.sex in (\"Male\", \"male\", \"M\", \"m\"):\n            sex = \"M\"\n        elif sub.sex in (\"Female\", \"female\", \"F\", \"f\"):\n            sex = \"F\"\n        else:\n            sex = \"U\"\n        subject_dict[\"sex\"] = sex\n        subject_dict[\"species\"] = sub.species\n        cls.insert1(subject_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_subject/#src.spyglass.common.common_subject.Subject.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Get the subject information from the NWBFile and insert it into the Subject table.</p> Source code in <code>src/spyglass/common/common_subject.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Get the subject information from the NWBFile and insert it into the Subject table.\"\"\"\n    sub = nwbf.subject\n    if sub is None:\n        print(\"No subject metadata found.\\n\")\n        return\n    subject_dict = dict()\n    subject_dict[\"subject_id\"] = sub.subject_id\n    subject_dict[\"age\"] = sub.age\n    subject_dict[\"description\"] = sub.description\n    subject_dict[\"genotype\"] = sub.genotype\n    if sub.sex in (\"Male\", \"male\", \"M\", \"m\"):\n        sex = \"M\"\n    elif sub.sex in (\"Female\", \"female\", \"F\", \"f\"):\n        sex = \"F\"\n    else:\n        sex = \"U\"\n    subject_dict[\"sex\"] = sex\n    subject_dict[\"species\"] = sub.species\n    cls.insert1(subject_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/", "title": "common_task.py", "text": ""}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.Task", "title": "<code>Task</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@schema\nclass Task(dj.Manual):\n    definition = \"\"\"\n     task_name: varchar(80)\n     ---\n     task_description = NULL: varchar(2000)    # description of this task\n     task_type = NULL: varchar(2000)           # type of task\n     task_subtype = NULL: varchar(2000)        # subtype of task\n     \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert tasks from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        \"\"\"\n        tasks_mod = nwbf.processing.get(\"tasks\")\n        if tasks_mod is None:\n            print(f\"No tasks processing module found in {nwbf}\\n\")\n            return\n        for task in tasks_mod.data_interfaces.values():\n            if cls.check_task_table(task):\n                cls.insert_from_task_table(task)\n\n    @classmethod\n    def insert_from_task_table(cls, task_table):\n\"\"\"Insert tasks from a pynwb DynamicTable containing task metadata.\n\n        Duplicate tasks will not be added.\n\n        Parameters\n        ----------\n        task_table : pynwb.core.DynamicTable\n            The table representing task metadata.\n        \"\"\"\n        taskdf = task_table.to_dataframe()\n        for task_entry in taskdf.iterrows():\n            task_dict = dict()\n            task_dict[\"task_name\"] = task_entry[1].task_name\n            task_dict[\"task_description\"] = task_entry[1].task_description\n            cls.insert1(task_dict, skip_duplicates=True)\n\n    @classmethod\n    def check_task_table(cls, task_table):\n\"\"\"Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.\n\n\n        The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name' and\n        'task_description'.\n\n        Parameters\n        ----------\n        task_table : pynwb.core.DynamicTable\n            The table representing task metadata.\n\n        Returns\n        -------\n        bool\n            Whether the DynamicTable conforms to the expected format for loading data into the Task table.\n        \"\"\"\n        return (\n            isinstance(task_table, pynwb.core.DynamicTable)\n            and hasattr(task_table, \"task_name\")\n            and hasattr(task_table, \"task_description\")\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.Task.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert tasks from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert tasks from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    \"\"\"\n    tasks_mod = nwbf.processing.get(\"tasks\")\n    if tasks_mod is None:\n        print(f\"No tasks processing module found in {nwbf}\\n\")\n        return\n    for task in tasks_mod.data_interfaces.values():\n        if cls.check_task_table(task):\n            cls.insert_from_task_table(task)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.Task.insert_from_task_table", "title": "<code>insert_from_task_table(task_table)</code>  <code>classmethod</code>", "text": "<p>Insert tasks from a pynwb DynamicTable containing task metadata.</p> <p>Duplicate tasks will not be added.</p> <p>Parameters:</p> Name Type Description Default <code>task_table</code> <code>pynwb.core.DynamicTable</code> <p>The table representing task metadata.</p> required Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@classmethod\ndef insert_from_task_table(cls, task_table):\n\"\"\"Insert tasks from a pynwb DynamicTable containing task metadata.\n\n    Duplicate tasks will not be added.\n\n    Parameters\n    ----------\n    task_table : pynwb.core.DynamicTable\n        The table representing task metadata.\n    \"\"\"\n    taskdf = task_table.to_dataframe()\n    for task_entry in taskdf.iterrows():\n        task_dict = dict()\n        task_dict[\"task_name\"] = task_entry[1].task_name\n        task_dict[\"task_description\"] = task_entry[1].task_description\n        cls.insert1(task_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.Task.check_task_table", "title": "<code>check_task_table(task_table)</code>  <code>classmethod</code>", "text": "<p>Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.</p> <p>The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name' and 'task_description'.</p> <p>Parameters:</p> Name Type Description Default <code>task_table</code> <code>pynwb.core.DynamicTable</code> <p>The table representing task metadata.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the DynamicTable conforms to the expected format for loading data into the Task table.</p> Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@classmethod\ndef check_task_table(cls, task_table):\n\"\"\"Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.\n\n\n    The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name' and\n    'task_description'.\n\n    Parameters\n    ----------\n    task_table : pynwb.core.DynamicTable\n        The table representing task metadata.\n\n    Returns\n    -------\n    bool\n        Whether the DynamicTable conforms to the expected format for loading data into the Task table.\n    \"\"\"\n    return (\n        isinstance(task_table, pynwb.core.DynamicTable)\n        and hasattr(task_table, \"task_name\")\n        and hasattr(task_table, \"task_description\")\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.IntervalList", "title": "<code>IntervalList</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@schema\nclass IntervalList(dj.Manual):\n    definition = \"\"\"\n    # Time intervals used for analysis\n    -&gt; Session\n    interval_list_name: varchar(200)  # descriptive name of this interval list\n    ---\n    valid_times: longblob  # numpy array with start and end times for each interval\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n        The interval list name for each epoch is set to the first tag for the epoch.\n        If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n        (0-indexed) of the epoch in the epochs table.\n        The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n        [start time, stop time] for each epoch.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        nwb_file_name : str\n            The file name of the NWB file, used as a primary key to the Session table.\n        \"\"\"\n        if nwbf.epochs is None:\n            print(\"No epochs found in NWB file.\")\n            return\n        epochs = nwbf.epochs.to_dataframe()\n        for epoch_index, epoch_data in epochs.iterrows():\n            epoch_dict = dict()\n            epoch_dict[\"nwb_file_name\"] = nwb_file_name\n            if epoch_data.tags[0]:\n                epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n            else:\n                epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                    epoch_index\n                )\n            epoch_dict[\"valid_times\"] = np.asarray(\n                [[epoch_data.start_time, epoch_data.stop_time]]\n            )\n            cls.insert1(epoch_dict, skip_duplicates=True)\n\n    def plot_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=figsize)\n        interval_count = 0\n        for row in interval_list.itertuples(index=False):\n            for interval in row.valid_times:\n                ax.plot(interval, [interval_count, interval_count])\n                ax.scatter(\n                    interval,\n                    [interval_count, interval_count],\n                    alpha=0.8,\n                    zorder=2,\n                )\n            interval_count += 1\n        ax.set_yticks(np.arange(interval_list.shape[0]))\n        ax.set_yticklabels(interval_list.interval_list_name)\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n\n    def plot_epoch_pos_raw_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=(30, 3))\n\n        raw_data_valid_times = interval_list.loc[\n            interval_list.interval_list_name == \"raw data valid times\"\n        ].valid_times\n        interval_y = 1\n\n        for interval in np.asarray(raw_data_valid_times)[0]:\n            ax.plot(interval, [interval_y, interval_y])\n            ax.scatter(interval, [interval_y, interval_y], alpha=0.8, zorder=2)\n\n        epoch_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n        interval_y = 2\n        for epoch, valid_times in zip(\n            epoch_valid_times.index, epoch_valid_times\n        ):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        pos_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^pos \\d+ valid times$\", axis=0)\n            .valid_times\n        ).sort_index(key=lambda index: [int(name.split()[1]) for name in index])\n        interval_y = 0\n        for epoch, valid_times in zip(pos_valid_times.index, pos_valid_times):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch.strip(\" valid times\"),\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        ax.set_ylim((-0.25, 2.25))\n        ax.set_yticks(np.arange(3))\n        ax.set_yticklabels([\"pos valid times\", \"raw data valid times\", \"epoch\"])\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_interval.IntervalList.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, *, nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add each entry in the NWB file epochs table to the IntervalList table.</p> <p>The interval list name for each epoch is set to the first tag for the epoch. If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index (0-indexed) of the epoch in the epochs table. The start time and stop time of the epoch are stored in the valid_times field as a numpy array of [start time, stop time] for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>nwb_file_name</code> <code>str</code> <p>The file name of the NWB file, used as a primary key to the Session table.</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n    The interval list name for each epoch is set to the first tag for the epoch.\n    If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n    (0-indexed) of the epoch in the epochs table.\n    The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n    [start time, stop time] for each epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    nwb_file_name : str\n        The file name of the NWB file, used as a primary key to the Session table.\n    \"\"\"\n    if nwbf.epochs is None:\n        print(\"No epochs found in NWB file.\")\n        return\n    epochs = nwbf.epochs.to_dataframe()\n    for epoch_index, epoch_data in epochs.iterrows():\n        epoch_dict = dict()\n        epoch_dict[\"nwb_file_name\"] = nwb_file_name\n        if epoch_data.tags[0]:\n            epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n        else:\n            epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                epoch_index\n            )\n        epoch_dict[\"valid_times\"] = np.asarray(\n            [[epoch_data.start_time, epoch_data.stop_time]]\n        )\n        cls.insert1(epoch_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.get_nwb_file", "title": "<code>get_nwb_file(nwb_file_path)</code>", "text": "<p>Return an NWBFile object with the given file path in read mode.    If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Path to the NWB file.</p> required <p>Returns:</p> Name Type Description <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>NWB file object for the given path opened in read mode.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_file(nwb_file_path):\n\"\"\"Return an NWBFile object with the given file path in read mode.\n       If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.\n\n    Parameters\n    ----------\n    nwb_file_path : str\n        Path to the NWB file.\n\n    Returns\n    -------\n    nwbfile : pynwb.NWBFile\n        NWB file object for the given path opened in read mode.\n    \"\"\"\n    _, nwbfile = __open_nwb_files.get(nwb_file_path, (None, None))\n    nwb_uri = None\n    nwb_raw_uri = None\n    if nwbfile is None:\n        # check to see if the file exists\n        if not os.path.exists(nwb_file_path):\n            print(\n                f\"NWB file {nwb_file_path} does not exist locally; checking kachery\"\n            )\n            # first try the analysis files\n            from ..sharing.sharing_kachery import AnalysisNwbfileKachery\n\n            # the download functions assume just the filename, so we need to get that from the path\n            if not AnalysisNwbfileKachery.download_file(\n                os.path.basename(nwb_file_path)\n            ):\n                return None\n        # now open the file\n        io = pynwb.NWBHDF5IO(\n            path=nwb_file_path, mode=\"r\", load_namespaces=True\n        )  # keep file open\n        nwbfile = io.read()\n        __open_nwb_files[nwb_file_path] = (io, nwbfile)\n\n    return nwbfile\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.Nwbfile", "title": "<code>Nwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass Nwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files.\n    nwb_file_name: varchar(255)   # name of the NWB file\n    ---\n    nwb_file_abs_path: filepath@raw\n    INDEX (nwb_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    @classmethod\n    def insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The relative path to the NWB file.\n        \"\"\"\n        nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n        assert os.path.exists(\n            nwb_file_abs_path\n        ), f\"File does not exist: {nwb_file_abs_path}\"\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n        cls.insert1(key, skip_duplicates=True)\n\n    @staticmethod\n    def get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n        Returns\n        -------\n        nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n        nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n        return str(nwb_file_abspath)\n\n    @staticmethod\n    def add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n        The NWB_LOCK_FILE environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n        \"\"\"\n        key = {\"nwb_file_name\": nwb_file_name}\n        # check to make sure the file exists\n        assert (\n            len((Nwbfile() &amp; key).fetch()) &gt; 0\n        ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n        lock_file.write(f\"{nwb_file_name}\\n\")\n        lock_file.close()\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        This does not delete the files themselves unless delete_files=True is specified\n        Run this after deleting the Nwbfile() entries themselves.\n        \"\"\"\n        schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_nwbfile.Nwbfile.insert_from_relative_file_name", "title": "<code>insert_from_relative_file_name(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Insert a new session from an existing NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The relative path to the NWB file.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The relative path to the NWB file.\n    \"\"\"\n    nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n    assert os.path.exists(\n        nwb_file_abs_path\n    ), f\"File does not exist: {nwb_file_abs_path}\"\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n    cls.insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_nwbfile.Nwbfile.get_abs_path", "title": "<code>get_abs_path(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored raw NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required <p>Returns:</p> Name Type Description <code>nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n    Returns\n    -------\n    nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n    nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n    return str(nwb_file_abspath)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_nwbfile.Nwbfile.add_to_lock", "title": "<code>add_to_lock(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Add the specified NWB file to the file with the list of NWB files to be locked.</p> <p>The NWB_LOCK_FILE environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n    The NWB_LOCK_FILE environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n    \"\"\"\n    key = {\"nwb_file_name\": nwb_file_name}\n    # check to make sure the file exists\n    assert (\n        len((Nwbfile() &amp; key).fetch()) &gt; 0\n    ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n    lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n    lock_file.write(f\"{nwb_file_name}\\n\")\n    lock_file.close()\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_nwbfile.Nwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>This does not delete the files themselves unless delete_files=True is specified Run this after deleting the Nwbfile() entries themselves.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    This does not delete the files themselves unless delete_files=True is specified\n    Run this after deleting the Nwbfile() entries themselves.\n    \"\"\"\n    schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.TaskEpoch", "title": "<code>TaskEpoch</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@schema\nclass TaskEpoch(dj.Imported):\n    # Tasks, session and time intervals\n    definition = \"\"\"\n     -&gt; Session\n     epoch: int  # the session epoch for this task and apparatus(1 based)\n     ---\n     -&gt; Task\n     -&gt; [nullable] CameraDevice\n     -&gt; IntervalList\n     task_environment = NULL: varchar(200)  # the environment the animal was in\n     camera_names : blob # list of keys corresponding to entry in CameraDevice\n     \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile().get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        camera_names = dict()\n        # the tasks refer to the camera_id which is unique for the NWB file but not for CameraDevice schema, so we\n        # need to look up the right camera\n        # map camera ID (in camera name) to camera_name\n        for device in nwbf.devices.values():\n            if isinstance(device, ndx_franklab_novela.CameraDevice):\n                # get the camera ID\n                camera_id = int(str.split(device.name)[1])\n                camera_names[camera_id] = device.camera_name\n\n        # find the task modules and for each one, add the task to the Task schema if it isn't there\n        # and then add an entry for each epoch\n        tasks_mod = nwbf.processing.get(\"tasks\")\n        if tasks_mod is None:\n            print(f\"No tasks processing module found in {nwbf}\\n\")\n            return\n\n        for task in tasks_mod.data_interfaces.values():\n            if self.check_task_table(task):\n                # check if the task is in the Task table and if not, add it\n                Task.insert_from_task_table(task)\n                key[\"task_name\"] = task.task_name[0]\n\n                # get the CameraDevice used for this task (primary key is camera name so we need\n                # to map from ID to name)\n                camera_ids = task.camera_id[0]\n                valid_camera_ids = [\n                    camera_id\n                    for camera_id in camera_ids\n                    if camera_id in camera_names.keys()\n                ]\n                if valid_camera_ids:\n                    key[\"camera_names\"] = [\n                        {\"camera_name\": camera_names[camera_id]}\n                        for camera_id in valid_camera_ids\n                    ]\n                else:\n                    print(\n                        f\"No camera device found with ID {camera_ids} in NWB file {nwbf}\\n\"\n                    )\n                # Add task environment\n                if hasattr(task, \"task_environment\"):\n                    key[\"task_environment\"] = task.task_environment[0]\n\n                # get the interval list for this task, which corresponds to the matching epoch for the raw data.\n                # Users should define more restrictive intervals as required for analyses\n                session_intervals = (\n                    IntervalList() &amp; {\"nwb_file_name\": nwb_file_name}\n                ).fetch(\"interval_list_name\")\n                for epoch in task.task_epochs[0]:\n                    # TODO in beans file, task_epochs[0] is 1x2 dset of ints, so epoch would be an int\n                    key[\"epoch\"] = epoch\n                    target_interval = str(epoch).zfill(2)\n                    for interval in session_intervals:\n                        if (\n                            target_interval in interval\n                        ):  # TODO this is not true for the beans file\n                            break\n                    # TODO case when interval is not found is not handled\n                    key[\"interval_list_name\"] = interval\n                    self.insert1(key)\n\n    @classmethod\n    def update_entries(cls, restrict={}):\n        existing_entries = (cls &amp; restrict).fetch(\"KEY\")\n        for row in existing_entries:\n            if (cls &amp; row).fetch1(\"camera_names\"):\n                continue\n            row[\"camera_names\"] = [\n                {\"camera_name\": (cls &amp; row).fetch1(\"camera_name\")}\n            ]\n            cls.update1(row=row)\n\n    @classmethod\n    def check_task_table(cls, task_table):\n\"\"\"Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.\n\n        The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name',\n        'task_description', 'camera_id', 'and 'task_epochs'.\n\n        Parameters\n        ----------\n        task_table : pynwb.core.DynamicTable\n            The table representing task metadata.\n\n        Returns\n        -------\n        bool\n            Whether the DynamicTable conforms to the expected format for loading data into the TaskEpoch table.\n        \"\"\"\n\n        # TODO this could be more strict and check data types, but really it should be schematized\n        return (\n            Task.check_task_table(task_table)\n            and hasattr(task_table, \"camera_id\")\n            and hasattr(task_table, \"task_epochs\")\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.TaskEpoch.check_task_table", "title": "<code>check_task_table(task_table)</code>  <code>classmethod</code>", "text": "<p>Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.</p> <p>The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name', 'task_description', 'camera_id', 'and 'task_epochs'.</p> <p>Parameters:</p> Name Type Description Default <code>task_table</code> <code>pynwb.core.DynamicTable</code> <p>The table representing task metadata.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the DynamicTable conforms to the expected format for loading data into the TaskEpoch table.</p> Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@classmethod\ndef check_task_table(cls, task_table):\n\"\"\"Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.\n\n    The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name',\n    'task_description', 'camera_id', 'and 'task_epochs'.\n\n    Parameters\n    ----------\n    task_table : pynwb.core.DynamicTable\n        The table representing task metadata.\n\n    Returns\n    -------\n    bool\n        Whether the DynamicTable conforms to the expected format for loading data into the TaskEpoch table.\n    \"\"\"\n\n    # TODO this could be more strict and check data types, but really it should be schematized\n    return (\n        Task.check_task_table(task_table)\n        and hasattr(task_table, \"camera_id\")\n        and hasattr(task_table, \"task_epochs\")\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_task.CameraDevice", "title": "<code>CameraDevice</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass CameraDevice(dj.Manual):\n    definition = \"\"\"\n    camera_name: varchar(80)\n    ---\n    meters_per_pixel = 0: float  # height / width of pixel in meters\n    manufacturer = \"\": varchar(2000)\n    model = \"\": varchar(2000)\n    lens = \"\": varchar(2000)\n    camera_id = -1: int\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert camera devices from an NWB file\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n\n        Returns\n        -------\n        device_name_list : list\n            List of camera device object names found in the NWB file.\n        \"\"\"\n        device_name_list = list()\n        for device in nwbf.devices.values():\n            if isinstance(device, ndx_franklab_novela.CameraDevice):\n                device_dict = dict()\n                # TODO ideally the ID is not encoded in the name formatted in a particular way\n                # device.name must have the form \"[any string without a space, usually camera] [int]\"\n                device_dict[\"camera_id\"] = int(str.split(device.name)[1])\n                device_dict[\"camera_name\"] = device.camera_name\n                device_dict[\"manufacturer\"] = device.manufacturer\n                device_dict[\"model\"] = device.model\n                device_dict[\"lens\"] = device.lens\n                device_dict[\"meters_per_pixel\"] = device.meters_per_pixel\n                cls.insert1(device_dict, skip_duplicates=True)\n                device_name_list.append(device_dict[\"camera_name\"])\n        if device_name_list:\n            print(f\"Inserted camera devices {device_name_list}\")\n        else:\n            print(\"No conforming camera device metadata found.\")\n        return device_name_list\n</code></pre>"}, {"location": "api/src/spyglass/common/common_task/#src.spyglass.common.common_device.CameraDevice.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert camera devices from an NWB file</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of camera device object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert camera devices from an NWB file\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n\n    Returns\n    -------\n    device_name_list : list\n        List of camera device object names found in the NWB file.\n    \"\"\"\n    device_name_list = list()\n    for device in nwbf.devices.values():\n        if isinstance(device, ndx_franklab_novela.CameraDevice):\n            device_dict = dict()\n            # TODO ideally the ID is not encoded in the name formatted in a particular way\n            # device.name must have the form \"[any string without a space, usually camera] [int]\"\n            device_dict[\"camera_id\"] = int(str.split(device.name)[1])\n            device_dict[\"camera_name\"] = device.camera_name\n            device_dict[\"manufacturer\"] = device.manufacturer\n            device_dict[\"model\"] = device.model\n            device_dict[\"lens\"] = device.lens\n            device_dict[\"meters_per_pixel\"] = device.meters_per_pixel\n            cls.insert1(device_dict, skip_duplicates=True)\n            device_name_list.append(device_dict[\"camera_name\"])\n    if device_name_list:\n        print(f\"Inserted camera devices {device_name_list}\")\n    else:\n        print(\"No conforming camera device metadata found.\")\n    return device_name_list\n</code></pre>"}, {"location": "api/src/spyglass/common/errors/", "title": "errors.py", "text": ""}, {"location": "api/src/spyglass/common/populate_all_common/", "title": "populate_all_common.py", "text": ""}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.populate_all_common.DIOEvents", "title": "<code>DIOEvents</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> Source code in <code>src/spyglass/common/common_dio.py</code> <pre><code>@schema\nclass DIOEvents(dj.Imported):\n    definition = \"\"\"\n    -&gt; Session\n    dio_event_name: varchar(80)   # the name assigned to this DIO event\n    ---\n    dio_object_id: varchar(40)    # the object id of the data in the NWB file\n    -&gt; IntervalList               # the list of intervals for this object\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        behav_events = get_data_interface(\n            nwbf, \"behavioral_events\", pynwb.behavior.BehavioralEvents\n        )\n        if behav_events is None:\n            print(\n                f\"No conforming behavioral events data interface found in {nwb_file_name}\\n\"\n            )\n            return\n\n        # the times for these events correspond to the valid times for the raw data\n        key[\"interval_list_name\"] = (\n            Raw() &amp; {\"nwb_file_name\": nwb_file_name}\n        ).fetch1(\"interval_list_name\")\n        for event_series in behav_events.time_series.values():\n            key[\"dio_event_name\"] = event_series.name\n            key[\"dio_object_id\"] = event_series.object_id\n            self.insert1(key, skip_duplicates=True)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    def plot_all_dio_events(self):\n\"\"\"Plot all DIO events in the session.\n\n        Examples\n        --------\n        &gt; (DIOEvents &amp; {'nwb_file_name': 'arthur20220314_.nwb'}).plot_all_dio_events()\n        &gt; (DIOEvents &amp; [{'nwb_file_name': \"arthur20220314_.nwb\"}, {\"nwb_file_name\": \"arthur20220316_.nwb\"}]).plot_all_dio_events()\n\n        \"\"\"\n        behavioral_events = self.fetch_nwb()\n        nwb_file_names = np.unique(\n            [event[\"nwb_file_name\"] for event in behavioral_events]\n        )\n        epoch_valid_times = (\n            pd.DataFrame(\n                IntervalList()\n                &amp; [\n                    {\"nwb_file_name\": nwb_file_name}\n                    for nwb_file_name in nwb_file_names\n                ]\n            )\n            .set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n\n        n_events = len(behavioral_events)\n\n        _, axes = plt.subplots(\n            n_events,\n            1,\n            figsize=(15, n_events * 0.3),\n            dpi=100,\n            sharex=True,\n            constrained_layout=True,\n        )\n\n        for ind, (ax, event) in enumerate(zip(axes.flat, behavioral_events)):\n            for epoch_name, epoch in epoch_valid_times.items():\n                start_time, stop_time = epoch.squeeze()\n                ax.axvspan(start_time, stop_time, alpha=0.5)\n                if ind == 0:\n                    ax.text(\n                        start_time + (stop_time - start_time) / 2,\n                        1.001,\n                        epoch_name,\n                        ha=\"center\",\n                        va=\"bottom\",\n                    )\n            ax.step(\n                np.asarray(event[\"dio\"].timestamps),\n                np.asarray(event[\"dio\"].data),\n                where=\"post\",\n                color=\"black\",\n            )\n            ax.set_ylabel(\n                event[\"dio_event_name\"], rotation=0, ha=\"right\", va=\"center\"\n            )\n            ax.set_yticks([])\n        ax.set_xlabel(\"Time\")\n\n        if len(nwb_file_names) == 1:\n            plt.suptitle(f\"DIO events in {nwb_file_names[0]}\")\n        else:\n            plt.suptitle(f\"DIO events in {', '.join(nwb_file_names)}\")\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.common_dio.DIOEvents.plot_all_dio_events", "title": "<code>plot_all_dio_events()</code>", "text": "<p>Plot all DIO events in the session.</p> <p>Examples:</p> <p>(DIOEvents &amp; {'nwb_file_name': 'arthur20220314_.nwb'}).plot_all_dio_events() (DIOEvents &amp; [{'nwb_file_name': \"arthur20220314_.nwb\"}, {\"nwb_file_name\": \"arthur20220316_.nwb\"}]).plot_all_dio_events()</p> Source code in <code>src/spyglass/common/common_dio.py</code> <pre><code>def plot_all_dio_events(self):\n\"\"\"Plot all DIO events in the session.\n\n    Examples\n    --------\n    &gt; (DIOEvents &amp; {'nwb_file_name': 'arthur20220314_.nwb'}).plot_all_dio_events()\n    &gt; (DIOEvents &amp; [{'nwb_file_name': \"arthur20220314_.nwb\"}, {\"nwb_file_name\": \"arthur20220316_.nwb\"}]).plot_all_dio_events()\n\n    \"\"\"\n    behavioral_events = self.fetch_nwb()\n    nwb_file_names = np.unique(\n        [event[\"nwb_file_name\"] for event in behavioral_events]\n    )\n    epoch_valid_times = (\n        pd.DataFrame(\n            IntervalList()\n            &amp; [\n                {\"nwb_file_name\": nwb_file_name}\n                for nwb_file_name in nwb_file_names\n            ]\n        )\n        .set_index(\"interval_list_name\")\n        .filter(regex=r\"^[0-9]\", axis=0)\n        .valid_times\n    )\n\n    n_events = len(behavioral_events)\n\n    _, axes = plt.subplots(\n        n_events,\n        1,\n        figsize=(15, n_events * 0.3),\n        dpi=100,\n        sharex=True,\n        constrained_layout=True,\n    )\n\n    for ind, (ax, event) in enumerate(zip(axes.flat, behavioral_events)):\n        for epoch_name, epoch in epoch_valid_times.items():\n            start_time, stop_time = epoch.squeeze()\n            ax.axvspan(start_time, stop_time, alpha=0.5)\n            if ind == 0:\n                ax.text(\n                    start_time + (stop_time - start_time) / 2,\n                    1.001,\n                    epoch_name,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n        ax.step(\n            np.asarray(event[\"dio\"].timestamps),\n            np.asarray(event[\"dio\"].data),\n            where=\"post\",\n            color=\"black\",\n        )\n        ax.set_ylabel(\n            event[\"dio_event_name\"], rotation=0, ha=\"right\", va=\"center\"\n        )\n        ax.set_yticks([])\n    ax.set_xlabel(\"Time\")\n\n    if len(nwb_file_names) == 1:\n        plt.suptitle(f\"DIO events in {nwb_file_names[0]}\")\n    else:\n        plt.suptitle(f\"DIO events in {', '.join(nwb_file_names)}\")\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.populate_all_common.PositionSource", "title": "<code>PositionSource</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass PositionSource(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    -&gt; IntervalList\n    ---\n    source: varchar(200)            # source of data; current options are \"trodes\" and \"dlc\" (deep lab cut)\n    import_file_name: varchar(2000)  # path to import file if importing position data\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwb_file_name):\n\"\"\"Given an NWB file name, get the spatial series and interval lists from the file, add the interval\n        lists to the IntervalList table, and populate the RawPosition table if possible.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        pos_dict = get_all_spatial_series(nwbf, verbose=True)\n        if pos_dict is not None:\n            for epoch in pos_dict:\n                pdict = pos_dict[epoch]\n                pos_interval_list_name = cls.get_pos_interval_name(epoch)\n\n                # create the interval list and insert it\n                interval_dict = dict()\n                interval_dict[\"nwb_file_name\"] = nwb_file_name\n                interval_dict[\"interval_list_name\"] = pos_interval_list_name\n                interval_dict[\"valid_times\"] = pdict[\"valid_times\"]\n                IntervalList().insert1(interval_dict, skip_duplicates=True)\n\n                # add this interval list to the table\n                key = dict()\n                key[\"nwb_file_name\"] = nwb_file_name\n                key[\"interval_list_name\"] = pos_interval_list_name\n                key[\"source\"] = \"trodes\"\n                key[\"import_file_name\"] = \"\"\n                cls.insert1(key)\n\n    @staticmethod\n    def get_pos_interval_name(pos_epoch_num):\n        return f\"pos {pos_epoch_num} valid times\"\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.common_behav.PositionSource.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Given an NWB file name, get the spatial series and interval lists from the file, add the interval lists to the IntervalList table, and populate the RawPosition table if possible.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file.</p> required Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwb_file_name):\n\"\"\"Given an NWB file name, get the spatial series and interval lists from the file, add the interval\n    lists to the IntervalList table, and populate the RawPosition table if possible.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    nwbf = get_nwb_file(nwb_file_abspath)\n\n    pos_dict = get_all_spatial_series(nwbf, verbose=True)\n    if pos_dict is not None:\n        for epoch in pos_dict:\n            pdict = pos_dict[epoch]\n            pos_interval_list_name = cls.get_pos_interval_name(epoch)\n\n            # create the interval list and insert it\n            interval_dict = dict()\n            interval_dict[\"nwb_file_name\"] = nwb_file_name\n            interval_dict[\"interval_list_name\"] = pos_interval_list_name\n            interval_dict[\"valid_times\"] = pdict[\"valid_times\"]\n            IntervalList().insert1(interval_dict, skip_duplicates=True)\n\n            # add this interval list to the table\n            key = dict()\n            key[\"nwb_file_name\"] = nwb_file_name\n            key[\"interval_list_name\"] = pos_interval_list_name\n            key[\"source\"] = \"trodes\"\n            key[\"import_file_name\"] = \"\"\n            cls.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.populate_all_common.Nwbfile", "title": "<code>Nwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass Nwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files.\n    nwb_file_name: varchar(255)   # name of the NWB file\n    ---\n    nwb_file_abs_path: filepath@raw\n    INDEX (nwb_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    @classmethod\n    def insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The relative path to the NWB file.\n        \"\"\"\n        nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n        assert os.path.exists(\n            nwb_file_abs_path\n        ), f\"File does not exist: {nwb_file_abs_path}\"\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n        cls.insert1(key, skip_duplicates=True)\n\n    @staticmethod\n    def get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n        Returns\n        -------\n        nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n        nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n        return str(nwb_file_abspath)\n\n    @staticmethod\n    def add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n        The NWB_LOCK_FILE environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n        \"\"\"\n        key = {\"nwb_file_name\": nwb_file_name}\n        # check to make sure the file exists\n        assert (\n            len((Nwbfile() &amp; key).fetch()) &gt; 0\n        ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n        lock_file.write(f\"{nwb_file_name}\\n\")\n        lock_file.close()\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        This does not delete the files themselves unless delete_files=True is specified\n        Run this after deleting the Nwbfile() entries themselves.\n        \"\"\"\n        schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.common_nwbfile.Nwbfile.insert_from_relative_file_name", "title": "<code>insert_from_relative_file_name(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Insert a new session from an existing NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The relative path to the NWB file.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The relative path to the NWB file.\n    \"\"\"\n    nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n    assert os.path.exists(\n        nwb_file_abs_path\n    ), f\"File does not exist: {nwb_file_abs_path}\"\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n    cls.insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.common_nwbfile.Nwbfile.get_abs_path", "title": "<code>get_abs_path(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored raw NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required <p>Returns:</p> Name Type Description <code>nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n    Returns\n    -------\n    nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n    nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n    return str(nwb_file_abspath)\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.common_nwbfile.Nwbfile.add_to_lock", "title": "<code>add_to_lock(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Add the specified NWB file to the file with the list of NWB files to be locked.</p> <p>The NWB_LOCK_FILE environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n    The NWB_LOCK_FILE environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n    \"\"\"\n    key = {\"nwb_file_name\": nwb_file_name}\n    # check to make sure the file exists\n    assert (\n        len((Nwbfile() &amp; key).fetch()) &gt; 0\n    ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n    lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n    lock_file.write(f\"{nwb_file_name}\\n\")\n    lock_file.close()\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.common_nwbfile.Nwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>This does not delete the files themselves unless delete_files=True is specified Run this after deleting the Nwbfile() entries themselves.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    This does not delete the files themselves unless delete_files=True is specified\n    Run this after deleting the Nwbfile() entries themselves.\n    \"\"\"\n    schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.populate_all_common.Electrode", "title": "<code>Electrode</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>@schema\nclass Electrode(dj.Imported):\n    definition = \"\"\"\n    -&gt; ElectrodeGroup\n    electrode_id: int                      # the unique number for this electrode\n    ---\n    -&gt; [nullable] Probe.Electrode\n    -&gt; BrainRegion\n    name = \"\": varchar(200)                 # unique label for each contact\n    original_reference_electrode = -1: int  # the configured reference electrode for this electrode\n    x = NULL: float                         # the x coordinate of the electrode position in the brain\n    y = NULL: float                         # the y coordinate of the electrode position in the brain\n    z = NULL: float                         # the z coordinate of the electrode position in the brain\n    filtering: varchar(2000)                # description of the signal filtering\n    impedance = NULL: float                 # electrode impedance\n    bad_channel = \"False\": enum(\"True\", \"False\")  # if electrode is \"good\" or \"bad\" as observed during recording\n    x_warped = NULL: float                  # x coordinate of electrode position warped to common template brain\n    y_warped = NULL: float                  # y coordinate of electrode position warped to common template brain\n    z_warped = NULL: float                  # z coordinate of electrode position warped to common template brain\n    contacts: varchar(200)                  # label of electrode contacts used for a bipolar signal - current workaround\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        config = get_config(nwb_file_abspath)\n\n        if \"Electrode\" in config:\n            electrode_config_dicts = {\n                electrode_dict[\"electrode_id\"]: electrode_dict\n                for electrode_dict in config[\"Electrode\"]\n            }\n        else:\n            electrode_config_dicts = dict()\n\n        electrodes = nwbf.electrodes.to_dataframe()\n        for elect_id, elect_data in electrodes.iterrows():\n            key[\"electrode_id\"] = elect_id\n            key[\"name\"] = str(elect_id)\n            key[\"electrode_group_name\"] = elect_data.group_name\n            key[\"region_id\"] = BrainRegion.fetch_add(\n                region_name=elect_data.group.location\n            )\n            key[\"x\"] = elect_data.x\n            key[\"y\"] = elect_data.y\n            key[\"z\"] = elect_data.z\n            key[\"x_warped\"] = 0\n            key[\"y_warped\"] = 0\n            key[\"z_warped\"] = 0\n            key[\"contacts\"] = \"\"\n            key[\"filtering\"] = elect_data.filtering\n            key[\"impedance\"] = elect_data.get(\"imp\")\n\n            # rough check of whether the electrodes table was created by rec_to_nwb and has\n            # the appropriate custom columns used by rec_to_nwb\n            # TODO this could be better resolved by making an extension for the electrodes table\n            if (\n                isinstance(elect_data.group.device, ndx_franklab_novela.Probe)\n                and \"probe_shank\" in elect_data\n                and \"probe_electrode\" in elect_data\n                and \"bad_channel\" in elect_data\n                and \"ref_elect_id\" in elect_data\n            ):\n                key[\"probe_id\"] = elect_data.group.device.probe_type\n                key[\"probe_shank\"] = elect_data.probe_shank\n                key[\"probe_electrode\"] = elect_data.probe_electrode\n                key[\"bad_channel\"] = (\n                    \"True\" if elect_data.bad_channel else \"False\"\n                )\n                key[\"original_reference_electrode\"] = elect_data.ref_elect_id\n\n            # override with information from the config YAML based on primary key (electrode id)\n            if elect_id in electrode_config_dicts:\n                # check whether the Probe.Electrode being referenced exists\n                query = Probe.Electrode &amp; electrode_config_dicts[elect_id]\n                if len(query) == 0:\n                    warnings.warn(\n                        f\"No Probe.Electrode exists that matches the data: {electrode_config_dicts[elect_id]}. \"\n                        f\"The config YAML for Electrode with electrode_id {elect_id} will be ignored.\"\n                    )\n                else:\n                    key.update(electrode_config_dicts[elect_id])\n\n            self.insert1(key, skip_duplicates=True)\n\n    @classmethod\n    def create_from_config(cls, nwb_file_name: str):\n\"\"\"Create or update Electrode entries from what is specified in the config YAML file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        config = get_config(nwb_file_abspath)\n        if \"Electrode\" not in config:\n            return\n\n        # map electrode id to dictionary of electrode information from config YAML\n        electrode_dicts = {\n            electrode_dict[\"electrode_id\"]: electrode_dict\n            for electrode_dict in config[\"Electrode\"]\n        }\n\n        electrodes = nwbf.electrodes.to_dataframe()\n        for nwbfile_elect_id, elect_data in electrodes.iterrows():\n            if nwbfile_elect_id in electrode_dicts:\n                # use the information in the electrodes table to start and then add (or overwrite) values from the\n                # config YAML\n                key = dict()\n                key[\"nwb_file_name\"] = nwb_file_name\n                key[\"name\"] = str(nwbfile_elect_id)\n                key[\"electrode_group_name\"] = elect_data.group_name\n                key[\"region_id\"] = BrainRegion.fetch_add(\n                    region_name=elect_data.group.location\n                )\n                key[\"x\"] = elect_data.x\n                key[\"y\"] = elect_data.y\n                key[\"z\"] = elect_data.z\n                key[\"x_warped\"] = 0\n                key[\"y_warped\"] = 0\n                key[\"z_warped\"] = 0\n                key[\"contacts\"] = \"\"\n                key[\"filtering\"] = elect_data.filtering\n                key[\"impedance\"] = elect_data.get(\"imp\")\n                key.update(electrode_dicts[nwbfile_elect_id])\n                query = Electrode &amp; {\"electrode_id\": nwbfile_elect_id}\n                if len(query):\n                    cls.update1(key)\n                    print(f\"Updated Electrode with ID {nwbfile_elect_id}.\")\n                else:\n                    cls.insert1(\n                        key, skip_duplicates=True, allow_direct_insert=True\n                    )\n                    print(f\"Inserted Electrode with ID {nwbfile_elect_id}.\")\n            else:\n                warnings.warn(\n                    f\"Electrode ID {nwbfile_elect_id} exists in the NWB file but has no corresponding \"\n                    \"config YAML entry.\"\n                )\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.common_ephys.Electrode.create_from_config", "title": "<code>create_from_config(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Create or update Electrode entries from what is specified in the config YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file.</p> required Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>@classmethod\ndef create_from_config(cls, nwb_file_name: str):\n\"\"\"Create or update Electrode entries from what is specified in the config YAML file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    nwbf = get_nwb_file(nwb_file_abspath)\n    config = get_config(nwb_file_abspath)\n    if \"Electrode\" not in config:\n        return\n\n    # map electrode id to dictionary of electrode information from config YAML\n    electrode_dicts = {\n        electrode_dict[\"electrode_id\"]: electrode_dict\n        for electrode_dict in config[\"Electrode\"]\n    }\n\n    electrodes = nwbf.electrodes.to_dataframe()\n    for nwbfile_elect_id, elect_data in electrodes.iterrows():\n        if nwbfile_elect_id in electrode_dicts:\n            # use the information in the electrodes table to start and then add (or overwrite) values from the\n            # config YAML\n            key = dict()\n            key[\"nwb_file_name\"] = nwb_file_name\n            key[\"name\"] = str(nwbfile_elect_id)\n            key[\"electrode_group_name\"] = elect_data.group_name\n            key[\"region_id\"] = BrainRegion.fetch_add(\n                region_name=elect_data.group.location\n            )\n            key[\"x\"] = elect_data.x\n            key[\"y\"] = elect_data.y\n            key[\"z\"] = elect_data.z\n            key[\"x_warped\"] = 0\n            key[\"y_warped\"] = 0\n            key[\"z_warped\"] = 0\n            key[\"contacts\"] = \"\"\n            key[\"filtering\"] = elect_data.filtering\n            key[\"impedance\"] = elect_data.get(\"imp\")\n            key.update(electrode_dicts[nwbfile_elect_id])\n            query = Electrode &amp; {\"electrode_id\": nwbfile_elect_id}\n            if len(query):\n                cls.update1(key)\n                print(f\"Updated Electrode with ID {nwbfile_elect_id}.\")\n            else:\n                cls.insert1(\n                    key, skip_duplicates=True, allow_direct_insert=True\n                )\n                print(f\"Inserted Electrode with ID {nwbfile_elect_id}.\")\n        else:\n            warnings.warn(\n                f\"Electrode ID {nwbfile_elect_id} exists in the NWB file but has no corresponding \"\n                \"config YAML entry.\"\n            )\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.populate_all_common.RawPosition", "title": "<code>RawPosition</code>", "text": "<p>         Bases: <code>dj.Imported</code></p>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.populate_all_common.RawPosition--notes", "title": "Notes", "text": "<p>The position timestamps come from: .pos_cameraHWSync.dat. If PTP is not used, the position timestamps are inferred by finding the closest timestamps from the neural recording via the trodes time.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass RawPosition(dj.Imported):\n\"\"\"\n\n    Notes\n    -----\n    The position timestamps come from: .pos_cameraHWSync.dat.\n    If PTP is not used, the position timestamps are inferred by finding the\n    closest timestamps from the neural recording via the trodes time.\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionSource\n    ---\n    raw_position_object_id: varchar(40)    # the object id of the spatial series for this epoch in the NWB file\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        # TODO refactor this. this calculates sampling rate (unused here) and is expensive to do twice\n        pos_dict = get_all_spatial_series(nwbf)\n        for epoch in pos_dict:\n            if key[\n                \"interval_list_name\"\n            ] == PositionSource.get_pos_interval_name(epoch):\n                pdict = pos_dict[epoch]\n                key[\"raw_position_object_id\"] = pdict[\"raw_position_object_id\"]\n                self.insert1(key)\n                break\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    def fetch1_dataframe(self):\n        raw_position_nwb = self.fetch_nwb()[0][\"raw_position\"]\n        return pd.DataFrame(\n            data=raw_position_nwb.data,\n            index=pd.Index(raw_position_nwb.timestamps, name=\"time\"),\n            columns=raw_position_nwb.description.split(\", \"),\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.populate_all_common.TaskEpoch", "title": "<code>TaskEpoch</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@schema\nclass TaskEpoch(dj.Imported):\n    # Tasks, session and time intervals\n    definition = \"\"\"\n     -&gt; Session\n     epoch: int  # the session epoch for this task and apparatus(1 based)\n     ---\n     -&gt; Task\n     -&gt; [nullable] CameraDevice\n     -&gt; IntervalList\n     task_environment = NULL: varchar(200)  # the environment the animal was in\n     camera_names : blob # list of keys corresponding to entry in CameraDevice\n     \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile().get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        camera_names = dict()\n        # the tasks refer to the camera_id which is unique for the NWB file but not for CameraDevice schema, so we\n        # need to look up the right camera\n        # map camera ID (in camera name) to camera_name\n        for device in nwbf.devices.values():\n            if isinstance(device, ndx_franklab_novela.CameraDevice):\n                # get the camera ID\n                camera_id = int(str.split(device.name)[1])\n                camera_names[camera_id] = device.camera_name\n\n        # find the task modules and for each one, add the task to the Task schema if it isn't there\n        # and then add an entry for each epoch\n        tasks_mod = nwbf.processing.get(\"tasks\")\n        if tasks_mod is None:\n            print(f\"No tasks processing module found in {nwbf}\\n\")\n            return\n\n        for task in tasks_mod.data_interfaces.values():\n            if self.check_task_table(task):\n                # check if the task is in the Task table and if not, add it\n                Task.insert_from_task_table(task)\n                key[\"task_name\"] = task.task_name[0]\n\n                # get the CameraDevice used for this task (primary key is camera name so we need\n                # to map from ID to name)\n                camera_ids = task.camera_id[0]\n                valid_camera_ids = [\n                    camera_id\n                    for camera_id in camera_ids\n                    if camera_id in camera_names.keys()\n                ]\n                if valid_camera_ids:\n                    key[\"camera_names\"] = [\n                        {\"camera_name\": camera_names[camera_id]}\n                        for camera_id in valid_camera_ids\n                    ]\n                else:\n                    print(\n                        f\"No camera device found with ID {camera_ids} in NWB file {nwbf}\\n\"\n                    )\n                # Add task environment\n                if hasattr(task, \"task_environment\"):\n                    key[\"task_environment\"] = task.task_environment[0]\n\n                # get the interval list for this task, which corresponds to the matching epoch for the raw data.\n                # Users should define more restrictive intervals as required for analyses\n                session_intervals = (\n                    IntervalList() &amp; {\"nwb_file_name\": nwb_file_name}\n                ).fetch(\"interval_list_name\")\n                for epoch in task.task_epochs[0]:\n                    # TODO in beans file, task_epochs[0] is 1x2 dset of ints, so epoch would be an int\n                    key[\"epoch\"] = epoch\n                    target_interval = str(epoch).zfill(2)\n                    for interval in session_intervals:\n                        if (\n                            target_interval in interval\n                        ):  # TODO this is not true for the beans file\n                            break\n                    # TODO case when interval is not found is not handled\n                    key[\"interval_list_name\"] = interval\n                    self.insert1(key)\n\n    @classmethod\n    def update_entries(cls, restrict={}):\n        existing_entries = (cls &amp; restrict).fetch(\"KEY\")\n        for row in existing_entries:\n            if (cls &amp; row).fetch1(\"camera_names\"):\n                continue\n            row[\"camera_names\"] = [\n                {\"camera_name\": (cls &amp; row).fetch1(\"camera_name\")}\n            ]\n            cls.update1(row=row)\n\n    @classmethod\n    def check_task_table(cls, task_table):\n\"\"\"Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.\n\n        The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name',\n        'task_description', 'camera_id', 'and 'task_epochs'.\n\n        Parameters\n        ----------\n        task_table : pynwb.core.DynamicTable\n            The table representing task metadata.\n\n        Returns\n        -------\n        bool\n            Whether the DynamicTable conforms to the expected format for loading data into the TaskEpoch table.\n        \"\"\"\n\n        # TODO this could be more strict and check data types, but really it should be schematized\n        return (\n            Task.check_task_table(task_table)\n            and hasattr(task_table, \"camera_id\")\n            and hasattr(task_table, \"task_epochs\")\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.common_task.TaskEpoch.check_task_table", "title": "<code>check_task_table(task_table)</code>  <code>classmethod</code>", "text": "<p>Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.</p> <p>The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name', 'task_description', 'camera_id', 'and 'task_epochs'.</p> <p>Parameters:</p> Name Type Description Default <code>task_table</code> <code>pynwb.core.DynamicTable</code> <p>The table representing task metadata.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the DynamicTable conforms to the expected format for loading data into the TaskEpoch table.</p> Source code in <code>src/spyglass/common/common_task.py</code> <pre><code>@classmethod\ndef check_task_table(cls, task_table):\n\"\"\"Check whether the pynwb DynamicTable containing task metadata conforms to the expected format.\n\n    The table should be an instance of pynwb.core.DynamicTable and contain the columns 'task_name',\n    'task_description', 'camera_id', 'and 'task_epochs'.\n\n    Parameters\n    ----------\n    task_table : pynwb.core.DynamicTable\n        The table representing task metadata.\n\n    Returns\n    -------\n    bool\n        Whether the DynamicTable conforms to the expected format for loading data into the TaskEpoch table.\n    \"\"\"\n\n    # TODO this could be more strict and check data types, but really it should be schematized\n    return (\n        Task.check_task_table(task_table)\n        and hasattr(task_table, \"camera_id\")\n        and hasattr(task_table, \"task_epochs\")\n    )\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.populate_all_common.StateScriptFile", "title": "<code>StateScriptFile</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass StateScriptFile(dj.Imported):\n    definition = \"\"\"\n    -&gt; TaskEpoch\n    ---\n    file_object_id: varchar(40)  # the object id of the file object\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Add a new row to the StateScriptFile table. Requires keys \"nwb_file_name\", \"file_object_id\".\"\"\"\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        associated_files = nwbf.processing.get(\n            \"associated_files\"\n        ) or nwbf.processing.get(\"associated files\")\n        if associated_files is None:\n            print(\n                f'Unable to import StateScriptFile: no processing module named \"associated_files\" '\n                f\"found in {nwb_file_name}.\"\n            )\n            return\n\n        for associated_file_obj in associated_files.data_interfaces.values():\n            if not isinstance(\n                associated_file_obj, ndx_franklab_novela.AssociatedFiles\n            ):\n                print(\n                    f'Data interface {associated_file_obj.name} within \"associated_files\" processing module is not '\n                    f\"of expected type ndx_franklab_novela.AssociatedFiles\\n\"\n                )\n                return\n            # parse the task_epochs string\n            # TODO update associated_file_obj.task_epochs to be an array of 1-based ints,\n            # not a comma-separated string of ints\n            epoch_list = associated_file_obj.task_epochs.split(\",\")\n            # only insert if this is the statescript file\n            print(associated_file_obj.description)\n            if (\n                \"statescript\".upper() in associated_file_obj.description.upper()\n                or \"state_script\".upper()\n                in associated_file_obj.description.upper()\n                or \"state script\".upper()\n                in associated_file_obj.description.upper()\n            ):\n                # find the file associated with this epoch\n                if str(key[\"epoch\"]) in epoch_list:\n                    key[\"file_object_id\"] = associated_file_obj.object_id\n                    self.insert1(key)\n            else:\n                print(\"not a statescript file\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.common_behav.StateScriptFile.make", "title": "<code>make(key)</code>", "text": "<p>Add a new row to the StateScriptFile table. Requires keys \"nwb_file_name\", \"file_object_id\".</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>def make(self, key):\n\"\"\"Add a new row to the StateScriptFile table. Requires keys \"nwb_file_name\", \"file_object_id\".\"\"\"\n    nwb_file_name = key[\"nwb_file_name\"]\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    nwbf = get_nwb_file(nwb_file_abspath)\n\n    associated_files = nwbf.processing.get(\n        \"associated_files\"\n    ) or nwbf.processing.get(\"associated files\")\n    if associated_files is None:\n        print(\n            f'Unable to import StateScriptFile: no processing module named \"associated_files\" '\n            f\"found in {nwb_file_name}.\"\n        )\n        return\n\n    for associated_file_obj in associated_files.data_interfaces.values():\n        if not isinstance(\n            associated_file_obj, ndx_franklab_novela.AssociatedFiles\n        ):\n            print(\n                f'Data interface {associated_file_obj.name} within \"associated_files\" processing module is not '\n                f\"of expected type ndx_franklab_novela.AssociatedFiles\\n\"\n            )\n            return\n        # parse the task_epochs string\n        # TODO update associated_file_obj.task_epochs to be an array of 1-based ints,\n        # not a comma-separated string of ints\n        epoch_list = associated_file_obj.task_epochs.split(\",\")\n        # only insert if this is the statescript file\n        print(associated_file_obj.description)\n        if (\n            \"statescript\".upper() in associated_file_obj.description.upper()\n            or \"state_script\".upper()\n            in associated_file_obj.description.upper()\n            or \"state script\".upper()\n            in associated_file_obj.description.upper()\n        ):\n            # find the file associated with this epoch\n            if str(key[\"epoch\"]) in epoch_list:\n                key[\"file_object_id\"] = associated_file_obj.object_id\n                self.insert1(key)\n        else:\n            print(\"not a statescript file\")\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.populate_all_common.VideoFile", "title": "<code>VideoFile</code>", "text": "<p>         Bases: <code>dj.Imported</code></p>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.populate_all_common.VideoFile--notes", "title": "Notes", "text": "<p>The video timestamps come from: videoTimeStamps.cameraHWSync if PTP is used. If PTP is not used, the video timestamps come from videoTimeStamps.cameraHWFrameCount .</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass VideoFile(dj.Imported):\n\"\"\"\n\n    Notes\n    -----\n    The video timestamps come from: videoTimeStamps.cameraHWSync if PTP is used.\n    If PTP is not used, the video timestamps come from videoTimeStamps.cameraHWFrameCount .\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; TaskEpoch\n    video_file_num = 0: int\n    ---\n    camera_name: varchar(80)\n    video_file_object_id: varchar(40)  # the object id of the file object\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        videos = get_data_interface(\n            nwbf, \"video\", pynwb.behavior.BehavioralEvents\n        )\n\n        if videos is None:\n            print(f\"No video data interface found in {nwb_file_name}\\n\")\n            return\n        else:\n            videos = videos.time_series\n\n        # get the interval for the current TaskEpoch\n        interval_list_name = (TaskEpoch() &amp; key).fetch1(\"interval_list_name\")\n        valid_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n\n        is_found = False\n        for ind, video in enumerate(videos.values()):\n            if isinstance(video, pynwb.image.ImageSeries):\n                video = [video]\n            for video_obj in video:\n                # check to see if the times for this video_object are largely overlapping with the task epoch times\n                if len(\n                    interval_list_contains(valid_times, video_obj.timestamps)\n                    &gt; 0.9 * len(video_obj.timestamps)\n                ):\n                    key[\"video_file_num\"] = ind\n                    camera_name = video_obj.device.camera_name\n                    if CameraDevice &amp; {\"camera_name\": camera_name}:\n                        key[\"camera_name\"] = video_obj.device.camera_name\n                    else:\n                        raise KeyError(\n                            f\"No camera with camera_name: {camera_name} found in CameraDevice table.\"\n                        )\n                    key[\"video_file_object_id\"] = video_obj.object_id\n                    self.insert1(key)\n                    is_found = True\n\n        if not is_found:\n            print(f\"No video found corresponding to epoch {interval_list_name}\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    @classmethod\n    def update_entries(cls, restrict={}):\n        existing_entries = (cls &amp; restrict).fetch(\"KEY\")\n        for row in existing_entries:\n            if (cls &amp; row).fetch1(\"camera_name\"):\n                continue\n            video_nwb = (cls &amp; row).fetch_nwb()[0]\n            if len(video_nwb) != 1:\n                raise ValueError(\n                    f\"expecting 1 video file per entry, but {len(video_nwb)} files found\"\n                )\n            row[\"camera_name\"] = video_nwb[0][\"video_file\"].device.camera_name\n            cls.update1(row=row)\n\n    @classmethod\n    def get_abs_path(cls, key: Dict):\n\"\"\"Return the absolute path for a stored video file given a key with the nwb_file_name and epoch number\n\n        The SPYGLASS_VIDEO_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        key : dict\n            dictionary with nwb_file_name and epoch as keys\n\n        Returns\n        -------\n        nwb_video_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        video_dir = pathlib.Path(os.getenv(\"SPYGLASS_VIDEO_DIR\", None))\n        assert video_dir is not None, \"You must set SPYGLASS_VIDEO_DIR\"\n        if not video_dir.exists():\n            raise OSError(\"SPYGLASS_VIDEO_DIR does not exist\")\n        video_info = (cls &amp; key).fetch1()\n        nwb_path = Nwbfile.get_abs_path(key[\"nwb_file_name\"])\n        nwbf = get_nwb_file(nwb_path)\n        nwb_video = nwbf.objects[video_info[\"video_file_object_id\"]]\n        video_filename = nwb_video.name\n        # see if the file exists and is stored in the base analysis dir\n        nwb_video_file_abspath = pathlib.Path(\n            f\"{video_dir}/{pathlib.Path(video_filename)}\"\n        )\n        if nwb_video_file_abspath.exists():\n            return nwb_video_file_abspath.as_posix()\n        else:\n            raise FileNotFoundError(\n                f\"video file with filename: {video_filename} \"\n                f\"does not exist in {video_dir}/\"\n            )\n</code></pre>"}, {"location": "api/src/spyglass/common/populate_all_common/#src.spyglass.common.common_behav.VideoFile.get_abs_path", "title": "<code>get_abs_path(key)</code>  <code>classmethod</code>", "text": "<p>Return the absolute path for a stored video file given a key with the nwb_file_name and epoch number</p> <p>The SPYGLASS_VIDEO_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>dictionary with nwb_file_name and epoch as keys</p> required <p>Returns:</p> Name Type Description <code>nwb_video_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@classmethod\ndef get_abs_path(cls, key: Dict):\n\"\"\"Return the absolute path for a stored video file given a key with the nwb_file_name and epoch number\n\n    The SPYGLASS_VIDEO_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    key : dict\n        dictionary with nwb_file_name and epoch as keys\n\n    Returns\n    -------\n    nwb_video_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    video_dir = pathlib.Path(os.getenv(\"SPYGLASS_VIDEO_DIR\", None))\n    assert video_dir is not None, \"You must set SPYGLASS_VIDEO_DIR\"\n    if not video_dir.exists():\n        raise OSError(\"SPYGLASS_VIDEO_DIR does not exist\")\n    video_info = (cls &amp; key).fetch1()\n    nwb_path = Nwbfile.get_abs_path(key[\"nwb_file_name\"])\n    nwbf = get_nwb_file(nwb_path)\n    nwb_video = nwbf.objects[video_info[\"video_file_object_id\"]]\n    video_filename = nwb_video.name\n    # see if the file exists and is stored in the base analysis dir\n    nwb_video_file_abspath = pathlib.Path(\n        f\"{video_dir}/{pathlib.Path(video_filename)}\"\n    )\n    if nwb_video_file_abspath.exists():\n        return nwb_video_file_abspath.as_posix()\n    else:\n        raise FileNotFoundError(\n            f\"video file with filename: {video_filename} \"\n            f\"does not exist in {video_dir}/\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/common/signal_processing/", "title": "signal_processing.py", "text": ""}, {"location": "api/src/spyglass/common/signal_processing/#src.spyglass.common.signal_processing.hilbert_decomp", "title": "<code>hilbert_decomp(lfp_band_object, sampling_rate=1)</code>", "text": "<p>generates the analytical decomposition of the signals in the lfp_band_object</p> <p>:param lfp_band_object: bandpass filtered LFP :type lfp_band_object: pynwb electrical series :param sampling_rate: bandpass filtered LFP sampling rate (defaults to 1; only used for instantaneous frequency) :type sampling_rate: int :return: envelope, phase, frequency :rtype: pynwb electrical series objects</p> Source code in <code>src/spyglass/common/signal_processing.py</code> <pre><code>def hilbert_decomp(lfp_band_object, sampling_rate=1):\n\"\"\"generates the analytical decomposition of the signals in the lfp_band_object\n\n    :param lfp_band_object: bandpass filtered LFP\n    :type lfp_band_object: pynwb electrical series\n    :param sampling_rate: bandpass filtered LFP sampling rate (defaults to 1; only used for instantaneous frequency)\n    :type sampling_rate: int\n    :return: envelope, phase, frequency\n    :rtype: pynwb electrical series objects\n    \"\"\"\n    analytical_signal = signal.hilbert(lfp_band_object.data, axis=0)\n\n    eseries_name = \"envelope\"\n    envelope = pynwb.ecephys.ElectricalSeries(\n        name=eseries_name,\n        data=np.abs(analytical_signal),\n        electrodes=lfp_band_object.electrodes,\n        timestamps=lfp_band_object.timestamps,\n    )\n\n    eseries_name = \"phase\"\n    instantaneous_phase = np.unwrap(np.angle(analytical_signal))\n    phase = pynwb.ecephys.ElectricalSeries(\n        name=eseries_name,\n        data=instantaneous_phase,\n        electrodes=lfp_band_object.electrodes,\n        timestamps=lfp_band_object.timestamps,\n    )\n\n    eseries_name = \"frequency\"\n    instantaneous_frequency = (\n        np.diff(instantaneous_phase) / (2.0 * np.pi) * sampling_rate\n    )\n    frequency = pynwb.ecephys.ElectricalSeries(\n        name=eseries_name,\n        data=instantaneous_frequency,\n        electrodes=lfp_band_object.electrodes,\n        timestamps=lfp_band_object.timestamps,\n    )\n    return envelope, phase, frequency\n</code></pre>"}, {"location": "api/src/spyglass/common/prepopulate/prepopulate/", "title": "prepopulate.py", "text": ""}, {"location": "api/src/spyglass/common/prepopulate/prepopulate/#src.spyglass.common.prepopulate.prepopulate.prepopulate_default", "title": "<code>prepopulate_default()</code>", "text": "<p>Prepopulate the database with the default values in SPYGLASS_BASE_DIR/entries.yaml.</p> Source code in <code>src/spyglass/common/prepopulate/prepopulate.py</code> <pre><code>def prepopulate_default():\n\"\"\"Prepopulate the database with the default values in SPYGLASS_BASE_DIR/entries.yaml.\"\"\"\n    base_dir = os.getenv(\"SPYGLASS_BASE_DIR\", None)\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n    yaml_path = pathlib.Path(base_dir) / \"entries.yaml\"\n    if os.path.exists(yaml_path):\n        populate_from_yaml(yaml_path)\n</code></pre>"}, {"location": "api/src/spyglass/common/prepopulate/prepopulate/#src.spyglass.common.prepopulate.prepopulate.populate_from_yaml", "title": "<code>populate_from_yaml(yaml_path)</code>", "text": "<p>Populate the database from specially formatted YAML files.</p> Source code in <code>src/spyglass/common/prepopulate/prepopulate.py</code> <pre><code>def populate_from_yaml(yaml_path: str):\n\"\"\"Populate the database from specially formatted YAML files.\"\"\"\n    if not os.path.exists(yaml_path):\n        raise ValueError(f\"There is no file found with the path: {yaml_path}\")\n    with open(yaml_path, \"r\") as stream:\n        d = yaml.safe_load(stream)\n\n    for table_name, table_entries in d.items():\n        table_cls = _get_table_cls(table_name)\n        for entry_dict in table_entries:\n            # test whether an entity with the primary key(s) already exists in the table\n            if not issubclass(table_cls, (dj.Manual, dj.Lookup, dj.Part)):\n                raise ValueError(\n                    f\"Prepopulate YAML ('{yaml_path}') contains table '{table_name}' that cannot be \"\n                    \"prepopulated. Only Manual and Lookup tables can be prepopulated.\"\n                )\n            if hasattr(table_cls, \"fetch_add\"):\n                # if the table has defined a fetch_add method, use that instead of insert1. this is useful for\n                # tables where the primary key is an ID that auto-increments.\n                # first check whether an entry exists with the same information.\n                query = table_cls &amp; entry_dict\n                if not query:\n                    print(\n                        f\"Populate: Populating table {table_cls.__name__} with data {entry_dict} using fetch_add.\"\n                    )\n                    table_cls.fetch_add(**entry_dict)\n                continue\n\n            primary_key_values = {\n                k: v\n                for k, v in entry_dict.items()\n                if k in table_cls.primary_key\n            }\n            if not primary_key_values:\n                print(\n                    f\"Populate: No primary key provided in data {entry_dict} for table {table_cls.__name__}\"\n                )\n                continue\n            if primary_key_values not in table_cls.fetch(\n                *table_cls.primary_key, as_dict=True\n            ):\n                print(\n                    f\"Populate: Populating table {table_cls.__name__} with data {entry_dict} using insert1.\"\n                )\n                table_cls.insert1(entry_dict)\n            else:\n                logging.info(\n                    f\"Populate: Entry in {table_cls.__name__} with primary keys {primary_key_values} already exists.\"\n                )\n</code></pre>"}, {"location": "api/src/spyglass/data_import/insert_sessions/", "title": "insert_sessions.py", "text": ""}, {"location": "api/src/spyglass/data_import/insert_sessions/#src.spyglass.data_import.insert_sessions.check_env", "title": "<code>check_env()</code>", "text": "<p>Check whether environment variables have been set properly. Raise an exception if not.</p> Source code in <code>src/spyglass/data_import/storage_dirs.py</code> <pre><code>def check_env():\n\"\"\"Check whether environment variables have been set properly.\n    Raise an exception if not.\n    \"\"\"\n    base_dir()\n</code></pre>"}, {"location": "api/src/spyglass/data_import/insert_sessions/#src.spyglass.data_import.insert_sessions.insert_sessions", "title": "<code>insert_sessions(nwb_file_names)</code>", "text": "<p>Populate the dj database with new sessions.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_names</code> <code>str or List of str</code> <p>File paths (relative to $SPYGLASS_BASE_DIR) pointing to existing .nwb files. Each file represents a session.</p> required Source code in <code>src/spyglass/data_import/insert_sessions.py</code> <pre><code>def insert_sessions(nwb_file_names: Union[str, List[str]]):\n\"\"\"\n    Populate the dj database with new sessions.\n\n    Parameters\n    ----------\n    nwb_file_names : str or List of str\n        File paths (relative to $SPYGLASS_BASE_DIR) pointing to\n        existing .nwb files. Each file represents a session.\n    \"\"\"\n    check_env()\n\n    if isinstance(nwb_file_names, str):\n        nwb_file_names = [nwb_file_names]\n\n    for nwb_file_name in nwb_file_names:\n        assert not nwb_file_name.startswith(\n            \"/\"\n        ), f\"You must use relative paths. nwb_file_name: {nwb_file_name}\"\n\n        # file name for the copied raw data\n        out_nwb_file_name = os.path.splitext(nwb_file_name)[0] + \"_.nwb\"\n\n        # Check whether the file already exists in the Nwbfile table\n        if len(Nwbfile() &amp; {\"nwb_file_name\": out_nwb_file_name}):\n            warnings.warn(\n                f\"Cannot insert data from {nwb_file_name}: {out_nwb_file_name} is already in Nwbfile table.\"\n            )\n            continue\n\n        # Make a copy of the NWB file that ends with '_'.\n        # This has everything except the raw data but has a link to the raw data in the original file\n        copy_nwb_link_raw_ephys(nwb_file_name, out_nwb_file_name)\n        Nwbfile().insert_from_relative_file_name(out_nwb_file_name)\n        populate_all_common(out_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/data_import/insert_sessions/#src.spyglass.data_import.insert_sessions.Nwbfile", "title": "<code>Nwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass Nwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files.\n    nwb_file_name: varchar(255)   # name of the NWB file\n    ---\n    nwb_file_abs_path: filepath@raw\n    INDEX (nwb_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    @classmethod\n    def insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The relative path to the NWB file.\n        \"\"\"\n        nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n        assert os.path.exists(\n            nwb_file_abs_path\n        ), f\"File does not exist: {nwb_file_abs_path}\"\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n        cls.insert1(key, skip_duplicates=True)\n\n    @staticmethod\n    def get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n        Returns\n        -------\n        nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n        nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n        return str(nwb_file_abspath)\n\n    @staticmethod\n    def add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n        The NWB_LOCK_FILE environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n        \"\"\"\n        key = {\"nwb_file_name\": nwb_file_name}\n        # check to make sure the file exists\n        assert (\n            len((Nwbfile() &amp; key).fetch()) &gt; 0\n        ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n        lock_file.write(f\"{nwb_file_name}\\n\")\n        lock_file.close()\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        This does not delete the files themselves unless delete_files=True is specified\n        Run this after deleting the Nwbfile() entries themselves.\n        \"\"\"\n        schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/data_import/insert_sessions/#src.spyglass.common.common_nwbfile.Nwbfile.insert_from_relative_file_name", "title": "<code>insert_from_relative_file_name(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Insert a new session from an existing NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The relative path to the NWB file.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The relative path to the NWB file.\n    \"\"\"\n    nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n    assert os.path.exists(\n        nwb_file_abs_path\n    ), f\"File does not exist: {nwb_file_abs_path}\"\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n    cls.insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/data_import/insert_sessions/#src.spyglass.common.common_nwbfile.Nwbfile.get_abs_path", "title": "<code>get_abs_path(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored raw NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required <p>Returns:</p> Name Type Description <code>nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n    Returns\n    -------\n    nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n    nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n    return str(nwb_file_abspath)\n</code></pre>"}, {"location": "api/src/spyglass/data_import/insert_sessions/#src.spyglass.common.common_nwbfile.Nwbfile.add_to_lock", "title": "<code>add_to_lock(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Add the specified NWB file to the file with the list of NWB files to be locked.</p> <p>The NWB_LOCK_FILE environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n    The NWB_LOCK_FILE environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n    \"\"\"\n    key = {\"nwb_file_name\": nwb_file_name}\n    # check to make sure the file exists\n    assert (\n        len((Nwbfile() &amp; key).fetch()) &gt; 0\n    ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n    lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n    lock_file.write(f\"{nwb_file_name}\\n\")\n    lock_file.close()\n</code></pre>"}, {"location": "api/src/spyglass/data_import/insert_sessions/#src.spyglass.common.common_nwbfile.Nwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>This does not delete the files themselves unless delete_files=True is specified Run this after deleting the Nwbfile() entries themselves.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    This does not delete the files themselves unless delete_files=True is specified\n    Run this after deleting the Nwbfile() entries themselves.\n    \"\"\"\n    schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/data_import/insert_sessions/#src.spyglass.data_import.insert_sessions.get_raw_eseries", "title": "<code>get_raw_eseries(nwbfile)</code>", "text": "<p>Return all ElectricalSeries in the acquisition group of an NWB file.</p> <p>ElectricalSeries found within LFP objects in the acquisition will also be returned.</p> <p>Parameters:</p> Name Type Description Default <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>The NWB file object to search in.</p> required <p>Returns:</p> Name Type Description <code>ret</code> <code>list</code> <p>A list of all ElectricalSeries in the acquisition group of an NWB file</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_raw_eseries(nwbfile):\n\"\"\"Return all ElectricalSeries in the acquisition group of an NWB file.\n\n    ElectricalSeries found within LFP objects in the acquisition will also be returned.\n\n    Parameters\n    ----------\n    nwbfile : pynwb.NWBFile\n        The NWB file object to search in.\n\n    Returns\n    -------\n    ret : list\n        A list of all ElectricalSeries in the acquisition group of an NWB file\n    \"\"\"\n    ret = []\n    for nwb_object in nwbfile.acquisition.values():\n        if isinstance(nwb_object, pynwb.ecephys.ElectricalSeries):\n            ret.append(nwb_object)\n        elif isinstance(nwb_object, pynwb.ecephys.LFP):\n            ret.extend(nwb_object.electrical_series.values())\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/data_import/storage_dirs/", "title": "storage_dirs.py", "text": ""}, {"location": "api/src/spyglass/data_import/storage_dirs/#src.spyglass.data_import.storage_dirs.check_env", "title": "<code>check_env()</code>", "text": "<p>Check whether environment variables have been set properly. Raise an exception if not.</p> Source code in <code>src/spyglass/data_import/storage_dirs.py</code> <pre><code>def check_env():\n\"\"\"Check whether environment variables have been set properly.\n    Raise an exception if not.\n    \"\"\"\n    base_dir()\n</code></pre>"}, {"location": "api/src/spyglass/data_import/storage_dirs/#src.spyglass.data_import.storage_dirs.base_dir", "title": "<code>base_dir()</code>", "text": "<p>Get the base directory from $SPYGLASS_BASE_DIR</p> <p>Returns:</p> Type Description <code>str</code> <p>base directory</p> Source code in <code>src/spyglass/data_import/storage_dirs.py</code> <pre><code>def base_dir():\n\"\"\"Get the base directory from $SPYGLASS_BASE_DIR\n\n    Returns\n    -------\n    str\n        base directory\n    \"\"\"\n    p = os.getenv(\"SPYGLASS_BASE_DIR\", None)\n    assert (\n        p is not None\n    ), \"\"\"\n    You must set the SPYGLASS_BASE_DIR environment variable.\n    \"\"\"\n    return p\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/", "title": "clusterless.py", "text": "<p>Pipeline for decoding the animal's mental position and some category of interest from unclustered spikes and spike waveform features. See [1] for details.</p>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless--references", "title": "References", "text": "<p>[1] Denovellis, E. L. et al. Hippocampal replay of experience at real-world speeds. eLife 10, e64505 (2021).</p>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.MarkParameters", "title": "<code>MarkParameters</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Defines the type of spike waveform feature computed for a given spike time.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass MarkParameters(dj.Manual):\n\"\"\"Defines the type of spike waveform feature computed for a given spike\n    time.\"\"\"\n\n    definition = \"\"\"\n    mark_param_name : varchar(80) # a name for this set of parameters\n    ---\n    # the type of mark. Currently only 'amplitude' is supported\n    mark_type = 'amplitude':  varchar(40)\n    mark_param_dict:    BLOB    # dictionary of parameters for the mark extraction function\n    \"\"\"\n\n    def insert_default(self):\n\"\"\"Insert the default parameter set\n\n        Examples\n        --------\n        {'peak_sign': 'neg', 'threshold' : 100}\n        corresponds to negative going waveforms of at least 100 uV size\n        \"\"\"\n        default_dict = {}\n        self.insert1(\n            {\"mark_param_name\": \"default\", \"mark_param_dict\": default_dict},\n            skip_duplicates=True,\n        )\n\n    @staticmethod\n    def supported_mark_type(mark_type):\n\"\"\"checks whether the requested mark type is supported.\n        Currently only 'amplitude\" is supported.\n\n        Parameters\n        ----------\n        mark_type : str\n\n        \"\"\"\n        supported_types = [\"amplitude\"]\n        if mark_type in supported_types:\n            return True\n        return False\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.MarkParameters.insert_default", "title": "<code>insert_default()</code>", "text": "<p>Insert the default parameter set</p> <p>Examples:</p> <p>{'peak_sign': 'neg', 'threshold' : 100} corresponds to negative going waveforms of at least 100 uV size</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>def insert_default(self):\n\"\"\"Insert the default parameter set\n\n    Examples\n    --------\n    {'peak_sign': 'neg', 'threshold' : 100}\n    corresponds to negative going waveforms of at least 100 uV size\n    \"\"\"\n    default_dict = {}\n    self.insert1(\n        {\"mark_param_name\": \"default\", \"mark_param_dict\": default_dict},\n        skip_duplicates=True,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.MarkParameters.supported_mark_type", "title": "<code>supported_mark_type(mark_type)</code>  <code>staticmethod</code>", "text": "<p>checks whether the requested mark type is supported. Currently only 'amplitude\" is supported.</p> <p>Parameters:</p> Name Type Description Default <code>mark_type</code> <code>str</code> required Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@staticmethod\ndef supported_mark_type(mark_type):\n\"\"\"checks whether the requested mark type is supported.\n    Currently only 'amplitude\" is supported.\n\n    Parameters\n    ----------\n    mark_type : str\n\n    \"\"\"\n    supported_types = [\"amplitude\"]\n    if mark_type in supported_types:\n        return True\n    return False\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.UnitMarks", "title": "<code>UnitMarks</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>For each spike time, compute a spike waveform feature associated with that spike. Used for clusterless decoding.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass UnitMarks(dj.Computed):\n\"\"\"For each spike time, compute a spike waveform feature associated with that\n    spike. Used for clusterless decoding.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; UnitMarkParameters\n    ---\n    -&gt; AnalysisNwbfile\n    marks_object_id: varchar(40) # the NWB object that stores the marks\n    \"\"\"\n\n    def make(self, key):\n        # get the list of mark parameters\n        mark_param = (MarkParameters &amp; key).fetch1()\n\n        # check that the mark type is supported\n        if not MarkParameters().supported_mark_type(mark_param[\"mark_type\"]):\n            Warning(\n                f'Mark type {mark_param[\"mark_type\"]} not supported; skipping'\n            )\n            return\n\n        # retrieve the units from the NWB file\n        nwb_units = (CuratedSpikeSorting() &amp; key).fetch_nwb()[0][\"units\"]\n\n        recording = Curation.get_recording(key)\n        if recording.get_num_segments() &gt; 1:\n            recording = si.concatenate_recordings([recording])\n        sorting = Curation.get_curated_sorting(key)\n        waveform_extractor_name = (\n            f'{key[\"nwb_file_name\"]}_{str(uuid.uuid4())[0:8]}_'\n            f'{key[\"curation_id\"]}_clusterless_waveforms'\n        )\n        waveform_extractor_path = str(\n            Path(os.environ[\"SPYGLASS_WAVEFORMS_DIR\"])\n            / Path(waveform_extractor_name)\n        )\n        if os.path.exists(waveform_extractor_path):\n            shutil.rmtree(waveform_extractor_path)\n\n        WAVEFORM_PARAMS = {\n            \"ms_before\": 0.5,\n            \"ms_after\": 0.5,\n            \"max_spikes_per_unit\": None,\n            \"n_jobs\": 5,\n            \"total_memory\": \"5G\",\n        }\n        waveform_extractor = si.extract_waveforms(\n            recording=recording,\n            sorting=sorting,\n            folder=waveform_extractor_path,\n            **WAVEFORM_PARAMS,\n        )\n\n        if mark_param[\"mark_type\"] == \"amplitude\":\n            sorter = (CuratedSpikeSorting() &amp; key).fetch1(\"sorter\")\n            if sorter == \"clusterless_thresholder\":\n                estimate_peak_time = False\n            else:\n                estimate_peak_time = True\n\n            try:\n                peak_sign = mark_param[\"mark_param_dict\"][\"peak_sign\"]\n            except KeyError:\n                peak_sign = \"neg\"\n\n            marks = np.concatenate(\n                [\n                    UnitMarks._get_peak_amplitude(\n                        waveform=waveform_extractor.get_waveforms(unit_id),\n                        peak_sign=peak_sign,\n                        estimate_peak_time=estimate_peak_time,\n                    )\n                    for unit_id in nwb_units.index\n                ],\n                axis=0,\n            )\n\n            timestamps = np.concatenate(np.asarray(nwb_units[\"spike_times\"]))\n            sorted_timestamp_ind = np.argsort(timestamps)\n            marks = marks[sorted_timestamp_ind]\n            timestamps = timestamps[sorted_timestamp_ind]\n\n        if \"threshold\" in mark_param[\"mark_param_dict\"]:\n            timestamps, marks = UnitMarks._threshold(\n                timestamps, marks, mark_param[\"mark_param_dict\"]\n            )\n\n        # create a new AnalysisNwbfile and a timeseries for the marks and save\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        nwb_object = pynwb.TimeSeries(\n            name=\"marks\",\n            data=marks,\n            unit=\"uV\",\n            timestamps=timestamps,\n            description=\"spike features for clusterless decoding\",\n        )\n        key[\"marks_object_id\"] = AnalysisNwbfile().add_nwb_object(\n            key[\"analysis_file_name\"], nwb_object\n        )\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], key[\"analysis_file_name\"])\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n\"\"\"Convenience function for returning the marks in a readable format\"\"\"\n        return self.fetch_dataframe()[0]\n\n    def fetch_dataframe(self):\n        return [self._convert_to_dataframe(data) for data in self.fetch_nwb()]\n\n    @staticmethod\n    def _convert_to_dataframe(nwb_data):\n        n_marks = nwb_data[\"marks\"].data.shape[1]\n        columns = [f\"amplitude_{ind:04d}\" for ind in range(n_marks)]\n        return pd.DataFrame(\n            nwb_data[\"marks\"].data,\n            index=pd.Index(nwb_data[\"marks\"].timestamps, name=\"time\"),\n            columns=columns,\n        )\n\n    @staticmethod\n    def _get_peak_amplitude(\n        waveform, peak_sign=\"neg\", estimate_peak_time=False\n    ):\n\"\"\"Returns the amplitudes of all channels at the time of the peak\n        amplitude across channels.\n\n        Parameters\n        ----------\n        waveform : array-like, shape (n_spikes, n_time, n_channels)\n        peak_sign : ('pos', 'neg', 'both'), optional\n            Direction of the peak in the waveform\n        estimate_peak_time : bool, optional\n            Find the peak times for each spike because some spikesorters do not\n            align the spike time (at index n_time // 2) to the peak\n\n        Returns\n        -------\n        peak_amplitudes : array-like, shape (n_spikes, n_channels)\n\n        \"\"\"\n        if estimate_peak_time:\n            if peak_sign == \"neg\":\n                peak_inds = np.argmin(np.min(waveform, axis=2), axis=1)\n            elif peak_sign == \"pos\":\n                peak_inds = np.argmax(np.max(waveform, axis=2), axis=1)\n            elif peak_sign == \"both\":\n                peak_inds = np.argmax(np.max(np.abs(waveform), axis=2), axis=1)\n\n            # Get mode of peaks to find the peak time\n            values, counts = np.unique(peak_inds, return_counts=True)\n            spike_peak_ind = values[counts.argmax()]\n        else:\n            spike_peak_ind = waveform.shape[1] // 2\n\n        return waveform[:, spike_peak_ind]\n\n    @staticmethod\n    def _threshold(timestamps, marks, mark_param_dict):\n\"\"\"Filter the marks by an amplitude threshold\n\n        Parameters\n        ----------\n        timestamps : array-like, shape (n_time,)\n        marks : array-like, shape (n_time, n_channels)\n        mark_param_dict : dict\n\n        Returns\n        -------\n        filtered_timestamps : array-like, shape (n_filtered_time,)\n        filtered_marks : array-like, shape (n_filtered_time, n_channels)\n\n        \"\"\"\n        if mark_param_dict[\"peak_sign\"] == \"neg\":\n            include = np.min(marks, axis=1) &lt;= -1 * mark_param_dict[\"threshold\"]\n        elif mark_param_dict[\"peak_sign\"] == \"pos\":\n            include = np.max(marks, axis=1) &gt;= mark_param_dict[\"threshold\"]\n        elif mark_param_dict[\"peak_sign\"] == \"both\":\n            include = (\n                np.max(np.abs(marks), axis=1) &gt;= mark_param_dict[\"threshold\"]\n            )\n        return timestamps[include], marks[include]\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.UnitMarks.fetch1_dataframe", "title": "<code>fetch1_dataframe()</code>", "text": "<p>Convenience function for returning the marks in a readable format</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>def fetch1_dataframe(self):\n\"\"\"Convenience function for returning the marks in a readable format\"\"\"\n    return self.fetch_dataframe()[0]\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.UnitMarksIndicatorSelection", "title": "<code>UnitMarksIndicatorSelection</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Bins the spike times and associated spike waveform features for a given time interval into regular time bins determined by the sampling rate.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass UnitMarksIndicatorSelection(dj.Lookup):\n\"\"\"Bins the spike times and associated spike waveform features for a given\n    time interval into regular time bins determined by the sampling rate.\"\"\"\n\n    definition = \"\"\"\n    -&gt; UnitMarks\n    -&gt; IntervalList\n    sampling_rate=500 : float\n    ---\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.UnitMarksIndicator", "title": "<code>UnitMarksIndicator</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Bins the spike times and associated spike waveform features into regular time bins according to the sampling rate. Features that fall into the same time bin are averaged.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass UnitMarksIndicator(dj.Computed):\n\"\"\"Bins the spike times and associated spike waveform features into regular\n    time bins according to the sampling rate. Features that fall into the same\n    time bin are averaged.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; UnitMarks\n    -&gt; UnitMarksIndicatorSelection\n    ---\n    -&gt; AnalysisNwbfile\n    marks_indicator_object_id: varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        # TODO: intersection of sort interval and interval list\n        interval_times = (IntervalList &amp; key).fetch1(\"valid_times\")\n\n        sampling_rate = (UnitMarksIndicatorSelection &amp; key).fetch(\n            \"sampling_rate\"\n        )\n\n        marks_df = (UnitMarks &amp; key).fetch1_dataframe()\n\n        time = self.get_time_bins_from_interval(interval_times, sampling_rate)\n\n        # Bin marks into time bins. No spike bins will have NaN\n        marks_df = marks_df.loc[time.min() : time.max()]\n        time_index = np.digitize(marks_df.index, time[1:-1])\n        marks_indicator_df = (\n            marks_df.groupby(time[time_index])\n            .mean()\n            .reindex(index=pd.Index(time, name=\"time\"))\n        )\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"analysis_file_name\"] = nwb_analysis_file.create(\n            key[\"nwb_file_name\"]\n        )\n\n        key[\"marks_indicator_object_id\"] = nwb_analysis_file.add_nwb_object(\n            analysis_file_name=key[\"analysis_file_name\"],\n            nwb_object=marks_indicator_df.reset_index(),\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n    @staticmethod\n    def get_time_bins_from_interval(interval_times, sampling_rate):\n\"\"\"Picks the superset of the interval\"\"\"\n        start_time, end_time = interval_times[0][0], interval_times[-1][-1]\n        n_samples = int(np.ceil((end_time - start_time) * sampling_rate)) + 1\n\n        return np.linspace(start_time, end_time, n_samples)\n\n    @staticmethod\n    def plot_all_marks(marks_indicators: xr.DataArray, plot_size=5, s=10):\n\"\"\"Plots 2D slices of each of the spike features against each other\n        for all electrodes.\n\n        Parameters\n        ----------\n        marks_indicators : xr.DataArray, shape (n_time, n_electrodes, n_features)\n            Spike times and associated spike waveform features binned into\n        \"\"\"\n        for electrode_ind in marks_indicators.electrodes:\n            marks = (\n                marks_indicators.sel(electrodes=electrode_ind)\n                .dropna(\"time\", how=\"all\")\n                .dropna(\"marks\")\n            )\n            n_features = len(marks.marks)\n            fig, axes = plt.subplots(\n                n_features,\n                n_features,\n                constrained_layout=True,\n                sharex=True,\n                sharey=True,\n                figsize=(plot_size * n_features, plot_size * n_features),\n            )\n            for ax_ind1, feature1 in enumerate(marks.marks):\n                for ax_ind2, feature2 in enumerate(marks.marks):\n                    try:\n                        axes[ax_ind1, ax_ind2].scatter(\n                            marks.sel(marks=feature1),\n                            marks.sel(marks=feature2),\n                            s=s,\n                        )\n                    except TypeError:\n                        axes.scatter(\n                            marks.sel(marks=feature1),\n                            marks.sel(marks=feature2),\n                            s=s,\n                        )\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        return self.fetch_dataframe()[0]\n\n    def fetch_dataframe(self):\n        return [\n            data[\"marks_indicator\"].set_index(\"time\")\n            for data in self.fetch_nwb()\n        ]\n\n    def fetch_xarray(self):\n        # sort_group_electrodes = (\n        #     SortGroup.SortGroupElectrode() &amp;\n        #     pd.DataFrame(self).to_dict('records'))\n        # brain_region = (sort_group_electrodes * Electrode *\n        #                 BrainRegion).fetch('region_name')\n\n        marks_indicators = (\n            xr.concat(\n                [\n                    df.to_xarray().to_array(\"marks\")\n                    for df in self.fetch_dataframe()\n                ],\n                dim=\"electrodes\",\n            )\n            .transpose(\"time\", \"marks\", \"electrodes\")\n            .assign_coords({\"electrodes\": self.fetch(\"sort_group_id\")})\n            .sortby([\"electrodes\", \"marks\"])\n        )\n\n        # hacky way to keep the marks in order\n        def reformat_name(name):\n            mark_type, number = name.split(\"_\")\n            return f\"{mark_type}_{int(number):04d}\"\n\n        new_mark_names = [\n            reformat_name(name) for name in marks_indicators.marks.values\n        ]\n\n        return marks_indicators.assign_coords({\"marks\": new_mark_names}).sortby(\n            [\"electrodes\", \"marks\"]\n        )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.UnitMarksIndicator.get_time_bins_from_interval", "title": "<code>get_time_bins_from_interval(interval_times, sampling_rate)</code>  <code>staticmethod</code>", "text": "<p>Picks the superset of the interval</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@staticmethod\ndef get_time_bins_from_interval(interval_times, sampling_rate):\n\"\"\"Picks the superset of the interval\"\"\"\n    start_time, end_time = interval_times[0][0], interval_times[-1][-1]\n    n_samples = int(np.ceil((end_time - start_time) * sampling_rate)) + 1\n\n    return np.linspace(start_time, end_time, n_samples)\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.UnitMarksIndicator.plot_all_marks", "title": "<code>plot_all_marks(marks_indicators, plot_size=5, s=10)</code>  <code>staticmethod</code>", "text": "<p>Plots 2D slices of each of the spike features against each other for all electrodes.</p> <p>Parameters:</p> Name Type Description Default <code>marks_indicators</code> <code>xr.DataArray, shape(n_time, n_electrodes, n_features)</code> <p>Spike times and associated spike waveform features binned into</p> required Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@staticmethod\ndef plot_all_marks(marks_indicators: xr.DataArray, plot_size=5, s=10):\n\"\"\"Plots 2D slices of each of the spike features against each other\n    for all electrodes.\n\n    Parameters\n    ----------\n    marks_indicators : xr.DataArray, shape (n_time, n_electrodes, n_features)\n        Spike times and associated spike waveform features binned into\n    \"\"\"\n    for electrode_ind in marks_indicators.electrodes:\n        marks = (\n            marks_indicators.sel(electrodes=electrode_ind)\n            .dropna(\"time\", how=\"all\")\n            .dropna(\"marks\")\n        )\n        n_features = len(marks.marks)\n        fig, axes = plt.subplots(\n            n_features,\n            n_features,\n            constrained_layout=True,\n            sharex=True,\n            sharey=True,\n            figsize=(plot_size * n_features, plot_size * n_features),\n        )\n        for ax_ind1, feature1 in enumerate(marks.marks):\n            for ax_ind2, feature2 in enumerate(marks.marks):\n                try:\n                    axes[ax_ind1, ax_ind2].scatter(\n                        marks.sel(marks=feature1),\n                        marks.sel(marks=feature2),\n                        s=s,\n                    )\n                except TypeError:\n                    axes.scatter(\n                        marks.sel(marks=feature1),\n                        marks.sel(marks=feature2),\n                        s=s,\n                    )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.ClusterlessClassifierParameters", "title": "<code>ClusterlessClassifierParameters</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Decodes the animal's mental position and some category of interest from unclustered spikes and spike waveform features</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass ClusterlessClassifierParameters(dj.Manual):\n\"\"\"Decodes the animal's mental position and some category of interest\n    from unclustered spikes and spike waveform features\n    \"\"\"\n\n    definition = \"\"\"\n    classifier_param_name : varchar(80) # a name for this set of parameters\n    ---\n    classifier_params :   BLOB    # initialization parameters\n    fit_params :          BLOB    # fit parameters\n    predict_params :      BLOB    # prediction parameters\n    \"\"\"\n\n    def insert_default(self):\n        (\n            classifier_parameters,\n            fit_parameters,\n            predict_parameters,\n        ) = make_default_decoding_parameters_cpu()\n        self.insert1(\n            {\n                \"classifier_param_name\": \"default_decoding_cpu\",\n                \"classifier_params\": classifier_parameters,\n                \"fit_params\": fit_parameters,\n                \"predict_params\": predict_parameters,\n            },\n            skip_duplicates=True,\n        )\n\n        (\n            classifier_parameters,\n            fit_parameters,\n            predict_parameters,\n        ) = make_default_decoding_parameters_gpu()\n        self.insert1(\n            {\n                \"classifier_param_name\": \"default_decoding_gpu\",\n                \"classifier_params\": classifier_parameters,\n                \"fit_params\": fit_parameters,\n                \"predict_params\": predict_parameters,\n            },\n            skip_duplicates=True,\n        )\n\n    def insert1(self, key, **kwargs):\n        super().insert1(convert_classes_to_dict(key), **kwargs)\n\n    def fetch1(self, *args, **kwargs):\n        return restore_classes(super().fetch1(*args, **kwargs))\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.MultiunitFiringRate", "title": "<code>MultiunitFiringRate</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Computes the population multiunit firing rate from the spikes in MarksIndicator.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass MultiunitFiringRate(dj.Computed):\n\"\"\"Computes the population multiunit firing rate from the spikes in\n    MarksIndicator.\"\"\"\n\n    definition = \"\"\"\n    -&gt; UnitMarksIndicator\n    ---\n    -&gt; AnalysisNwbfile\n    multiunit_firing_rate_object_id: varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        marks = (UnitMarksIndicator &amp; key).fetch_xarray()\n        multiunit_spikes = (np.any(~np.isnan(marks.values), axis=1)).astype(\n            float\n        )\n        multiunit_firing_rate = pd.DataFrame(\n            get_multiunit_population_firing_rate(\n                multiunit_spikes, key[\"sampling_rate\"]\n            ),\n            index=marks.time,\n            columns=[\"firing_rate\"],\n        )\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"analysis_file_name\"] = nwb_analysis_file.create(\n            key[\"nwb_file_name\"]\n        )\n\n        key[\n            \"multiunit_firing_rate_object_id\"\n        ] = nwb_analysis_file.add_nwb_object(\n            analysis_file_name=key[\"analysis_file_name\"],\n            nwb_object=multiunit_firing_rate.reset_index(),\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        return self.fetch_dataframe()[0]\n\n    def fetch_dataframe(self):\n        return [\n            data[\"multiunit_firing_rate\"].set_index(\"time\")\n            for data in self.fetch_nwb()\n        ]\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.MultiunitHighSynchronyEventsParameters", "title": "<code>MultiunitHighSynchronyEventsParameters</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Parameters for extracting times of high mulitunit activity during immobility.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass MultiunitHighSynchronyEventsParameters(dj.Manual):\n\"\"\"Parameters for extracting times of high mulitunit activity during immobility.\"\"\"\n\n    definition = \"\"\"\n    param_name : varchar(80) # a name for this set of parameters\n    ---\n    minimum_duration = 0.015 :  float # minimum duration of event (in seconds)\n    zscore_threshold = 2.0 : float    # threshold event must cross to be considered (in std. dev.)\n    close_event_threshold = 0.0 :  float # events closer than this will be excluded (in seconds)\n    \"\"\"\n\n    def insert_default(self):\n        self.insert1(\n            {\n                \"param_name\": \"default\",\n                \"minimum_duration\": 0.015,\n                \"zscore_threshold\": 2.0,\n                \"close_event_threshold\": 0.0,\n            },\n            skip_duplicates=True,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.MultiunitHighSynchronyEvents", "title": "<code>MultiunitHighSynchronyEvents</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Finds times of high mulitunit activity during immobility.</p> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>@schema\nclass MultiunitHighSynchronyEvents(dj.Computed):\n\"\"\"Finds times of high mulitunit activity during immobility.\"\"\"\n\n    definition = \"\"\"\n    -&gt; MultiunitHighSynchronyEventsParameters\n    -&gt; UnitMarksIndicator\n    -&gt; IntervalPositionInfo\n    ---\n    -&gt; AnalysisNwbfile\n    multiunit_hse_times_object_id: varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        marks = (UnitMarksIndicator &amp; key).fetch_xarray()\n        multiunit_spikes = (np.any(~np.isnan(marks.values), axis=1)).astype(\n            float\n        )\n        position_info = (IntervalPositionInfo() &amp; key).fetch1_dataframe()\n\n        params = (MultiunitHighSynchronyEventsParameters &amp; key).fetch1()\n\n        multiunit_high_synchrony_times = multiunit_HSE_detector(\n            marks.time.values,\n            multiunit_spikes,\n            position_info.head_speed.values,\n            sampling_frequency=key[\"sampling_rate\"],\n            **params,\n        )\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"analysis_file_name\"] = nwb_analysis_file.create(\n            key[\"nwb_file_name\"]\n        )\n\n        key[\"multiunit_hse_times_object_id\"] = nwb_analysis_file.add_nwb_object(\n            analysis_file_name=key[\"analysis_file_name\"],\n            nwb_object=multiunit_high_synchrony_times.reset_index(),\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.get_decoding_data_for_epoch", "title": "<code>get_decoding_data_for_epoch(nwb_file_name, interval_list_name, position_info_param_name='default_decoding', additional_mark_keys={})</code>", "text": "<p>Collects necessary data for decoding.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> required <code>interval_list_name</code> <code>str</code> required <code>position_info_param_name</code> <code>str</code> <code>'default_decoding'</code> <code>additional_mark_keys</code> <code>dict</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>position_info</code> <code>pd.DataFrame, shape(n_time, n_columns)</code> <code>marks</code> <code>xr.DataArray, shape(n_time, n_marks, n_electrodes)</code> <code>valid_slices</code> <code>list[slice]</code> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>def get_decoding_data_for_epoch(\n    nwb_file_name: str,\n    interval_list_name: str,\n    position_info_param_name: str = \"default_decoding\",\n    additional_mark_keys: dict = {},\n) -&gt; tuple[pd.DataFrame, xr.DataArray, list[slice]]:\n\"\"\"Collects necessary data for decoding.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n    interval_list_name : str\n    position_info_param_name : str, optional\n    additional_mark_keys : dict, optional\n\n    Returns\n    -------\n    position_info : pd.DataFrame, shape (n_time, n_columns)\n    marks : xr.DataArray, shape (n_time, n_marks, n_electrodes)\n    valid_slices : list[slice]\n\n    \"\"\"\n\n    valid_ephys_position_times_by_epoch = (\n        get_valid_ephys_position_times_by_epoch(nwb_file_name)\n    )\n    valid_ephys_position_times = valid_ephys_position_times_by_epoch[\n        interval_list_name\n    ]\n    valid_slices = convert_valid_times_to_slice(valid_ephys_position_times)\n    position_interval_name = (\n        convert_epoch_interval_name_to_position_interval_name(\n            interval_list_name\n        )\n    )\n\n    position_info = (\n        IntervalPositionInfo()\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_list_name\": position_interval_name,\n            \"position_info_param_name\": position_info_param_name,\n        }\n    ).fetch1_dataframe()\n\n    position_info = pd.concat(\n        [position_info.loc[times] for times in valid_slices]\n    )\n\n    marks = (\n        (\n            UnitMarksIndicator()\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"interval_list_name\": position_interval_name,\n                **additional_mark_keys,\n            }\n        )\n    ).fetch_xarray()\n\n    marks = xr.concat(\n        [marks.sel(time=times) for times in valid_slices], dim=\"time\"\n    )\n\n    return position_info, marks, valid_slices\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.get_data_for_multiple_epochs", "title": "<code>get_data_for_multiple_epochs(nwb_file_name, epoch_names, position_info_param_name='default_decoding', additional_mark_keys={})</code>", "text": "<p>Collects necessary data for decoding multiple environments</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> required <code>epoch_names</code> <code>list[str]</code> required <code>position_info_param_name</code> <code>str</code> <code>'default_decoding'</code> <code>additional_mark_keys</code> <code>dict</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>position_info</code> <code>pd.DataFrame, shape(n_time, n_columns)</code> <code>marks</code> <code>xr.DataArray, shape(n_time, n_marks, n_electrodes)</code> <code>valid_slices</code> <code>dict[str, list[slice]]</code> <code>environment_labels</code> <code>np.ndarray, shape(n_time)</code> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>def get_data_for_multiple_epochs(\n    nwb_file_name: str,\n    epoch_names: list[str],\n    position_info_param_name=\"default_decoding\",\n    additional_mark_keys: dict = {},\n) -&gt; tuple[pd.DataFrame, xr.DataArray, dict[str, list[slice]], np.ndarray]:\n\"\"\"Collects necessary data for decoding multiple environments\n\n    Parameters\n    ----------\n    nwb_file_name : str\n    epoch_names : list[str]\n    position_info_param_name : str, optional\n    additional_mark_keys : dict, optional\n\n    Returns\n    -------\n    position_info : pd.DataFrame, shape (n_time, n_columns)\n    marks : xr.DataArray, shape (n_time, n_marks, n_electrodes)\n    valid_slices : dict[str, list[slice]]\n    environment_labels : np.ndarray, shape (n_time,)\n\n    \"\"\"\n    data = []\n    environment_labels = []\n\n    for epoch in epoch_names:\n        data.append(\n            get_decoding_data_for_epoch(\n                nwb_file_name,\n                epoch,\n                position_info_param_name=position_info_param_name,\n                additional_mark_keys=additional_mark_keys,\n            )\n        )\n        n_time = data[-1][0].shape[0]\n        environment_labels.append([epoch] * n_time)\n\n    environment_labels = np.concatenate(environment_labels, axis=0)\n    position_info, marks, valid_slices = list(zip(*data))\n    position_info = pd.concat(position_info, axis=0)\n    marks = xr.concat(marks, dim=\"time\")\n    valid_slices = {\n        epoch: valid_slice\n        for epoch, valid_slice in zip(epoch_names, valid_slices)\n    }\n\n    assert position_info.shape[0] == marks.shape[0]\n\n    return position_info, marks, valid_slices, environment_labels\n</code></pre>"}, {"location": "api/src/spyglass/decoding/clusterless/#src.spyglass.decoding.clusterless.populate_mark_indicators", "title": "<code>populate_mark_indicators(spikesorting_selection_keys, mark_param_name='default', position_info_param_name='default_decoding')</code>", "text": "<p>Populate mark indicators for all units in the given spike sorting selection.</p> <p>This function is a way to do several pipeline steps at once. It will:   1. Populate the SpikeSortingSelection table   2. Populate the SpikeSorting table   3. Populate the Curation table   4. Populate the CuratedSpikeSortingSelection table   5. Populate UnitMarks   6. Compute UnitMarksIndicator for each position epoch</p> <p>Parameters:</p> Name Type Description Default <code>spikesorting_selection_keys</code> <code>dict</code> required <code>mark_param_name</code> <code>str</code> <code>'default'</code> <code>position_info_param_name</code> <code>str</code> <code>'default_decoding'</code> Source code in <code>src/spyglass/decoding/clusterless.py</code> <pre><code>def populate_mark_indicators(\n    spikesorting_selection_keys: dict,\n    mark_param_name: str = \"default\",\n    position_info_param_name: str = \"default_decoding\",\n):\n\"\"\"Populate mark indicators for all units in the given spike sorting selection.\n\n    This function is a way to do several pipeline steps at once. It will:\n      1. Populate the SpikeSortingSelection table\n      2. Populate the SpikeSorting table\n      3. Populate the Curation table\n      4. Populate the CuratedSpikeSortingSelection table\n      5. Populate UnitMarks\n      6. Compute UnitMarksIndicator for each position epoch\n\n    Parameters\n    ----------\n    spikesorting_selection_keys : dict\n    mark_param_name : str, optional\n    position_info_param_name : str, optional\n    \"\"\"\n    spikesorting_selection_keys = deepcopy(spikesorting_selection_keys)\n    # Populate spike sorting\n    SpikeSortingSelection().insert(\n        spikesorting_selection_keys,\n        skip_duplicates=True,\n    )\n    SpikeSorting.populate(spikesorting_selection_keys)\n\n    # Skip any curation\n    curation_keys = [\n        Curation.insert_curation(key) for key in spikesorting_selection_keys\n    ]\n\n    CuratedSpikeSortingSelection().insert(curation_keys, skip_duplicates=True)\n    CuratedSpikeSorting.populate(CuratedSpikeSortingSelection() &amp; curation_keys)\n\n    # Populate marks\n    mark_parameters_keys = pd.DataFrame(CuratedSpikeSorting &amp; curation_keys)\n    mark_parameters_keys[\"mark_param_name\"] = mark_param_name\n    mark_parameters_keys = mark_parameters_keys.loc[\n        :, UnitMarkParameters.primary_key\n    ].to_dict(\"records\")\n    UnitMarkParameters().insert(mark_parameters_keys, skip_duplicates=True)\n    UnitMarks.populate(UnitMarkParameters &amp; mark_parameters_keys)\n\n    # Compute mark indicators for each position epoch\n    nwb_file_name = spikesorting_selection_keys[0][\"nwb_file_name\"]\n    position_interval_names = (\n        IntervalPositionInfo()\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"position_info_param_name\": position_info_param_name,\n        }\n    ).fetch(\"interval_list_name\")\n\n    for interval_name in tqdm(position_interval_names):\n        position_interval = IntervalList &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_list_name\": interval_name,\n        }\n\n        marks_selection = (UnitMarks &amp; mark_parameters_keys) * position_interval\n        marks_selection = (\n            pd.DataFrame(marks_selection)\n            .loc[:, marks_selection.primary_key]\n            .to_dict(\"records\")\n        )\n        UnitMarksIndicatorSelection.insert(\n            marks_selection, skip_duplicates=True\n        )\n        UnitMarksIndicator.populate(marks_selection)\n</code></pre>"}, {"location": "api/src/spyglass/decoding/core/", "title": "core.py", "text": ""}, {"location": "api/src/spyglass/decoding/core/#src.spyglass.decoding.core.get_valid_ephys_position_times_from_interval", "title": "<code>get_valid_ephys_position_times_from_interval(interval_list_name, nwb_file_name)</code>", "text": "<p>Finds the intersection of the valid times for the interval list, the valid times for the ephys data, and the valid times for the position data.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list_name</code> <code>str</code> required <code>nwb_file_name</code> <code>str</code> required <p>Returns:</p> Name Type Description <code>valid_ephys_position_times</code> <code>np.ndarray, shape(n_valid_times, 2)</code> Source code in <code>src/spyglass/decoding/core.py</code> <pre><code>def get_valid_ephys_position_times_from_interval(\n    interval_list_name: str, nwb_file_name: str\n) -&gt; np.ndarray:\n\"\"\"Finds the intersection of the valid times for the interval list, the valid times for the ephys data,\n    and the valid times for the position data.\n\n    Parameters\n    ----------\n    interval_list_name : str\n    nwb_file_name : str\n\n    Returns\n    -------\n    valid_ephys_position_times : np.ndarray, shape (n_valid_times, 2)\n\n    \"\"\"\n    interval_valid_times = (\n        IntervalList\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_list_name\": interval_list_name,\n        }\n    ).fetch1(\"valid_times\")\n\n    position_interval_names = (\n        RawPosition\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n        }\n    ).fetch(\"interval_list_name\")\n    position_interval_names = position_interval_names[\n        np.argsort(\n            [\n                int(name.strip(\"pos valid time\"))\n                for name in position_interval_names\n            ]\n        )\n    ]\n    valid_pos_times = [\n        (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"interval_list_name\": pos_interval_name,\n            }\n        ).fetch1(\"valid_times\")\n        for pos_interval_name in position_interval_names\n    ]\n\n    valid_ephys_times = (\n        IntervalList\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_list_name\": \"raw data valid times\",\n        }\n    ).fetch1(\"valid_times\")\n\n    return interval_list_intersect(\n        interval_list_intersect(interval_valid_times, valid_ephys_times),\n        np.concatenate(valid_pos_times),\n    )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/core/#src.spyglass.decoding.core.get_epoch_interval_names", "title": "<code>get_epoch_interval_names(nwb_file_name)</code>", "text": "<p>Find the interval names that are epochs.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> required <p>Returns:</p> Name Type Description <code>epoch_names</code> <code>list[str]</code> <p>List of interval names that are epochs.</p> Source code in <code>src/spyglass/decoding/core.py</code> <pre><code>def get_epoch_interval_names(nwb_file_name: str) -&gt; list[str]:\n\"\"\"Find the interval names that are epochs.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n\n    Returns\n    -------\n    epoch_names : list[str]\n        List of interval names that are epochs.\n    \"\"\"\n    interval_list = pd.DataFrame(\n        IntervalList() &amp; {\"nwb_file_name\": nwb_file_name}\n    )\n\n    interval_list = interval_list.loc[\n        interval_list.interval_list_name.str.contains(\n            r\"^(?:\\d+)_(?:\\w+)$\", regex=True, na=False\n        )\n    ]\n\n    return interval_list.interval_list_name.tolist()\n</code></pre>"}, {"location": "api/src/spyglass/decoding/core/#src.spyglass.decoding.core.get_valid_ephys_position_times_by_epoch", "title": "<code>get_valid_ephys_position_times_by_epoch(nwb_file_name)</code>", "text": "<p>Get the valid ephys position times for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> required <p>Returns:</p> Name Type Description <code>valid_ephys_position_times_by_epoch</code> <code>dict[str, np.ndarray]</code> <p>Dictionary of epoch names and valid ephys position times.</p> Source code in <code>src/spyglass/decoding/core.py</code> <pre><code>def get_valid_ephys_position_times_by_epoch(\n    nwb_file_name: str,\n) -&gt; dict[str, np.ndarray]:\n\"\"\"Get the valid ephys position times for each epoch.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n\n    Returns\n    -------\n    valid_ephys_position_times_by_epoch : dict[str, np.ndarray]\n        Dictionary of epoch names and valid ephys position times.\n\n    \"\"\"\n    return {\n        epoch: get_valid_ephys_position_times_from_interval(\n            epoch, nwb_file_name\n        )\n        for epoch in get_epoch_interval_names(nwb_file_name)\n    }\n</code></pre>"}, {"location": "api/src/spyglass/decoding/core/#src.spyglass.decoding.core.convert_epoch_interval_name_to_position_interval_name", "title": "<code>convert_epoch_interval_name_to_position_interval_name(epoch_interval_name)</code>", "text": "<p>Converts the epoch interval name to the position interval name.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_interval_name</code> <code>str</code> required <p>Returns:</p> Name Type Description <code>position_interval_name</code> <code>str</code> Source code in <code>src/spyglass/decoding/core.py</code> <pre><code>def convert_epoch_interval_name_to_position_interval_name(\n    epoch_interval_name: str,\n) -&gt; str:\n\"\"\"Converts the epoch interval name to the position interval name.\n\n    Parameters\n    ----------\n    epoch_interval_name : str\n\n    Returns\n    -------\n    position_interval_name : str\n    \"\"\"\n\n    pos_interval_number = int(epoch_interval_name.split(\"_\")[0]) - 1\n    return f\"pos {pos_interval_number} valid times\"\n</code></pre>"}, {"location": "api/src/spyglass/decoding/core/#src.spyglass.decoding.core.convert_valid_times_to_slice", "title": "<code>convert_valid_times_to_slice(valid_times)</code>", "text": "<p>Converts the valid times to a list of slices so that arrays can be indexed easily.</p> <p>Parameters:</p> Name Type Description Default <code>valid_times</code> <code>np.ndarray, shape(n_valid_times, 2)</code> <p>Start and end times for each valid time.</p> required <p>Returns:</p> Name Type Description <code>valid_time_slices</code> <code>list[slice]</code> Source code in <code>src/spyglass/decoding/core.py</code> <pre><code>def convert_valid_times_to_slice(valid_times: np.ndarray) -&gt; list[slice]:\n\"\"\"Converts the valid times to a list of slices so that arrays can be indexed easily.\n\n    Parameters\n    ----------\n    valid_times : np.ndarray, shape (n_valid_times, 2)\n        Start and end times for each valid time.\n\n    Returns\n    -------\n    valid_time_slices : list[slice]\n\n    \"\"\"\n    return [slice(times[0], times[1]) for times in valid_times]\n</code></pre>"}, {"location": "api/src/spyglass/decoding/core/#src.spyglass.decoding.core.create_model_for_multiple_epochs", "title": "<code>create_model_for_multiple_epochs(epoch_names, env_kwargs)</code>", "text": "<p>Creates the observation model, environment, and continuous transition types for multiple epochs for decoding</p> <p>Parameters:</p> Name Type Description Default <code>epoch_names</code> <code>list[str], length(n_epochs)</code> required <code>env_kwargs</code> <code>dict</code> <p>Environment keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>observation_models</code> <code>tuple[list[ObservationModel]</code> <p>Observation model for each epoch.</p> <code>environments</code> <code>list[Environment]</code> <p>Environment for each epoch.</p> <code>continuous_transition_types</code> <code>list[list[object]]]</code> <p>Continuous transition types for each epoch.</p> Source code in <code>src/spyglass/decoding/core.py</code> <pre><code>def create_model_for_multiple_epochs(\n    epoch_names: list[str], env_kwargs: dict\n) -&gt; tuple[list[ObservationModel], list[Environment], list[list[object]]]:\n\"\"\"Creates the observation model, environment, and continuous transition types for multiple epochs for decoding\n\n    Parameters\n    ----------\n    epoch_names : list[str], length (n_epochs)\n    env_kwargs : dict\n        Environment keyword arguments.\n\n    Returns\n    -------\n    observation_models: tuple[list[ObservationModel]\n        Observation model for each epoch.\n    environments : list[Environment]\n        Environment for each epoch.\n    continuous_transition_types : list[list[object]]]\n        Continuous transition types for each epoch.\n\n    \"\"\"\n    observation_models = []\n    environments = []\n    continuous_transition_types = []\n\n    for epoch in epoch_names:\n        observation_models.append(ObservationModel(epoch))\n        environments.append(Environment(epoch, **env_kwargs))\n\n    for epoch1 in epoch_names:\n        continuous_transition_types.append([])\n        for epoch2 in epoch_names:\n            if epoch1 == epoch2:\n                continuous_transition_types[-1].append(\n                    RandomWalk(epoch1, use_diffusion=False)\n                )\n            else:\n                continuous_transition_types[-1].append(Uniform(epoch1, epoch2))\n\n    return observation_models, environments, continuous_transition_types\n</code></pre>"}, {"location": "api/src/spyglass/decoding/dj_decoder_conversion/", "title": "dj_decoder_conversion.py", "text": "<p>Converts decoder classes into dictionaries and dictionaries into classes so that datajoint can store them in tables.</p>"}, {"location": "api/src/spyglass/decoding/dj_decoder_conversion/#src.spyglass.decoding.dj_decoder_conversion.restore_classes", "title": "<code>restore_classes(params)</code>", "text": "<p>Converts a dictionary of parameters into a dictionary of classes since datajoint cannot handle classes</p> Source code in <code>src/spyglass/decoding/dj_decoder_conversion.py</code> <pre><code>def restore_classes(params: dict) -&gt; dict:\n\"\"\"Converts a dictionary of parameters into a dictionary of classes since datajoint cannot handle classes\"\"\"\n    continuous_state_transition_types = {\n        \"RandomWalk\": RandomWalk,\n        \"RandomWalkDirection1\": RandomWalkDirection1,\n        \"RandomWalkDirection2\": RandomWalkDirection2,\n        \"Uniform\": Uniform,\n        \"Identity\": Identity,\n    }\n\n    discrete_state_transition_types = {\n        \"DiagonalDiscrete\": DiagonalDiscrete,\n        \"UniformDiscrete\": UniformDiscrete,\n        \"RandomDiscrete\": RandomDiscrete,\n        \"UserDefinedDiscrete\": UserDefinedDiscrete,\n    }\n\n    initial_conditions_types = {\n        \"UniformInitialConditions\": UniformInitialConditions,\n        \"UniformOneEnvironmentInitialConditions\": UniformOneEnvironmentInitialConditions,\n    }\n\n    params[\"classifier_params\"][\"continuous_transition_types\"] = [\n        [\n            _convert_dict_to_class(st, continuous_state_transition_types)\n            for st in sts\n        ]\n        for sts in params[\"classifier_params\"][\"continuous_transition_types\"]\n    ]\n    params[\"classifier_params\"][\"environments\"] = [\n        _convert_env_dict(env_params)\n        for env_params in params[\"classifier_params\"][\"environments\"]\n    ]\n    params[\"classifier_params\"][\n        \"discrete_transition_type\"\n    ] = _convert_dict_to_class(\n        params[\"classifier_params\"][\"discrete_transition_type\"],\n        discrete_state_transition_types,\n    )\n    params[\"classifier_params\"][\n        \"initial_conditions_type\"\n    ] = _convert_dict_to_class(\n        params[\"classifier_params\"][\"initial_conditions_type\"],\n        initial_conditions_types,\n    )\n\n    if params[\"classifier_params\"][\"observation_models\"] is not None:\n        params[\"classifier_params\"][\"observation_models\"] = [\n            ObservationModel(obs)\n            for obs in params[\"classifier_params\"][\"observation_models\"]\n        ]\n\n    return params\n</code></pre>"}, {"location": "api/src/spyglass/decoding/dj_decoder_conversion/#src.spyglass.decoding.dj_decoder_conversion.convert_classes_to_dict", "title": "<code>convert_classes_to_dict(key)</code>", "text": "<p>Converts the classifier parameters into a dictionary so that datajoint can store it.</p> Source code in <code>src/spyglass/decoding/dj_decoder_conversion.py</code> <pre><code>def convert_classes_to_dict(key: dict) -&gt; dict:\n\"\"\"Converts the classifier parameters into a dictionary so that datajoint can store it.\"\"\"\n    try:\n        key[\"classifier_params\"][\"environments\"] = [\n            _convert_environment_to_dict(env)\n            for env in key[\"classifier_params\"][\"environments\"]\n        ]\n    except TypeError:\n        key[\"classifier_params\"][\"environments\"] = [\n            _convert_environment_to_dict(\n                key[\"classifier_params\"][\"environments\"]\n            )\n        ]\n    key[\"classifier_params\"][\n        \"continuous_transition_types\"\n    ] = _convert_transitions_to_dict(\n        key[\"classifier_params\"][\"continuous_transition_types\"]\n    )\n    key[\"classifier_params\"][\"discrete_transition_type\"] = _to_dict(\n        key[\"classifier_params\"][\"discrete_transition_type\"]\n    )\n    key[\"classifier_params\"][\"initial_conditions_type\"] = _to_dict(\n        key[\"classifier_params\"][\"initial_conditions_type\"]\n    )\n\n    if key[\"classifier_params\"][\"observation_models\"] is not None:\n        key[\"classifier_params\"][\"observation_models\"] = [\n            vars(obs) for obs in key[\"classifier_params\"][\"observation_models\"]\n        ]\n\n    try:\n        key[\"classifier_params\"][\n            \"clusterless_algorithm_params\"\n        ] = _convert_algorithm_params(\n            key[\"classifier_params\"][\"clusterless_algorithm_params\"]\n        )\n    except KeyError:\n        pass\n\n    return key\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/", "title": "sorted_spikes.py", "text": "<p>Pipeline for decoding the animal's mental position and some category of interest from clustered spikes times. See [1] for details.</p>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes--references", "title": "References", "text": "<p>[1] Denovellis, E. L. et al. Hippocampal replay of experience at real-world speeds. eLife 10, e64505 (2021).</p>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.SortedSpikesIndicatorSelection", "title": "<code>SortedSpikesIndicatorSelection</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Bins spike times into regular intervals given by the sampling rate. Start and stop time of the interval are defined by the interval list.</p> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>@schema\nclass SortedSpikesIndicatorSelection(dj.Lookup):\n\"\"\"Bins spike times into regular intervals given by the sampling rate.\n    Start and stop time of the interval are defined by the interval list.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; CuratedSpikeSorting\n    -&gt; IntervalList\n    sampling_rate=500 : float\n    ---\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.SortedSpikesIndicator", "title": "<code>SortedSpikesIndicator</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Bins spike times into regular intervals given by the sampling rate. Useful for GLMs and for decoding.</p> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>@schema\nclass SortedSpikesIndicator(dj.Computed):\n\"\"\"Bins spike times into regular intervals given by the sampling rate.\n    Useful for GLMs and for decoding.\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; SortedSpikesIndicatorSelection\n    ---\n    -&gt; AnalysisNwbfile\n    spike_indicator_object_id: varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        pprint.pprint(key)\n        # TODO: intersection of sort interval and interval list\n        interval_times = (IntervalList &amp; key).fetch1(\"valid_times\")\n\n        sampling_rate = (SortedSpikesIndicatorSelection &amp; key).fetch(\n            \"sampling_rate\"\n        )\n\n        time = self.get_time_bins_from_interval(interval_times, sampling_rate)\n\n        spikes_nwb = (CuratedSpikeSorting &amp; key).fetch_nwb()\n        # restrict to cases with units\n        spikes_nwb = [entry for entry in spikes_nwb if \"units\" in entry]\n        spike_times_list = [\n            np.asarray(n_trode[\"units\"][\"spike_times\"])\n            for n_trode in spikes_nwb\n        ]\n        if len(spike_times_list) &gt; 0:  # if units\n            spikes = np.concatenate(spike_times_list)\n\n            # Bin spikes into time bins\n            spike_indicator = []\n            for spike_times in spikes:\n                spike_times = spike_times[\n                    (spike_times &gt; time[0]) &amp; (spike_times &lt;= time[-1])\n                ]\n                spike_indicator.append(\n                    np.bincount(\n                        np.digitize(spike_times, time[1:-1]),\n                        minlength=time.shape[0],\n                    )\n                )\n\n            column_names = np.concatenate(\n                [\n                    [\n                        f'{n_trode[\"sort_group_id\"]:04d}_{unit_number:04d}'\n                        for unit_number in n_trode[\"units\"].index\n                    ]\n                    for n_trode in spikes_nwb\n                ]\n            )\n            spike_indicator = pd.DataFrame(\n                np.stack(spike_indicator, axis=1),\n                index=pd.Index(time, name=\"time\"),\n                columns=column_names,\n            )\n\n            # Insert into analysis nwb file\n            nwb_analysis_file = AnalysisNwbfile()\n            key[\"analysis_file_name\"] = nwb_analysis_file.create(\n                key[\"nwb_file_name\"]\n            )\n\n            key[\"spike_indicator_object_id\"] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=spike_indicator.reset_index(),\n            )\n\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n\n            self.insert1(key)\n\n    @staticmethod\n    def get_time_bins_from_interval(interval_times, sampling_rate):\n\"\"\"Gets the superset of the interval.\"\"\"\n        start_time, end_time = interval_times[0][0], interval_times[-1][-1]\n        n_samples = int(np.ceil((end_time - start_time) * sampling_rate)) + 1\n\n        return np.linspace(start_time, end_time, n_samples)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        return self.fetch_dataframe()[0]\n\n    def fetch_dataframe(self):\n        return pd.concat(\n            [\n                data[\"spike_indicator\"].set_index(\"time\")\n                for data in self.fetch_nwb()\n            ],\n            axis=1,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.SortedSpikesIndicator.get_time_bins_from_interval", "title": "<code>get_time_bins_from_interval(interval_times, sampling_rate)</code>  <code>staticmethod</code>", "text": "<p>Gets the superset of the interval.</p> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>@staticmethod\ndef get_time_bins_from_interval(interval_times, sampling_rate):\n\"\"\"Gets the superset of the interval.\"\"\"\n    start_time, end_time = interval_times[0][0], interval_times[-1][-1]\n    n_samples = int(np.ceil((end_time - start_time) * sampling_rate)) + 1\n\n    return np.linspace(start_time, end_time, n_samples)\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.SortedSpikesClassifierParameters", "title": "<code>SortedSpikesClassifierParameters</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Stores parameters for decoding with sorted spikes</p> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>@schema\nclass SortedSpikesClassifierParameters(dj.Manual):\n\"\"\"Stores parameters for decoding with sorted spikes\"\"\"\n\n    definition = \"\"\"\n    classifier_param_name : varchar(80) # a name for this set of parameters\n    ---\n    classifier_params :   BLOB    # initialization parameters\n    fit_params :          BLOB    # fit parameters\n    predict_params :      BLOB    # prediction parameters\n    \"\"\"\n\n    def insert_default(self):\n        (\n            classifier_parameters,\n            fit_parameters,\n            predict_parameters,\n        ) = make_default_decoding_parameters_cpu()\n        self.insert1(\n            {\n                \"classifier_param_name\": \"default_decoding_cpu\",\n                \"classifier_params\": classifier_parameters,\n                \"fit_params\": fit_parameters,\n                \"predict_params\": predict_parameters,\n            },\n            skip_duplicates=True,\n        )\n\n        (\n            classifier_parameters,\n            fit_parameters,\n            predict_parameters,\n        ) = make_default_decoding_parameters_gpu()\n        self.insert1(\n            {\n                \"classifier_param_name\": \"default_decoding_gpu\",\n                \"classifier_params\": classifier_parameters,\n                \"fit_params\": fit_parameters,\n                \"predict_params\": predict_parameters,\n            },\n            skip_duplicates=True,\n        )\n\n    def insert1(self, key, **kwargs):\n        super().insert1(convert_classes_to_dict(key), **kwargs)\n\n    def fetch1(self, *args, **kwargs):\n        return restore_classes(super().fetch1(*args, **kwargs))\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.get_spike_indicator", "title": "<code>get_spike_indicator(key, time_range, sampling_rate=500.0)</code>", "text": "<p>For a given key, returns a dataframe with the spike indicator for each unit</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> required <code>time_range</code> <code>tuple[float, float]</code> <p>Start and end time of the spike indicator</p> required <code>sampling_rate</code> <code>float</code> <code>500.0</code> <p>Returns:</p> Name Type Description <code>spike_indicator</code> <code>pd.DataFrame, shape(n_time, n_units)</code> <p>A dataframe with the spike indicator for each unit</p> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>def get_spike_indicator(\n    key: dict, time_range: tuple[float, float], sampling_rate: float = 500.0\n) -&gt; pd.DataFrame:\n\"\"\"For a given key, returns a dataframe with the spike indicator for each unit\n\n    Parameters\n    ----------\n    key : dict\n    time_range : tuple[float, float]\n        Start and end time of the spike indicator\n    sampling_rate : float, optional\n\n    Returns\n    -------\n    spike_indicator : pd.DataFrame, shape (n_time, n_units)\n        A dataframe with the spike indicator for each unit\n    \"\"\"\n    start_time, end_time = time_range\n    n_samples = int(np.ceil((end_time - start_time) * sampling_rate)) + 1\n    time = np.linspace(start_time, end_time, n_samples)\n\n    spike_indicator = dict()\n    spikes_nwb_table = CuratedSpikeSorting() &amp; key\n\n    for n_trode in spikes_nwb_table.fetch_nwb():\n        try:\n            for unit_id, unit_spike_times in n_trode[\"units\"][\n                \"spike_times\"\n            ].items():\n                unit_spike_times = unit_spike_times[\n                    (unit_spike_times &gt; time[0])\n                    &amp; (unit_spike_times &lt;= time[-1])\n                ]\n                unit_name = f'{n_trode[\"sort_group_id\"]:04d}_{unit_id:04d}'\n                spike_indicator[unit_name] = np.bincount(\n                    np.digitize(unit_spike_times, time[1:-1]),\n                    minlength=time.shape[0],\n                )\n        except KeyError:\n            pass\n\n    return pd.DataFrame(\n        spike_indicator,\n        index=pd.Index(time, name=\"time\"),\n    )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.get_decoding_data_for_epoch", "title": "<code>get_decoding_data_for_epoch(nwb_file_name, interval_list_name, position_info_param_name='default', additional_spike_keys={})</code>", "text": "<p>Collects the data needed for decoding</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> required <code>interval_list_name</code> <code>str</code> required <code>position_info_param_name</code> <code>str</code> <code>'default'</code> <code>additional_spike_keys</code> <code>dict</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>position_info</code> <code>pd.DataFrame, shape(n_time, n_position_features)</code> <code>spikes</code> <code>pd.DataFrame, shape(n_time, n_units)</code> <code>valid_slices</code> <code>list[slice]</code> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>def get_decoding_data_for_epoch(\n    nwb_file_name: str,\n    interval_list_name: str,\n    position_info_param_name: str = \"default\",\n    additional_spike_keys: dict = {},\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, list[slice]]:\n\"\"\"Collects the data needed for decoding\n\n    Parameters\n    ----------\n    nwb_file_name : str\n    interval_list_name : str\n    position_info_param_name : str, optional\n    additional_spike_keys : dict, optional\n\n    Returns\n    -------\n    position_info : pd.DataFrame, shape (n_time, n_position_features)\n    spikes : pd.DataFrame, shape (n_time, n_units)\n    valid_slices : list[slice]\n\n    \"\"\"\n    # valid slices\n    valid_ephys_position_times_by_epoch = (\n        get_valid_ephys_position_times_by_epoch(nwb_file_name)\n    )\n    valid_ephys_position_times = valid_ephys_position_times_by_epoch[\n        interval_list_name\n    ]\n    valid_slices = convert_valid_times_to_slice(valid_ephys_position_times)\n\n    # position interval\n    position_interval_name = (\n        convert_epoch_interval_name_to_position_interval_name(\n            interval_list_name\n        )\n    )\n\n    # spikes\n    valid_times = np.asarray(\n        [(times.start, times.stop) for times in valid_slices]\n    )\n\n    curated_spikes_key = {\n        \"nwb_file_name\": nwb_file_name,\n        **additional_spike_keys,\n    }\n    spikes = get_spike_indicator(\n        curated_spikes_key,\n        (valid_times.min(), valid_times.max()),\n        sampling_rate=500,\n    )\n    spikes = pd.concat([spikes.loc[times] for times in valid_slices])\n\n    # position\n    position_info = (\n        IntervalPositionInfo()\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_list_name\": position_interval_name,\n            \"position_info_param_name\": position_info_param_name,\n        }\n    ).fetch1_dataframe()\n    new_time = spikes.index.to_numpy()\n    new_index = pd.Index(\n        np.unique(np.concatenate((position_info.index, new_time))), name=\"time\"\n    )\n    position_info = (\n        position_info.reindex(index=new_index)\n        .interpolate(method=\"linear\")\n        .reindex(index=new_time)\n    )\n\n    return position_info, spikes, valid_slices\n</code></pre>"}, {"location": "api/src/spyglass/decoding/sorted_spikes/#src.spyglass.decoding.sorted_spikes.get_data_for_multiple_epochs", "title": "<code>get_data_for_multiple_epochs(nwb_file_name, epoch_names, position_info_param_name='decoding', additional_spike_keys={})</code>", "text": "<p>Collects the data needed for decoding for multiple epochs</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> required <code>epoch_names</code> <code>list</code> required <code>position_info_param_name</code> <code>str</code> <code>'decoding'</code> <code>additional_spike_keys</code> <code>dict</code> <code>{}</code> <p>Returns:</p> Name Type Description <code>position_info</code> <code>pd.DataFrame, shape(n_time, n_position_features)</code> <code>spikes</code> <code>pd.DataFrame, shape(n_time, n_units)</code> <code>valid_slices</code> <code>list[slice]</code> <code>environment_labels</code> <code>np.ndarray, shape(n_time)</code> <p>The environment label for each time point</p> <code>sort_group_ids</code> <code>np.ndarray, shape(n_units)</code> <p>The sort group of each unit</p> Source code in <code>src/spyglass/decoding/sorted_spikes.py</code> <pre><code>def get_data_for_multiple_epochs(\n    nwb_file_name: str,\n    epoch_names: list,\n    position_info_param_name: str = \"decoding\",\n    additional_spike_keys: dict = {},\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, list[slice], np.ndarray, np.ndarray]:\n\"\"\"Collects the data needed for decoding for multiple epochs\n\n    Parameters\n    ----------\n    nwb_file_name : str\n    epoch_names : list\n    position_info_param_name : str, optional\n    additional_spike_keys : dict, optional\n\n    Returns\n    -------\n    position_info : pd.DataFrame, shape (n_time, n_position_features)\n    spikes : pd.DataFrame, shape (n_time, n_units)\n    valid_slices : list[slice]\n    environment_labels : np.ndarray, shape (n_time,)\n        The environment label for each time point\n    sort_group_ids : np.ndarray, shape (n_units,)\n        The sort group of each unit\n    \"\"\"\n    data = []\n    environment_labels = []\n\n    for epoch in epoch_names:\n        print(epoch)\n        data.append(\n            get_decoding_data_for_epoch(\n                nwb_file_name,\n                epoch,\n                position_info_param_name=position_info_param_name,\n                additional_spike_keys=additional_spike_keys,\n            )\n        )\n        n_time = data[-1][0].shape[0]\n        environment_labels.append([epoch] * n_time)\n\n    environment_labels = np.concatenate(environment_labels, axis=0)\n    position_info, spikes, valid_slices = list(zip(*data))\n    position_info = pd.concat(position_info, axis=0)\n    spikes = pd.concat(spikes, axis=0)\n    valid_slices = {\n        epoch: valid_slice\n        for epoch, valid_slice in zip(epoch_names, valid_slices)\n    }\n\n    assert position_info.shape[0] == spikes.shape[0]\n\n    sort_group_ids = np.asarray(\n        [int(col.split(\"_\")[0]) for col in spikes.columns]\n    )\n\n    return (\n        position_info,\n        spikes,\n        valid_slices,\n        environment_labels,\n        sort_group_ids,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/visualization/", "title": "visualization.py", "text": ""}, {"location": "api/src/spyglass/decoding/visualization_1D_view/", "title": "visualization_1D_view.py", "text": ""}, {"location": "api/src/spyglass/decoding/visualization_1D_view/#src.spyglass.decoding.visualization_1D_view.create_1D_decode_view", "title": "<code>create_1D_decode_view(posterior, linear_position=None, ref_time_sec=None)</code>", "text": "<p>Creates a view of an interactive heatmap of position vs. time.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>xr.DataArray, shape(n_time, n_position_bins)</code> required <code>linear_position</code> <code>np.ndarray, shape(n_time)</code> <code>None</code> <code>ref_time_sec</code> <code>np.float64</code> <p>Reference time for the purpose of offsetting the start time</p> <code>None</code> <p>Returns:</p> Name Type Description <code>view</code> <code>vvf.DecodedLinearPositionData</code> Source code in <code>src/spyglass/decoding/visualization_1D_view.py</code> <pre><code>def create_1D_decode_view(\n    posterior: xr.DataArray,\n    linear_position: np.ndarray = None,\n    ref_time_sec: Union[np.float64, None] = None,\n) -&gt; vvf.DecodedLinearPositionData:\n\"\"\"Creates a view of an interactive heatmap of position vs. time.\n\n    Parameters\n    ----------\n    posterior : xr.DataArray, shape (n_time, n_position_bins)\n    linear_position : np.ndarray, shape (n_time, ), optional\n    ref_time_sec : np.float64, optional\n        Reference time for the purpose of offsetting the start time\n\n    Returns\n    -------\n    view : vvf.DecodedLinearPositionData\n\n    \"\"\"\n    if linear_position is not None:\n        linear_position = np.asarray(linear_position).squeeze()\n\n    trimmed_posterior = discretize_and_trim(posterior)\n    observations_per_time = get_observations_per_time(\n        trimmed_posterior, posterior\n    )\n    sampling_freq = get_sampling_freq(posterior.time)\n    start_time_sec = posterior.time.values[0]\n    if ref_time_sec is not None:\n        start_time_sec = start_time_sec - ref_time_sec\n\n    trimmed_bin_center_index = get_trimmed_bin_center_index(\n        posterior.position.values, trimmed_posterior.position.values\n    )\n\n    return vvf.DecodedLinearPositionData(\n        values=trimmed_posterior.values,\n        positions=trimmed_bin_center_index,\n        frame_bounds=observations_per_time,\n        positions_key=posterior.position.values.astype(np.float32),\n        observed_positions=linear_position,\n        start_time_sec=start_time_sec,\n        sampling_frequency=sampling_freq,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/decoding/visualization_2D_view/", "title": "visualization_2D_view.py", "text": ""}, {"location": "api/src/spyglass/decoding/visualization_2D_view/#src.spyglass.decoding.visualization_2D_view.create_2D_decode_view", "title": "<code>create_2D_decode_view(position_time, position, posterior, bin_size, head_dir=None)</code>", "text": "<p>Creates a 2D decoding movie view</p> <p>Parameters:</p> Name Type Description Default <code>position_time</code> <code>np.ndarray, shape(n_time)</code> required <code>position</code> <code>np.ndarray, shape(n_time, 2)</code> required <code>posterior</code> <code>xr.DataArray, shape(n_time, n_position_bins)</code> required <code>bin_size</code> <code>float</code> required <code>head_dir</code> <code>np.ndarray</code> <code>None</code> <p>Returns:</p> Name Type Description <code>view</code> <code>vvf.TrackPositionAnimationV1</code> Source code in <code>src/spyglass/decoding/visualization_2D_view.py</code> <pre><code>def create_2D_decode_view(\n    position_time: np.ndarray,\n    position: np.ndarray,\n    posterior: xr.DataArray,\n    bin_size: float,\n    head_dir: np.ndarray = None,\n) -&gt; vvf.TrackPositionAnimationV1:\n\"\"\"Creates a 2D decoding movie view\n\n    Parameters\n    ----------\n    position_time : np.ndarray, shape (n_time,)\n    position : np.ndarray, shape (n_time, 2)\n    posterior : xr.DataArray, shape (n_time, n_position_bins)\n    bin_size : float\n    head_dir : np.ndarray, optional\n\n    Returns\n    -------\n    view : vvf.TrackPositionAnimationV1\n\n    \"\"\"\n    assert (\n        position_time.shape[0] == position.shape[0]\n    ), \"position_time and position must have the same length\"\n    assert (\n        posterior.shape[0] == position.shape[0]\n    ), \"posterior and position must have the same length\"\n\n    position_time = np.squeeze(np.asarray(position_time)).copy()\n    position = np.asarray(position)\n    if head_dir is not None:\n        head_dir = np.squeeze(np.asarray(head_dir))\n\n    track_width, track_height, upper_left_points = make_track(\n        position, bin_size=bin_size\n    )\n\n    data = create_static_track_animation(\n        ul_corners=upper_left_points,\n        track_rect_height=track_height,\n        track_rect_width=track_width,\n        timestamps=position_time,\n        positions=position.T,\n        head_dir=head_dir,\n        compute_real_time_rate=True,\n    )\n    data[\"decodedData\"] = process_decoded_data(posterior)\n\n    return create_track_animation_object(static_track_animation=data)\n</code></pre>"}, {"location": "api/src/spyglass/figurl_views/SpikeSortingRecordingView/", "title": "SpikeSortingRecordingView.py", "text": ""}, {"location": "api/src/spyglass/figurl_views/SpikeSortingRecordingView/#src.spyglass.figurl_views.SpikeSortingRecordingView.SpikeSortingRecording", "title": "<code>SpikeSortingRecording</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>@schema\nclass SpikeSortingRecording(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingRecordingSelection\n    ---\n    recording_path: varchar(1000)\n    -&gt; IntervalList.proj(sort_interval_list_name='interval_list_name')\n    \"\"\"\n\n    def make(self, key):\n        sort_interval_valid_times = self._get_sort_interval_valid_times(key)\n        recording = self._get_filtered_recording(key)\n        recording_name = self._get_recording_name(key)\n\n        tmp_key = {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": recording_name,\n            \"valid_times\": sort_interval_valid_times,\n        }\n        IntervalList.insert1(tmp_key, replace=True)\n\n        # store the list of valid times for the sort\n        key[\"sort_interval_list_name\"] = tmp_key[\"interval_list_name\"]\n\n        # Path to files that will hold the recording extractors\n        recording_folder = Path(os.getenv(\"SPYGLASS_RECORDING_DIR\"))\n        key[\"recording_path\"] = str(recording_folder / Path(recording_name))\n        if os.path.exists(key[\"recording_path\"]):\n            shutil.rmtree(key[\"recording_path\"])\n        recording = recording.save(\n            folder=key[\"recording_path\"], chunk_duration=\"10000ms\", n_jobs=8\n        )\n\n        self.insert1(key)\n\n    @staticmethod\n    def _get_recording_name(key):\n        recording_name = (\n            key[\"nwb_file_name\"]\n            + \"_\"\n            + key[\"sort_interval_name\"]\n            + \"_\"\n            + str(key[\"sort_group_id\"])\n            + \"_\"\n            + key[\"preproc_params_name\"]\n        )\n        return recording_name\n\n    @staticmethod\n    def _get_recording_timestamps(recording):\n        if recording.get_num_segments() &gt; 1:\n            frames_per_segment = [0]\n            for i in range(recording.get_num_segments()):\n                frames_per_segment.append(\n                    recording.get_num_frames(segment_index=i)\n                )\n\n            cumsum_frames = np.cumsum(frames_per_segment)\n            total_frames = np.sum(frames_per_segment)\n\n            timestamps = np.zeros((total_frames,))\n            for i in range(recording.get_num_segments()):\n                timestamps[\n                    cumsum_frames[i] : cumsum_frames[i + 1]\n                ] = recording.get_times(segment_index=i)\n        else:\n            timestamps = recording.get_times()\n        return timestamps\n\n    def _get_sort_interval_valid_times(self, key):\n\"\"\"Identifies the intersection between sort interval specified by the user\n        and the valid times (times for which neural data exist)\n\n        Parameters\n        ----------\n        key: dict\n            specifies a (partially filled) entry of SpikeSorting table\n\n        Returns\n        -------\n        sort_interval_valid_times: ndarray of tuples\n            (start, end) times for valid stretches of the sorting interval\n\n        \"\"\"\n        sort_interval = (\n            SortInterval\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_interval_name\": key[\"sort_interval_name\"],\n            }\n        ).fetch1(\"sort_interval\")\n        interval_list_name = (SpikeSortingRecordingSelection &amp; key).fetch1(\n            \"interval_list_name\"\n        )\n        valid_interval_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n        valid_sort_times = interval_list_intersect(\n            sort_interval, valid_interval_times\n        )\n        # Exclude intervals shorter than specified length\n        params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        if \"min_segment_length\" in params:\n            valid_sort_times = intervals_by_length(\n                valid_sort_times, min_length=params[\"min_segment_length\"]\n            )\n        return valid_sort_times\n\n    def _get_filtered_recording(self, key: dict):\n\"\"\"Filters and references a recording\n        * Loads the NWB file created during insertion as a spikeinterface Recording\n        * Slices recording in time (interval) and space (channels);\n          recording chunks from disjoint intervals are concatenated\n        * Applies referencing and bandpass filtering\n\n        Parameters\n        ----------\n        key: dict,\n            primary key of SpikeSortingRecording table\n\n        Returns\n        -------\n        recording: si.Recording\n        \"\"\"\n\n        nwb_file_abs_path = Nwbfile().get_abs_path(key[\"nwb_file_name\"])\n        recording = se.read_nwb_recording(\n            nwb_file_abs_path, load_time_vector=True\n        )\n\n        valid_sort_times = self._get_sort_interval_valid_times(key)\n        # shape is (N, 2)\n        valid_sort_times_indices = np.array(\n            [\n                np.searchsorted(recording.get_times(), interval)\n                for interval in valid_sort_times\n            ]\n        )\n        # join intervals of indices that are adjacent\n        valid_sort_times_indices = reduce(\n            union_adjacent_index, valid_sort_times_indices\n        )\n        if valid_sort_times_indices.ndim == 1:\n            valid_sort_times_indices = np.expand_dims(\n                valid_sort_times_indices, 0\n            )\n\n        # create an AppendRecording if there is more than one disjoint sort interval\n        if len(valid_sort_times_indices) &gt; 1:\n            recordings_list = []\n            for interval_indices in valid_sort_times_indices:\n                recording_single = recording.frame_slice(\n                    start_frame=interval_indices[0],\n                    end_frame=interval_indices[1],\n                )\n                recordings_list.append(recording_single)\n            recording = si.append_recordings(recordings_list)\n        else:\n            recording = recording.frame_slice(\n                start_frame=valid_sort_times_indices[0][0],\n                end_frame=valid_sort_times_indices[0][1],\n            )\n\n        channel_ids = (\n            SortGroup.SortGroupElectrode\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch(\"electrode_id\")\n        ref_channel_id = (\n            SortGroup\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch1(\"sort_reference_electrode_id\")\n        channel_ids = np.setdiff1d(channel_ids, ref_channel_id)\n\n        # include ref channel in first slice, then exclude it in second slice\n        if ref_channel_id &gt;= 0:\n            channel_ids_ref = np.append(channel_ids, ref_channel_id)\n            recording = recording.channel_slice(channel_ids=channel_ids_ref)\n\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"single\", ref_channel_ids=ref_channel_id\n            )\n            recording = recording.channel_slice(channel_ids=channel_ids)\n        elif ref_channel_id == -2:\n            recording = recording.channel_slice(channel_ids=channel_ids)\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"global\", operator=\"median\"\n            )\n        else:\n            raise ValueError(\"Invalid reference channel ID\")\n        filter_params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        recording = si.preprocessing.bandpass_filter(\n            recording,\n            freq_min=filter_params[\"frequency_min\"],\n            freq_max=filter_params[\"frequency_max\"],\n        )\n\n        # if the sort group is a tetrode, change the channel location\n        # note that this is a workaround that would be deprecated when spikeinterface uses 3D probe locations\n        probe_type = []\n        electrode_group = []\n        for channel_id in channel_ids:\n            probe_type.append(\n                (\n                    Electrode * Probe\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"probe_type\")\n            )\n            electrode_group.append(\n                (\n                    Electrode\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"electrode_group_name\")\n            )\n        if (\n            all(p == \"tetrode_12.5\" for p in probe_type)\n            and len(probe_type) == 4\n            and all(eg == electrode_group[0] for eg in electrode_group)\n        ):\n            tetrode = pi.Probe(ndim=2)\n            position = [[0, 0], [0, 12.5], [12.5, 0], [12.5, 12.5]]\n            tetrode.set_contacts(\n                position, shapes=\"circle\", shape_params={\"radius\": 6.25}\n            )\n            tetrode.set_contact_ids(channel_ids)\n            tetrode.set_device_channel_indices(np.arange(4))\n            recording = recording.set_probe(tetrode, in_place=True)\n\n        return recording\n</code></pre>"}, {"location": "api/src/spyglass/figurl_views/SpikeSortingView/", "title": "SpikeSortingView.py", "text": ""}, {"location": "api/src/spyglass/figurl_views/prepare_spikesortingview_data/", "title": "prepare_spikesortingview_data.py", "text": ""}, {"location": "api/src/spyglass/lfp/lfp_merge/", "title": "lfp_merge.py", "text": ""}, {"location": "api/src/spyglass/lfp/lfp_merge/#src.spyglass.lfp.lfp_merge.LFPOutput", "title": "<code>LFPOutput</code>", "text": "<p>         Bases: <code>Merge</code></p> Source code in <code>src/spyglass/lfp/lfp_merge.py</code> <pre><code>@schema\nclass LFPOutput(Merge):\n    definition = \"\"\"\n    merge_id: uuid\n    ---\n    source: varchar(32)\n    \"\"\"\n\n    class LFPV1(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        ---\n        -&gt; LFPV1\n        \"\"\"\n\n    class ImportedLFPV1(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        ---\n        -&gt; ImportedLFPV1\n        \"\"\"\n\n    class CommonLFP(dj.Part):\n\"\"\"Table to pass-through legacy LFP\"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        ---\n        -&gt; CommonLFP\n        \"\"\"\n\n    def fetch1_dataframe(self, *attrs, **kwargs):\n        nwb_lfp = self.fetch_nwb()[0]\n        return pd.DataFrame(\n            nwb_lfp[\"lfp\"].data,\n            index=pd.Index(nwb_lfp[\"lfp\"].timestamps, name=\"time\"),\n        )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/lfp_merge/#src.spyglass.lfp.lfp_merge.LFPOutput.CommonLFP", "title": "<code>CommonLFP</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Table to pass-through legacy LFP</p> Source code in <code>src/spyglass/lfp/lfp_merge.py</code> <pre><code>class CommonLFP(dj.Part):\n\"\"\"Table to pass-through legacy LFP\"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    ---\n    -&gt; CommonLFP\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp/", "title": "lfp.py", "text": ""}, {"location": "api/src/spyglass/lfp/v1/lfp/#src.spyglass.lfp.v1.lfp.LFPV1", "title": "<code>LFPV1</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>The filtered LFP data</p> Source code in <code>src/spyglass/lfp/v1/lfp.py</code> <pre><code>@schema\nclass LFPV1(dj.Computed):\n\"\"\"The filtered LFP data\"\"\"\n\n    definition = \"\"\"\n    -&gt; LFPSelection             # the user's selection of LFP data to be filtered\n    ---\n    -&gt; AnalysisNwbfile          # the name of the nwb file with the lfp data\n    -&gt; IntervalList             # the final interval list of valid times for the data\n    lfp_object_id: varchar(40)  # the NWB object ID for loading this object from the file\n    lfp_sampling_rate: float    # the sampling rate, in HZ\n    \"\"\"\n\n    def make(self, key):\n        DECIMATION_FACTOR = 1000\n        # get the NWB object with the data\n        nwbf_key = {\"nwb_file_name\": key[\"nwb_file_name\"]}\n        rawdata = (Raw &amp; nwbf_key).fetch_nwb()[0][\"raw\"]\n        sampling_rate, raw_interval_list_name = (Raw &amp; nwbf_key).fetch1(\n            \"sampling_rate\", \"interval_list_name\"\n        )\n        sampling_rate = int(np.round(sampling_rate))\n\n        # to get the list of valid times, we need to combine those from the user with those from the\n        # raw data\n        orig_key = copy.deepcopy(key)\n        orig_key[\"interval_list_name\"] = key[\"target_interval_list_name\"]\n        user_valid_times = (IntervalList() &amp; orig_key).fetch1(\"valid_times\")\n        # we remove the extra entry so we can insert this into the LFPOutput table.\n        del orig_key[\"interval_list_name\"]\n\n        raw_valid_times = (\n            IntervalList()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": raw_interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n        valid_times = interval_list_intersect(\n            user_valid_times,\n            raw_valid_times,\n            min_length=MIN_LFP_INTERVAL_DURATION,\n        )\n        print(\n            f\"LFP: found {len(valid_times)} intervals &gt; {MIN_LFP_INTERVAL_DURATION} sec long.\"\n        )\n\n        # target 1 KHz sampling rate\n        decimation = sampling_rate // DECIMATION_FACTOR\n\n        # get the LFP filter that matches the raw data\n        filter = (\n            FirFilterParameters()\n            &amp; {\"filter_name\": key[\"filter_name\"]}\n            &amp; {\"filter_sampling_rate\": sampling_rate}\n        ).fetch(as_dict=True)[0]\n\n        # there should only be one filter that matches, so we take the first of the dictionaries\n        key[\"filter_name\"] = filter[\"filter_name\"]\n        key[\"filter_sampling_rate\"] = filter[\"filter_sampling_rate\"]\n\n        filter_coeff = filter[\"filter_coeff\"]\n        if len(filter_coeff) == 0:\n            print(\n                f\"Error in LFP: no filter found with data sampling rate of {sampling_rate}\"\n            )\n            return None\n        # get the list of selected LFP Channels from LFPElectrode\n        electrode_keys = (LFPElectrodeGroup.LFPElectrode &amp; key).fetch(\"KEY\")\n        electrode_id_list = list(k[\"electrode_id\"] for k in electrode_keys)\n        electrode_id_list.sort()\n\n        lfp_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n\n        lfp_file_abspath = AnalysisNwbfile().get_abs_path(lfp_file_name)\n        (\n            lfp_object_id,\n            timestamp_interval,\n        ) = FirFilterParameters().filter_data_nwb(\n            lfp_file_abspath,\n            rawdata,\n            filter_coeff,\n            valid_times,\n            electrode_id_list,\n            decimation,\n        )\n\n        # now that the LFP is filtered and in the file, add the file to the AnalysisNwbfile table\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], lfp_file_name)\n\n        key[\"analysis_file_name\"] = lfp_file_name\n        key[\"lfp_object_id\"] = lfp_object_id\n        key[\"lfp_sampling_rate\"] = sampling_rate // decimation\n\n        # finally, we need to censor the valid times to account for the downsampling\n        lfp_valid_times = interval_list_censor(valid_times, timestamp_interval)\n\n        # add an interval list for the LFP valid times, skipping duplicates\n        key[\"interval_list_name\"] = \"_\".join(\n            (\n                \"lfp\",\n                key[\"lfp_electrode_group_name\"],\n                key[\"target_interval_list_name\"],\n                \"valid times\",\n            )\n        )\n        IntervalList.insert1(\n            {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n                \"valid_times\": lfp_valid_times,\n            },\n            replace=True,\n        )\n        self.insert1(key)\n\n        # finally, we insert this into the LFP output table.\n        from spyglass.lfp.lfp_merge import LFPOutput\n\n        orig_key[\"analysis_file_name\"] = lfp_file_name\n        orig_key[\"lfp_object_id\"] = lfp_object_id\n        LFPOutput.insert1(orig_key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self, *attrs, **kwargs):\n        nwb_lfp = self.fetch_nwb()[0]\n        return pd.DataFrame(\n            nwb_lfp[\"lfp\"].data,\n            index=pd.Index(nwb_lfp[\"lfp\"].timestamps, name=\"time\"),\n        )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp/#src.spyglass.lfp.v1.lfp.LFPElectrodeGroup", "title": "<code>LFPElectrodeGroup</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/lfp/v1/lfp.py</code> <pre><code>@schema\nclass LFPElectrodeGroup(dj.Manual):\n    definition = \"\"\"\n     -&gt; Session                             # the session to which this LFP belongs\n     lfp_electrode_group_name: varchar(200) # the name of this group of electrodes\n     \"\"\"\n\n    class LFPElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; LFPElectrodeGroup # the group of electrodes to be filtered\n        -&gt; Electrode        # the electrode to be filtered\n        \"\"\"\n\n    @staticmethod\n    def create_lfp_electrode_group(\n        nwb_file_name: str, group_name: str, electrode_list: list[int]\n    ):\n\"\"\"Adds an LFPElectrodeGroup and the individual electrodes\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the nwb file (e.g. the session)\n        group_name : str\n            The name of this group (&lt; 200 char)\n        electrode_list : list\n            A list of the electrode ids to include in this group.\n        \"\"\"\n        # remove the session and then recreate the session and Electrode list\n        # check to see if the user allowed the deletion\n        key = {\n            \"nwb_file_name\": nwb_file_name,\n            \"lfp_electrode_group_name\": group_name,\n        }\n        LFPElectrodeGroup().insert1(key, skip_duplicates=True)\n\n        # TODO: do this in a better way\n        all_electrodes = (Electrode() &amp; {\"nwb_file_name\": nwb_file_name}).fetch(\n            as_dict=True\n        )\n        primary_key = Electrode.primary_key\n        for e in all_electrodes:\n            # create a dictionary so we can insert the electrodes\n            if e[\"electrode_id\"] in electrode_list:\n                lfpelectdict = {k: v for k, v in e.items() if k in primary_key}\n                lfpelectdict[\"lfp_electrode_group_name\"] = group_name\n                LFPElectrodeGroup().LFPElectrode.insert1(\n                    lfpelectdict, skip_duplicates=True\n                )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp/#src.spyglass.lfp.v1.lfp.LFPElectrodeGroup.create_lfp_electrode_group", "title": "<code>create_lfp_electrode_group(nwb_file_name, group_name, electrode_list)</code>  <code>staticmethod</code>", "text": "<p>Adds an LFPElectrodeGroup and the individual electrodes</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the nwb file (e.g. the session)</p> required <code>group_name</code> <code>str</code> <p>The name of this group (&lt; 200 char)</p> required <code>electrode_list</code> <code>list</code> <p>A list of the electrode ids to include in this group.</p> required Source code in <code>src/spyglass/lfp/v1/lfp.py</code> <pre><code>@staticmethod\ndef create_lfp_electrode_group(\n    nwb_file_name: str, group_name: str, electrode_list: list[int]\n):\n\"\"\"Adds an LFPElectrodeGroup and the individual electrodes\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the nwb file (e.g. the session)\n    group_name : str\n        The name of this group (&lt; 200 char)\n    electrode_list : list\n        A list of the electrode ids to include in this group.\n    \"\"\"\n    # remove the session and then recreate the session and Electrode list\n    # check to see if the user allowed the deletion\n    key = {\n        \"nwb_file_name\": nwb_file_name,\n        \"lfp_electrode_group_name\": group_name,\n    }\n    LFPElectrodeGroup().insert1(key, skip_duplicates=True)\n\n    # TODO: do this in a better way\n    all_electrodes = (Electrode() &amp; {\"nwb_file_name\": nwb_file_name}).fetch(\n        as_dict=True\n    )\n    primary_key = Electrode.primary_key\n    for e in all_electrodes:\n        # create a dictionary so we can insert the electrodes\n        if e[\"electrode_id\"] in electrode_list:\n            lfpelectdict = {k: v for k, v in e.items() if k in primary_key}\n            lfpelectdict[\"lfp_electrode_group_name\"] = group_name\n            LFPElectrodeGroup().LFPElectrode.insert1(\n                lfpelectdict, skip_duplicates=True\n            )\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp/#src.spyglass.lfp.v1.lfp.LFPSelection", "title": "<code>LFPSelection</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>The user's selection of LFP data to be filtered</p> <p>This table is used to select the LFP data to be filtered.  The user can select the LFP data by specifying the electrode group and the interval list to be used. The interval list is used to select the times from the raw data that will be filtered.  The user can also specify the filter to be used.</p> <p>The LFP data is filtered and downsampled to 1 KHz.  The filtered data is stored in the AnalysisNwbfile table.  The valid times for the filtered data are stored in the IntervalList table.</p> Source code in <code>src/spyglass/lfp/v1/lfp.py</code> <pre><code>@schema\nclass LFPSelection(dj.Manual):\n\"\"\"The user's selection of LFP data to be filtered\n\n    This table is used to select the LFP data to be filtered.  The user can select\n    the LFP data by specifying the electrode group and the interval list to be used.\n    The interval list is used to select the times from the raw data that will be\n    filtered.  The user can also specify the filter to be used.\n\n    The LFP data is filtered and downsampled to 1 KHz.  The filtered data is stored\n    in the AnalysisNwbfile table.  The valid times for the filtered data are stored\n    in the IntervalList table.\n    \"\"\"\n\n    definition = \"\"\"\n     -&gt; LFPElectrodeGroup                                                  # the group of electrodes to be filtered\n     -&gt; IntervalList.proj(target_interval_list_name='interval_list_name')  # the original set of times to be filtered\n     -&gt; FirFilterParameters                                                # the filter to be used\n     \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact/", "title": "lfp_artifact.py", "text": ""}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact/#src.spyglass.lfp.v1.lfp_artifact.LFPArtifactDetectionParameters", "title": "<code>LFPArtifactDetectionParameters</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/lfp/v1/lfp_artifact.py</code> <pre><code>@schema\nclass LFPArtifactDetectionParameters(dj.Manual):\n    definition = \"\"\"\n    # Parameters for detecting LFP artifact times within a LFP group.\n    artifact_params_name: varchar(200)\n    ---\n    artifact_params: blob  # dictionary of parameters\n    \"\"\"\n\n    def insert_default(self):\n\"\"\"Insert the default artifact parameters with an appropriate parameter dict.\"\"\"\n        artifact_params = {\n            \"artifact_detection_algorithm\": \"difference\",\n            \"artifact_detection_algorithm_params\": {\n                \"amplitude_thresh_1st\": 500,  # must be None or &gt;= 0\n                \"proportion_above_thresh_1st\": 0.1,\n                \"amplitude_thresh_2nd\": 1000,  # must be None or &gt;= 0\n                \"proportion_above_thresh_2nd\": 0.05,\n                \"removal_window_ms\": 10.0,  # in milliseconds\n                \"local_window_ms\": 40.0,  # in milliseconds\n            },\n        }\n\n        self.insert1(\n            [\"default_difference\", artifact_params], skip_duplicates=True\n        )\n\n        artifact_params_none = {\n            \"artifact_detection_algorithm\": \"difference\",\n            \"artifact_detection_algorithm_params\": {\n                \"amplitude_thresh_1st\": None,  # must be None or &gt;= 0\n                \"proportion_above_thresh_1st\": None,\n                \"amplitude_thresh_2nd\": None,  # must be None or &gt;= 0\n                \"proportion_above_thresh_2nd\": None,\n                \"removal_window_ms\": None,  # in milliseconds\n                \"local_window_ms\": None,  # in milliseconds\n            },\n        }\n        self.insert1([\"none\", artifact_params_none], skip_duplicates=True)\n\n        artifact_params_mad = {\n            \"artifact_detection_algorithm\": \"mad\",\n            \"artifact_detection_algorithm_params\": {\n                \"mad_thresh\": 6.0,  # akin to z-score standard deviations if the distribution is normal\n                \"proportion_above_thresh\": 0.1,\n                \"removal_window_ms\": 10.0,  # in milliseconds\n            },\n        }\n        self.insert1([\"default_mad\", artifact_params_mad], skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact/#src.spyglass.lfp.v1.lfp_artifact.LFPArtifactDetectionParameters.insert_default", "title": "<code>insert_default()</code>", "text": "<p>Insert the default artifact parameters with an appropriate parameter dict.</p> Source code in <code>src/spyglass/lfp/v1/lfp_artifact.py</code> <pre><code>def insert_default(self):\n\"\"\"Insert the default artifact parameters with an appropriate parameter dict.\"\"\"\n    artifact_params = {\n        \"artifact_detection_algorithm\": \"difference\",\n        \"artifact_detection_algorithm_params\": {\n            \"amplitude_thresh_1st\": 500,  # must be None or &gt;= 0\n            \"proportion_above_thresh_1st\": 0.1,\n            \"amplitude_thresh_2nd\": 1000,  # must be None or &gt;= 0\n            \"proportion_above_thresh_2nd\": 0.05,\n            \"removal_window_ms\": 10.0,  # in milliseconds\n            \"local_window_ms\": 40.0,  # in milliseconds\n        },\n    }\n\n    self.insert1(\n        [\"default_difference\", artifact_params], skip_duplicates=True\n    )\n\n    artifact_params_none = {\n        \"artifact_detection_algorithm\": \"difference\",\n        \"artifact_detection_algorithm_params\": {\n            \"amplitude_thresh_1st\": None,  # must be None or &gt;= 0\n            \"proportion_above_thresh_1st\": None,\n            \"amplitude_thresh_2nd\": None,  # must be None or &gt;= 0\n            \"proportion_above_thresh_2nd\": None,\n            \"removal_window_ms\": None,  # in milliseconds\n            \"local_window_ms\": None,  # in milliseconds\n        },\n    }\n    self.insert1([\"none\", artifact_params_none], skip_duplicates=True)\n\n    artifact_params_mad = {\n        \"artifact_detection_algorithm\": \"mad\",\n        \"artifact_detection_algorithm_params\": {\n            \"mad_thresh\": 6.0,  # akin to z-score standard deviations if the distribution is normal\n            \"proportion_above_thresh\": 0.1,\n            \"removal_window_ms\": 10.0,  # in milliseconds\n        },\n    }\n    self.insert1([\"default_mad\", artifact_params_mad], skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact_MAD_detection/", "title": "lfp_artifact_MAD_detection.py", "text": ""}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact_MAD_detection/#src.spyglass.lfp.v1.lfp_artifact_MAD_detection.mad_artifact_detector", "title": "<code>mad_artifact_detector(recording, mad_thresh=6.0, proportion_above_thresh=0.1, removal_window_ms=10.0, sampling_frequency=1000.0)</code>", "text": "<p>Detect LFP artifacts using the median absolute deviation method.</p> <p>Parameters:</p> Name Type Description Default <code>recording</code> <code>RecordingExtractor</code> <p>The recording extractor object</p> required <code>mad_thresh</code> <code>float</code> <p>Threshold on the median absolute deviation scaled LFPs, defaults to 6.0</p> <code>6.0</code> <code>proportion_above_thresh</code> <code>float</code> <p>Proportion of electrodes that need to be above the threshold, defaults to 1.0</p> <code>0.1</code> <code>removal_window_ms</code> <code>float</code> <p>Width of the window in milliseconds to mask out per artifact (window/2 removed on each side of threshold crossing), defaults to 1 ms</p> <code>10.0</code> <code>sampling_frequency</code> <code>float</code> <p>Sampling frequency of the recording extractor, defaults to 1000.0</p> <code>1000.0</code> <p>Returns:</p> Name Type Description <code>artifact_removed_valid_times</code> <code>np.ndarray</code> <p>Intervals of valid times where artifacts were not detected, unit: seconds</p> <code>artifact_intervals</code> <code>np.ndarray</code> <p>Intervals in which artifacts are detected (including removal windows), unit: seconds</p> Source code in <code>src/spyglass/lfp/v1/lfp_artifact_MAD_detection.py</code> <pre><code>def mad_artifact_detector(\n    recording: None,\n    mad_thresh: float = 6.0,\n    proportion_above_thresh: float = 0.1,\n    removal_window_ms: float = 10.0,\n    sampling_frequency: float = 1000.0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n\"\"\"Detect LFP artifacts using the median absolute deviation method.\n\n    Parameters\n    ----------\n    recording : RecordingExtractor\n        The recording extractor object\n    mad_thresh : float, optional\n        Threshold on the median absolute deviation scaled LFPs, defaults to 6.0\n    proportion_above_thresh : float, optional\n        Proportion of electrodes that need to be above the threshold, defaults to 1.0\n    removal_window_ms : float, optional\n        Width of the window in milliseconds to mask out per artifact\n        (window/2 removed on each side of threshold crossing), defaults to 1 ms\n    sampling_frequency : float, optional\n        Sampling frequency of the recording extractor, defaults to 1000.0\n\n    Returns\n    -------\n    artifact_removed_valid_times : np.ndarray\n        Intervals of valid times where artifacts were not detected, unit: seconds\n    artifact_intervals : np.ndarray\n        Intervals in which artifacts are detected (including removal windows), unit: seconds\n\n    \"\"\"\n\n    timestamps = np.asarray(recording.timestamps)\n    lfps = np.asarray(recording.data)\n\n    mad = median_abs_deviation(lfps, axis=0, nan_policy=\"omit\", scale=\"normal\")\n    is_artifact = _is_above_proportion_thresh(\n        _mad_scale_lfps(lfps, mad), mad_thresh, proportion_above_thresh\n    )\n\n    MILLISECONDS_PER_SECOND = 1000.0\n    half_removal_window_s = (removal_window_ms / MILLISECONDS_PER_SECOND) * 0.5\n    half_removal_window_idx = int(half_removal_window_s * sampling_frequency)\n    is_artifact = _extend_array_by_window(is_artifact, half_removal_window_idx)\n\n    artifact_intervals_s = _get_time_intervals_from_bool_array(\n        is_artifact, timestamps\n    )\n\n    valid_times = _get_time_intervals_from_bool_array(~is_artifact, timestamps)\n\n    return valid_times, artifact_intervals_s\n</code></pre>"}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact_difference_detection/", "title": "lfp_artifact_difference_detection.py", "text": ""}, {"location": "api/src/spyglass/lfp/v1/lfp_artifact_difference_detection/#src.spyglass.lfp.v1.lfp_artifact_difference_detection.difference_artifact_detector", "title": "<code>difference_artifact_detector(recording, amplitude_thresh_1st=None, amplitude_thresh_2nd=None, proportion_above_thresh_1st=1.0, proportion_above_thresh_2nd=1.0, removal_window_ms=1.0, local_window_ms=1.0, sampling_frequency=1000.0)</code>", "text": "<p>Detects times during which artifacts do and do not occur.</p> <p>Artifacts are defined as periods where the absolute value of the change in LFP exceeds amplitude change thresholds on the proportion of channels specified, with the period extended by the removal_window_ms/2 on each side. amplitude change threshold values of None are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>recording</code> <code>lfp eseries</code> required <code>zscore_thresh</code> <code>float</code> <p>Stdev threshold for exclusion, should be &gt;=0, defaults to None</p> required <code>amplitude_thresh</code> <code>float</code> <p>Amplitude (ad units) threshold for exclusion, should be &gt;=0, defaults to None</p> required <code>proportion_above_thresh</code> <code>float, optional, should be&gt;0 and &lt;=1</code> <p>Proportion of electrodes that need to have threshold crossings, defaults to 1</p> required <code>removal_window_ms</code> <code>float</code> <p>Width of the window in milliseconds to mask out per artifact (window/2 removed on each side of threshold crossing), defaults to 1 ms</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>artifact_removed_valid_times</code> <code>np.ndarray</code> <p>Intervals of valid times where artifacts were not detected, unit: seconds</p> <code>artifact_intervals</code> <code>np.ndarray</code> <p>Intervals in which artifacts are detected (including removal windows), unit: seconds</p> Source code in <code>src/spyglass/lfp/v1/lfp_artifact_difference_detection.py</code> <pre><code>def difference_artifact_detector(\n    recording: None,\n    amplitude_thresh_1st: Union[float, None] = None,\n    amplitude_thresh_2nd: Union[float, None] = None,\n    proportion_above_thresh_1st: float = 1.0,\n    proportion_above_thresh_2nd: float = 1.0,\n    removal_window_ms: float = 1.0,\n    local_window_ms: float = 1.0,\n    sampling_frequency: float = 1000.0,\n):\n\"\"\"Detects times during which artifacts do and do not occur.\n\n    Artifacts are defined as periods where the absolute value of the change in LFP exceeds\n    amplitude change thresholds on the proportion of channels specified,\n    with the period extended by the removal_window_ms/2 on each side. amplitude change\n    threshold values of None are ignored.\n\n    Parameters\n    ----------\n    recording : lfp eseries\n    zscore_thresh : float, optional\n        Stdev threshold for exclusion, should be &gt;=0, defaults to None\n    amplitude_thresh : float, optional\n        Amplitude (ad units) threshold for exclusion, should be &gt;=0, defaults to None\n    proportion_above_thresh : float, optional, should be&gt;0 and &lt;=1\n        Proportion of electrodes that need to have threshold crossings, defaults to 1\n    removal_window_ms : float, optional\n        Width of the window in milliseconds to mask out per artifact\n        (window/2 removed on each side of threshold crossing), defaults to 1 ms\n\n    Returns\n    -------\n    artifact_removed_valid_times : np.ndarray\n        Intervals of valid times where artifacts were not detected, unit: seconds\n    artifact_intervals : np.ndarray\n        Intervals in which artifacts are detected (including removal windows), unit: seconds\n    \"\"\"\n\n    valid_timestamps = recording.timestamps\n\n    local_window = int(local_window_ms / 2)\n\n    # if both thresholds are None, we skip artifact detection\n    if amplitude_thresh_1st is None:\n        recording_interval = np.asarray(\n            [valid_timestamps[0], valid_timestamps[-1]]\n        )\n        artifact_times_empty = np.asarray([])\n        print(\"Amplitude threshold is None, skipping artifact detection\")\n        return recording_interval, artifact_times_empty\n\n    # verify threshold parameters\n    (\n        amplitude_thresh_1st,\n        amplitude_thresh_2nd,\n        proportion_above_thresh_1st,\n        proportion_above_thresh_2nd,\n    ) = _check_artifact_thresholds(\n        amplitude_thresh_1st,\n        amplitude_thresh_2nd,\n        proportion_above_thresh_1st,\n        proportion_above_thresh_2nd,\n    )\n\n    # want to detect frames without parallel processing\n    # compute the number of electrodes that have to be above threshold\n    nelect_above_1st = np.ceil(\n        proportion_above_thresh_1st * recording.data.shape[1]\n    )\n    nelect_above_2nd = np.ceil(\n        proportion_above_thresh_2nd * recording.data.shape[1]\n    )\n\n    # find the artifact occurrences using one or both thresholds, across channels\n    # replace with LFP artifact code\n    # is this supposed to be indices??\n    if amplitude_thresh_1st is not None:\n        # first find times with large amp change\n        artifact_boolean = np.sum(\n            (np.abs(np.diff(recording.data, axis=0)) &gt; amplitude_thresh_1st),\n            axis=1,\n        )\n        above_thresh_1st = np.where(artifact_boolean &gt;= nelect_above_1st)[0]\n\n        # second, find artifacts with large baseline change\n        big_artifacts = np.zeros(\n            (recording.data.shape[1], above_thresh_1st.shape[0])\n        )\n        for art_count in np.arange(above_thresh_1st.shape[0]):\n            if above_thresh_1st[art_count] &lt;= local_window:\n                local_min = local_max = above_thresh_1st[art_count]\n            else:\n                local_max = np.max(\n                    recording.data[\n                        above_thresh_1st[art_count]\n                        - local_window : above_thresh_1st[art_count]\n                        + local_window,\n                        :,\n                    ],\n                    axis=0,\n                )\n                local_min = np.min(\n                    recording.data[\n                        above_thresh_1st[art_count]\n                        - local_window : above_thresh_1st[art_count]\n                        + local_window,\n                        :,\n                    ],\n                    axis=0,\n                )\n            big_artifacts[:, art_count] = (\n                np.abs(local_max - local_min) &gt; amplitude_thresh_2nd\n            )\n\n        # sum columns in big artficat, then compare to nelect_above_2nd\n        above_thresh = above_thresh_1st[\n            np.sum(big_artifacts, axis=0) &gt;= nelect_above_2nd\n        ]\n\n    artifact_frames = above_thresh.copy()\n    print(\"detected \", artifact_frames.shape[0], \" artifacts\")\n\n    # turn ms to remove total into s to remove from either side of each detected artifact\n    half_removal_window_s = removal_window_ms / 1000 * 0.5\n\n    if len(artifact_frames) == 0:\n        recording_interval = np.asarray(\n            [[valid_timestamps[0], valid_timestamps[-1]]]\n        )\n        artifact_times_empty = np.asarray([])\n        print(\"No artifacts detected.\")\n        return recording_interval, artifact_times_empty\n\n    artifact_intervals = interval_from_inds(artifact_frames)\n\n    artifact_intervals_s = np.zeros(\n        (len(artifact_intervals), 2), dtype=np.float64\n    )\n    for interval_idx, interval in enumerate(artifact_intervals):\n        artifact_intervals_s[interval_idx] = [\n            valid_timestamps[interval[0]] - half_removal_window_s,\n            valid_timestamps[interval[1]] + half_removal_window_s,\n        ]\n    artifact_intervals_s = reduce(_union_concat, artifact_intervals_s)\n\n    valid_intervals = get_valid_intervals(\n        valid_timestamps, sampling_frequency, 1.5, 0.000001\n    )\n\n    # these are artifact times - need to subtract these from valid timestamps\n    artifact_valid_times = interval_list_intersect(\n        valid_intervals, artifact_intervals_s\n    )\n\n    # note: this is a slow step\n    list_triggers = []\n    for interval in artifact_valid_times:\n        list_triggers.append(\n            np.arange(\n                np.searchsorted(valid_timestamps, interval[0]),\n                np.searchsorted(valid_timestamps, interval[1]),\n            )\n        )\n\n    new_array = np.array(np.concatenate(list_triggers))\n\n    new_timestamps = np.delete(valid_timestamps, new_array)\n\n    artifact_removed_valid_times = get_valid_intervals(\n        new_timestamps, sampling_frequency, 1.5, 0.000001\n    )\n\n    return artifact_removed_valid_times, artifact_intervals_s\n</code></pre>"}, {"location": "api/src/spyglass/lfp_band/lfp_band_merge/", "title": "lfp_band_merge.py", "text": ""}, {"location": "api/src/spyglass/lfp_band/v1/lfp_band/", "title": "lfp_band.py", "text": ""}, {"location": "api/src/spyglass/lfp_band/v1/lfp_band/#src.spyglass.lfp_band.v1.lfp_band.LFPBandSelection", "title": "<code>LFPBandSelection</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>The user's selection of LFP data to be filtered in a given frequency band.</p> Source code in <code>src/spyglass/lfp_band/v1/lfp_band.py</code> <pre><code>@schema\nclass LFPBandSelection(dj.Manual):\n\"\"\"The user's selection of LFP data to be filtered in a given frequency band.\"\"\"\n\n    definition = \"\"\"\n    -&gt; LFPOutput.proj(lfp_merge_id='merge_id')                                # the LFP data to be filtered\n    -&gt; FirFilterParameters                                                # the filter to use for the data\n    -&gt; IntervalList.proj(target_interval_list_name='interval_list_name')  # the original set of times to be filtered\n    lfp_band_sampling_rate: int                                           # the sampling rate for this band\n    ---\n    min_interval_len = 1.0: float  # the minimum length of a valid interval to filter\n    \"\"\"\n\n    class LFPBandElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; LFPBandSelection # the LFP band selection\n        -&gt; LFPElectrodeGroup.LFPElectrode  # the LFP electrode to be filtered\n        reference_elect_id = -1: int  # the reference electrode to use; -1 for no reference\n        ---\n        \"\"\"\n\n    def set_lfp_band_electrodes(\n        self,\n        nwb_file_name: str,\n        lfp_merge_id: int,\n        electrode_list: list[int],\n        filter_name: str,\n        interval_list_name: str,\n        reference_electrode_list: list[int],\n        lfp_band_sampling_rate: int,\n    ):\n\"\"\"Sets the electrodes to be filtered for a given LFP\n\n        Parameters\n        ----------\n        nwb_file_name: str\n            The name of the NWB file containing the LFP data\n        merge_id: int\n            The uuid of the LFP data to be filtered\n        electrode_list: list\n            A list of the electrodes to be filtered\n        filter_name: str\n            The name of the filter to be used\n        interval_list_name: str\n            The name of the interval list to be used\n        reference_electrode_list: list\n            A list of the reference electrodes to be used\n        lfp_band_sampling_rate: int\n        \"\"\"\n        # Error checks on parameters\n        # electrode_list\n\n        lfp_key = {\"merge_id\": lfp_merge_id}\n        lfp_output_table = LFPOutput &amp; lfp_key\n        lfp_part_table = lfp_output_table.get_part_table()\n        print(lfp_part_table)\n\n        query = LFPElectrodeGroup().LFPElectrode() &amp; lfp_key\n        available_electrodes = query.fetch(\"electrode_id\")\n        if not np.all(np.isin(electrode_list, available_electrodes)):\n            raise ValueError(\n                \"All elements in electrode_list must be valid electrode_ids in the LFPElectodeGroup table\"\n            )\n        # sampling rate\n        lfp_sampling_rate = lfp_part_table.fetch1(\"filter_sampling_rate\")\n        decimation = lfp_sampling_rate // lfp_band_sampling_rate\n        if lfp_sampling_rate // decimation != lfp_band_sampling_rate:\n            raise ValueError(\n                f\"lfp_band_sampling rate {lfp_band_sampling_rate} is not an integer divisor of lfp \"\n                f\"samping rate {lfp_sampling_rate}\"\n            )\n        # filter\n        filter_query = FirFilterParameters() &amp; {\n            \"filter_name\": filter_name,\n            \"filter_sampling_rate\": lfp_sampling_rate,\n        }\n        if not filter_query:\n            raise ValueError(\n                f\"filter {filter_name}, sampling rate {lfp_sampling_rate} is not in the FirFilterParameters table\"\n            )\n        # interval_list\n        interval_query = IntervalList() &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"interval_name\": interval_list_name,\n        }\n        if not interval_query:\n            raise ValueError(\n                f\"interval list {interval_list_name} is not in the IntervalList table; the list must be \"\n                \"added before this function is called\"\n            )\n        # reference_electrode_list\n        if len(reference_electrode_list) != 1 and len(\n            reference_electrode_list\n        ) != len(electrode_list):\n            raise ValueError(\n                \"reference_electrode_list must contain either 1 or len(electrode_list) elements\"\n            )\n        # add a -1 element to the list to allow for the no reference option\n        available_electrodes = np.append(available_electrodes, [-1])\n        if not np.all(np.isin(reference_electrode_list, available_electrodes)):\n            raise ValueError(\n                \"All elements in reference_electrode_list must be valid electrode_ids in the LFPSelection \"\n                \"table\"\n            )\n\n        # make a list of all the references\n        ref_list = np.zeros((len(electrode_list),))\n        ref_list[:] = reference_electrode_list\n\n        key = dict(\n            nwb_file_name=nwb_file_name,\n            lfp_merge_id=lfp_merge_id,\n            filter_name=filter_name,\n            filter_sampling_rate=lfp_sampling_rate,\n            target_interval_list_name=interval_list_name,\n            lfp_band_sampling_rate=lfp_sampling_rate // decimation,\n        )\n        # insert an entry into the main LFPBandSelectionTable\n        self.insert1(key, skip_duplicates=True)\n\n        key[\"lfp_electrode_group_name\"] = lfp_part_table.fetch1(\n            \"lfp_electrode_group_name\"\n        )\n        # iterate through all of the new elements and add them\n        for e, r in zip(electrode_list, ref_list):\n            elect_key = (\n                LFPElectrodeGroup.LFPElectrode\n                &amp; {\n                    \"nwb_file_name\": nwb_file_name,\n                    \"lfp_electrode_group_name\": key[\"lfp_electrode_group_name\"],\n                    \"electrode_id\": e,\n                }\n            ).fetch1(\"KEY\")\n            for item in elect_key:\n                key[item] = elect_key[item]\n            query = Electrode &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"electrode_id\": e,\n            }\n            key[\"reference_elect_id\"] = r\n            self.LFPBandElectrode().insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/lfp_band/v1/lfp_band/#src.spyglass.lfp_band.v1.lfp_band.LFPBandSelection.set_lfp_band_electrodes", "title": "<code>set_lfp_band_electrodes(nwb_file_name, lfp_merge_id, electrode_list, filter_name, interval_list_name, reference_electrode_list, lfp_band_sampling_rate)</code>", "text": "<p>Sets the electrodes to be filtered for a given LFP</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file containing the LFP data</p> required <code>merge_id</code> <p>The uuid of the LFP data to be filtered</p> required <code>electrode_list</code> <code>list[int]</code> <p>A list of the electrodes to be filtered</p> required <code>filter_name</code> <code>str</code> <p>The name of the filter to be used</p> required <code>interval_list_name</code> <code>str</code> <p>The name of the interval list to be used</p> required <code>reference_electrode_list</code> <code>list[int]</code> <p>A list of the reference electrodes to be used</p> required <code>lfp_band_sampling_rate</code> <code>int</code> required Source code in <code>src/spyglass/lfp_band/v1/lfp_band.py</code> <pre><code>def set_lfp_band_electrodes(\n    self,\n    nwb_file_name: str,\n    lfp_merge_id: int,\n    electrode_list: list[int],\n    filter_name: str,\n    interval_list_name: str,\n    reference_electrode_list: list[int],\n    lfp_band_sampling_rate: int,\n):\n\"\"\"Sets the electrodes to be filtered for a given LFP\n\n    Parameters\n    ----------\n    nwb_file_name: str\n        The name of the NWB file containing the LFP data\n    merge_id: int\n        The uuid of the LFP data to be filtered\n    electrode_list: list\n        A list of the electrodes to be filtered\n    filter_name: str\n        The name of the filter to be used\n    interval_list_name: str\n        The name of the interval list to be used\n    reference_electrode_list: list\n        A list of the reference electrodes to be used\n    lfp_band_sampling_rate: int\n    \"\"\"\n    # Error checks on parameters\n    # electrode_list\n\n    lfp_key = {\"merge_id\": lfp_merge_id}\n    lfp_output_table = LFPOutput &amp; lfp_key\n    lfp_part_table = lfp_output_table.get_part_table()\n    print(lfp_part_table)\n\n    query = LFPElectrodeGroup().LFPElectrode() &amp; lfp_key\n    available_electrodes = query.fetch(\"electrode_id\")\n    if not np.all(np.isin(electrode_list, available_electrodes)):\n        raise ValueError(\n            \"All elements in electrode_list must be valid electrode_ids in the LFPElectodeGroup table\"\n        )\n    # sampling rate\n    lfp_sampling_rate = lfp_part_table.fetch1(\"filter_sampling_rate\")\n    decimation = lfp_sampling_rate // lfp_band_sampling_rate\n    if lfp_sampling_rate // decimation != lfp_band_sampling_rate:\n        raise ValueError(\n            f\"lfp_band_sampling rate {lfp_band_sampling_rate} is not an integer divisor of lfp \"\n            f\"samping rate {lfp_sampling_rate}\"\n        )\n    # filter\n    filter_query = FirFilterParameters() &amp; {\n        \"filter_name\": filter_name,\n        \"filter_sampling_rate\": lfp_sampling_rate,\n    }\n    if not filter_query:\n        raise ValueError(\n            f\"filter {filter_name}, sampling rate {lfp_sampling_rate} is not in the FirFilterParameters table\"\n        )\n    # interval_list\n    interval_query = IntervalList() &amp; {\n        \"nwb_file_name\": nwb_file_name,\n        \"interval_name\": interval_list_name,\n    }\n    if not interval_query:\n        raise ValueError(\n            f\"interval list {interval_list_name} is not in the IntervalList table; the list must be \"\n            \"added before this function is called\"\n        )\n    # reference_electrode_list\n    if len(reference_electrode_list) != 1 and len(\n        reference_electrode_list\n    ) != len(electrode_list):\n        raise ValueError(\n            \"reference_electrode_list must contain either 1 or len(electrode_list) elements\"\n        )\n    # add a -1 element to the list to allow for the no reference option\n    available_electrodes = np.append(available_electrodes, [-1])\n    if not np.all(np.isin(reference_electrode_list, available_electrodes)):\n        raise ValueError(\n            \"All elements in reference_electrode_list must be valid electrode_ids in the LFPSelection \"\n            \"table\"\n        )\n\n    # make a list of all the references\n    ref_list = np.zeros((len(electrode_list),))\n    ref_list[:] = reference_electrode_list\n\n    key = dict(\n        nwb_file_name=nwb_file_name,\n        lfp_merge_id=lfp_merge_id,\n        filter_name=filter_name,\n        filter_sampling_rate=lfp_sampling_rate,\n        target_interval_list_name=interval_list_name,\n        lfp_band_sampling_rate=lfp_sampling_rate // decimation,\n    )\n    # insert an entry into the main LFPBandSelectionTable\n    self.insert1(key, skip_duplicates=True)\n\n    key[\"lfp_electrode_group_name\"] = lfp_part_table.fetch1(\n        \"lfp_electrode_group_name\"\n    )\n    # iterate through all of the new elements and add them\n    for e, r in zip(electrode_list, ref_list):\n        elect_key = (\n            LFPElectrodeGroup.LFPElectrode\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"lfp_electrode_group_name\": key[\"lfp_electrode_group_name\"],\n                \"electrode_id\": e,\n            }\n        ).fetch1(\"KEY\")\n        for item in elect_key:\n            key[item] = elect_key[item]\n        query = Electrode &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"electrode_id\": e,\n        }\n        key[\"reference_elect_id\"] = r\n        self.LFPBandElectrode().insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/lfp_band/v1/lfp_band/#src.spyglass.lfp_band.v1.lfp_band.LFPBandV1", "title": "<code>LFPBandV1</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/lfp_band/v1/lfp_band.py</code> <pre><code>@schema\nclass LFPBandV1(dj.Computed):\n    definition = \"\"\"\n    -&gt; LFPBandSelection              # the LFP band selection\n    ---\n    -&gt; AnalysisNwbfile               # the name of the nwb file with the lfp data\n    -&gt; IntervalList                  # the final interval list of valid times for the data\n    lfp_band_object_id: varchar(40)  # the NWB object ID for loading this object from the file\n    \"\"\"\n\n    def make(self, key):\n        # get the NWB object with the lfp data; FIX: change to fetch with additional infrastructure\n        lfp_key = {\"merge_id\": key[\"lfp_merge_id\"]}\n        lfp_output_table = LFPOutput &amp; lfp_key\n        lfp_part_table = lfp_output_table.get_part_table()\n        lfp_object = lfp_output_table.fetch_nwb()[0][\"lfp\"]\n\n        # get the electrodes to be filtered and their references\n        lfp_band_elect_id, lfp_band_ref_id = (\n            LFPBandSelection().LFPBandElectrode() &amp; key\n        ).fetch(\"electrode_id\", \"reference_elect_id\")\n\n        # sort the electrodes to make sure they are in ascending order\n        lfp_band_elect_id = np.asarray(lfp_band_elect_id)\n        lfp_band_ref_id = np.asarray(lfp_band_ref_id)\n        lfp_sort_order = np.argsort(lfp_band_elect_id)\n        lfp_band_elect_id = lfp_band_elect_id[lfp_sort_order]\n        lfp_band_ref_id = lfp_band_ref_id[lfp_sort_order]\n\n        lfp_sampling_rate = lfp_part_table.fetch1(\"filter_sampling_rate\")\n        interval_list_name, lfp_band_sampling_rate = (\n            LFPBandSelection() &amp; key\n        ).fetch1(\"target_interval_list_name\", \"lfp_band_sampling_rate\")\n        valid_times = (\n            IntervalList()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n        # the valid_times for this interval may be slightly beyond the valid times for the lfp itself,\n        # so we have to intersect the two\n        lfp_interval_list = lfp_part_table.fetch1(\"target_interval_list_name\")\n        lfp_valid_times = (\n            IntervalList()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": lfp_interval_list,\n            }\n        ).fetch1(\"valid_times\")\n        min_length = (LFPBandSelection &amp; key).fetch1(\"min_interval_len\")\n        lfp_band_valid_times = interval_list_intersect(\n            valid_times, lfp_valid_times, min_length=min_length\n        )\n\n        filter_name, filter_sampling_rate, lfp_band_sampling_rate = (\n            LFPBandSelection() &amp; key\n        ).fetch1(\n            \"filter_name\", \"filter_sampling_rate\", \"lfp_band_sampling_rate\"\n        )\n\n        decimation = int(lfp_sampling_rate) // lfp_band_sampling_rate\n\n        # load in the timestamps\n        timestamps = np.asarray(lfp_object.timestamps)\n        # get the indices of the first timestamp and the last timestamp that are within the valid times\n        included_indices = interval_list_contains_ind(\n            lfp_band_valid_times, timestamps\n        )\n        # pad the indices by 1 on each side to avoid message in filter_data\n        if included_indices[0] &gt; 0:\n            included_indices[0] -= 1\n        if included_indices[-1] != len(timestamps) - 1:\n            included_indices[-1] += 1\n\n        timestamps = timestamps[included_indices[0] : included_indices[-1]]\n\n        # load all the data to speed filtering\n        lfp_data = np.asarray(\n            lfp_object.data[included_indices[0] : included_indices[-1], :],\n            dtype=type(lfp_object.data[0][0]),\n        )\n\n        # get the indices of the electrodes to be filtered and the references\n        lfp_band_elect_index = get_electrode_indices(\n            lfp_object, lfp_band_elect_id\n        )\n        lfp_band_ref_index = get_electrode_indices(lfp_object, lfp_band_ref_id)\n\n        # subtract off the references for the selected channels\n        for index, elect_index in enumerate(lfp_band_elect_index):\n            if lfp_band_ref_id[index] != -1:\n                lfp_data[:, elect_index] = (\n                    lfp_data[:, elect_index]\n                    - lfp_data[:, lfp_band_ref_index[index]]\n                )\n\n        # get the LFP filter that matches the raw data\n        filter = (\n            FirFilterParameters()\n            &amp; {\"filter_name\": filter_name}\n            &amp; {\"filter_sampling_rate\": filter_sampling_rate}\n        ).fetch(as_dict=True)\n        if len(filter) == 0:\n            raise ValueError(\n                f\"Filter {filter_name} and sampling_rate {lfp_band_sampling_rate} does not exit in the \"\n                \"FirFilterParameters table\"\n            )\n\n        filter_coeff = filter[0][\"filter_coeff\"]\n        if len(filter_coeff) == 0:\n            print(\n                f\"Error in LFPBand: no filter found with data sampling rate of {lfp_band_sampling_rate}\"\n            )\n            return None\n\n        # create the analysis nwb file to store the results.\n        lfp_band_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n        lfp_band_file_abspath = AnalysisNwbfile().get_abs_path(\n            lfp_band_file_name\n        )\n        # filter the data and write to an the nwb file\n        filtered_data, new_timestamps = FirFilterParameters().filter_data(\n            timestamps,\n            lfp_data,\n            filter_coeff,\n            lfp_band_valid_times,\n            lfp_band_elect_index,\n            decimation,\n        )\n\n        # now that the LFP is filtered, we create an electrical series for it and add it to the file\n        with pynwb.NWBHDF5IO(\n            path=lfp_band_file_abspath, mode=\"a\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the indices of the electrodes in the electrode table of the file to get the right values\n            elect_index = get_electrode_indices(nwbf, lfp_band_elect_id)\n            electrode_table_region = nwbf.create_electrode_table_region(\n                elect_index, \"filtered electrode table\"\n            )\n            eseries_name = \"filtered data\"\n            # TODO: use datatype of data\n            es = pynwb.ecephys.ElectricalSeries(\n                name=eseries_name,\n                data=filtered_data,\n                electrodes=electrode_table_region,\n                timestamps=new_timestamps,\n            )\n            lfp = pynwb.ecephys.LFP(electrical_series=es)\n            ecephys_module = nwbf.create_processing_module(\n                name=\"ecephys\",\n                description=f\"LFP data processed with {filter_name}\",\n            )\n            ecephys_module.add(lfp)\n            io.write(nwbf)\n            lfp_band_object_id = es.object_id\n        #\n        # add the file to the AnalysisNwbfile table\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], lfp_band_file_name)\n        key[\"analysis_file_name\"] = lfp_band_file_name\n        key[\"lfp_band_object_id\"] = lfp_band_object_id\n\n        # finally, we need to censor the valid times to account for the downsampling if this is the first time we've\n        # downsampled these data\n        key[\"interval_list_name\"] = (\n            interval_list_name\n            + \" lfp band \"\n            + str(lfp_band_sampling_rate)\n            + \"Hz\"\n        )\n        tmp_valid_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n            }\n        ).fetch(\"valid_times\")\n        if len(tmp_valid_times) == 0:\n            lfp_band_valid_times = interval_list_censor(\n                lfp_band_valid_times, new_timestamps\n            )\n            # add an interval list for the LFP valid times\n            IntervalList.insert1(\n                {\n                    \"nwb_file_name\": key[\"nwb_file_name\"],\n                    \"interval_list_name\": key[\"interval_list_name\"],\n                    \"valid_times\": lfp_band_valid_times,\n                }\n            )\n        else:\n            # check that the valid times are the same\n            assert np.isclose(\n                tmp_valid_times[0], lfp_band_valid_times\n            ).all(), (\n                \"previously saved lfp band times do not match current times\"\n            )\n\n        self.insert1(key)\n\n        from spyglass.lfp_band.lfp_band_merge import LFPBandOutput\n\n        lfp_band_output_key = {\n            \"merge_id\": uuid.uuid1(),\n            \"analysis_file_name\": lfp_band_file_name,\n            \"lfp_band_object_id\": lfp_band_object_id,\n        }\n        LFPBandOutput.insert1(lfp_band_output_key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self, *attrs, **kwargs):\n\"\"\"Fetches the filtered data as a dataframe\"\"\"\n        filtered_nwb = self.fetch_nwb()[0]\n        return pd.DataFrame(\n            filtered_nwb[\"filtered_data\"].data,\n            index=pd.Index(\n                filtered_nwb[\"filtered_data\"].timestamps, name=\"time\"\n            ),\n        )\n\n    def compute_analytic_signal(self, electrode_list: list[int], **kwargs):\n\"\"\"Computes the hilbert transform of a given LFPBand signal using scipy.signal.hilbert\n\n        Parameters\n        ----------\n        electrode_list: list[int]\n            A list of the electrodes to compute the hilbert transform of\n\n        Returns\n        -------\n        analytic_signal_df: pd.DataFrame\n            DataFrame containing hilbert transform of signal\n\n        Raises\n        ------\n        ValueError\n            If any electrodes passed to electrode_list are invalid for the dataset\n        \"\"\"\n\n        filtered_band = self.fetch_nwb()[0][\"filtered_data\"]\n        electrode_index = np.isin(\n            filtered_band.electrodes.data[:], electrode_list\n        )\n        if len(electrode_list) != np.sum(electrode_index):\n            raise ValueError(\n                \"Some of the electrodes specified in electrode_list are missing in the current LFPBand table.\"\n            )\n        analytic_signal_df = pd.DataFrame(\n            hilbert(filtered_band.data[:, electrode_index], axis=0),\n            index=pd.Index(filtered_band.timestamps, name=\"time\"),\n            columns=[f\"electrode {e}\" for e in electrode_list],\n        )\n        return analytic_signal_df\n\n    def compute_signal_phase(\n        self, electrode_list: list[int] = None, **kwargs\n    ) -&gt; pd.DataFrame:\n\"\"\"Computes the phase of a given LFPBand signals using the hilbert transform\n\n        Parameters\n        ----------\n        electrode_list : list[int], optional\n            A list of the electrodes to compute the phase of, by default None\n\n        Returns\n        -------\n        signal_phase_df : pd.DataFrame\n            DataFrame containing the phase of the signals\n        \"\"\"\n        if electrode_list is None:\n            electrode_list = []\n\n        analytic_signal_df = self.compute_analytic_signal(\n            electrode_list, **kwargs\n        )\n\n        return pd.DataFrame(\n            np.angle(analytic_signal_df) + np.pi,\n            columns=analytic_signal_df.columns,\n            index=analytic_signal_df.index,\n        )\n\n    def compute_signal_power(\n        self, electrode_list: list[int] = None, **kwargs\n    ) -&gt; pd.DataFrame:\n\"\"\"Computes the power of a given LFPBand signals using the hilbert transform\n\n        Parameters\n        ----------\n        electrode_list : list[int], optional\n            A list of the electrodes to compute the power of, by default None\n\n        Returns\n        -------\n        signal_power_df : pd.DataFrame\n            DataFrame containing the power of the signals\n        \"\"\"\n        if electrode_list is None:\n            electrode_list = []\n\n        analytic_signal_df = self.compute_analytic_signal(\n            electrode_list, **kwargs\n        )\n\n        return pd.DataFrame(\n            np.abs(analytic_signal_df) ** 2,\n            columns=analytic_signal_df.columns,\n            index=analytic_signal_df.index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/lfp_band/v1/lfp_band/#src.spyglass.lfp_band.v1.lfp_band.LFPBandV1.fetch1_dataframe", "title": "<code>fetch1_dataframe(*attrs, **kwargs)</code>", "text": "<p>Fetches the filtered data as a dataframe</p> Source code in <code>src/spyglass/lfp_band/v1/lfp_band.py</code> <pre><code>def fetch1_dataframe(self, *attrs, **kwargs):\n\"\"\"Fetches the filtered data as a dataframe\"\"\"\n    filtered_nwb = self.fetch_nwb()[0]\n    return pd.DataFrame(\n        filtered_nwb[\"filtered_data\"].data,\n        index=pd.Index(\n            filtered_nwb[\"filtered_data\"].timestamps, name=\"time\"\n        ),\n    )\n</code></pre>"}, {"location": "api/src/spyglass/lfp_band/v1/lfp_band/#src.spyglass.lfp_band.v1.lfp_band.LFPBandV1.compute_analytic_signal", "title": "<code>compute_analytic_signal(electrode_list, **kwargs)</code>", "text": "<p>Computes the hilbert transform of a given LFPBand signal using scipy.signal.hilbert</p> <p>Parameters:</p> Name Type Description Default <code>electrode_list</code> <code>list[int]</code> <p>A list of the electrodes to compute the hilbert transform of</p> required <p>Returns:</p> Name Type Description <code>analytic_signal_df</code> <code>pd.DataFrame</code> <p>DataFrame containing hilbert transform of signal</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any electrodes passed to electrode_list are invalid for the dataset</p> Source code in <code>src/spyglass/lfp_band/v1/lfp_band.py</code> <pre><code>def compute_analytic_signal(self, electrode_list: list[int], **kwargs):\n\"\"\"Computes the hilbert transform of a given LFPBand signal using scipy.signal.hilbert\n\n    Parameters\n    ----------\n    electrode_list: list[int]\n        A list of the electrodes to compute the hilbert transform of\n\n    Returns\n    -------\n    analytic_signal_df: pd.DataFrame\n        DataFrame containing hilbert transform of signal\n\n    Raises\n    ------\n    ValueError\n        If any electrodes passed to electrode_list are invalid for the dataset\n    \"\"\"\n\n    filtered_band = self.fetch_nwb()[0][\"filtered_data\"]\n    electrode_index = np.isin(\n        filtered_band.electrodes.data[:], electrode_list\n    )\n    if len(electrode_list) != np.sum(electrode_index):\n        raise ValueError(\n            \"Some of the electrodes specified in electrode_list are missing in the current LFPBand table.\"\n        )\n    analytic_signal_df = pd.DataFrame(\n        hilbert(filtered_band.data[:, electrode_index], axis=0),\n        index=pd.Index(filtered_band.timestamps, name=\"time\"),\n        columns=[f\"electrode {e}\" for e in electrode_list],\n    )\n    return analytic_signal_df\n</code></pre>"}, {"location": "api/src/spyglass/lfp_band/v1/lfp_band/#src.spyglass.lfp_band.v1.lfp_band.LFPBandV1.compute_signal_phase", "title": "<code>compute_signal_phase(electrode_list=None, **kwargs)</code>", "text": "<p>Computes the phase of a given LFPBand signals using the hilbert transform</p> <p>Parameters:</p> Name Type Description Default <code>electrode_list</code> <code>list[int]</code> <p>A list of the electrodes to compute the phase of, by default None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>signal_phase_df</code> <code>pd.DataFrame</code> <p>DataFrame containing the phase of the signals</p> Source code in <code>src/spyglass/lfp_band/v1/lfp_band.py</code> <pre><code>def compute_signal_phase(\n    self, electrode_list: list[int] = None, **kwargs\n) -&gt; pd.DataFrame:\n\"\"\"Computes the phase of a given LFPBand signals using the hilbert transform\n\n    Parameters\n    ----------\n    electrode_list : list[int], optional\n        A list of the electrodes to compute the phase of, by default None\n\n    Returns\n    -------\n    signal_phase_df : pd.DataFrame\n        DataFrame containing the phase of the signals\n    \"\"\"\n    if electrode_list is None:\n        electrode_list = []\n\n    analytic_signal_df = self.compute_analytic_signal(\n        electrode_list, **kwargs\n    )\n\n    return pd.DataFrame(\n        np.angle(analytic_signal_df) + np.pi,\n        columns=analytic_signal_df.columns,\n        index=analytic_signal_df.index,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/lfp_band/v1/lfp_band/#src.spyglass.lfp_band.v1.lfp_band.LFPBandV1.compute_signal_power", "title": "<code>compute_signal_power(electrode_list=None, **kwargs)</code>", "text": "<p>Computes the power of a given LFPBand signals using the hilbert transform</p> <p>Parameters:</p> Name Type Description Default <code>electrode_list</code> <code>list[int]</code> <p>A list of the electrodes to compute the power of, by default None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>signal_power_df</code> <code>pd.DataFrame</code> <p>DataFrame containing the power of the signals</p> Source code in <code>src/spyglass/lfp_band/v1/lfp_band.py</code> <pre><code>def compute_signal_power(\n    self, electrode_list: list[int] = None, **kwargs\n) -&gt; pd.DataFrame:\n\"\"\"Computes the power of a given LFPBand signals using the hilbert transform\n\n    Parameters\n    ----------\n    electrode_list : list[int], optional\n        A list of the electrodes to compute the power of, by default None\n\n    Returns\n    -------\n    signal_power_df : pd.DataFrame\n        DataFrame containing the power of the signals\n    \"\"\"\n    if electrode_list is None:\n        electrode_list = []\n\n    analytic_signal_df = self.compute_analytic_signal(\n        electrode_list, **kwargs\n    )\n\n    return pd.DataFrame(\n        np.abs(analytic_signal_df) ** 2,\n        columns=analytic_signal_df.columns,\n        index=analytic_signal_df.index,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/lock/file_lock/", "title": "file_lock.py", "text": ""}, {"location": "api/src/spyglass/lock/file_lock/#src.spyglass.lock.file_lock.NwbfileLock", "title": "<code>NwbfileLock</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/lock/file_lock.py</code> <pre><code>@schema\nclass NwbfileLock(dj.Manual):\n    definition = \"\"\"\n    -&gt; Nwbfile\n    \"\"\"\n\n    def populate_from_lock_file(self):\n\"\"\"\n        Reads from the NWB_LOCK_FILE (defined by an environment variable),\n        adds the entries to this schema, and then removes the file\n        \"\"\"\n        if os.path.exists(os.getenv(\"NWB_LOCK_FILE\")):\n            lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"r\")\n            for line in lock_file:\n                print(line)\n                key = {\"nwb_file_name\": line.strip()}\n                self.insert1(key, skip_duplicates=\"True\")\n            lock_file.close()\n            os.remove(os.getenv(\"NWB_LOCK_FILE\"))\n</code></pre>"}, {"location": "api/src/spyglass/lock/file_lock/#src.spyglass.lock.file_lock.NwbfileLock.populate_from_lock_file", "title": "<code>populate_from_lock_file()</code>", "text": "<p>Reads from the NWB_LOCK_FILE (defined by an environment variable), adds the entries to this schema, and then removes the file</p> Source code in <code>src/spyglass/lock/file_lock.py</code> <pre><code>def populate_from_lock_file(self):\n\"\"\"\n    Reads from the NWB_LOCK_FILE (defined by an environment variable),\n    adds the entries to this schema, and then removes the file\n    \"\"\"\n    if os.path.exists(os.getenv(\"NWB_LOCK_FILE\")):\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"r\")\n        for line in lock_file:\n            print(line)\n            key = {\"nwb_file_name\": line.strip()}\n            self.insert1(key, skip_duplicates=\"True\")\n        lock_file.close()\n        os.remove(os.getenv(\"NWB_LOCK_FILE\"))\n</code></pre>"}, {"location": "api/src/spyglass/lock/file_lock/#src.spyglass.lock.file_lock.AnalysisNwbfileLock", "title": "<code>AnalysisNwbfileLock</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/lock/file_lock.py</code> <pre><code>@schema\nclass AnalysisNwbfileLock(dj.Manual):\n    definition = \"\"\"\n    -&gt; AnalysisNwbfile\n    \"\"\"\n\n    def populate_from_lock_file(self):\n\"\"\"Reads from the ANALYSIS_LOCK_FILE (defined by an environment variable), adds the entries to this schema, and\n        then removes the file\n        \"\"\"\n        if os.path.exists(os.getenv(\"ANALYSIS_LOCK_FILE\")):\n            lock_file = open(os.getenv(\"ANALYSIS_LOCK_FILE\"), \"r\")\n            for line in lock_file:\n                key = {\"analysis_file_name\": line.strip()}\n                self.insert1(key, skip_duplicates=\"True\")\n            os.remove(os.getenv(\"ANALYSIS_LOCK_FILE\"))\n</code></pre>"}, {"location": "api/src/spyglass/lock/file_lock/#src.spyglass.lock.file_lock.AnalysisNwbfileLock.populate_from_lock_file", "title": "<code>populate_from_lock_file()</code>", "text": "<p>Reads from the ANALYSIS_LOCK_FILE (defined by an environment variable), adds the entries to this schema, and then removes the file</p> Source code in <code>src/spyglass/lock/file_lock.py</code> <pre><code>def populate_from_lock_file(self):\n\"\"\"Reads from the ANALYSIS_LOCK_FILE (defined by an environment variable), adds the entries to this schema, and\n    then removes the file\n    \"\"\"\n    if os.path.exists(os.getenv(\"ANALYSIS_LOCK_FILE\")):\n        lock_file = open(os.getenv(\"ANALYSIS_LOCK_FILE\"), \"r\")\n        for line in lock_file:\n            key = {\"analysis_file_name\": line.strip()}\n            self.insert1(key, skip_duplicates=\"True\")\n        os.remove(os.getenv(\"ANALYSIS_LOCK_FILE\"))\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/", "title": "position_merge.py", "text": ""}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.IntervalList", "title": "<code>IntervalList</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@schema\nclass IntervalList(dj.Manual):\n    definition = \"\"\"\n    # Time intervals used for analysis\n    -&gt; Session\n    interval_list_name: varchar(200)  # descriptive name of this interval list\n    ---\n    valid_times: longblob  # numpy array with start and end times for each interval\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n        The interval list name for each epoch is set to the first tag for the epoch.\n        If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n        (0-indexed) of the epoch in the epochs table.\n        The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n        [start time, stop time] for each epoch.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        nwb_file_name : str\n            The file name of the NWB file, used as a primary key to the Session table.\n        \"\"\"\n        if nwbf.epochs is None:\n            print(\"No epochs found in NWB file.\")\n            return\n        epochs = nwbf.epochs.to_dataframe()\n        for epoch_index, epoch_data in epochs.iterrows():\n            epoch_dict = dict()\n            epoch_dict[\"nwb_file_name\"] = nwb_file_name\n            if epoch_data.tags[0]:\n                epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n            else:\n                epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                    epoch_index\n                )\n            epoch_dict[\"valid_times\"] = np.asarray(\n                [[epoch_data.start_time, epoch_data.stop_time]]\n            )\n            cls.insert1(epoch_dict, skip_duplicates=True)\n\n    def plot_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=figsize)\n        interval_count = 0\n        for row in interval_list.itertuples(index=False):\n            for interval in row.valid_times:\n                ax.plot(interval, [interval_count, interval_count])\n                ax.scatter(\n                    interval,\n                    [interval_count, interval_count],\n                    alpha=0.8,\n                    zorder=2,\n                )\n            interval_count += 1\n        ax.set_yticks(np.arange(interval_list.shape[0]))\n        ax.set_yticklabels(interval_list.interval_list_name)\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n\n    def plot_epoch_pos_raw_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=(30, 3))\n\n        raw_data_valid_times = interval_list.loc[\n            interval_list.interval_list_name == \"raw data valid times\"\n        ].valid_times\n        interval_y = 1\n\n        for interval in np.asarray(raw_data_valid_times)[0]:\n            ax.plot(interval, [interval_y, interval_y])\n            ax.scatter(interval, [interval_y, interval_y], alpha=0.8, zorder=2)\n\n        epoch_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n        interval_y = 2\n        for epoch, valid_times in zip(\n            epoch_valid_times.index, epoch_valid_times\n        ):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        pos_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^pos \\d+ valid times$\", axis=0)\n            .valid_times\n        ).sort_index(key=lambda index: [int(name.split()[1]) for name in index])\n        interval_y = 0\n        for epoch, valid_times in zip(pos_valid_times.index, pos_valid_times):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch.strip(\" valid times\"),\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        ax.set_ylim((-0.25, 2.25))\n        ax.set_yticks(np.arange(3))\n        ax.set_yticklabels([\"pos valid times\", \"raw data valid times\", \"epoch\"])\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.common.common_interval.IntervalList.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, *, nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add each entry in the NWB file epochs table to the IntervalList table.</p> <p>The interval list name for each epoch is set to the first tag for the epoch. If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index (0-indexed) of the epoch in the epochs table. The start time and stop time of the epoch are stored in the valid_times field as a numpy array of [start time, stop time] for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>nwb_file_name</code> <code>str</code> <p>The file name of the NWB file, used as a primary key to the Session table.</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n    The interval list name for each epoch is set to the first tag for the epoch.\n    If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n    (0-indexed) of the epoch in the epochs table.\n    The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n    [start time, stop time] for each epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    nwb_file_name : str\n        The file name of the NWB file, used as a primary key to the Session table.\n    \"\"\"\n    if nwbf.epochs is None:\n        print(\"No epochs found in NWB file.\")\n        return\n    epochs = nwbf.epochs.to_dataframe()\n    for epoch_index, epoch_data in epochs.iterrows():\n        epoch_dict = dict()\n        epoch_dict[\"nwb_file_name\"] = nwb_file_name\n        if epoch_data.tags[0]:\n            epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n        else:\n            epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                epoch_index\n            )\n        epoch_dict[\"valid_times\"] = np.asarray(\n            [[epoch_data.start_time, epoch_data.stop_time]]\n        )\n        cls.insert1(epoch_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.DLCPoseEstimationSelection", "title": "<code>DLCPoseEstimationSelection</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@schema\nclass DLCPoseEstimationSelection(dj.Manual):\n    definition = \"\"\"\n    -&gt; VideoFile                           # Session -&gt; Recording + File part table\n    -&gt; DLCModel                                    # Must specify a DLC project_path\n    ---\n    task_mode='load' : enum('load', 'trigger')  # load results or trigger computation\n    video_path : varchar(120)                   # path to video file\n    pose_estimation_output_dir='': varchar(255) # output dir relative to the root dir\n    pose_estimation_params=null  : longblob     # analyze_videos params, if not default\n    \"\"\"\n\n    @classmethod\n    def get_video_crop(cls, video_path):\n\"\"\"\n        Queries the user to determine the cropping parameters for a given video\n\n        Parameters\n        ----------\n        video_path : str\n            path to the video file\n\n        Returns\n        -------\n        crop_ints : list\n            list of 4 integers [x min, x max, y min, y max]\n        \"\"\"\n\n        cap = cv2.VideoCapture(video_path)\n        _, frame = cap.read()\n        fig, ax = plt.subplots(figsize=(20, 10))\n        ax.imshow(frame)\n        xlims = ax.get_xlim()\n        ylims = ax.get_ylim()\n        ax.set_xticks(np.arange(xlims[0], xlims[-1], 50))\n        ax.set_yticks(np.arange(ylims[0], ylims[-1], -50))\n        ax.grid(visible=True, color=\"white\", lw=0.5, alpha=0.5)\n        display(fig)\n        crop_input = input(\n            \"Please enter the crop parameters for your video in format xmin, xmax, ymin, ymax, or 'none'\\n\"\n        )\n        plt.close()\n        if crop_input.lower() == \"none\":\n            return None\n        crop_ints = [int(val) for val in crop_input.split(\",\")]\n        assert all(isinstance(val, int) for val in crop_ints)\n        return crop_ints\n\n    @classmethod\n    def insert_estimation_task(\n        cls,\n        key,\n        task_mode=\"trigger\",\n        params: dict = None,\n        check_crop=None,\n        skip_duplicates=True,\n    ):\n\"\"\"\n        Insert PoseEstimationTask in inferred output dir.\n        From Datajoint Elements\n\n        Parameters\n        ----------\n        key: DataJoint key specifying a pairing of VideoRecording and Model.\n        task_mode (bool): Default 'trigger' computation. Or 'load' existing results.\n        params (dict): Optional. Parameters passed to DLC's analyze_videos:\n            videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference,\n            dynamic, robust_nframes, allow_growth, use_shelve\n        \"\"\"\n        from .dlc_utils import check_videofile, get_video_path\n\n        video_path, video_filename, _, _ = get_video_path(key)\n        output_dir = infer_output_dir(key)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n        ) as logger:\n            logger.logger.info(\"Pose Estimation Selection\")\n            video_dir = os.path.dirname(video_path) + \"/\"\n            logger.logger.info(\"video_dir: %s\", video_dir)\n            video_path = check_videofile(\n                video_path=video_dir, video_filename=video_filename\n            )[0]\n            if check_crop is not None:\n                params[\"cropping\"] = cls.get_video_crop(\n                    video_path=video_path.as_posix()\n                )\n            cls.insert1(\n                {\n                    **key,\n                    \"task_mode\": task_mode,\n                    \"pose_estimation_params\": params,\n                    \"video_path\": video_path,\n                    \"pose_estimation_output_dir\": output_dir,\n                },\n                skip_duplicates=skip_duplicates,\n            )\n        logger.logger.info(\"inserted entry into Pose Estimation Selection\")\n        return {**key, \"task_mode\": task_mode}\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimationSelection.get_video_crop", "title": "<code>get_video_crop(video_path)</code>  <code>classmethod</code>", "text": "<p>Queries the user to determine the cropping parameters for a given video</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>path to the video file</p> required <p>Returns:</p> Name Type Description <code>crop_ints</code> <code>list</code> <p>list of 4 integers [x min, x max, y min, y max]</p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@classmethod\ndef get_video_crop(cls, video_path):\n\"\"\"\n    Queries the user to determine the cropping parameters for a given video\n\n    Parameters\n    ----------\n    video_path : str\n        path to the video file\n\n    Returns\n    -------\n    crop_ints : list\n        list of 4 integers [x min, x max, y min, y max]\n    \"\"\"\n\n    cap = cv2.VideoCapture(video_path)\n    _, frame = cap.read()\n    fig, ax = plt.subplots(figsize=(20, 10))\n    ax.imshow(frame)\n    xlims = ax.get_xlim()\n    ylims = ax.get_ylim()\n    ax.set_xticks(np.arange(xlims[0], xlims[-1], 50))\n    ax.set_yticks(np.arange(ylims[0], ylims[-1], -50))\n    ax.grid(visible=True, color=\"white\", lw=0.5, alpha=0.5)\n    display(fig)\n    crop_input = input(\n        \"Please enter the crop parameters for your video in format xmin, xmax, ymin, ymax, or 'none'\\n\"\n    )\n    plt.close()\n    if crop_input.lower() == \"none\":\n        return None\n    crop_ints = [int(val) for val in crop_input.split(\",\")]\n    assert all(isinstance(val, int) for val in crop_ints)\n    return crop_ints\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimationSelection.insert_estimation_task", "title": "<code>insert_estimation_task(key, task_mode='trigger', params=None, check_crop=None, skip_duplicates=True)</code>  <code>classmethod</code>", "text": "<p>Insert PoseEstimationTask in inferred output dir. From Datajoint Elements</p> <p>Parameters:</p> Name Type Description Default <code>key</code> required <code>task_mode</code> <code>'trigger'</code> <code>params</code> <code>dict</code> <p>videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference, dynamic, robust_nframes, allow_growth, use_shelve</p> <code>None</code> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@classmethod\ndef insert_estimation_task(\n    cls,\n    key,\n    task_mode=\"trigger\",\n    params: dict = None,\n    check_crop=None,\n    skip_duplicates=True,\n):\n\"\"\"\n    Insert PoseEstimationTask in inferred output dir.\n    From Datajoint Elements\n\n    Parameters\n    ----------\n    key: DataJoint key specifying a pairing of VideoRecording and Model.\n    task_mode (bool): Default 'trigger' computation. Or 'load' existing results.\n    params (dict): Optional. Parameters passed to DLC's analyze_videos:\n        videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference,\n        dynamic, robust_nframes, allow_growth, use_shelve\n    \"\"\"\n    from .dlc_utils import check_videofile, get_video_path\n\n    video_path, video_filename, _, _ = get_video_path(key)\n    output_dir = infer_output_dir(key)\n    with OutputLogger(\n        name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n        path=f\"{output_dir.as_posix()}/log.log\",\n    ) as logger:\n        logger.logger.info(\"Pose Estimation Selection\")\n        video_dir = os.path.dirname(video_path) + \"/\"\n        logger.logger.info(\"video_dir: %s\", video_dir)\n        video_path = check_videofile(\n            video_path=video_dir, video_filename=video_filename\n        )[0]\n        if check_crop is not None:\n            params[\"cropping\"] = cls.get_video_crop(\n                video_path=video_path.as_posix()\n            )\n        cls.insert1(\n            {\n                **key,\n                \"task_mode\": task_mode,\n                \"pose_estimation_params\": params,\n                \"video_path\": video_path,\n                \"pose_estimation_output_dir\": output_dir,\n            },\n            skip_duplicates=skip_duplicates,\n        )\n    logger.logger.info(\"inserted entry into Pose Estimation Selection\")\n    return {**key, \"task_mode\": task_mode}\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.PositionOutput", "title": "<code>PositionOutput</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Table to identify source of Position Information from upstream options (e.g. DLC, Trodes, etc...) To add another upstream option, a new Part table should be added in the same syntax as DLCPos and TrodesPos and</p> <p>Note: all part tables need to be named using the source+\"Pos\" convention i.e. if the source='DLC', then the table is DLCPos</p> Source code in <code>src/spyglass/position/position_merge.py</code> <pre><code>@schema\nclass PositionOutput(dj.Manual):\n\"\"\"\n    Table to identify source of Position Information from upstream options\n    (e.g. DLC, Trodes, etc...) To add another upstream option, a new Part table\n    should be added in the same syntax as DLCPos and TrodesPos and\n\n    Note: all part tables need to be named using the source+\"Pos\" convention\n    i.e. if the source='DLC', then the table is DLCPos\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; IntervalList\n    source: varchar(40)\n    version: int\n    position_id: int\n    ---\n    \"\"\"\n\n    class DLCPosV1(dj.Part):\n\"\"\"\n        Table to pass-through upstream DLC Pose Estimation information\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; PositionOutput\n        -&gt; DLCPosV1\n        ---\n        -&gt; AnalysisNwbfile\n        position_object_id : varchar(80)\n        orientation_object_id : varchar(80)\n        velocity_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n    class TrodesPosV1(dj.Part):\n\"\"\"\n        Table to pass-through upstream Trodes Position Tracking information\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; PositionOutput\n        -&gt; TrodesPosV1\n        ---\n        -&gt; AnalysisNwbfile\n        position_object_id : varchar(80)\n        orientation_object_id : varchar(80)\n        velocity_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n    class CommonPos(dj.Part):\n\"\"\"\n        Table to pass-through upstream Trodes Position Tracking information\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; PositionOutput\n        -&gt; CommonIntervalPositionInfo\n        ---\n        -&gt; AnalysisNwbfile\n        position_object_id : varchar(80)\n        orientation_object_id : varchar(80)\n        velocity_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n    def insert1(self, key, params: Dict = None, **kwargs):\n\"\"\"Overrides insert1 to also insert into specific part table.\n\n        Parameters\n        ----------\n        key : Dict\n            key specifying the entry to insert\n        params : Dict, optional\n            A dictionary containing all table entries\n            not specified by the parent table (PosMerge)\n        \"\"\"\n        assert (\n            key[\"source\"] in _valid_data_sources\n        ), f\"source needs to be one of {_valid_data_sources}\"\n        position_id = key.get(\"position_id\", None)\n        if position_id is None:\n            key[\"position_id\"] = (\n                dj.U().aggr(self &amp; key, n=\"max(position_id)\").fetch1(\"n\") or 0\n            ) + 1\n        else:\n            id = (self &amp; key).fetch(\"position_id\")\n            if len(id) &gt; 0:\n                position_id = max(id) + 1\n            else:\n                position_id = max(0, position_id)\n            key[\"position_id\"] = position_id\n        super().insert1(key, **kwargs)\n        source = key[\"source\"]\n        if source in [\"Common\"]:\n            table_name = f\"{source}Pos\"\n        else:\n            version = key[\"version\"]\n            table_name = f\"{source}PosV{version}\"\n        part_table = getattr(self, table_name)\n        # TODO: The parent table to refer to is hard-coded here, expecting it to be the second\n        # Table in the definition. This could be more flexible.\n        if params:\n            table_query = (\n                dj.FreeTable(dj.conn(), full_table_name=part_table.parents()[1])\n                &amp; key\n                &amp; params\n            )\n        else:\n            table_query = (\n                dj.FreeTable(dj.conn(), full_table_name=part_table.parents()[1])\n                &amp; key\n            )\n        if any(\n            \"head\" in col\n            for col in list(table_query.fetch().dtype.fields.keys())\n        ):\n            (\n                analysis_file_name,\n                position_object_id,\n                orientation_object_id,\n                velocity_object_id,\n            ) = table_query.fetch1(\n                \"analysis_file_name\",\n                \"head_position_object_id\",\n                \"head_orientation_object_id\",\n                \"head_velocity_object_id\",\n            )\n        else:\n            (\n                analysis_file_name,\n                position_object_id,\n                orientation_object_id,\n                velocity_object_id,\n            ) = table_query.fetch1(\n                \"analysis_file_name\",\n                \"position_object_id\",\n                \"orientation_object_id\",\n                \"velocity_object_id\",\n            )\n        part_table.insert1(\n            {\n                **key,\n                \"analysis_file_name\": analysis_file_name,\n                \"position_object_id\": position_object_id,\n                \"orientation_object_id\": orientation_object_id,\n                \"velocity_object_id\": velocity_object_id,\n                **params,\n            },\n        )\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        source = self.fetch1(\"source\")\n        if source in [\"Common\"]:\n            table_name = f\"{source}Pos\"\n        else:\n            version = self.fetch1(\"version\")\n            table_name = f\"{source}PosV{version}\"\n        part_table = getattr(self, table_name) &amp; self\n        return part_table.fetch_nwb()\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(nwb_data[\"position\"].get_spatial_series().timestamps),\n            name=\"time\",\n        )\n        if (\n            \"video_frame_ind\"\n            in nwb_data[\"velocity\"].fields[\"time_series\"].keys()\n        ):\n            COLUMNS = [\n                \"video_frame_ind\",\n                \"position_x\",\n                \"position_y\",\n                \"orientation\",\n                \"velocity_x\",\n                \"velocity_y\",\n                \"speed\",\n            ]\n            return pd.DataFrame(\n                np.concatenate(\n                    (\n                        np.asarray(\n                            nwb_data[\"velocity\"]\n                            .get_timeseries(\"video_frame_ind\")\n                            .data,\n                            dtype=int,\n                        )[:, np.newaxis],\n                        np.asarray(\n                            nwb_data[\"position\"].get_spatial_series().data\n                        ),\n                        np.asarray(\n                            nwb_data[\"orientation\"].get_spatial_series().data\n                        )[:, np.newaxis],\n                        np.asarray(\n                            nwb_data[\"velocity\"].get_timeseries(\"velocity\").data\n                        ),\n                    ),\n                    axis=1,\n                ),\n                columns=COLUMNS,\n                index=index,\n            )\n        else:\n            COLUMNS = [\n                \"position_x\",\n                \"position_y\",\n                \"orientation\",\n                \"velocity_x\",\n                \"velocity_y\",\n                \"speed\",\n            ]\n            return pd.DataFrame(\n                np.concatenate(\n                    (\n                        np.asarray(\n                            nwb_data[\"position\"].get_spatial_series().data\n                        ),\n                        np.asarray(\n                            nwb_data[\"orientation\"].get_spatial_series().data\n                        )[:, np.newaxis],\n                        np.asarray(nwb_data[\"velocity\"].get_timeseries().data),\n                    ),\n                    axis=1,\n                ),\n                columns=COLUMNS,\n                index=index,\n            )\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.PositionOutput.DLCPosV1", "title": "<code>DLCPosV1</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Table to pass-through upstream DLC Pose Estimation information</p> Source code in <code>src/spyglass/position/position_merge.py</code> <pre><code>class DLCPosV1(dj.Part):\n\"\"\"\n    Table to pass-through upstream DLC Pose Estimation information\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionOutput\n    -&gt; DLCPosV1\n    ---\n    -&gt; AnalysisNwbfile\n    position_object_id : varchar(80)\n    orientation_object_id : varchar(80)\n    velocity_object_id : varchar(80)\n    \"\"\"\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self,\n            (AnalysisNwbfile, \"analysis_file_abs_path\"),\n            *attrs,\n            **kwargs,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.PositionOutput.TrodesPosV1", "title": "<code>TrodesPosV1</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Table to pass-through upstream Trodes Position Tracking information</p> Source code in <code>src/spyglass/position/position_merge.py</code> <pre><code>class TrodesPosV1(dj.Part):\n\"\"\"\n    Table to pass-through upstream Trodes Position Tracking information\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionOutput\n    -&gt; TrodesPosV1\n    ---\n    -&gt; AnalysisNwbfile\n    position_object_id : varchar(80)\n    orientation_object_id : varchar(80)\n    velocity_object_id : varchar(80)\n    \"\"\"\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self,\n            (AnalysisNwbfile, \"analysis_file_abs_path\"),\n            *attrs,\n            **kwargs,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.PositionOutput.CommonPos", "title": "<code>CommonPos</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Table to pass-through upstream Trodes Position Tracking information</p> Source code in <code>src/spyglass/position/position_merge.py</code> <pre><code>class CommonPos(dj.Part):\n\"\"\"\n    Table to pass-through upstream Trodes Position Tracking information\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionOutput\n    -&gt; CommonIntervalPositionInfo\n    ---\n    -&gt; AnalysisNwbfile\n    position_object_id : varchar(80)\n    orientation_object_id : varchar(80)\n    velocity_object_id : varchar(80)\n    \"\"\"\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self,\n            (AnalysisNwbfile, \"analysis_file_abs_path\"),\n            *attrs,\n            **kwargs,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.PositionOutput.insert1", "title": "<code>insert1(key, params=None, **kwargs)</code>", "text": "<p>Overrides insert1 to also insert into specific part table.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Dict</code> <p>key specifying the entry to insert</p> required <code>params</code> <code>Dict</code> <p>A dictionary containing all table entries not specified by the parent table (PosMerge)</p> <code>None</code> Source code in <code>src/spyglass/position/position_merge.py</code> <pre><code>def insert1(self, key, params: Dict = None, **kwargs):\n\"\"\"Overrides insert1 to also insert into specific part table.\n\n    Parameters\n    ----------\n    key : Dict\n        key specifying the entry to insert\n    params : Dict, optional\n        A dictionary containing all table entries\n        not specified by the parent table (PosMerge)\n    \"\"\"\n    assert (\n        key[\"source\"] in _valid_data_sources\n    ), f\"source needs to be one of {_valid_data_sources}\"\n    position_id = key.get(\"position_id\", None)\n    if position_id is None:\n        key[\"position_id\"] = (\n            dj.U().aggr(self &amp; key, n=\"max(position_id)\").fetch1(\"n\") or 0\n        ) + 1\n    else:\n        id = (self &amp; key).fetch(\"position_id\")\n        if len(id) &gt; 0:\n            position_id = max(id) + 1\n        else:\n            position_id = max(0, position_id)\n        key[\"position_id\"] = position_id\n    super().insert1(key, **kwargs)\n    source = key[\"source\"]\n    if source in [\"Common\"]:\n        table_name = f\"{source}Pos\"\n    else:\n        version = key[\"version\"]\n        table_name = f\"{source}PosV{version}\"\n    part_table = getattr(self, table_name)\n    # TODO: The parent table to refer to is hard-coded here, expecting it to be the second\n    # Table in the definition. This could be more flexible.\n    if params:\n        table_query = (\n            dj.FreeTable(dj.conn(), full_table_name=part_table.parents()[1])\n            &amp; key\n            &amp; params\n        )\n    else:\n        table_query = (\n            dj.FreeTable(dj.conn(), full_table_name=part_table.parents()[1])\n            &amp; key\n        )\n    if any(\n        \"head\" in col\n        for col in list(table_query.fetch().dtype.fields.keys())\n    ):\n        (\n            analysis_file_name,\n            position_object_id,\n            orientation_object_id,\n            velocity_object_id,\n        ) = table_query.fetch1(\n            \"analysis_file_name\",\n            \"head_position_object_id\",\n            \"head_orientation_object_id\",\n            \"head_velocity_object_id\",\n        )\n    else:\n        (\n            analysis_file_name,\n            position_object_id,\n            orientation_object_id,\n            velocity_object_id,\n        ) = table_query.fetch1(\n            \"analysis_file_name\",\n            \"position_object_id\",\n            \"orientation_object_id\",\n            \"velocity_object_id\",\n        )\n    part_table.insert1(\n        {\n            **key,\n            \"analysis_file_name\": analysis_file_name,\n            \"position_object_id\": position_object_id,\n            \"orientation_object_id\": orientation_object_id,\n            \"velocity_object_id\": velocity_object_id,\n            **params,\n        },\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.DLCPosV1", "title": "<code>DLCPosV1</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Combines upstream DLCCentroid and DLCOrientation entries into a single entry with a single Analysis NWB file</p> Source code in <code>src/spyglass/position/v1/position_dlc_selection.py</code> <pre><code>@schema\nclass DLCPosV1(dj.Computed):\n\"\"\"\n    Combines upstream DLCCentroid and DLCOrientation\n    entries into a single entry with a single Analysis NWB file\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCPosSelection\n    ---\n    -&gt; AnalysisNwbfile\n    position_object_id      : varchar(80)\n    orientation_object_id   : varchar(80)\n    velocity_object_id      : varchar(80)\n    pose_eval_result        : longblob\n    \"\"\"\n\n    def make(self, key):\n        key[\"pose_eval_result\"] = self.evaluate_pose_estimation(key)\n        position_nwb_data = (DLCCentroid &amp; key).fetch_nwb()[0]\n        orientation_nwb_data = (DLCOrientation &amp; key).fetch_nwb()[0]\n        position_object = position_nwb_data[\"dlc_position\"].spatial_series[\n            \"position\"\n        ]\n        velocity_object = position_nwb_data[\"dlc_velocity\"].time_series[\n            \"velocity\"\n        ]\n        video_frame_object = position_nwb_data[\"dlc_velocity\"].time_series[\n            \"video_frame_ind\"\n        ]\n        orientation_object = orientation_nwb_data[\n            \"dlc_orientation\"\n        ].spatial_series[\"orientation\"]\n        position = pynwb.behavior.Position()\n        orientation = pynwb.behavior.CompassDirection()\n        velocity = pynwb.behavior.BehavioralTimeSeries()\n        position.create_spatial_series(\n            name=position_object.name,\n            timestamps=np.asarray(position_object.timestamps),\n            conversion=position_object.conversion,\n            data=np.asarray(position_object.data),\n            reference_frame=position_object.reference_frame,\n            comments=position_object.comments,\n            description=position_object.description,\n        )\n        orientation.create_spatial_series(\n            name=orientation_object.name,\n            timestamps=np.asarray(orientation_object.timestamps),\n            conversion=orientation_object.conversion,\n            data=np.asarray(orientation_object.data),\n            reference_frame=orientation_object.reference_frame,\n            comments=orientation_object.comments,\n            description=orientation_object.description,\n        )\n        velocity.create_timeseries(\n            name=velocity_object.name,\n            timestamps=np.asarray(velocity_object.timestamps),\n            conversion=velocity_object.conversion,\n            unit=velocity_object.unit,\n            data=np.asarray(velocity_object.data),\n            comments=velocity_object.comments,\n            description=velocity_object.description,\n        )\n        velocity.create_timeseries(\n            name=video_frame_object.name,\n            unit=video_frame_object.unit,\n            timestamps=np.asarray(video_frame_object.timestamps),\n            data=np.asarray(video_frame_object.data),\n            description=video_frame_object.description,\n            comments=video_frame_object.comments,\n        )\n        # Add to Analysis NWB file\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"orientation_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], orientation\n        )\n        key[\"position_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], position\n        )\n        key[\"velocity_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], velocity\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n        from ..position_merge import PositionOutput\n\n        key[\"source\"] = \"DLC\"\n        key[\"version\"] = 1\n        dlc_key = key.copy()\n        del dlc_key[\"pose_eval_result\"]\n        key[\"interval_list_name\"] = f\"pos {key['epoch']-1} valid times\"\n        valid_fields = PositionOutput().fetch().dtype.fields.keys()\n        entries_to_delete = [\n            entry for entry in key.keys() if entry not in valid_fields\n        ]\n        for entry in entries_to_delete:\n            del key[entry]\n\n        PositionOutput().insert1(key=key, params=dlc_key, skip_duplicates=True)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(nwb_data[\"position\"].get_spatial_series().timestamps),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"position_x\",\n            \"position_y\",\n            \"orientation\",\n            \"velocity_x\",\n            \"velocity_y\",\n            \"speed\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"velocity\"]\n                        .time_series[\"video_frame_ind\"]\n                        .data,\n                        dtype=int,\n                    )[:, np.newaxis],\n                    np.asarray(nwb_data[\"position\"].get_spatial_series().data),\n                    np.asarray(\n                        nwb_data[\"orientation\"].get_spatial_series().data\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"velocity\"].time_series[\"velocity\"].data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n\n    @classmethod\n    def evaluate_pose_estimation(cls, key):\n        likelihood_thresh = []\n        valid_fields = (\n            DLCSmoothInterpCohort.BodyPart().fetch().dtype.fields.keys()\n        )\n        centroid_key = {k: val for k, val in key.items() if k in valid_fields}\n        centroid_key[\"dlc_si_cohort_selection_name\"] = key[\n            \"dlc_si_cohort_centroid\"\n        ]\n        orientation_key = centroid_key.copy()\n        orientation_key[\"dlc_si_cohort_selection_name\"] = key[\n            \"dlc_si_cohort_orientation\"\n        ]\n        centroid_bodyparts, centroid_si_params = (\n            DLCSmoothInterpCohort.BodyPart &amp; centroid_key\n        ).fetch(\"bodypart\", \"dlc_si_params_name\")\n        orientation_bodyparts, orientation_si_params = (\n            DLCSmoothInterpCohort.BodyPart &amp; orientation_key\n        ).fetch(\"bodypart\", \"dlc_si_params_name\")\n        for param in np.unique(\n            np.concatenate((centroid_si_params, orientation_si_params))\n        ):\n            likelihood_thresh.append(\n                (\n                    DLCSmoothInterpParams() &amp; {\"dlc_si_params_name\": param}\n                ).fetch1(\"params\")[\"likelihood_thresh\"]\n            )\n\n        if len(np.unique(likelihood_thresh)) &gt; 1:\n            raise ValueError(\"more than one likelihood threshold used\")\n        like_thresh = likelihood_thresh[0]\n        bodyparts = np.unique([*centroid_bodyparts, *orientation_bodyparts])\n        fields = list(DLCPoseEstimation.BodyPart.fetch().dtype.fields.keys())\n        pose_estimation_key = {k: v for k, v in key.items() if k in fields}\n        pose_estimation_df = pd.concat(\n            {\n                bodypart: (\n                    DLCPoseEstimation.BodyPart()\n                    &amp; {**pose_estimation_key, **{\"bodypart\": bodypart}}\n                ).fetch1_dataframe()\n                for bodypart in bodyparts.tolist()\n            },\n            axis=1,\n        )\n        df_filter = {\n            bodypart: pose_estimation_df[bodypart][\"likelihood\"] &lt; like_thresh\n            for bodypart in bodyparts\n            if bodypart in pose_estimation_df.columns\n        }\n        sub_thresh_ind_dict = {\n            bodypart: {\n                \"inds\": np.where(\n                    ~np.isnan(\n                        pose_estimation_df[bodypart][\"likelihood\"].where(\n                            df_filter[bodypart]\n                        )\n                    )\n                )[0],\n            }\n            for bodypart in bodyparts\n        }\n        sub_thresh_percent_dict = {\n            bodypart: (\n                len(\n                    np.where(\n                        ~np.isnan(\n                            pose_estimation_df[bodypart][\"likelihood\"].where(\n                                df_filter[bodypart]\n                            )\n                        )\n                    )[0]\n                )\n                / len(pose_estimation_df)\n            )\n            * 100\n            for bodypart in bodyparts\n        }\n        return sub_thresh_percent_dict\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.CommonIntervalPositionInfo", "title": "<code>CommonIntervalPositionInfo</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Computes the smoothed head position, orientation and velocity for a given interval.</p> Source code in <code>src/spyglass/common/common_position.py</code> <pre><code>@schema\nclass IntervalPositionInfo(dj.Computed):\n\"\"\"Computes the smoothed head position, orientation and velocity for a given\n    interval.\"\"\"\n\n    definition = \"\"\"\n    -&gt; IntervalPositionInfoSelection\n    ---\n    -&gt; AnalysisNwbfile\n    head_position_object_id : varchar(40)\n    head_orientation_object_id : varchar(40)\n    head_velocity_object_id : varchar(40)\n    \"\"\"\n\n    def make(self, key):\n        print(f\"Computing position for: {key}\")\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        raw_position = (\n            RawPosition()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n            }\n        ).fetch_nwb()[0]\n        position_info_parameters = (\n            PositionInfoParameters()\n            &amp; {\"position_info_param_name\": key[\"position_info_param_name\"]}\n        ).fetch1()\n\n        head_position = pynwb.behavior.Position()\n        head_orientation = pynwb.behavior.CompassDirection()\n        head_velocity = pynwb.behavior.BehavioralTimeSeries()\n\n        METERS_PER_CM = 0.01\n\n        try:\n            # calculate the processed position\n            spatial_series = raw_position[\"raw_position\"]\n            position_info = self.calculate_position_info_from_spatial_series(\n                spatial_series,\n                position_info_parameters[\"max_separation\"],\n                position_info_parameters[\"max_speed\"],\n                position_info_parameters[\"speed_smoothing_std_dev\"],\n                position_info_parameters[\"position_smoothing_duration\"],\n                position_info_parameters[\"head_orient_smoothing_std_dev\"],\n                position_info_parameters[\"led1_is_front\"],\n                position_info_parameters[\"is_upsampled\"],\n                position_info_parameters[\"upsampling_sampling_rate\"],\n                position_info_parameters[\"upsampling_interpolation_method\"],\n            )\n\n            # create nwb objects for insertion into analysis nwb file\n            head_position.create_spatial_series(\n                name=\"head_position\",\n                timestamps=position_info[\"time\"],\n                conversion=METERS_PER_CM,\n                data=position_info[\"head_position\"],\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"head_x_position, head_y_position\",\n            )\n\n            head_orientation.create_spatial_series(\n                name=\"head_orientation\",\n                timestamps=position_info[\"time\"],\n                conversion=1.0,\n                data=position_info[\"head_orientation\"],\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"head_orientation\",\n            )\n\n            head_velocity.create_timeseries(\n                name=\"head_velocity\",\n                timestamps=position_info[\"time\"],\n                conversion=METERS_PER_CM,\n                unit=\"m/s\",\n                data=np.concatenate(\n                    (\n                        position_info[\"velocity\"],\n                        position_info[\"speed\"][:, np.newaxis],\n                    ),\n                    axis=1,\n                ),\n                comments=spatial_series.comments,\n                description=\"head_x_velocity, head_y_velocity, head_speed\",\n            )\n        except ValueError as e:\n            print(e)\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n\n        key[\"head_position_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], head_position\n        )\n        key[\"head_orientation_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], head_orientation\n        )\n        key[\"head_velocity_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], head_velocity\n        )\n\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], key[\"analysis_file_name\"])\n\n        self.insert1(key)\n\n    @staticmethod\n    def calculate_position_info_from_spatial_series(\n        spatial_series,\n        max_LED_separation,\n        max_plausible_speed,\n        speed_smoothing_std_dev,\n        position_smoothing_duration,\n        head_orient_smoothing_std_dev,\n        led1_is_front,\n        is_upsampled,\n        upsampling_sampling_rate,\n        upsampling_interpolation_method,\n    ):\n        CM_TO_METERS = 100\n\n        # Get spatial series properties\n        time = np.asarray(spatial_series.timestamps)  # seconds\n        position = np.asarray(\n            pd.DataFrame(\n                spatial_series.data,\n                columns=spatial_series.description.split(\", \"),\n            ).loc[:, [\"xloc\", \"yloc\", \"xloc2\", \"yloc2\"]]\n        )  # meters\n\n        # remove NaN times\n        is_nan_time = np.isnan(time)\n        position = position[~is_nan_time]\n        time = time[~is_nan_time]\n\n        dt = np.median(np.diff(time))\n        sampling_rate = 1 / dt\n        meters_to_pixels = spatial_series.conversion\n\n        # Define LEDs\n        if led1_is_front:\n            front_LED = position[:, [0, 1]].astype(float)\n            back_LED = position[:, [2, 3]].astype(float)\n        else:\n            back_LED = position[:, [0, 1]].astype(float)\n            front_LED = position[:, [2, 3]].astype(float)\n\n        # Convert to cm\n        back_LED *= meters_to_pixels * CM_TO_METERS\n        front_LED *= meters_to_pixels * CM_TO_METERS\n\n        # Set points to NaN where the front and back LEDs are too separated\n        dist_between_LEDs = get_distance(back_LED, front_LED)\n        is_too_separated = dist_between_LEDs &gt;= max_LED_separation\n\n        back_LED[is_too_separated] = np.nan\n        front_LED[is_too_separated] = np.nan\n\n        # Calculate speed\n        front_LED_speed = get_speed(\n            front_LED,\n            time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )\n        back_LED_speed = get_speed(\n            back_LED,\n            time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )\n\n        # Set to points to NaN where the speed is too fast\n        is_too_fast = (front_LED_speed &gt; max_plausible_speed) | (\n            back_LED_speed &gt; max_plausible_speed\n        )\n        back_LED[is_too_fast] = np.nan\n        front_LED[is_too_fast] = np.nan\n\n        # Interpolate the NaN points\n        back_LED = interpolate_nan(back_LED)\n        front_LED = interpolate_nan(front_LED)\n\n        # Smooth\n        moving_average_window = int(position_smoothing_duration * sampling_rate)\n        back_LED = bottleneck.move_mean(\n            back_LED, window=moving_average_window, axis=0, min_count=1\n        )\n        front_LED = bottleneck.move_mean(\n            front_LED, window=moving_average_window, axis=0, min_count=1\n        )\n\n        if is_upsampled:\n            position_df = pd.DataFrame(\n                {\n                    \"time\": time,\n                    \"back_LED_x\": back_LED[:, 0],\n                    \"back_LED_y\": back_LED[:, 1],\n                    \"front_LED_x\": front_LED[:, 0],\n                    \"front_LED_y\": front_LED[:, 1],\n                }\n            ).set_index(\"time\")\n\n            upsampling_start_time = time[0]\n            upsampling_end_time = time[-1]\n\n            n_samples = (\n                int(\n                    np.ceil(\n                        (upsampling_end_time - upsampling_start_time)\n                        * upsampling_sampling_rate\n                    )\n                )\n                + 1\n            )\n            new_time = np.linspace(\n                upsampling_start_time, upsampling_end_time, n_samples\n            )\n            new_index = pd.Index(\n                np.unique(np.concatenate((position_df.index, new_time))),\n                name=\"time\",\n            )\n            position_df = (\n                position_df.reindex(index=new_index)\n                .interpolate(method=upsampling_interpolation_method)\n                .reindex(index=new_time)\n            )\n\n            time = np.asarray(position_df.index)\n            back_LED = np.asarray(\n                position_df.loc[:, [\"back_LED_x\", \"back_LED_y\"]]\n            )\n            front_LED = np.asarray(\n                position_df.loc[:, [\"front_LED_x\", \"front_LED_y\"]]\n            )\n\n            sampling_rate = upsampling_sampling_rate\n\n        # Calculate head position, head orientation, velocity, speed\n        head_position = get_centriod(back_LED, front_LED)  # cm\n\n        head_orientation = get_angle(back_LED, front_LED)  # radians\n        is_nan = np.isnan(head_orientation)\n\n        # Unwrap orientation before smoothing\n        head_orientation[~is_nan] = np.unwrap(head_orientation[~is_nan])\n        head_orientation[~is_nan] = gaussian_smooth(\n            head_orientation[~is_nan],\n            head_orient_smoothing_std_dev,\n            sampling_rate,\n            axis=0,\n            truncate=8,\n        )\n        # convert back to between -pi and pi\n        head_orientation[~is_nan] = np.angle(\n            np.exp(1j * head_orientation[~is_nan])\n        )\n\n        velocity = get_velocity(\n            head_position,\n            time=time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )  # cm/s\n        speed = np.sqrt(np.sum(velocity**2, axis=1))  # cm/s\n\n        return {\n            \"time\": time,\n            \"head_position\": head_position,\n            \"head_orientation\": head_orientation,\n            \"velocity\": velocity,\n            \"speed\": speed,\n        }\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(\n                nwb_data[\"head_position\"].get_spatial_series().timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"head_position_x\",\n            \"head_position_y\",\n            \"head_orientation\",\n            \"head_velocity_x\",\n            \"head_velocity_y\",\n            \"head_speed\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"head_position\"].get_spatial_series().data\n                    ),\n                    np.asarray(\n                        nwb_data[\"head_orientation\"].get_spatial_series().data\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"head_velocity\"]\n                        .time_series[\"head_velocity\"]\n                        .data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.TrodesPosV1", "title": "<code>TrodesPosV1</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Table to calculate the position based on Trodes tracking</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@schema\nclass TrodesPosV1(dj.Computed):\n\"\"\"\n    Table to calculate the position based on Trodes tracking\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; TrodesPosSelection\n    ---\n    -&gt; AnalysisNwbfile\n    position_object_id : varchar(80)\n    orientation_object_id : varchar(80)\n    velocity_object_id : varchar(80)\n    \"\"\"\n\n    def make(self, key):\n        print(f\"Computing position for: {key}\")\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        raw_position = (RawPosition() &amp; key).fetch_nwb()[0]\n        position_info_parameters = (TrodesPosParams() &amp; key).fetch1(\"params\")\n        position = pynwb.behavior.Position()\n        orientation = pynwb.behavior.CompassDirection()\n        velocity = pynwb.behavior.BehavioralTimeSeries()\n\n        METERS_PER_CM = 0.01\n        raw_pos_df = pd.DataFrame(\n            data=raw_position[\"raw_position\"].data,\n            index=pd.Index(\n                raw_position[\"raw_position\"].timestamps, name=\"time\"\n            ),\n            columns=raw_position[\"raw_position\"].description.split(\", \"),\n        )\n        try:\n            # calculate the processed position\n            spatial_series = raw_position[\"raw_position\"]\n            position_info = self.calculate_position_info_from_spatial_series(\n                spatial_series,\n                position_info_parameters[\"max_separation\"],\n                position_info_parameters[\"max_speed\"],\n                position_info_parameters[\"speed_smoothing_std_dev\"],\n                position_info_parameters[\"position_smoothing_duration\"],\n                position_info_parameters[\"orient_smoothing_std_dev\"],\n                position_info_parameters[\"led1_is_front\"],\n                position_info_parameters[\"is_upsampled\"],\n                position_info_parameters[\"upsampling_sampling_rate\"],\n                position_info_parameters[\"upsampling_interpolation_method\"],\n            )\n            # create nwb objects for insertion into analysis nwb file\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=position_info[\"time\"],\n                conversion=METERS_PER_CM,\n                data=position_info[\"position\"],\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"x_position, y_position\",\n            )\n\n            orientation.create_spatial_series(\n                name=\"orientation\",\n                timestamps=position_info[\"time\"],\n                conversion=1.0,\n                data=position_info[\"orientation\"],\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"orientation\",\n            )\n\n            velocity.create_timeseries(\n                name=\"velocity\",\n                timestamps=position_info[\"time\"],\n                conversion=METERS_PER_CM,\n                unit=\"m/s\",\n                data=np.concatenate(\n                    (\n                        position_info[\"velocity\"],\n                        position_info[\"speed\"][:, np.newaxis],\n                    ),\n                    axis=1,\n                ),\n                comments=spatial_series.comments,\n                description=\"x_velocity, y_velocity, speed\",\n            )\n            try:\n                velocity.create_timeseries(\n                    name=\"video_frame_ind\",\n                    unit=\"index\",\n                    timestamps=position_info[\"time\"],\n                    data=raw_pos_df.video_frame_ind.to_numpy(),\n                    description=\"video_frame_ind\",\n                    comments=spatial_series.comments,\n                )\n            except AttributeError:\n                print(\n                    \"No video frame index found. Assuming all camera frames are present.\"\n                )\n                velocity.create_timeseries(\n                    name=\"video_frame_ind\",\n                    unit=\"index\",\n                    timestamps=position_info[\"time\"],\n                    data=np.arange(len(position_info[\"time\"])),\n                    description=\"video_frame_ind\",\n                    comments=spatial_series.comments,\n                )\n        except ValueError:\n            pass\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n\n        key[\"position_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], position\n        )\n        key[\"orientation_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], orientation\n        )\n        key[\"velocity_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], velocity\n        )\n\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], key[\"analysis_file_name\"])\n\n        self.insert1(key)\n        from ..position_merge import PositionOutput\n\n        key[\"source\"] = \"Trodes\"\n        key[\"version\"] = 1\n        trodes_key = key.copy()\n        valid_fields = PositionOutput().fetch().dtype.fields.keys()\n        entries_to_delete = [\n            entry for entry in key.keys() if entry not in valid_fields\n        ]\n        for entry in entries_to_delete:\n            del key[entry]\n        PositionOutput().insert1(\n            key=key, params=trodes_key, skip_duplicates=True\n        )\n\n    @staticmethod\n    def calculate_position_info_from_spatial_series(\n        spatial_series,\n        max_LED_separation,\n        max_plausible_speed,\n        speed_smoothing_std_dev,\n        position_smoothing_duration,\n        orient_smoothing_std_dev,\n        led1_is_front,\n        is_upsampled,\n        upsampling_sampling_rate,\n        upsampling_interpolation_method,\n    ):\n        CM_TO_METERS = 100\n\n        # Get spatial series properties\n        time = np.asarray(spatial_series.timestamps)  # seconds\n        position = np.asarray(\n            pd.DataFrame(\n                spatial_series.data,\n                columns=spatial_series.description.split(\", \"),\n            ).loc[:, [\"xloc\", \"yloc\", \"xloc2\", \"yloc2\"]]\n        )  # meters\n\n        # remove NaN times\n        is_nan_time = np.isnan(time)\n        position = position[~is_nan_time]\n        time = time[~is_nan_time]\n\n        dt = np.median(np.diff(time))\n        sampling_rate = 1 / dt\n        meters_to_pixels = spatial_series.conversion\n\n        # Define LEDs\n        if led1_is_front:\n            front_LED = position[:, [0, 1]].astype(float)\n            back_LED = position[:, [2, 3]].astype(float)\n        else:\n            back_LED = position[:, [0, 1]].astype(float)\n            front_LED = position[:, [2, 3]].astype(float)\n\n        # Convert to cm\n        back_LED *= meters_to_pixels * CM_TO_METERS\n        front_LED *= meters_to_pixels * CM_TO_METERS\n\n        # Set points to NaN where the front and back LEDs are too separated\n        dist_between_LEDs = get_distance(back_LED, front_LED)\n        is_too_separated = dist_between_LEDs &gt;= max_LED_separation\n\n        back_LED[is_too_separated] = np.nan\n        front_LED[is_too_separated] = np.nan\n\n        # Calculate speed\n        front_LED_speed = get_speed(\n            front_LED,\n            time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )\n        back_LED_speed = get_speed(\n            back_LED,\n            time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )\n\n        # Set to points to NaN where the speed is too fast\n        is_too_fast = (front_LED_speed &gt; max_plausible_speed) | (\n            back_LED_speed &gt; max_plausible_speed\n        )\n        back_LED[is_too_fast] = np.nan\n        front_LED[is_too_fast] = np.nan\n\n        # Interpolate the NaN points\n        back_LED = interpolate_nan(back_LED)\n        front_LED = interpolate_nan(front_LED)\n\n        # Smooth\n        moving_average_window = int(position_smoothing_duration * sampling_rate)\n        back_LED = bottleneck.move_mean(\n            back_LED, window=moving_average_window, axis=0, min_count=1\n        )\n        front_LED = bottleneck.move_mean(\n            front_LED, window=moving_average_window, axis=0, min_count=1\n        )\n\n        if is_upsampled:\n            position_df = pd.DataFrame(\n                {\n                    \"time\": time,\n                    \"back_LED_x\": back_LED[:, 0],\n                    \"back_LED_y\": back_LED[:, 1],\n                    \"front_LED_x\": front_LED[:, 0],\n                    \"front_LED_y\": front_LED[:, 1],\n                }\n            ).set_index(\"time\")\n\n            upsampling_start_time = time[0]\n            upsampling_end_time = time[-1]\n\n            n_samples = (\n                int(\n                    np.ceil(\n                        (upsampling_end_time - upsampling_start_time)\n                        * upsampling_sampling_rate\n                    )\n                )\n                + 1\n            )\n            new_time = np.linspace(\n                upsampling_start_time, upsampling_end_time, n_samples\n            )\n            new_index = pd.Index(\n                np.unique(np.concatenate((position_df.index, new_time))),\n                name=\"time\",\n            )\n            position_df = (\n                position_df.reindex(index=new_index)\n                .interpolate(method=upsampling_interpolation_method)\n                .reindex(index=new_time)\n            )\n\n            time = np.asarray(position_df.index)\n            back_LED = np.asarray(\n                position_df.loc[:, [\"back_LED_x\", \"back_LED_y\"]]\n            )\n            front_LED = np.asarray(\n                position_df.loc[:, [\"front_LED_x\", \"front_LED_y\"]]\n            )\n\n            sampling_rate = upsampling_sampling_rate\n\n        # Calculate position, orientation, velocity, speed\n        position = get_centriod(back_LED, front_LED)  # cm\n\n        orientation = get_angle(back_LED, front_LED)  # radians\n        is_nan = np.isnan(orientation)\n\n        # Unwrap orientation before smoothing\n        orientation[~is_nan] = np.unwrap(orientation[~is_nan])\n        orientation[~is_nan] = gaussian_smooth(\n            orientation[~is_nan],\n            orient_smoothing_std_dev,\n            sampling_rate,\n            axis=0,\n            truncate=8,\n        )\n        # convert back to between -pi and pi\n        orientation[~is_nan] = np.angle(np.exp(1j * orientation[~is_nan]))\n\n        velocity = get_velocity(\n            position,\n            time=time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )  # cm/s\n        speed = np.sqrt(np.sum(velocity**2, axis=1))  # cm/s\n\n        return {\n            \"time\": time,\n            \"position\": position,\n            \"orientation\": orientation,\n            \"velocity\": velocity,\n            \"speed\": speed,\n        }\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(nwb_data[\"position\"].get_spatial_series().timestamps),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"position_x\",\n            \"position_y\",\n            \"orientation\",\n            \"velocity_x\",\n            \"velocity_y\",\n            \"speed\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"velocity\"]\n                        .time_series[\"video_frame_ind\"]\n                        .data,\n                        dtype=int,\n                    )[:, np.newaxis],\n                    np.asarray(nwb_data[\"position\"].get_spatial_series().data),\n                    np.asarray(\n                        nwb_data[\"orientation\"].get_spatial_series().data\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"velocity\"].time_series[\"velocity\"].data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.get_video_path", "title": "<code>get_video_path(key)</code>", "text": "<p>Given nwb_file_name and interval_list_name returns specified video file filename and path</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Dictionary containing nwb_file_name and interval_list_name as keys</p> required <p>Returns:</p> Name Type Description <code>video_filepath</code> <code>str</code> <p>path to the video file, including video filename</p> <code>video_filename</code> <code>str</code> <p>filename of the video</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_video_path(key):\n\"\"\"\n    Given nwb_file_name and interval_list_name returns specified\n    video file filename and path\n\n    Parameters\n    ----------\n    key : dict\n        Dictionary containing nwb_file_name and interval_list_name as keys\n\n    Returns\n    -------\n    video_filepath : str\n        path to the video file, including video filename\n    video_filename : str\n        filename of the video\n    \"\"\"\n    import pynwb\n\n    from ...common.common_behav import VideoFile\n\n    video_info = (\n        VideoFile()\n        &amp; {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": key[\"epoch\"]}\n    ).fetch1()\n    nwb_path = (\n        f\"{os.getenv('SPYGLASS_BASE_DIR')}/raw/{video_info['nwb_file_name']}\"\n    )\n    with pynwb.NWBHDF5IO(path=nwb_path, mode=\"r\") as in_out:\n        nwb_file = in_out.read()\n        nwb_video = nwb_file.objects[video_info[\"video_file_object_id\"]]\n        video_filepath = VideoFile.get_abs_path(\n            {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": key[\"epoch\"]}\n        )\n        video_dir = os.path.dirname(video_filepath) + \"/\"\n        video_filename = video_filepath.split(video_dir)[-1]\n        meters_per_pixel = nwb_video.device.meters_per_pixel\n        timestamps = np.asarray(nwb_video.timestamps)\n    return video_dir, video_filename, meters_per_pixel, timestamps\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.PositionVideo", "title": "<code>PositionVideo</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Creates a video of the computed head position and orientation as well as the original LED positions overlaid on the video of the animal.</p> <p>Use for debugging the effect of position extraction parameters.</p> Source code in <code>src/spyglass/position/position_merge.py</code> <pre><code>@schema\nclass PositionVideo(dj.Computed):\n\"\"\"Creates a video of the computed head position and orientation as well as\n    the original LED positions overlaid on the video of the animal.\n\n    Use for debugging the effect of position extraction parameters.\"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionVideoSelection\n    ---\n    \"\"\"\n\n    def make(self, key):\n        assert key[\"plot\"] in [\"DLC\", \"Trodes\", \"Common\", \"All\"]\n        M_TO_CM = 100\n        output_dir, position_ids = (PositionVideoSelection &amp; key).fetch1(\n            \"output_dir\", \"position_ids\"\n        )\n\n        print(\"Loading position data...\")\n        # raw_position_df = (\n        #     RawPosition()\n        #     &amp; {\n        #         \"nwb_file_name\": key[\"nwb_file_name\"],\n        #         \"interval_list_name\": key[\"interval_list_name\"],\n        #     }\n        # ).fetch1_dataframe()\n        query = {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": key[\"interval_list_name\"],\n        }\n        if key[\"plot\"] == \"DLC\":\n            assert position_ids[\"dlc_position_id\"]\n            pos_df = (\n                PositionOutput()\n                &amp; {\n                    **query,\n                    \"source\": \"DLC\",\n                    \"position_id\": position_ids[\"dlc_position_id\"],\n                }\n            ).fetch1_dataframe()\n        elif key[\"plot\"] == \"Trodes\":\n            assert position_ids[\"trodes_position_id\"]\n            pos_df = (\n                PositionOutput()\n                &amp; {\n                    **query,\n                    \"source\": \"Trodes\",\n                    \"position_id\": position_ids[\"trodes_position_id\"],\n                }\n            ).fetch1_dataframe()\n        elif key[\"plot\"] == \"Common\":\n            assert position_ids[\"common_position_id\"]\n            pos_df = (\n                PositionOutput()\n                &amp; {\n                    **query,\n                    \"source\": \"Common\",\n                    \"position_id\": position_ids[\"common_position_id\"],\n                }\n            ).fetch1_dataframe()\n        elif key[\"plot\"] == \"All\":\n            # Check which entries exist in PositionOutput\n            merge_dict = {}\n            if \"dlc_position_id\" in position_ids:\n                if (\n                    len(\n                        PositionOutput()\n                        &amp; {\n                            **query,\n                            \"source\": \"DLC\",\n                            \"position_id\": position_ids[\"dlc_position_id\"],\n                        }\n                    )\n                    &gt; 0\n                ):\n                    dlc_df = (\n                        (\n                            PositionOutput()\n                            &amp; {\n                                **query,\n                                \"source\": \"DLC\",\n                                \"position_id\": position_ids[\"dlc_position_id\"],\n                            }\n                        )\n                        .fetch1_dataframe()\n                        .drop(columns=[\"velocity_x\", \"velocity_y\", \"speed\"])\n                    )\n                    merge_dict[\"DLC\"] = dlc_df\n            if \"trodes_position_id\" in position_ids:\n                if (\n                    len(\n                        PositionOutput()\n                        &amp; {\n                            **query,\n                            \"source\": \"Trodes\",\n                            \"position_id\": position_ids[\"trodes_position_id\"],\n                        }\n                    )\n                    &gt; 0\n                ):\n                    trodes_df = (\n                        (\n                            PositionOutput()\n                            &amp; {\n                                **query,\n                                \"source\": \"Trodes\",\n                                \"position_id\": position_ids[\n                                    \"trodes_position_id\"\n                                ],\n                            }\n                        )\n                        .fetch1_dataframe()\n                        .drop(columns=[\"velocity_x\", \"velocity_y\", \"speed\"])\n                    )\n                    merge_dict[\"Trodes\"] = trodes_df\n            if \"common_position_id\" in position_ids:\n                if (\n                    len(\n                        PositionOutput()\n                        &amp; {\n                            **query,\n                            \"source\": \"Common\",\n                            \"position_id\": position_ids[\"common_position_id\"],\n                        }\n                    )\n                    &gt; 0\n                ):\n                    common_df = (\n                        (\n                            PositionOutput()\n                            &amp; {\n                                **query,\n                                \"source\": \"Common\",\n                                \"position_id\": position_ids[\n                                    \"common_position_id\"\n                                ],\n                            }\n                        )\n                        .fetch1_dataframe()\n                        .drop(columns=[\"velocity_x\", \"velocity_y\", \"speed\"])\n                    )\n                    merge_dict[\"Common\"] = common_df\n            pos_df = ft.reduce(\n                lambda left, right,: pd.merge(\n                    left[1],\n                    right[1],\n                    left_index=True,\n                    right_index=True,\n                    suffixes=[f\"_{left[0]}\", f\"_{right[0]}\"],\n                ),\n                merge_dict.items(),\n            )\n        print(\"Loading video data...\")\n        epoch = (\n            int(\n                key[\"interval_list_name\"]\n                .replace(\"pos \", \"\")\n                .replace(\" valid times\", \"\")\n            )\n            + 1\n        )\n\n        (\n            video_path,\n            video_filename,\n            meters_per_pixel,\n            video_time,\n        ) = get_video_path(\n            {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": epoch}\n        )\n        video_dir = os.path.dirname(video_path) + \"/\"\n        video_frame_col_name = [\n            col for col in pos_df.columns if \"video_frame_ind\" in col\n        ]\n        video_frame_inds = (\n            pos_df[video_frame_col_name[0]].astype(int).to_numpy()\n        )\n        if key[\"plot\"] in [\"DLC\", \"All\"]:\n            temp_key = (PositionOutput.DLCPosV1 &amp; key).fetch1(\"KEY\")\n            video_path = (DLCPoseEstimationSelection &amp; temp_key).fetch1(\n                \"video_path\"\n            )\n        else:\n            video_path = check_videofile(\n                video_dir, key[\"output_dir\"], video_filename\n            )[0]\n\n        nwb_base_filename = key[\"nwb_file_name\"].replace(\".nwb\", \"\")\n        output_video_filename = Path(\n            f\"{Path(output_dir).as_posix()}/{nwb_base_filename}{epoch:02d}_\"\n            f\"{key['plot']}_pos_overlay.mp4\"\n        ).as_posix()\n\n        # centroids = {'red': np.asarray(raw_position_df[['xloc', 'yloc']]),\n        #              'green':  np.asarray(raw_position_df[['xloc2', 'yloc2']])}\n        position_mean_dict = {}\n        if key[\"plot\"] in [\"DLC\", \"Trodes\", \"Common\"]:\n            position_mean_dict[key[\"plot\"]][\"position\"] = np.asarray(\n                pos_df[[\"position_x\", \"position_y\"]]\n            )\n            position_mean_dict[key[\"plot\"]][\"orientation\"] = np.asarray(\n                pos_df[[\"orientation\"]]\n            )\n        elif key[\"plot\"] == \"All\":\n            position_mean_dict = {\n                source: {\n                    \"position\": np.asarray(\n                        pos_df[[f\"position_x_{source}\", f\"position_y_{source}\"]]\n                    ),\n                    \"orientation\": np.asarray(\n                        pos_df[[f\"orientation_{source}\"]]\n                    ),\n                }\n                for source in merge_dict.keys()\n            }\n        position_time = np.asarray(pos_df.index)\n        cm_per_pixel = meters_per_pixel * M_TO_CM\n        print(\"Making video...\")\n\n        make_video(\n            video_path,\n            video_frame_inds,\n            position_mean_dict,\n            video_time,\n            position_time,\n            processor=\"opencv\",\n            output_video_filename=output_video_filename,\n            cm_to_pixels=cm_per_pixel,\n            disable_progressbar=False,\n        )\n        self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/position_merge/#src.spyglass.position.position_merge.check_videofile", "title": "<code>check_videofile(video_path, output_path=os.getenv('DLC_VIDEO_PATH'), video_filename=None, video_filetype='h264')</code>", "text": "<p>Checks the file extension of a video file to make sure it is .mp4 for DeepLabCut processes. Converts to MP4 if not already.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str or PosixPath object</code> <p>path to directory of the existing video file without filename</p> required <code>output_path</code> <code>str or PosixPath object</code> <p>path to directory where converted video will be saved</p> <code>os.getenv('DLC_VIDEO_PATH')</code> <code>video_filename</code> <code>str, Optional</code> <p>filename of the video to convert, if not provided, video_filetype must be and all video files of video_filetype in the directory will be converted</p> <code>None</code> <code>video_filetype</code> <code>str or List, Default 'h264', Optional</code> <p>If video_filename is not provided, all videos of this filetype will be converted to .mp4</p> <code>'h264'</code> <p>Returns:</p> Name Type Description <code>output_files</code> <code>List of PosixPath objects</code> <p>paths to converted video file(s)</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def check_videofile(\n    video_path: Union[str, pathlib.PosixPath],\n    output_path: Union[str, pathlib.PosixPath] = os.getenv(\"DLC_VIDEO_PATH\"),\n    video_filename: str = None,\n    video_filetype: str = \"h264\",\n):\n\"\"\"\n    Checks the file extension of a video file to make sure it is .mp4 for\n    DeepLabCut processes. Converts to MP4 if not already.\n\n    Parameters\n    ----------\n    video_path : str or PosixPath object\n        path to directory of the existing video file without filename\n    output_path : str or PosixPath object\n        path to directory where converted video will be saved\n    video_filename : str, Optional\n        filename of the video to convert, if not provided, video_filetype must be\n        and all video files of video_filetype in the directory will be converted\n    video_filetype : str or List, Default 'h264', Optional\n        If video_filename is not provided,\n        all videos of this filetype will be converted to .mp4\n\n    Returns\n    -------\n    output_files : List of PosixPath objects\n        paths to converted video file(s)\n    \"\"\"\n\n    if not video_filename:\n        video_files = pathlib.Path(video_path).glob(f\"*.{video_filetype}\")\n    else:\n        video_files = [pathlib.Path(f\"{video_path}/{video_filename}\")]\n    output_files = []\n    for video_filepath in video_files:\n        if video_filepath.exists():\n            if video_filepath.suffix == \".mp4\":\n                output_files.append(video_filepath)\n                continue\n        video_file = (\n            video_filepath.as_posix()\n            .rsplit(video_filepath.parent.as_posix(), maxsplit=1)[-1]\n            .split(\"/\")[-1]\n        )\n        output_files.append(\n            _convert_mp4(video_file, video_path, output_path, videotype=\"mp4\")\n        )\n    return output_files\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_decorators/", "title": "dlc_decorators.py", "text": ""}, {"location": "api/src/spyglass/position/v1/dlc_reader/", "title": "dlc_reader.py", "text": ""}, {"location": "api/src/spyglass/position/v1/dlc_reader/#src.spyglass.position.v1.dlc_reader.read_yaml", "title": "<code>read_yaml(fullpath, filename='*')</code>", "text": "<p>Return contents of yml in fullpath. If available, defer to DJ-saved version</p> <p>Parameters:</p> Name Type Description Default <code>fullpath</code> required <code>filename</code> <code>'*'</code> <p>Returns filepath and contents as dict</p> Source code in <code>src/spyglass/position/v1/dlc_reader.py</code> <pre><code>def read_yaml(fullpath, filename=\"*\"):\n\"\"\"Return contents of yml in fullpath. If available, defer to DJ-saved version\n\n    Parameters\n    ----------\n    fullpath: String or pathlib path. Directory with yaml files\n    filename: String. Filename, no extension. Permits wildcards.\n\n    Returns filepath and contents as dict\n    \"\"\"\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    # Take the DJ-saved if there. If not, return list of available\n    yml_paths = list(Path(fullpath).glob(\"dj_dlc_config.yaml\")) or sorted(\n        list(Path(fullpath).glob(f\"{filename}.y*ml\"))\n    )\n\n    assert (  # If more than 1 and not DJ-saved,\n        len(yml_paths) == 1\n    ), f\"Found more yaml files than expected: {len(yml_paths)}\\n{fullpath}\"\n\n    return yml_paths[0], read_config(yml_paths[0])\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_reader/#src.spyglass.position.v1.dlc_reader.save_yaml", "title": "<code>save_yaml(output_dir, config_dict, filename='dj_dlc_config', mkdir=True)</code>", "text": "<p>Save config_dict to output_path as filename.yaml. By default, preserves original.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> required <code>config_dict</code> required <code>filename</code> <p>Set to 'config' to overwrite original file. If extension is included, removed and replaced with \"yaml\".</p> <code>'dj_dlc_config'</code> <code>mkdir</code> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>path of saved file as string - due to DLC func preference for strings</p> Source code in <code>src/spyglass/position/v1/dlc_reader.py</code> <pre><code>def save_yaml(output_dir, config_dict, filename=\"dj_dlc_config\", mkdir=True):\n\"\"\"Save config_dict to output_path as filename.yaml. By default, preserves original.\n\n    Parameters\n    ----------\n    output_dir: where to save yaml file\n    config_dict: dict of config params or element-deeplabcut model.Model dict\n    filename: Optional, default 'dj_dlc_config' or preserve original 'config'\n              Set to 'config' to overwrite original file.\n              If extension is included, removed and replaced with \"yaml\".\n    mkdir (bool): Optional, True. Make new directory if output_dir not exist\n\n    Returns\n    -------\n    str\n        path of saved file as string - due to DLC func preference for strings\n    \"\"\"\n    from deeplabcut.utils.auxiliaryfunctions import write_config\n\n    if \"config_template\" in config_dict:  # if passed full model.Model dict\n        config_dict = config_dict[\"config_template\"]\n    if mkdir:\n        Path(output_dir).mkdir(exist_ok=True)\n    if \".\" in filename:  # if user provided extension, remove\n        filename = filename.split(\".\")[0]\n\n    output_filepath = Path(output_dir) / f\"{filename}.yaml\"\n    write_config(output_filepath, config_dict)\n    return str(output_filepath)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_reader/#src.spyglass.position.v1.dlc_reader.do_pose_estimation", "title": "<code>do_pose_estimation(video_filepaths, dlc_model, project_path, output_dir, videotype='', gputouse=None, save_as_csv=False, batchsize=None, cropping=None, TFGPUinference=True, dynamic=(False, 0.5, 10), robust_nframes=False, allow_growth=False, use_shelve=False)</code>", "text": "<p>Launch DLC's analyze_videos within element-deeplabcut</p> <p>Other optional parameters may be set other than those described below. See deeplabcut.analyze_videos parameters for descriptions/defaults.</p> <p>Parameters:</p> Name Type Description Default <code>video_filepaths</code> required <code>dlc_model</code> required <code>project_path</code> required <code>output_dir</code> required Source code in <code>src/spyglass/position/v1/dlc_reader.py</code> <pre><code>def do_pose_estimation(\n    video_filepaths,\n    dlc_model,\n    project_path,\n    output_dir,\n    videotype=\"\",\n    gputouse=None,\n    save_as_csv=False,\n    batchsize=None,\n    cropping=None,\n    TFGPUinference=True,\n    dynamic=(False, 0.5, 10),\n    robust_nframes=False,\n    allow_growth=False,\n    use_shelve=False,\n):\n\"\"\"Launch DLC's analyze_videos within element-deeplabcut\n\n    Other optional parameters may be set other than those described below. See\n    deeplabcut.analyze_videos parameters for descriptions/defaults.\n\n    Parameters\n    ----------\n    video_filepaths: list of videos to analyze\n    dlc_model: element-deeplabcut dlc.Model dict\n    project_path: path to project config.yml\n    output_dir: where to save output\n    \"\"\"\n    from deeplabcut.pose_estimation_tensorflow import analyze_videos\n\n    # ---- Build and save DLC configuration (yaml) file ----\n    dlc_config = dlc_model[\"config_template\"]\n    dlc_project_path = Path(project_path)\n    dlc_config[\"project_path\"] = dlc_project_path.as_posix()\n\n    # ---- Write config files ----\n    # To output dir: Important for loading/parsing output in datajoint\n    _ = save_yaml(output_dir, dlc_config)\n    # To project dir: Required by DLC to run the analyze_videos\n    if dlc_project_path != output_dir:\n        config_filepath = save_yaml(dlc_project_path, dlc_config)\n    # ---- Trigger DLC prediction job ----\n    analyze_videos(\n        config=config_filepath,\n        videos=video_filepaths,\n        shuffle=dlc_model[\"shuffle\"],\n        trainingsetindex=dlc_model[\"trainingsetindex\"],\n        destfolder=output_dir,\n        modelprefix=dlc_model[\"model_prefix\"],\n        videotype=videotype,\n        gputouse=gputouse,\n        save_as_csv=save_as_csv,\n        batchsize=batchsize,\n        cropping=cropping,\n        TFGPUinference=TFGPUinference,\n        dynamic=dynamic,\n        robust_nframes=robust_nframes,\n        allow_growth=allow_growth,\n        use_shelve=use_shelve,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/", "title": "dlc_utils.py", "text": ""}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.OutputLogger", "title": "<code>OutputLogger</code>", "text": "<p>A class to wrap a logging.Logger object in order to provide context manager capabilities.</p> <p>This class uses contextlib.redirect_stdout to temporarily redirect sys.stdout and thus print statements to the log file instead of, or as well as the console.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <code>logging.Logger</code> <p>logger object</p> <code>name</code> <code>str</code> <p>name of logger</p> <code>level</code> <code>int</code> <p>level of logging that the logger is set to handle</p>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.OutputLogger--methods", "title": "Methods", "text": "<p>setup_logger(name_logfile, path_logfile, print_console=False)     initialize or get logger object with name_logfile     that writes to path_logfile</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with OutputLogger(name, path, print_console=True) as logger:\n...    print(\"this will print to logfile\")\n...    logger.logger.info(\"this will log to the logfile\")\n... print(\"this will print to the console\")\n... logger.logger.info(\"this will log to the logfile\")\n</code></pre> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>class OutputLogger:\n\"\"\"\n    A class to wrap a logging.Logger object in order to provide context manager capabilities.\n\n    This class uses contextlib.redirect_stdout to temporarily redirect sys.stdout and thus\n    print statements to the log file instead of, or as well as the console.\n\n    Attributes\n    ----------\n    logger : logging.Logger\n        logger object\n    name : str\n        name of logger\n    level : int\n        level of logging that the logger is set to handle\n\n    Methods\n    -------\n    setup_logger(name_logfile, path_logfile, print_console=False)\n        initialize or get logger object with name_logfile\n        that writes to path_logfile\n\n    Examples\n    --------\n    &gt;&gt;&gt; with OutputLogger(name, path, print_console=True) as logger:\n    ...    print(\"this will print to logfile\")\n    ...    logger.logger.info(\"this will log to the logfile\")\n    ... print(\"this will print to the console\")\n    ... logger.logger.info(\"this will log to the logfile\")\n\n    \"\"\"\n\n    def __init__(self, name, path, level=\"INFO\", **kwargs):\n        self.logger = self.setup_logger(name, path, **kwargs)\n        self.name = self.logger.name\n        self.level = getattr(logging, level)\n\n    def setup_logger(\n        self, name_logfile, path_logfile, print_console=False\n    ) -&gt; logging.Logger:\n\"\"\"\n        Sets up a logger for that outputs to a file, and optionally, the console\n\n        Parameters\n        ----------\n        name_logfile : str\n            name of the logfile to use\n        path_logfile : str\n            path to the file that should be used as the file handler\n        print_console : bool, default-False\n            if True, prints to console as well as log file.\n\n        Returns\n        -------\n        logger : logging.Logger\n            the logger object with specified handlers\n        \"\"\"\n\n        logger = logging.getLogger(name_logfile)\n        # check to see if handlers already exist for this logger\n        if logger.handlers:\n            for handler in logger.handlers:\n                # if it's a file handler\n                # type is used instead of isinstance,\n                # which doesn't work properly with logging.StreamHandler\n                if type(handler) == logging.FileHandler:\n                    # if paths don't match, change file handler path\n                    if not os.path.samefile(handler.baseFilename, path_logfile):\n                        handler.close()\n                        logger.removeHandler(handler)\n                        file_handler = self._get_file_handler(path_logfile)\n                        logger.addHandler(file_handler)\n                # if a stream handler exists and\n                # if print_console is False remove streamHandler\n                if type(handler) == logging.StreamHandler:\n                    if not print_console:\n                        handler.close()\n                        logger.removeHandler(handler)\n            if print_console and not any(\n                type(handler) == logging.StreamHandler\n                for handler in logger.handlers\n            ):\n                logger.addHandler(self._get_stream_handler())\n\n        else:\n            file_handler = self._get_file_handler(path_logfile)\n            logger.addHandler(file_handler)\n            if print_console:\n                logger.addHandler(self._get_stream_handler())\n        logger.setLevel(logging.INFO)\n        return logger\n\n    def _get_file_handler(self, path):\n        output_dir = pathlib.Path(os.path.dirname(path))\n        if not os.path.exists(output_dir):\n            output_dir.mkdir(parents=True, exist_ok=True)\n        file_handler = logging.FileHandler(path, mode=\"a\")\n        file_handler.setFormatter(self._get_formatter())\n        return file_handler\n\n    def _get_stream_handler(self):\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(self._get_formatter())\n        return stream_handler\n\n    def _get_formatter(self):\n        return logging.Formatter(\n            \"[%(asctime)s] in %(pathname)s, line %(lineno)d: %(message)s\",\n            datefmt=\"%d-%b-%y %H:%M:%S\",\n        )\n\n    def write(self, msg):\n        if msg and not msg.isspace():\n            self.logger.log(self.level, msg)\n\n    def flush(self):\n        pass\n\n    def __enter__(self):\n        self._redirector = redirect_stdout(self)\n        self._redirector.__enter__()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        # let contextlib do any exception handling here\n        self._redirector.__exit__(exc_type, exc_value, traceback)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.OutputLogger.setup_logger", "title": "<code>setup_logger(name_logfile, path_logfile, print_console=False)</code>", "text": "<p>Sets up a logger for that outputs to a file, and optionally, the console</p> <p>Parameters:</p> Name Type Description Default <code>name_logfile</code> <code>str</code> <p>name of the logfile to use</p> required <code>path_logfile</code> <code>str</code> <p>path to the file that should be used as the file handler</p> required <code>print_console</code> <code>bool, default-False</code> <p>if True, prints to console as well as log file.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>logger</code> <code>logging.Logger</code> <p>the logger object with specified handlers</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def setup_logger(\n    self, name_logfile, path_logfile, print_console=False\n) -&gt; logging.Logger:\n\"\"\"\n    Sets up a logger for that outputs to a file, and optionally, the console\n\n    Parameters\n    ----------\n    name_logfile : str\n        name of the logfile to use\n    path_logfile : str\n        path to the file that should be used as the file handler\n    print_console : bool, default-False\n        if True, prints to console as well as log file.\n\n    Returns\n    -------\n    logger : logging.Logger\n        the logger object with specified handlers\n    \"\"\"\n\n    logger = logging.getLogger(name_logfile)\n    # check to see if handlers already exist for this logger\n    if logger.handlers:\n        for handler in logger.handlers:\n            # if it's a file handler\n            # type is used instead of isinstance,\n            # which doesn't work properly with logging.StreamHandler\n            if type(handler) == logging.FileHandler:\n                # if paths don't match, change file handler path\n                if not os.path.samefile(handler.baseFilename, path_logfile):\n                    handler.close()\n                    logger.removeHandler(handler)\n                    file_handler = self._get_file_handler(path_logfile)\n                    logger.addHandler(file_handler)\n            # if a stream handler exists and\n            # if print_console is False remove streamHandler\n            if type(handler) == logging.StreamHandler:\n                if not print_console:\n                    handler.close()\n                    logger.removeHandler(handler)\n        if print_console and not any(\n            type(handler) == logging.StreamHandler\n            for handler in logger.handlers\n        ):\n            logger.addHandler(self._get_stream_handler())\n\n    else:\n        file_handler = self._get_file_handler(path_logfile)\n        logger.addHandler(file_handler)\n        if print_console:\n            logger.addHandler(self._get_stream_handler())\n    logger.setLevel(logging.INFO)\n    return logger\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.get_dlc_processed_data_dir", "title": "<code>get_dlc_processed_data_dir()</code>", "text": "<p>Returns session_dir relative to custom 'dlc_output_dir' root</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_dlc_processed_data_dir() -&gt; str:\n\"\"\"Returns session_dir relative to custom 'dlc_output_dir' root\"\"\"\n    if \"custom\" in dj.config:\n        if \"dlc_output_dir\" in dj.config[\"custom\"]:\n            dlc_output_dir = dj.config.get(\"custom\", {}).get(\"dlc_output_dir\")\n    if dlc_output_dir:\n        return pathlib.Path(dlc_output_dir)\n    else:\n        return pathlib.Path(\"/nimbus/deeplabcut/output/\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.find_full_path", "title": "<code>find_full_path(root_directories, relative_path)</code>", "text": "<p>from Datajoint Elements - unused Given a relative path, search and return the full-path  from provided potential root directories (in the given order)     :param root_directories: potential root directories     :param relative_path: the relative path to find the valid root directory     :return: full-path (pathlib.Path object)</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def find_full_path(root_directories, relative_path):\n\"\"\"\n    from Datajoint Elements - unused\n    Given a relative path, search and return the full-path\n     from provided potential root directories (in the given order)\n        :param root_directories: potential root directories\n        :param relative_path: the relative path to find the valid root directory\n        :return: full-path (pathlib.Path object)\n    \"\"\"\n    relative_path = _to_Path(relative_path)\n\n    if relative_path.exists():\n        return relative_path\n\n    # Turn to list if only a single root directory is provided\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [_to_Path(root_directories)]\n\n    for root_dir in root_directories:\n        if (_to_Path(root_dir) / relative_path).exists():\n            return _to_Path(root_dir) / relative_path\n\n    raise FileNotFoundError(\n        f\"No valid full-path found (from {root_directories})\"\n        f\" for {relative_path}\"\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.find_root_directory", "title": "<code>find_root_directory(root_directories, full_path)</code>", "text": "<p>From datajoint elements - unused Given multiple potential root directories and a full-path, search and return one directory that is the parent of the given path     :param root_directories: potential root directories     :param full_path: the full path to search the root directory     :return: root_directory (pathlib.Path object)</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def find_root_directory(root_directories, full_path):\n\"\"\"\n    From datajoint elements - unused\n    Given multiple potential root directories and a full-path,\n    search and return one directory that is the parent of the given path\n        :param root_directories: potential root directories\n        :param full_path: the full path to search the root directory\n        :return: root_directory (pathlib.Path object)\n    \"\"\"\n    full_path = _to_Path(full_path)\n\n    if not full_path.exists():\n        raise FileNotFoundError(f\"{full_path} does not exist!\")\n\n    # Turn to list if only a single root directory is provided\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [_to_Path(root_directories)]\n\n    try:\n        return next(\n            _to_Path(root_dir)\n            for root_dir in root_directories\n            if _to_Path(root_dir) in set(full_path.parents)\n        )\n\n    except StopIteration as exc:\n        raise FileNotFoundError(\n            f\"No valid root directory found (from {root_directories})\"\n            f\" for {full_path}\"\n        ) from exc\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.infer_output_dir", "title": "<code>infer_output_dir(key, makedir=True)</code>", "text": "<p>Return the expected pose_estimation_output_dir.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> required Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def infer_output_dir(key, makedir=True):\n\"\"\"Return the expected pose_estimation_output_dir.\n\n    Parameters\n    ----------\n    key: DataJoint key specifying a pairing of VideoFile and Model.\n    \"\"\"\n    # TODO: add check to make sure interval_list_name refers to a single epoch\n    # Or make key include epoch in and of itself instead of interval_list_name\n    nwb_file_name = key[\"nwb_file_name\"].split(\"_.\")[0]\n    output_dir = pathlib.Path(os.getenv(\"DLC_OUTPUT_PATH\")) / pathlib.Path(\n        f\"{nwb_file_name}/{nwb_file_name}_{key['epoch']:02}\"\n        f\"_model_\" + key[\"dlc_model_name\"].replace(\" \", \"-\")\n    )\n    if makedir is True:\n        if not os.path.exists(output_dir):\n            output_dir.mkdir(parents=True, exist_ok=True)\n    return output_dir\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.get_video_path", "title": "<code>get_video_path(key)</code>", "text": "<p>Given nwb_file_name and interval_list_name returns specified video file filename and path</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Dictionary containing nwb_file_name and interval_list_name as keys</p> required <p>Returns:</p> Name Type Description <code>video_filepath</code> <code>str</code> <p>path to the video file, including video filename</p> <code>video_filename</code> <code>str</code> <p>filename of the video</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_video_path(key):\n\"\"\"\n    Given nwb_file_name and interval_list_name returns specified\n    video file filename and path\n\n    Parameters\n    ----------\n    key : dict\n        Dictionary containing nwb_file_name and interval_list_name as keys\n\n    Returns\n    -------\n    video_filepath : str\n        path to the video file, including video filename\n    video_filename : str\n        filename of the video\n    \"\"\"\n    import pynwb\n\n    from ...common.common_behav import VideoFile\n\n    video_info = (\n        VideoFile()\n        &amp; {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": key[\"epoch\"]}\n    ).fetch1()\n    nwb_path = (\n        f\"{os.getenv('SPYGLASS_BASE_DIR')}/raw/{video_info['nwb_file_name']}\"\n    )\n    with pynwb.NWBHDF5IO(path=nwb_path, mode=\"r\") as in_out:\n        nwb_file = in_out.read()\n        nwb_video = nwb_file.objects[video_info[\"video_file_object_id\"]]\n        video_filepath = VideoFile.get_abs_path(\n            {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": key[\"epoch\"]}\n        )\n        video_dir = os.path.dirname(video_filepath) + \"/\"\n        video_filename = video_filepath.split(video_dir)[-1]\n        meters_per_pixel = nwb_video.device.meters_per_pixel\n        timestamps = np.asarray(nwb_video.timestamps)\n    return video_dir, video_filename, meters_per_pixel, timestamps\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.check_videofile", "title": "<code>check_videofile(video_path, output_path=os.getenv('DLC_VIDEO_PATH'), video_filename=None, video_filetype='h264')</code>", "text": "<p>Checks the file extension of a video file to make sure it is .mp4 for DeepLabCut processes. Converts to MP4 if not already.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str or PosixPath object</code> <p>path to directory of the existing video file without filename</p> required <code>output_path</code> <code>str or PosixPath object</code> <p>path to directory where converted video will be saved</p> <code>os.getenv('DLC_VIDEO_PATH')</code> <code>video_filename</code> <code>str, Optional</code> <p>filename of the video to convert, if not provided, video_filetype must be and all video files of video_filetype in the directory will be converted</p> <code>None</code> <code>video_filetype</code> <code>str or List, Default 'h264', Optional</code> <p>If video_filename is not provided, all videos of this filetype will be converted to .mp4</p> <code>'h264'</code> <p>Returns:</p> Name Type Description <code>output_files</code> <code>List of PosixPath objects</code> <p>paths to converted video file(s)</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def check_videofile(\n    video_path: Union[str, pathlib.PosixPath],\n    output_path: Union[str, pathlib.PosixPath] = os.getenv(\"DLC_VIDEO_PATH\"),\n    video_filename: str = None,\n    video_filetype: str = \"h264\",\n):\n\"\"\"\n    Checks the file extension of a video file to make sure it is .mp4 for\n    DeepLabCut processes. Converts to MP4 if not already.\n\n    Parameters\n    ----------\n    video_path : str or PosixPath object\n        path to directory of the existing video file without filename\n    output_path : str or PosixPath object\n        path to directory where converted video will be saved\n    video_filename : str, Optional\n        filename of the video to convert, if not provided, video_filetype must be\n        and all video files of video_filetype in the directory will be converted\n    video_filetype : str or List, Default 'h264', Optional\n        If video_filename is not provided,\n        all videos of this filetype will be converted to .mp4\n\n    Returns\n    -------\n    output_files : List of PosixPath objects\n        paths to converted video file(s)\n    \"\"\"\n\n    if not video_filename:\n        video_files = pathlib.Path(video_path).glob(f\"*.{video_filetype}\")\n    else:\n        video_files = [pathlib.Path(f\"{video_path}/{video_filename}\")]\n    output_files = []\n    for video_filepath in video_files:\n        if video_filepath.exists():\n            if video_filepath.suffix == \".mp4\":\n                output_files.append(video_filepath)\n                continue\n        video_file = (\n            video_filepath.as_posix()\n            .rsplit(video_filepath.parent.as_posix(), maxsplit=1)[-1]\n            .split(\"/\")[-1]\n        )\n        output_files.append(\n            _convert_mp4(video_file, video_path, output_path, videotype=\"mp4\")\n        )\n    return output_files\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.get_gpu_memory", "title": "<code>get_gpu_memory()</code>", "text": "<p>Queries the gpu cluster and returns the memory use for each core. This is used to evaluate which GPU cores are available to run jobs on (i.e. pose estimation, DLC model training)</p> <p>Returns:</p> Name Type Description <code>memory_use_values</code> <code>dict</code> <p>dictionary with core number as key and memory in use as value.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if subproccess command errors.</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_gpu_memory():\n\"\"\"Queries the gpu cluster and returns the memory use for each core.\n    This is used to evaluate which GPU cores are available to run jobs on\n    (i.e. pose estimation, DLC model training)\n\n    Returns\n    -------\n    memory_use_values : dict\n        dictionary with core number as key and memory in use as value.\n\n    Raises\n    ------\n    RuntimeError\n        if subproccess command errors.\n    \"\"\"\n\n    output_to_list = lambda x: x.decode(\"ascii\").split(\"\\n\")[:-1]\n    query_cmd = \"nvidia-smi --query-gpu=memory.used --format=csv\"\n    try:\n        memory_use_info = output_to_list(\n            subprocess.check_output(query_cmd.split(), stderr=subprocess.STDOUT)\n        )[1:]\n    except subprocess.CalledProcessError as err:\n        raise RuntimeError(\n            f\"command {err.cmd} return with error (code {err.returncode}): {err.output}\"\n        ) from err\n    memory_use_values = {\n        i: int(x.split()[0]) for i, x in enumerate(memory_use_info)\n    }\n    return memory_use_values\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.get_span_start_stop", "title": "<code>get_span_start_stop(indices)</code>", "text": "<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Type Description <code>_type_</code> <p>description</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_span_start_stop(indices):\n\"\"\"_summary_\n\n    Parameters\n    ----------\n    indices : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    span_inds = []\n    # Get start and stop index of spans of consecutive indices\n    for k, g in groupby(enumerate(indices), lambda x: x[1] - x[0]):\n        group = list(map(itemgetter(1), g))\n        span_inds.append((group[0], group[-1]))\n    return span_inds\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/dlc_utils/#src.spyglass.position.v1.dlc_utils.convert_to_pixels", "title": "<code>convert_to_pixels(data, frame_size, cm_to_pixels=1.0)</code>", "text": "<p>Converts from cm to pixels and flips the y-axis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray, shape(n_time, 2)</code> required <code>frame_size</code> <code>array_like, shape(2)</code> required <code>cm_to_pixels</code> <code>float</code> <code>1.0</code> <p>Returns:</p> Name Type Description <code>converted_data</code> <code>ndarray, shape(n_time, 2)</code> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def convert_to_pixels(data, frame_size, cm_to_pixels=1.0):\n\"\"\"Converts from cm to pixels and flips the y-axis.\n    Parameters\n    ----------\n    data : ndarray, shape (n_time, 2)\n    frame_size : array_like, shape (2,)\n    cm_to_pixels : float\n\n    Returns\n    -------\n    converted_data : ndarray, shape (n_time, 2)\n    \"\"\"\n    return data / cm_to_pixels\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/", "title": "position_dlc_centroid.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.DLCSmoothInterpParams", "title": "<code>DLCSmoothInterpParams</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Parameters for extracting the smoothed head position.</p> <p>Attributes:</p> Name Type Description <code>interpolate</code> <code>bool, default True</code> <p>whether to interpolate over NaN spans</p> <code>smooth</code> <code>bool, default True</code> <p>whether to smooth the dataset</p> <code>smoothing_params</code> <code>dict</code> <p>smoothing_duration : float, default 0.05 number of frames to smooth over: sampling_rate*smoothing_duration = num_frames</p> <code>interp_params</code> <code>dict</code> <p>max_cm_to_interp : int, default 20 maximum distance between high likelihood points on either side of a NaN span to interpolate over</p> <code>likelihood_thresh</code> <code>float, default 0.95</code> <p>likelihood below which to NaN and interpolate over</p> Source code in <code>src/spyglass/position/v1/position_dlc_position.py</code> <pre><code>@schema\nclass DLCSmoothInterpParams(dj.Manual):\n\"\"\"\n    Parameters for extracting the smoothed head position.\n\n    Attributes\n    ----------\n    interpolate : bool, default True\n        whether to interpolate over NaN spans\n    smooth : bool, default True\n        whether to smooth the dataset\n    smoothing_params : dict\n        smoothing_duration : float, default 0.05\n            number of frames to smooth over: sampling_rate*smoothing_duration = num_frames\n    interp_params : dict\n        max_cm_to_interp : int, default 20\n            maximum distance between high likelihood points on either side of a NaN span\n            to interpolate over\n    likelihood_thresh : float, default 0.95\n        likelihood below which to NaN and interpolate over\n    \"\"\"\n\n    definition = \"\"\"\n    dlc_si_params_name : varchar(80) # name for this set of parameters\n    ---\n    params: longblob # dictionary of parameters\n    \"\"\"\n\n    @classmethod\n    def insert_params(cls, params_name: str, params: dict, **kwargs):\n        cls.insert1(\n            {\"dlc_si_params_name\": params_name, \"params\": params},\n            **kwargs,\n        )\n\n    @classmethod\n    def insert_default(cls, **kwargs):\n        default_params = {\n            \"smooth\": True,\n            \"smoothing_params\": {\n                \"smoothing_duration\": 0.05,\n                \"smooth_method\": \"moving_avg\",\n            },\n            \"interpolate\": True,\n            \"likelihood_thresh\": 0.95,\n            \"interp_params\": {\"max_cm_to_interp\": 15},\n            \"max_cm_between_pts\": 20,\n            # This is for use when finding \"good spans\" and is how many indices to bridge in between good spans\n            # see inds_to_span in get_good_spans\n            \"num_inds_to_span\": 20,\n        }\n        cls.insert1(\n            {\"dlc_si_params_name\": \"default\", \"params\": default_params},\n            **kwargs,\n        )\n\n    @classmethod\n    def insert_nan_params(cls, **kwargs):\n        nan_params = {\n            \"smooth\": False,\n            \"interpolate\": False,\n            \"likelihood_thresh\": 0.95,\n            \"max_cm_between_pts\": 20,\n            \"num_inds_to_span\": 20,\n        }\n        cls.insert1(\n            {\"dlc_si_params_name\": \"just_nan\", \"params\": nan_params}, **kwargs\n        )\n\n    @classmethod\n    def get_default(cls):\n        query = cls &amp; {\"dlc_si_params_name\": \"default\"}\n        if not len(query) &gt; 0:\n            cls().insert_default(skip_duplicates=True)\n            default = (cls &amp; {\"dlc_si_params_name\": \"default\"}).fetch1()\n        else:\n            default = query.fetch1()\n        return default\n\n    @classmethod\n    def get_nan_params(cls):\n        query = cls &amp; {\"dlc_si_params_name\": \"just_nan\"}\n        if not len(query) &gt; 0:\n            cls().insert_nan_params(skip_duplicates=True)\n            nan_params = (cls &amp; {\"dlc_si_params_name\": \"just_nan\"}).fetch1()\n        else:\n            nan_params = query.fetch1()\n        return nan_params\n\n    @staticmethod\n    def get_available_methods():\n        return _key_to_smooth_func_dict.keys()\n\n    def insert1(self, key, **kwargs):\n        if \"params\" in key:\n            if not \"max_cm_between_pts\" in key[\"params\"]:\n                raise KeyError(\"max_cm_between_pts is a required parameter\")\n            if \"smooth\" in key[\"params\"]:\n                if key[\"params\"][\"smooth\"]:\n                    if \"smoothing_params\" in key[\"params\"]:\n                        if \"smooth_method\" in key[\"params\"][\"smoothing_params\"]:\n                            smooth_method = key[\"params\"][\"smoothing_params\"][\n                                \"smooth_method\"\n                            ]\n                            if smooth_method not in _key_to_smooth_func_dict:\n                                raise KeyError(\n                                    f\"smooth_method: {smooth_method} not an available method.\"\n                                )\n                        if (\n                            not \"smoothing_duration\"\n                            in key[\"params\"][\"smoothing_params\"]\n                        ):\n                            raise KeyError(\n                                \"smoothing_duration must be passed as a smoothing_params within key['params']\"\n                            )\n                        else:\n                            assert isinstance(\n                                key[\"params\"][\"smoothing_params\"][\n                                    \"smoothing_duration\"\n                                ],\n                                (float, int),\n                            ), \"smoothing_duration must be a float or int\"\n                    else:\n                        raise ValueError(\n                            \"smoothing_params not in key['params']\"\n                        )\n            if \"likelihood_thresh\" in key[\"params\"]:\n                assert isinstance(\n                    key[\"params\"][\"likelihood_thresh\"],\n                    float,\n                ), \"likelihood_thresh must be a float\"\n                assert (\n                    0 &lt; key[\"params\"][\"likelihood_thresh\"] &lt; 1\n                ), \"likelihood_thresh must be between 0 and 1\"\n            else:\n                raise ValueError(\n                    \"likelihood_thresh must be passed within key['params']\"\n                )\n        else:\n            raise KeyError(\"'params' must be in key\")\n        super().insert1(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.DLCCentroidParams", "title": "<code>DLCCentroidParams</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Parameters for calculating the centroid</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>@schema\nclass DLCCentroidParams(dj.Manual):\n\"\"\"\n    Parameters for calculating the centroid\n    \"\"\"\n\n    # TODO: whether to keep all params in a params dict\n    # or break out into individual secondary keys\n    definition = \"\"\"\n    dlc_centroid_params_name: varchar(80) # name for this set of parameters\n    ---\n    params: longblob\n    \"\"\"\n\n    _available_centroid_methods = [\n        \"four_led_centroid\",\n        \"two_pt_centroid\",\n        \"one_pt_centroid\",\n    ]\n    _four_led_labels = [\"greenLED\", \"redLED_L\", \"redLED_C\", \"redLED_R\"]\n    _two_pt_labels = [\"point1\", \"point2\"]\n\n    @classmethod\n    def insert_default(cls, **kwargs):\n\"\"\"\n        Inserts default centroid parameters. Assumes 2 LEDs tracked\n        \"\"\"\n        params = {\n            \"centroid_method\": \"two_pt_centroid\",\n            \"points\": {\n                \"point1\": \"greenLED\",\n                \"point2\": \"redLED_C\",\n            },\n            \"interpolate\": True,\n            \"interp_params\": {\"max_cm_to_interp\": 15},\n            \"smooth\": True,\n            \"smoothing_params\": {\n                \"smoothing_duration\": 0.05,\n                \"smooth_method\": \"moving_avg\",\n            },\n            \"max_LED_separation\": 12,\n            \"speed_smoothing_std_dev\": 0.100,\n        }\n        cls.insert1(\n            {\"dlc_centroid_params_name\": \"default\", \"params\": params}, **kwargs\n        )\n\n    @classmethod\n    def get_default(cls):\n        query = cls &amp; {\"dlc_centroid_params_name\": \"default\"}\n        if not len(query) &gt; 0:\n            cls().insert_default(skip_duplicates=True)\n            default = (cls &amp; {\"dlc_centroid_params_name\": \"default\"}).fetch1()\n        else:\n            default = query.fetch1()\n        return default\n\n    def insert1(self, key, **kwargs):\n\"\"\"\n        Check provided parameter dictionary to make sure\n        it contains all necessary items\n        \"\"\"\n        params = key[\"params\"]\n        if \"centroid_method\" in params:\n            if params[\"centroid_method\"] in self._available_centroid_methods:\n                if params[\"centroid_method\"] == \"four_led_centroid\":\n                    if any(\n                        x not in self._four_led_labels for x in params[\"points\"]\n                    ):\n                        raise KeyError(\n                            f\"Please make sure to specify all necessary labels: \"\n                            f\"{self._four_led_labels} \"\n                            f\"if using the 'four_led_centroid' method\"\n                        )\n                elif params[\"centroid_method\"] == \"two_pt_centroid\":\n                    if any(\n                        x not in self._two_pt_labels for x in params[\"points\"]\n                    ):\n                        raise KeyError(\n                            f\"Please make sure to specify all necessary labels: \"\n                            f\"{self._two_pt_labels} \"\n                            f\"if using the 'two_pt_centroid' method\"\n                        )\n                elif params[\"centroid_method\"] == \"one_pt_centroid\":\n                    if \"point1\" not in params[\"points\"]:\n                        raise KeyError(\n                            \"Please make sure to specify the necessary label: \"\n                            \"'point1' \"\n                            \"if using the 'one_pt_centroid' method\"\n                        )\n                else:\n                    raise Exception(\"This shouldn't happen lol oops\")\n            else:\n                raise ValueError(\n                    f\"The given 'centroid_method': {params['centroid_method']} \"\n                    f\"is not in the available methods: \"\n                    f\"{self._available_centroid_methods}\"\n                )\n        else:\n            raise KeyError(\n                \"'centroid_method' needs to be provided as a parameter\"\n            )\n\n        if \"max_LED_separation\" in params:\n            if not isinstance(params[\"max_LED_separation\"], (int, float)):\n                raise TypeError(\n                    f\"parameter 'max_LED_separation' is type: \"\n                    f\"{type(params['max_LED_separation'])}, \"\n                    f\"it should be one of type (float, int)\"\n                )\n        if \"smooth\" in params:\n            if params[\"smooth\"]:\n                if \"smoothing_params\" in params:\n                    if \"smooth_method\" in params[\"smoothing_params\"]:\n                        smooth_method = params[\"smoothing_params\"][\n                            \"smooth_method\"\n                        ]\n                        if smooth_method not in _key_to_smooth_func_dict:\n                            raise KeyError(\n                                f\"smooth_method: {smooth_method} not an available method.\"\n                            )\n                    if not \"smoothing_duration\" in params[\"smoothing_params\"]:\n                        raise KeyError(\n                            \"smoothing_duration must be passed as a smoothing_params within key['params']\"\n                        )\n                    else:\n                        assert isinstance(\n                            params[\"smoothing_params\"][\"smoothing_duration\"],\n                            (float, int),\n                        ), \"smoothing_duration must be a float or int\"\n                else:\n                    raise ValueError(\"smoothing_params not in key['params']\")\n\n        super().insert1(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.DLCCentroidParams.insert_default", "title": "<code>insert_default(**kwargs)</code>  <code>classmethod</code>", "text": "<p>Inserts default centroid parameters. Assumes 2 LEDs tracked</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>@classmethod\ndef insert_default(cls, **kwargs):\n\"\"\"\n    Inserts default centroid parameters. Assumes 2 LEDs tracked\n    \"\"\"\n    params = {\n        \"centroid_method\": \"two_pt_centroid\",\n        \"points\": {\n            \"point1\": \"greenLED\",\n            \"point2\": \"redLED_C\",\n        },\n        \"interpolate\": True,\n        \"interp_params\": {\"max_cm_to_interp\": 15},\n        \"smooth\": True,\n        \"smoothing_params\": {\n            \"smoothing_duration\": 0.05,\n            \"smooth_method\": \"moving_avg\",\n        },\n        \"max_LED_separation\": 12,\n        \"speed_smoothing_std_dev\": 0.100,\n    }\n    cls.insert1(\n        {\"dlc_centroid_params_name\": \"default\", \"params\": params}, **kwargs\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.DLCCentroidParams.insert1", "title": "<code>insert1(key, **kwargs)</code>", "text": "<p>Check provided parameter dictionary to make sure it contains all necessary items</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>def insert1(self, key, **kwargs):\n\"\"\"\n    Check provided parameter dictionary to make sure\n    it contains all necessary items\n    \"\"\"\n    params = key[\"params\"]\n    if \"centroid_method\" in params:\n        if params[\"centroid_method\"] in self._available_centroid_methods:\n            if params[\"centroid_method\"] == \"four_led_centroid\":\n                if any(\n                    x not in self._four_led_labels for x in params[\"points\"]\n                ):\n                    raise KeyError(\n                        f\"Please make sure to specify all necessary labels: \"\n                        f\"{self._four_led_labels} \"\n                        f\"if using the 'four_led_centroid' method\"\n                    )\n            elif params[\"centroid_method\"] == \"two_pt_centroid\":\n                if any(\n                    x not in self._two_pt_labels for x in params[\"points\"]\n                ):\n                    raise KeyError(\n                        f\"Please make sure to specify all necessary labels: \"\n                        f\"{self._two_pt_labels} \"\n                        f\"if using the 'two_pt_centroid' method\"\n                    )\n            elif params[\"centroid_method\"] == \"one_pt_centroid\":\n                if \"point1\" not in params[\"points\"]:\n                    raise KeyError(\n                        \"Please make sure to specify the necessary label: \"\n                        \"'point1' \"\n                        \"if using the 'one_pt_centroid' method\"\n                    )\n            else:\n                raise Exception(\"This shouldn't happen lol oops\")\n        else:\n            raise ValueError(\n                f\"The given 'centroid_method': {params['centroid_method']} \"\n                f\"is not in the available methods: \"\n                f\"{self._available_centroid_methods}\"\n            )\n    else:\n        raise KeyError(\n            \"'centroid_method' needs to be provided as a parameter\"\n        )\n\n    if \"max_LED_separation\" in params:\n        if not isinstance(params[\"max_LED_separation\"], (int, float)):\n            raise TypeError(\n                f\"parameter 'max_LED_separation' is type: \"\n                f\"{type(params['max_LED_separation'])}, \"\n                f\"it should be one of type (float, int)\"\n            )\n    if \"smooth\" in params:\n        if params[\"smooth\"]:\n            if \"smoothing_params\" in params:\n                if \"smooth_method\" in params[\"smoothing_params\"]:\n                    smooth_method = params[\"smoothing_params\"][\n                        \"smooth_method\"\n                    ]\n                    if smooth_method not in _key_to_smooth_func_dict:\n                        raise KeyError(\n                            f\"smooth_method: {smooth_method} not an available method.\"\n                        )\n                if not \"smoothing_duration\" in params[\"smoothing_params\"]:\n                    raise KeyError(\n                        \"smoothing_duration must be passed as a smoothing_params within key['params']\"\n                    )\n                else:\n                    assert isinstance(\n                        params[\"smoothing_params\"][\"smoothing_duration\"],\n                        (float, int),\n                    ), \"smoothing_duration must be a float or int\"\n            else:\n                raise ValueError(\"smoothing_params not in key['params']\")\n\n    super().insert1(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.DLCSmoothInterpCohort", "title": "<code>DLCSmoothInterpCohort</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Table to combine multiple bodyparts from DLCSmoothInterp to enable centroid/orientation calculations</p> Source code in <code>src/spyglass/position/v1/position_dlc_cohort.py</code> <pre><code>@schema\nclass DLCSmoothInterpCohort(dj.Computed):\n\"\"\"\n    Table to combine multiple bodyparts from DLCSmoothInterp\n    to enable centroid/orientation calculations\n    \"\"\"\n\n    # Need to ensure that nwb_file_name/epoch/interval list name endure as primary keys\n    definition = \"\"\"\n    -&gt; DLCSmoothInterpCohortSelection\n    ---\n    \"\"\"\n\n    class BodyPart(dj.Part):\n        definition = \"\"\"\n        -&gt; DLCSmoothInterpCohort\n        -&gt; DLCSmoothInterp\n        ---\n        -&gt; AnalysisNwbfile\n        dlc_smooth_interp_position_object_id : varchar(80)\n        dlc_smooth_interp_info_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n        def fetch1_dataframe(self):\n            nwb_data = self.fetch_nwb()[0]\n            index = pd.Index(\n                np.asarray(\n                    nwb_data[\"dlc_smooth_interp_position\"]\n                    .get_spatial_series()\n                    .timestamps\n                ),\n                name=\"time\",\n            )\n            COLUMNS = [\n                \"video_frame_ind\",\n                \"x\",\n                \"y\",\n            ]\n            return pd.DataFrame(\n                np.concatenate(\n                    (\n                        np.asarray(\n                            nwb_data[\"dlc_smooth_interp_info\"]\n                            .time_series[\"video_frame_ind\"]\n                            .data,\n                            dtype=int,\n                        )[:, np.newaxis],\n                        np.asarray(\n                            nwb_data[\"dlc_smooth_interp_position\"]\n                            .get_spatial_series()\n                            .data\n                        ),\n                    ),\n                    axis=1,\n                ),\n                columns=COLUMNS,\n                index=index,\n            )\n\n    def make(self, key):\n        from .dlc_utils import OutputLogger, infer_output_dir\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n            print_console=False,\n        ) as logger:\n            logger.logger.info(\"-----------------------\")\n            logger.logger.info(\"Bodypart Cohort\")\n            # from Jen Guidera\n            self.insert1(key)\n            cohort_selection = (DLCSmoothInterpCohortSelection &amp; key).fetch1()\n            table_entries = []\n            bodyparts_params_dict = cohort_selection.pop(\n                \"bodyparts_params_dict\"\n            )\n            temp_key = cohort_selection.copy()\n            for bodypart, params in bodyparts_params_dict.items():\n                temp_key[\"bodypart\"] = bodypart\n                temp_key[\"dlc_si_params_name\"] = params\n                table_entries.append((DLCSmoothInterp &amp; temp_key).fetch())\n            assert len(table_entries) == len(\n                bodyparts_params_dict\n            ), \"more entries found in DLCSmoothInterp than specified in bodyparts_params_dict\"\n            table_column_names = list(table_entries[0].dtype.fields.keys())\n            for table_entry in table_entries:\n                entry_key = {\n                    **{\n                        k: v for k, v in zip(table_column_names, table_entry[0])\n                    },\n                    **key,\n                }\n                DLCSmoothInterpCohort.BodyPart.insert1(\n                    entry_key, skip_duplicates=True\n                )\n        logger.logger.info(\"Inserted entry into DLCSmoothInterpCohort\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.RawPosition", "title": "<code>RawPosition</code>", "text": "<p>         Bases: <code>dj.Imported</code></p>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.RawPosition--notes", "title": "Notes", "text": "<p>The position timestamps come from: .pos_cameraHWSync.dat. If PTP is not used, the position timestamps are inferred by finding the closest timestamps from the neural recording via the trodes time.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass RawPosition(dj.Imported):\n\"\"\"\n\n    Notes\n    -----\n    The position timestamps come from: .pos_cameraHWSync.dat.\n    If PTP is not used, the position timestamps are inferred by finding the\n    closest timestamps from the neural recording via the trodes time.\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionSource\n    ---\n    raw_position_object_id: varchar(40)    # the object id of the spatial series for this epoch in the NWB file\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        # TODO refactor this. this calculates sampling rate (unused here) and is expensive to do twice\n        pos_dict = get_all_spatial_series(nwbf)\n        for epoch in pos_dict:\n            if key[\n                \"interval_list_name\"\n            ] == PositionSource.get_pos_interval_name(epoch):\n                pdict = pos_dict[epoch]\n                key[\"raw_position_object_id\"] = pdict[\"raw_position_object_id\"]\n                self.insert1(key)\n                break\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    def fetch1_dataframe(self):\n        raw_position_nwb = self.fetch_nwb()[0][\"raw_position\"]\n        return pd.DataFrame(\n            data=raw_position_nwb.data,\n            index=pd.Index(raw_position_nwb.timestamps, name=\"time\"),\n            columns=raw_position_nwb.description.split(\", \"),\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.DLCCentroidSelection", "title": "<code>DLCCentroidSelection</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Table to pair a cohort of bodypart entries with the parameters for calculating their centroid</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>@schema\nclass DLCCentroidSelection(dj.Manual):\n\"\"\"\n    Table to pair a cohort of bodypart entries with\n    the parameters for calculating their centroid\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCSmoothInterpCohort\n    -&gt; DLCCentroidParams\n    ---\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.DLCCentroid", "title": "<code>DLCCentroid</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Table to calculate the centroid of a group of bodyparts</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>@schema\nclass DLCCentroid(dj.Computed):\n\"\"\"\n    Table to calculate the centroid of a group of bodyparts\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCCentroidSelection\n    ---\n    -&gt; AnalysisNwbfile\n    dlc_position_object_id : varchar(80)\n    dlc_velocity_object_id : varchar(80)\n    \"\"\"\n\n    def make(self, key):\n        from .dlc_utils import OutputLogger, infer_output_dir\n\n        idx = pd.IndexSlice\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n            print_console=False,\n        ) as logger:\n            logger.logger.info(\"-----------------------\")\n            logger.logger.info(\"Centroid Calculation\")\n            # Get labels to smooth from Parameters table\n            cohort_entries = DLCSmoothInterpCohort.BodyPart &amp; key\n            params = (DLCCentroidParams() &amp; key).fetch1(\"params\")\n            centroid_method = params.pop(\"centroid_method\")\n            bodyparts_avail = cohort_entries.fetch(\"bodypart\")\n            speed_smoothing_std_dev = params.pop(\"speed_smoothing_std_dev\")\n            # TODO, generalize key naming\n            if centroid_method == \"four_led_centroid\":\n                centroid_func = _key_to_func_dict[centroid_method]\n                if \"greenLED\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"greenLED\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"greenLED\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"A green led needs to be specified for the 4 led centroid method\"\n                    )\n                if \"redLED_L\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"redLED_L\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"redLED_L\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"A left red led needs to be specified for the 4 led centroid method\"\n                    )\n                if \"redLED_C\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"redLED_C\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"redLED_C\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"A center red led needs to be specified for the 4 led centroid method\"\n                    )\n                if \"redLED_R\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"redLED_R\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"redLED_R\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"A right red led needs to be specified for the 4 led centroid method\"\n                    )\n                bodyparts_to_use = [\n                    params[\"points\"][\"greenLED\"],\n                    params[\"points\"][\"redLED_L\"],\n                    params[\"points\"][\"redLED_C\"],\n                    params[\"points\"][\"redLED_R\"],\n                ]\n\n            elif centroid_method == \"two_pt_centroid\":\n                centroid_func = _key_to_func_dict[centroid_method]\n                if \"point1\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"point1\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"point1\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"point1 needs to be specified for the 2 pt centroid method\"\n                    )\n                if \"point2\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"point2\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"point2\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"point2 needs to be specified for the 2 pt centroid method\"\n                    )\n                bodyparts_to_use = [\n                    params[\"points\"][\"point1\"],\n                    params[\"points\"][\"point2\"],\n                ]\n\n            elif centroid_method == \"one_pt_centroid\":\n                centroid_func = _key_to_func_dict[centroid_method]\n                if \"point1\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"point1\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"point1\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"point1 needs to be specified for the 1 pt centroid method\"\n                    )\n                bodyparts_to_use = [params[\"points\"][\"point1\"]]\n\n            else:\n                raise ValueError(\"Please specify a centroid method to use.\")\n            pos_df = pd.concat(\n                {\n                    bodypart: (\n                        DLCSmoothInterpCohort.BodyPart\n                        &amp; {**key, **{\"bodypart\": bodypart}}\n                    ).fetch1_dataframe()\n                    for bodypart in bodyparts_to_use\n                },\n                axis=1,\n            )\n            dt = np.median(np.diff(pos_df.index.to_numpy()))\n            sampling_rate = 1 / dt\n            logger.logger.info(\n                \"Calculating centroid with %s\", str(centroid_method)\n            )\n            centroid = centroid_func(pos_df, **params)\n            centroid_df = pd.DataFrame(\n                centroid,\n                columns=[\"x\", \"y\"],\n                index=pos_df.index.to_numpy(),\n            )\n            if params[\"interpolate\"]:\n                if np.any(np.isnan(centroid)):\n                    logger.logger.info(\"interpolating over NaNs\")\n                    nan_inds = (\n                        pd.isnull(centroid_df.loc[:, idx[(\"x\", \"y\")]])\n                        .any(axis=1)\n                        .to_numpy()\n                        .nonzero()[0]\n                    )\n                    nan_spans = get_span_start_stop(nan_inds)\n                    interp_df = interp_pos(\n                        centroid_df.copy(), nan_spans, **params[\"interp_params\"]\n                    )\n                else:\n                    logger.logger.info(\"no NaNs to interpolate over\")\n                    interp_df = centroid_df.copy()\n            else:\n                interp_df = centroid_df.copy()\n            if params[\"smooth\"]:\n                if \"smoothing_duration\" in params[\"smoothing_params\"]:\n                    smoothing_duration = params[\"smoothing_params\"].pop(\n                        \"smoothing_duration\"\n                    )\n                    dt = np.median(np.diff(pos_df.index.to_numpy()))\n                    sampling_rate = 1 / dt\n                    logger.logger.info(\"smoothing position\")\n                    smooth_func = _key_to_smooth_func_dict[\n                        params[\"smoothing_params\"][\"smooth_method\"]\n                    ]\n                    logger.logger.info(\n                        \"Smoothing using method: %s\",\n                        str(params[\"smoothing_params\"][\"smooth_method\"]),\n                    )\n                    final_df = smooth_func(\n                        interp_df,\n                        smoothing_duration=smoothing_duration,\n                        sampling_rate=sampling_rate,\n                        **params[\"smoothing_params\"],\n                    )\n                else:\n                    raise KeyError(\n                        \"smoothing_duration needs to be passed within smoothing_params\"\n                    )\n            else:\n                final_df = interp_df.copy()\n            logger.logger.info(\"getting velocity\")\n            velocity = get_velocity(\n                final_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                time=pos_df.index.to_numpy(),\n                sigma=speed_smoothing_std_dev,\n                sampling_frequency=sampling_rate,\n            )  # cm/s\n            speed = np.sqrt(np.sum(velocity**2, axis=1))  # cm/s\n            # Create dataframe\n            velocity_df = pd.DataFrame(\n                np.concatenate((velocity, speed[:, np.newaxis]), axis=1),\n                columns=[\"velocity_x\", \"velocity_y\", \"speed\"],\n                index=pos_df.index.to_numpy(),\n            )\n            total_nan = np.sum(\n                final_df.loc[:, idx[(\"x\", \"y\")]].isna().any(axis=1)\n            )\n            pretrack_nan = np.sum(\n                final_df.iloc[:1000].loc[:, idx[(\"x\", \"y\")]].isna().any(axis=1)\n            )\n            logger.logger.info(\"total NaNs in centroid dataset: %d\", total_nan)\n            logger.logger.info(\n                \"NaNs in centroid dataset before ind 1000: %d\", pretrack_nan\n            )\n            position = pynwb.behavior.Position()\n            velocity = pynwb.behavior.BehavioralTimeSeries()\n            spatial_series = (RawPosition() &amp; key).fetch_nwb()[0][\n                \"raw_position\"\n            ]\n            METERS_PER_CM = 0.01\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=final_df.index.to_numpy(),\n                conversion=METERS_PER_CM,\n                data=final_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"x_position, y_position\",\n            )\n            velocity.create_timeseries(\n                name=\"velocity\",\n                timestamps=velocity_df.index.to_numpy(),\n                conversion=METERS_PER_CM,\n                unit=\"m/s\",\n                data=velocity_df.loc[\n                    :, idx[(\"velocity_x\", \"velocity_y\", \"speed\")]\n                ].to_numpy(),\n                comments=spatial_series.comments,\n                description=\"x_velocity, y_velocity, speed\",\n            )\n            velocity.create_timeseries(\n                name=\"video_frame_ind\",\n                unit=\"index\",\n                timestamps=final_df.index.to_numpy(),\n                data=pos_df[\n                    pos_df.columns.levels[0][0]\n                ].video_frame_ind.to_numpy(),\n                description=\"video_frame_ind\",\n                comments=\"no comments\",\n            )\n            # Add to Analysis NWB file\n            key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                key[\"nwb_file_name\"]\n            )\n            nwb_analysis_file = AnalysisNwbfile()\n            key[\"dlc_position_object_id\"] = nwb_analysis_file.add_nwb_object(\n                key[\"analysis_file_name\"], position\n            )\n            key[\"dlc_velocity_object_id\"] = nwb_analysis_file.add_nwb_object(\n                key[\"analysis_file_name\"], velocity\n            )\n\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n            self.insert1(key)\n            logger.logger.info(\"inserted entry into DLCCentroid\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(\n                nwb_data[\"dlc_position\"].get_spatial_series().timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"position_x\",\n            \"position_y\",\n            \"velocity_x\",\n            \"velocity_y\",\n            \"speed\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"dlc_velocity\"]\n                        .time_series[\"video_frame_ind\"]\n                        .data,\n                        dtype=int,\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"dlc_position\"].get_spatial_series().data\n                    ),\n                    np.asarray(\n                        nwb_data[\"dlc_velocity\"].time_series[\"velocity\"].data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.four_led_centroid", "title": "<code>four_led_centroid(pos_df, **params)</code>", "text": "<p>Determines the centroid of 4 LEDS on an implant LED ring. Assumed to be the Green LED, and 3 red LEDs called: redLED_C, redLED_L, redLED_R By default, uses (greenled + redLED_C) / 2 to calculate centroid If Green LED is NaN, but red center LED is not,     then the red center LED is called the centroid If green and red center LEDs are NaN, but red left and red right LEDs are not,     then the centroid is (redLED_L + redLED_R) / 2 If red center LED is NaN, but the other 3 LEDS are not,     then the centroid is (greenled + (redLED_L + redLED_R) / 2) / 2 If red center and left LEDs are NaN, but green and red right LEDs are not,     then the centroid is (greenled + redLED_R) / 2 If red center and right LEDs are NaN, but green and red left LEDs are not,     then the centroid is (greenled + redLED_L) / 2 If all red LEDs are NaN, but green LED is not,     then the green LED is called the centroid If all LEDs are NaN, then the centroid is NaN</p> <p>Parameters:</p> Name Type Description Default <code>pos_df</code> <code>pd.DataFrame</code> <p>dataframe containing x and y position for each LED of interest, index is timestamps. Column names specified by params</p> required <code>**params</code> <code>dict</code> <p>contains 'greenLED' and 'redLED_C', 'redLED_R', 'redLED_L' keys, whose values specify the column names in <code>pos_df</code></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>centroid</code> <code>np.ndarray</code> <p>numpy array with shape (n_time, 2) centroid[0] is the x coord and centroid[1] is the y coord</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>def four_led_centroid(pos_df: pd.DataFrame, **params):\n\"\"\"\n    Determines the centroid of 4 LEDS on an implant LED ring.\n    Assumed to be the Green LED, and 3 red LEDs called: redLED_C, redLED_L, redLED_R\n    By default, uses (greenled + redLED_C) / 2 to calculate centroid\n    If Green LED is NaN, but red center LED is not,\n        then the red center LED is called the centroid\n    If green and red center LEDs are NaN, but red left and red right LEDs are not,\n        then the centroid is (redLED_L + redLED_R) / 2\n    If red center LED is NaN, but the other 3 LEDS are not,\n        then the centroid is (greenled + (redLED_L + redLED_R) / 2) / 2\n    If red center and left LEDs are NaN, but green and red right LEDs are not,\n        then the centroid is (greenled + redLED_R) / 2\n    If red center and right LEDs are NaN, but green and red left LEDs are not,\n        then the centroid is (greenled + redLED_L) / 2\n    If all red LEDs are NaN, but green LED is not,\n        then the green LED is called the centroid\n    If all LEDs are NaN, then the centroid is NaN\n\n    Parameters\n    ----------\n    pos_df : pd.DataFrame\n        dataframe containing x and y position for each LED of interest,\n        index is timestamps. Column names specified by params\n    **params : dict\n        contains 'greenLED' and 'redLED_C', 'redLED_R', 'redLED_L' keys,\n        whose values specify the column names in `pos_df`\n\n    Returns\n    -------\n    centroid : np.ndarray\n        numpy array with shape (n_time, 2)\n        centroid[0] is the x coord and centroid[1] is the y coord\n    \"\"\"\n    centroid = np.zeros(shape=(len(pos_df), 2))\n    idx = pd.IndexSlice\n    # TODO: this feels messy, clean-up\n    green_led = params[\"points\"].pop(\"greenLED\", None)\n    red_led_C = params[\"points\"].pop(\"redLED_C\", None)\n    red_led_L = params[\"points\"].pop(\"redLED_L\", None)\n    red_led_R = params[\"points\"].pop(\"redLED_R\", None)\n    green_nans = pos_df.loc[:, idx[green_led, (\"x\", \"y\")]].isna().any(axis=1)\n    red_C_nans = pos_df.loc[:, idx[red_led_C, (\"x\", \"y\")]].isna().any(axis=1)\n    red_L_nans = pos_df.loc[:, idx[red_led_L, (\"x\", \"y\")]].isna().any(axis=1)\n    red_R_nans = pos_df.loc[:, idx[red_led_R, (\"x\", \"y\")]].isna().any(axis=1)\n    # TODO: implement checks to make sure not rewriting previously set index in centroid\n    # If all given LEDs are not NaN\n    dist_between_green_red = get_distance(\n        pos_df.loc[:, idx[red_led_C, (\"x\", \"y\")]].to_numpy(),\n        pos_df.loc[:, idx[green_led, (\"x\", \"y\")]].to_numpy(),\n    )\n    g_c_is_too_separated = (\n        dist_between_green_red &gt;= params[\"max_LED_separation\"]\n    )\n    all_good_mask = reduce(\n        np.logical_and,\n        (\n            ~green_nans,\n            ~red_C_nans,\n            ~red_L_nans,\n            ~red_R_nans,\n            ~g_c_is_too_separated,\n        ),\n    )\n    centroid[all_good_mask] = [\n        *zip(\n            (\n                pos_df.loc[idx[all_good_mask], idx[red_led_C, \"x\"]]\n                + pos_df.loc[idx[all_good_mask], idx[green_led, \"x\"]]\n            )\n            / 2,\n            (\n                pos_df.loc[idx[all_good_mask], idx[red_led_C, \"y\"]]\n                + pos_df.loc[idx[all_good_mask], idx[green_led, \"y\"]]\n            )\n            / 2,\n        )\n    ]\n    # If green LED and red center LED are both not NaN\n    green_red_C = np.logical_and(\n        ~green_nans, ~red_C_nans, ~g_c_is_too_separated\n    )\n    if np.sum(green_red_C) &gt; 0:\n        centroid[green_red_C] = [\n            *zip(\n                (\n                    pos_df.loc[idx[green_red_C], idx[red_led_C, \"x\"]]\n                    + pos_df.loc[idx[green_red_C], idx[green_led, \"x\"]]\n                )\n                / 2,\n                (\n                    pos_df.loc[idx[green_red_C], idx[red_led_C, \"y\"]]\n                    + pos_df.loc[idx[green_red_C], idx[green_led, \"y\"]]\n                )\n                / 2,\n            )\n        ]\n    # If all given LEDs are NaN\n    all_bad_mask = reduce(\n        np.logical_and, (green_nans, red_C_nans, red_L_nans, red_R_nans)\n    )\n    centroid[all_bad_mask, :] = np.nan\n    # If green LED is NaN, but red center LED is not\n    no_green_red_C = np.logical_and(green_nans, ~red_C_nans)\n    if np.sum(no_green_red_C) &gt; 0:\n        centroid[no_green_red_C] = [\n            *zip(\n                pos_df.loc[idx[no_green_red_C], idx[red_led_C, \"x\"]],\n                pos_df.loc[idx[no_green_red_C], idx[red_led_C, \"y\"]],\n            )\n        ]\n    # If green and red center LEDs are NaN, but red left and red right LEDs are not\n    dist_between_left_right = get_distance(\n        pos_df.loc[:, idx[red_led_L, (\"x\", \"y\")]].to_numpy(),\n        pos_df.loc[:, idx[red_led_R, (\"x\", \"y\")]].to_numpy(),\n    )\n    l_r_is_too_separated = (\n        dist_between_left_right &gt;= params[\"max_LED_separation\"]\n    )\n    no_green_no_red_C_red_L_red_R = reduce(\n        np.logical_and,\n        (\n            green_nans,\n            red_C_nans,\n            ~red_L_nans,\n            ~red_R_nans,\n            ~l_r_is_too_separated,\n        ),\n    )\n    if np.sum(no_green_no_red_C_red_L_red_R) &gt; 0:\n        centroid[no_green_no_red_C_red_L_red_R] = [\n            *zip(\n                (\n                    pos_df.loc[\n                        idx[no_green_no_red_C_red_L_red_R], idx[red_led_L, \"x\"]\n                    ]\n                    + pos_df.loc[\n                        idx[no_green_no_red_C_red_L_red_R], idx[red_led_R, \"x\"]\n                    ]\n                )\n                / 2,\n                (\n                    pos_df.loc[\n                        idx[no_green_no_red_C_red_L_red_R], idx[red_led_L, \"y\"]\n                    ]\n                    + pos_df.loc[\n                        idx[no_green_no_red_C_red_L_red_R], idx[red_led_R, \"y\"]\n                    ]\n                )\n                / 2,\n            )\n        ]\n    # If red center LED is NaN, but green, red left, and right LEDs are not\n    dist_between_left_green = get_distance(\n        pos_df.loc[:, idx[red_led_L, (\"x\", \"y\")]].to_numpy(),\n        pos_df.loc[:, idx[green_led, (\"x\", \"y\")]].to_numpy(),\n    )\n    dist_between_right_green = get_distance(\n        pos_df.loc[:, idx[red_led_R, (\"x\", \"y\")]].to_numpy(),\n        pos_df.loc[:, idx[green_led, (\"x\", \"y\")]].to_numpy(),\n    )\n    l_g_is_too_separated = (\n        dist_between_left_green &gt;= params[\"max_LED_separation\"]\n    )\n    r_g_is_too_separated = (\n        dist_between_right_green &gt;= params[\"max_LED_separation\"]\n    )\n    green_red_L_red_R_no_red_C = reduce(\n        np.logical_and,\n        (\n            ~green_nans,\n            red_C_nans,\n            ~red_L_nans,\n            ~red_R_nans,\n            ~l_r_is_too_separated,\n            ~l_g_is_too_separated,\n            ~r_g_is_too_separated,\n        ),\n    )\n    if np.sum(green_red_L_red_R_no_red_C) &gt; 0:\n        midpoint = (\n            (\n                pos_df.loc[idx[green_red_L_red_R_no_red_C], idx[red_led_L, \"x\"]]\n                + pos_df.loc[\n                    idx[green_red_L_red_R_no_red_C], idx[red_led_R, \"x\"]\n                ]\n            )\n            / 2,\n            (\n                pos_df.loc[idx[green_red_L_red_R_no_red_C], idx[red_led_L, \"y\"]]\n                + pos_df.loc[\n                    idx[green_red_L_red_R_no_red_C], idx[red_led_R, \"y\"]\n                ]\n            )\n            / 2,\n        )\n        centroid[green_red_L_red_R_no_red_C] = [\n            *zip(\n                (\n                    midpoint[0]\n                    + pos_df.loc[\n                        idx[green_red_L_red_R_no_red_C], idx[green_led, \"x\"]\n                    ]\n                )\n                / 2,\n                (\n                    midpoint[1]\n                    + pos_df.loc[\n                        idx[green_red_L_red_R_no_red_C], idx[green_led, \"y\"]\n                    ]\n                )\n                / 2,\n            )\n        ]\n    # If red center and left LED is NaN, but green and red right LED are not\n    green_red_R_no_red_C_no_red_L = reduce(\n        np.logical_and,\n        (\n            ~green_nans,\n            red_C_nans,\n            red_L_nans,\n            ~red_R_nans,\n            ~r_g_is_too_separated,\n        ),\n    )\n    if np.sum(green_red_R_no_red_C_no_red_L) &gt; 0:\n        centroid[green_red_R_no_red_C_no_red_L] = [\n            *zip(\n                (\n                    pos_df.loc[\n                        idx[green_red_R_no_red_C_no_red_L], idx[red_led_R, \"x\"]\n                    ]\n                    + pos_df.loc[\n                        idx[green_red_R_no_red_C_no_red_L], idx[green_led, \"x\"]\n                    ]\n                )\n                / 2,\n                (\n                    pos_df.loc[\n                        idx[green_red_R_no_red_C_no_red_L], idx[red_led_R, \"y\"]\n                    ]\n                    + pos_df.loc[\n                        idx[green_red_R_no_red_C_no_red_L], idx[green_led, \"y\"]\n                    ]\n                )\n                / 2,\n            )\n        ]\n    # If red center and right LED is NaN, but green and red left LED are not\n    green_red_L_no_red_C_no_red_R = reduce(\n        np.logical_and,\n        (\n            ~green_nans,\n            red_C_nans,\n            ~red_L_nans,\n            red_R_nans,\n            ~l_g_is_too_separated,\n        ),\n    )\n    if np.sum(green_red_L_no_red_C_no_red_R) &gt; 0:\n        centroid[green_red_L_no_red_C_no_red_R] = [\n            *zip(\n                (\n                    pos_df.loc[\n                        idx[green_red_L_no_red_C_no_red_R], idx[red_led_L, \"x\"]\n                    ]\n                    + pos_df.loc[\n                        idx[green_red_L_no_red_C_no_red_R], idx[green_led, \"x\"]\n                    ]\n                )\n                / 2,\n                (\n                    pos_df.loc[\n                        idx[green_red_L_no_red_C_no_red_R], idx[red_led_L, \"y\"]\n                    ]\n                    + pos_df.loc[\n                        idx[green_red_L_no_red_C_no_red_R], idx[green_led, \"y\"]\n                    ]\n                )\n                / 2,\n            )\n        ]\n    # If all LEDS are NaN except red left LED\n    red_L_no_green_no_red_C_no_red_R = reduce(\n        np.logical_and, (green_nans, red_C_nans, ~red_L_nans, red_R_nans)\n    )\n    if np.sum(red_L_no_green_no_red_C_no_red_R) &gt; 0:\n        centroid[red_L_no_green_no_red_C_no_red_R] = [\n            *zip(\n                pos_df.loc[\n                    idx[red_L_no_green_no_red_C_no_red_R], idx[red_led_L, \"x\"]\n                ],\n                pos_df.loc[\n                    idx[red_L_no_green_no_red_C_no_red_R], idx[red_led_L, \"y\"]\n                ],\n            )\n        ]\n    # If all LEDS are NaN except red right LED\n    red_R_no_green_no_red_C_no_red_L = reduce(\n        np.logical_and, (green_nans, red_C_nans, red_L_nans, ~red_R_nans)\n    )\n    if np.sum(red_R_no_green_no_red_C_no_red_L) &gt; 0:\n        centroid[red_R_no_green_no_red_C_no_red_L] = [\n            *zip(\n                pos_df.loc[\n                    idx[red_R_no_green_no_red_C_no_red_L], idx[red_led_R, \"x\"]\n                ],\n                pos_df.loc[\n                    idx[red_R_no_green_no_red_C_no_red_L], idx[red_led_R, \"y\"]\n                ],\n            )\n        ]\n    # If all red LEDs are NaN, but green LED is not\n    green_no_red = reduce(\n        np.logical_and, (~green_nans, red_C_nans, red_L_nans, red_R_nans)\n    )\n    if np.sum(green_no_red) &gt; 0:\n        centroid[green_no_red] = [\n            *zip(\n                pos_df.loc[idx[green_no_red], idx[green_led, \"x\"]],\n                pos_df.loc[idx[green_no_red], idx[green_led, \"y\"]],\n            )\n        ]\n    too_separated_inds = reduce(\n        np.logical_or,\n        (\n            g_c_is_too_separated,\n            l_r_is_too_separated,\n            l_g_is_too_separated,\n            r_g_is_too_separated,\n        ),\n    )\n    if np.sum(too_separated_inds) &gt; 0:\n        centroid[too_separated_inds, :] = np.nan\n    return centroid\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.get_span_start_stop", "title": "<code>get_span_start_stop(indices)</code>", "text": "<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Type Description <code>_type_</code> <p>description</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_span_start_stop(indices):\n\"\"\"_summary_\n\n    Parameters\n    ----------\n    indices : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    span_inds = []\n    # Get start and stop index of spans of consecutive indices\n    for k, g in groupby(enumerate(indices), lambda x: x[1] - x[0]):\n        group = list(map(itemgetter(1), g))\n        span_inds.append((group[0], group[-1]))\n    return span_inds\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.two_pt_centroid", "title": "<code>two_pt_centroid(pos_df, **params)</code>", "text": "<p>Determines the centroid of 2 points using (point1 + point2) / 2 For a given timestamp, if one point is NaN, then the other point is assigned as the centroid. If both are NaN, the centroid is NaN</p> <p>Parameters:</p> Name Type Description Default <code>pos_df</code> <code>pd.DataFrame</code> <p>dataframe containing x and y position for each point of interest, index is timestamps. Column names specified by params</p> required <code>**params</code> <code>dict</code> <p>contains 'point1' and 'point2' keys, whose values specify the column names in <code>pos_df</code></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>centroid</code> <code>np.ndarray</code> <p>numpy array with shape (n_time, 2) centroid[0] is the x coord and centroid[1] is the y coord</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>def two_pt_centroid(pos_df: pd.DataFrame, **params):\n\"\"\"\n    Determines the centroid of 2 points using (point1 + point2) / 2\n    For a given timestamp, if one point is NaN,\n    then the other point is assigned as the centroid.\n    If both are NaN, the centroid is NaN\n\n    Parameters\n    ----------\n    pos_df : pd.DataFrame\n        dataframe containing x and y position for each point of interest,\n        index is timestamps. Column names specified by params\n    **params : dict\n        contains 'point1' and 'point2' keys,\n        whose values specify the column names in `pos_df`\n\n    Returns\n    -------\n    centroid : np.ndarray\n        numpy array with shape (n_time, 2)\n        centroid[0] is the x coord and centroid[1] is the y coord\n    \"\"\"\n\n    idx = pd.IndexSlice\n    centroid = np.zeros(shape=(len(pos_df), 2))\n    PT1 = params[\"points\"].pop(\"point1\", None)\n    PT2 = params[\"points\"].pop(\"point2\", None)\n    pt1_nans = pos_df.loc[:, idx[PT1, (\"x\", \"y\")]].isna().any(axis=1)\n    pt2_nans = pos_df.loc[:, idx[PT2, (\"x\", \"y\")]].isna().any(axis=1)\n    dist_between_points = get_distance(\n        pos_df.loc[:, idx[PT1, (\"x\", \"y\")]].to_numpy(),\n        pos_df.loc[:, idx[PT2, (\"x\", \"y\")]].to_numpy(),\n    )\n    is_too_separated = dist_between_points &gt;= params[\"max_LED_separation\"]\n    all_good_mask = np.logical_and(~pt1_nans, ~pt2_nans, ~is_too_separated)\n    centroid[all_good_mask] = [\n        *zip(\n            (\n                pos_df.loc[idx[all_good_mask], idx[PT1, \"x\"]]\n                + pos_df.loc[idx[all_good_mask], idx[PT2, \"x\"]]\n            )\n            / 2,\n            (\n                pos_df.loc[idx[all_good_mask], idx[PT1, \"y\"]]\n                + pos_df.loc[idx[all_good_mask], idx[PT2, \"y\"]]\n            )\n            / 2,\n        )\n    ]\n    # If only point1 is good\n    pt1_mask = np.logical_and(~pt1_nans, pt2_nans)\n    if np.sum(pt1_mask) &gt; 0:\n        centroid[pt1_mask] = [\n            *zip(\n                pos_df.loc[idx[pt1_mask], idx[PT1, \"x\"]],\n                pos_df.loc[idx[pt1_mask], idx[PT1, \"y\"]],\n            )\n        ]\n    # If only point2 is good\n    pt2_mask = np.logical_and(pt1_nans, ~pt2_nans)\n    if np.sum(pt2_mask) &gt; 0:\n        centroid[pt2_mask] = [\n            *zip(\n                pos_df.loc[idx[pt2_mask], idx[PT2, \"x\"]],\n                pos_df.loc[idx[pt2_mask], idx[PT2, \"y\"]],\n            )\n        ]\n    # If neither point is not NaN\n    all_bad_mask = np.logical_and(pt1_nans, pt2_nans)\n    centroid[all_bad_mask, :] = np.nan\n    # If LEDs are too far apart\n    centroid[is_too_separated, :] = np.nan\n\n    return centroid\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_centroid/#src.spyglass.position.v1.position_dlc_centroid.one_pt_centroid", "title": "<code>one_pt_centroid(pos_df, **params)</code>", "text": "<p>Passes through the provided point as the centroid For a given timestamp, if the point is NaN, then the centroid is NaN</p> <p>Parameters:</p> Name Type Description Default <code>pos_df</code> <code>pd.DataFrame</code> <p>dataframe containing x and y position for the point of interest, index is timestamps. Column name specified by params</p> required <code>**kwargs</code> <code>dict</code> <p>contains a 'point1' key, whose value specifies the column name in <code>pos_df</code></p> required <p>Returns:</p> Name Type Description <code>centroid</code> <code>np.ndarray</code> <p>numpy array with shape (n_time, 2) centroid[0] is the x coord and centroid[1] is the y coord</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>def one_pt_centroid(pos_df: pd.DataFrame, **params):\n\"\"\"\n    Passes through the provided point as the centroid\n    For a given timestamp, if the point is NaN,\n    then the centroid is NaN\n\n    Parameters\n    ----------\n    pos_df : pd.DataFrame\n        dataframe containing x and y position for the point of interest,\n        index is timestamps. Column name specified by params\n    **kwargs : dict\n        contains a 'point1' key,\n        whose value specifies the column name in `pos_df`\n\n    Returns\n    -------\n    centroid : np.ndarray\n        numpy array with shape (n_time, 2)\n        centroid[0] is the x coord and centroid[1] is the y coord\n    \"\"\"\n    idx = pd.IndexSlice\n    PT1 = params[\"points\"].pop(\"point1\", None)\n    centroid = pos_df.loc[:, idx[PT1, (\"x\", \"y\")]].to_numpy()\n    return centroid\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/", "title": "position_dlc_cohort.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.position.v1.position_dlc_cohort.DLCSmoothInterpCohortSelection", "title": "<code>DLCSmoothInterpCohortSelection</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Table to specify which combination of bodyparts from DLCSmoothInterp get combined into a cohort</p> Source code in <code>src/spyglass/position/v1/position_dlc_cohort.py</code> <pre><code>@schema\nclass DLCSmoothInterpCohortSelection(dj.Manual):\n\"\"\"\n    Table to specify which combination of bodyparts from DLCSmoothInterp\n    get combined into a cohort\n    \"\"\"\n\n    definition = \"\"\"\n    dlc_si_cohort_selection_name : varchar(120)\n    -&gt; DLCPoseEstimation\n    ---\n    bodyparts_params_dict   : blob      # Dict with bodypart as key and desired dlc_si_params_name as value\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.position.v1.position_dlc_cohort.DLCSmoothInterpCohort", "title": "<code>DLCSmoothInterpCohort</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Table to combine multiple bodyparts from DLCSmoothInterp to enable centroid/orientation calculations</p> Source code in <code>src/spyglass/position/v1/position_dlc_cohort.py</code> <pre><code>@schema\nclass DLCSmoothInterpCohort(dj.Computed):\n\"\"\"\n    Table to combine multiple bodyparts from DLCSmoothInterp\n    to enable centroid/orientation calculations\n    \"\"\"\n\n    # Need to ensure that nwb_file_name/epoch/interval list name endure as primary keys\n    definition = \"\"\"\n    -&gt; DLCSmoothInterpCohortSelection\n    ---\n    \"\"\"\n\n    class BodyPart(dj.Part):\n        definition = \"\"\"\n        -&gt; DLCSmoothInterpCohort\n        -&gt; DLCSmoothInterp\n        ---\n        -&gt; AnalysisNwbfile\n        dlc_smooth_interp_position_object_id : varchar(80)\n        dlc_smooth_interp_info_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n        def fetch1_dataframe(self):\n            nwb_data = self.fetch_nwb()[0]\n            index = pd.Index(\n                np.asarray(\n                    nwb_data[\"dlc_smooth_interp_position\"]\n                    .get_spatial_series()\n                    .timestamps\n                ),\n                name=\"time\",\n            )\n            COLUMNS = [\n                \"video_frame_ind\",\n                \"x\",\n                \"y\",\n            ]\n            return pd.DataFrame(\n                np.concatenate(\n                    (\n                        np.asarray(\n                            nwb_data[\"dlc_smooth_interp_info\"]\n                            .time_series[\"video_frame_ind\"]\n                            .data,\n                            dtype=int,\n                        )[:, np.newaxis],\n                        np.asarray(\n                            nwb_data[\"dlc_smooth_interp_position\"]\n                            .get_spatial_series()\n                            .data\n                        ),\n                    ),\n                    axis=1,\n                ),\n                columns=COLUMNS,\n                index=index,\n            )\n\n    def make(self, key):\n        from .dlc_utils import OutputLogger, infer_output_dir\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n            print_console=False,\n        ) as logger:\n            logger.logger.info(\"-----------------------\")\n            logger.logger.info(\"Bodypart Cohort\")\n            # from Jen Guidera\n            self.insert1(key)\n            cohort_selection = (DLCSmoothInterpCohortSelection &amp; key).fetch1()\n            table_entries = []\n            bodyparts_params_dict = cohort_selection.pop(\n                \"bodyparts_params_dict\"\n            )\n            temp_key = cohort_selection.copy()\n            for bodypart, params in bodyparts_params_dict.items():\n                temp_key[\"bodypart\"] = bodypart\n                temp_key[\"dlc_si_params_name\"] = params\n                table_entries.append((DLCSmoothInterp &amp; temp_key).fetch())\n            assert len(table_entries) == len(\n                bodyparts_params_dict\n            ), \"more entries found in DLCSmoothInterp than specified in bodyparts_params_dict\"\n            table_column_names = list(table_entries[0].dtype.fields.keys())\n            for table_entry in table_entries:\n                entry_key = {\n                    **{\n                        k: v for k, v in zip(table_column_names, table_entry[0])\n                    },\n                    **key,\n                }\n                DLCSmoothInterpCohort.BodyPart.insert1(\n                    entry_key, skip_duplicates=True\n                )\n        logger.logger.info(\"Inserted entry into DLCSmoothInterpCohort\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.position.v1.position_dlc_cohort.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.position.v1.position_dlc_cohort.DLCPoseEstimation", "title": "<code>DLCPoseEstimation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@schema\nclass DLCPoseEstimation(dj.Computed):\n    definition = \"\"\"\n    -&gt; DLCPoseEstimationSelection\n    ---\n    pose_estimation_time: datetime  # time of generation of this set of DLC results\n    meters_per_pixel : double       # conversion of meters per pixel for analyzed video\n    \"\"\"\n\n    class BodyPart(dj.Part):\n        definition = \"\"\" # uses DeepLabCut h5 output for body part position\n        -&gt; DLCPoseEstimation\n        -&gt; DLCModel.BodyPart\n        ---\n        -&gt; AnalysisNwbfile\n        dlc_pose_estimation_position_object_id : varchar(80)\n        dlc_pose_estimation_likelihood_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n        def fetch1_dataframe(self):\n            nwb_data = self.fetch_nwb()[0]\n            index = pd.Index(\n                np.asarray(\n                    nwb_data[\"dlc_pose_estimation_position\"]\n                    .get_spatial_series()\n                    .timestamps\n                ),\n                name=\"time\",\n            )\n            COLUMNS = [\n                \"video_frame_ind\",\n                \"x\",\n                \"y\",\n                \"likelihood\",\n            ]\n            return pd.DataFrame(\n                np.concatenate(\n                    (\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_likelihood\"]\n                            .time_series[\"video_frame_ind\"]\n                            .data,\n                            dtype=int,\n                        )[:, np.newaxis],\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_position\"]\n                            .get_spatial_series()\n                            .data\n                        ),\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_likelihood\"]\n                            .time_series[\"likelihood\"]\n                            .data\n                        )[:, np.newaxis],\n                    ),\n                    axis=1,\n                ),\n                columns=COLUMNS,\n                index=index,\n            )\n\n    def make(self, key):\n\"\"\".populate() method will launch training for each PoseEstimationTask\"\"\"\n        from . import dlc_reader\n        from .dlc_utils import get_video_path\n\n        METERS_PER_CM = 0.01\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n        ) as logger:\n            logger.logger.info(\"----------------------\")\n            logger.logger.info(\"Pose Estimation\")\n            # ID model and directories\n            dlc_model = (DLCModel &amp; key).fetch1()\n            bodyparts = (DLCModel.BodyPart &amp; key).fetch(\"bodypart\")\n            task_mode, analyze_video_params, video_path, output_dir = (\n                DLCPoseEstimationSelection &amp; key\n            ).fetch1(\n                \"task_mode\",\n                \"pose_estimation_params\",\n                \"video_path\",\n                \"pose_estimation_output_dir\",\n            )\n            analyze_video_params = analyze_video_params or {}\n\n            project_path = dlc_model[\"project_path\"]\n\n            # Trigger PoseEstimation\n            if task_mode == \"trigger\":\n                dlc_reader.do_pose_estimation(\n                    video_path,\n                    dlc_model,\n                    project_path,\n                    output_dir,\n                    **analyze_video_params,\n                )\n            dlc_result = dlc_reader.PoseEstimation(output_dir)\n            creation_time = datetime.fromtimestamp(\n                dlc_result.creation_time\n            ).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n            logger.logger.info(\"getting raw position\")\n            interval_list_name = f\"pos {key['epoch']-1} valid times\"\n            spatial_series = (\n                RawPosition()\n                &amp; {**key, \"interval_list_name\": interval_list_name}\n            ).fetch_nwb()[0][\"raw_position\"]\n            _, _, _, video_time = get_video_path(key)\n            pos_time = spatial_series.timestamps\n            # TODO: should get timestamps from VideoFile, but need the video_frame_ind from RawPosition,\n            # which also has timestamps\n            key[\"meters_per_pixel\"] = spatial_series.conversion\n\n            # Insert entry into DLCPoseEstimation\n            logger.logger.info(\n                \"Inserting %s, epoch %02d into DLCPoseEsimation\",\n                key[\"nwb_file_name\"],\n                key[\"epoch\"],\n            )\n            self.insert1({**key, \"pose_estimation_time\": creation_time})\n            meters_per_pixel = key[\"meters_per_pixel\"]\n            del key[\"meters_per_pixel\"]\n            body_parts = dlc_result.df.columns.levels[0]\n            body_parts_df = {}\n            # Insert dlc pose estimation into analysis NWB file for each body part.\n            for body_part in bodyparts:\n                if body_part in body_parts:\n                    body_parts_df[body_part] = pd.DataFrame.from_dict(\n                        {\n                            c: dlc_result.df.get(body_part).get(c).values\n                            for c in dlc_result.df.get(body_part).columns\n                        }\n                    )\n            idx = pd.IndexSlice\n            for body_part, part_df in body_parts_df.items():\n                logger.logger.info(\"converting to cm\")\n                part_df = convert_to_cm(part_df, meters_per_pixel)\n                logger.logger.info(\"adding timestamps to DataFrame\")\n                part_df = add_timestamps(\n                    part_df, pos_time=pos_time, video_time=video_time\n                )\n                key[\"bodypart\"] = body_part\n                key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                    key[\"nwb_file_name\"]\n                )\n                position = pynwb.behavior.Position()\n                likelihood = pynwb.behavior.BehavioralTimeSeries()\n                position.create_spatial_series(\n                    name=\"position\",\n                    timestamps=part_df.time.to_numpy(),\n                    conversion=METERS_PER_CM,\n                    data=part_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                    reference_frame=spatial_series.reference_frame,\n                    comments=spatial_series.comments,\n                    description=\"x_position, y_position\",\n                )\n                likelihood.create_timeseries(\n                    name=\"likelihood\",\n                    timestamps=part_df.time.to_numpy(),\n                    data=part_df.loc[:, idx[\"likelihood\"]].to_numpy(),\n                    unit=\"likelihood\",\n                    comments=\"no comments\",\n                    description=\"likelihood\",\n                )\n                likelihood.create_timeseries(\n                    name=\"video_frame_ind\",\n                    timestamps=part_df.time.to_numpy(),\n                    data=part_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                    unit=\"index\",\n                    comments=\"no comments\",\n                    description=\"video_frame_ind\",\n                )\n                nwb_analysis_file = AnalysisNwbfile()\n                key[\n                    \"dlc_pose_estimation_position_object_id\"\n                ] = nwb_analysis_file.add_nwb_object(\n                    analysis_file_name=key[\"analysis_file_name\"],\n                    nwb_object=position,\n                )\n                key[\n                    \"dlc_pose_estimation_likelihood_object_id\"\n                ] = nwb_analysis_file.add_nwb_object(\n                    analysis_file_name=key[\"analysis_file_name\"],\n                    nwb_object=likelihood,\n                )\n                nwb_analysis_file.add(\n                    nwb_file_name=key[\"nwb_file_name\"],\n                    analysis_file_name=key[\"analysis_file_name\"],\n                )\n                self.BodyPart.insert1(key)\n\n    def fetch_dataframe(self, *attrs, **kwargs):\n        entries = (self.BodyPart &amp; self).fetch(\"KEY\")\n        nwb_data_dict = {\n            entry[\"bodypart\"]: (self.BodyPart() &amp; entry).fetch_nwb()[0]\n            for entry in entries\n        }\n        index = pd.Index(\n            np.asarray(\n                nwb_data_dict[entries[0][\"bodypart\"]][\n                    \"dlc_pose_estimation_position\"\n                ]\n                .get_spatial_series()\n                .timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"x\",\n            \"y\",\n            \"likelihood\",\n        ]\n        return pd.concat(\n            {\n                entry[\"bodypart\"]: pd.DataFrame(\n                    np.concatenate(\n                        (\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_likelihood\"\n                                ]\n                                .time_series[\"video_frame_ind\"]\n                                .data,\n                                dtype=int,\n                            )[:, np.newaxis],\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_position\"\n                                ]\n                                .get_spatial_series()\n                                .data\n                            ),\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_likelihood\"\n                                ]\n                                .time_series[\"likelihood\"]\n                                .data\n                            )[:, np.newaxis],\n                        ),\n                        axis=1,\n                    ),\n                    columns=COLUMNS,\n                    index=index,\n                )\n                for entry in entries\n            },\n            axis=1,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimation.make", "title": "<code>make(key)</code>", "text": "<p>.populate() method will launch training for each PoseEstimationTask</p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>def make(self, key):\n\"\"\".populate() method will launch training for each PoseEstimationTask\"\"\"\n    from . import dlc_reader\n    from .dlc_utils import get_video_path\n\n    METERS_PER_CM = 0.01\n\n    output_dir = infer_output_dir(key=key, makedir=False)\n    with OutputLogger(\n        name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n        path=f\"{output_dir.as_posix()}/log.log\",\n    ) as logger:\n        logger.logger.info(\"----------------------\")\n        logger.logger.info(\"Pose Estimation\")\n        # ID model and directories\n        dlc_model = (DLCModel &amp; key).fetch1()\n        bodyparts = (DLCModel.BodyPart &amp; key).fetch(\"bodypart\")\n        task_mode, analyze_video_params, video_path, output_dir = (\n            DLCPoseEstimationSelection &amp; key\n        ).fetch1(\n            \"task_mode\",\n            \"pose_estimation_params\",\n            \"video_path\",\n            \"pose_estimation_output_dir\",\n        )\n        analyze_video_params = analyze_video_params or {}\n\n        project_path = dlc_model[\"project_path\"]\n\n        # Trigger PoseEstimation\n        if task_mode == \"trigger\":\n            dlc_reader.do_pose_estimation(\n                video_path,\n                dlc_model,\n                project_path,\n                output_dir,\n                **analyze_video_params,\n            )\n        dlc_result = dlc_reader.PoseEstimation(output_dir)\n        creation_time = datetime.fromtimestamp(\n            dlc_result.creation_time\n        ).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        logger.logger.info(\"getting raw position\")\n        interval_list_name = f\"pos {key['epoch']-1} valid times\"\n        spatial_series = (\n            RawPosition()\n            &amp; {**key, \"interval_list_name\": interval_list_name}\n        ).fetch_nwb()[0][\"raw_position\"]\n        _, _, _, video_time = get_video_path(key)\n        pos_time = spatial_series.timestamps\n        # TODO: should get timestamps from VideoFile, but need the video_frame_ind from RawPosition,\n        # which also has timestamps\n        key[\"meters_per_pixel\"] = spatial_series.conversion\n\n        # Insert entry into DLCPoseEstimation\n        logger.logger.info(\n            \"Inserting %s, epoch %02d into DLCPoseEsimation\",\n            key[\"nwb_file_name\"],\n            key[\"epoch\"],\n        )\n        self.insert1({**key, \"pose_estimation_time\": creation_time})\n        meters_per_pixel = key[\"meters_per_pixel\"]\n        del key[\"meters_per_pixel\"]\n        body_parts = dlc_result.df.columns.levels[0]\n        body_parts_df = {}\n        # Insert dlc pose estimation into analysis NWB file for each body part.\n        for body_part in bodyparts:\n            if body_part in body_parts:\n                body_parts_df[body_part] = pd.DataFrame.from_dict(\n                    {\n                        c: dlc_result.df.get(body_part).get(c).values\n                        for c in dlc_result.df.get(body_part).columns\n                    }\n                )\n        idx = pd.IndexSlice\n        for body_part, part_df in body_parts_df.items():\n            logger.logger.info(\"converting to cm\")\n            part_df = convert_to_cm(part_df, meters_per_pixel)\n            logger.logger.info(\"adding timestamps to DataFrame\")\n            part_df = add_timestamps(\n                part_df, pos_time=pos_time, video_time=video_time\n            )\n            key[\"bodypart\"] = body_part\n            key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                key[\"nwb_file_name\"]\n            )\n            position = pynwb.behavior.Position()\n            likelihood = pynwb.behavior.BehavioralTimeSeries()\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=part_df.time.to_numpy(),\n                conversion=METERS_PER_CM,\n                data=part_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"x_position, y_position\",\n            )\n            likelihood.create_timeseries(\n                name=\"likelihood\",\n                timestamps=part_df.time.to_numpy(),\n                data=part_df.loc[:, idx[\"likelihood\"]].to_numpy(),\n                unit=\"likelihood\",\n                comments=\"no comments\",\n                description=\"likelihood\",\n            )\n            likelihood.create_timeseries(\n                name=\"video_frame_ind\",\n                timestamps=part_df.time.to_numpy(),\n                data=part_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                unit=\"index\",\n                comments=\"no comments\",\n                description=\"video_frame_ind\",\n            )\n            nwb_analysis_file = AnalysisNwbfile()\n            key[\n                \"dlc_pose_estimation_position_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=position,\n            )\n            key[\n                \"dlc_pose_estimation_likelihood_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=likelihood,\n            )\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n            self.BodyPart.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.position.v1.position_dlc_cohort.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_cohort/#src.spyglass.position.v1.position_dlc_cohort.DLCSmoothInterp", "title": "<code>DLCSmoothInterp</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Interpolates across low likelihood periods and smooths the position Can take a few minutes.</p> Source code in <code>src/spyglass/position/v1/position_dlc_position.py</code> <pre><code>@schema\nclass DLCSmoothInterp(dj.Computed):\n\"\"\"\n    Interpolates across low likelihood periods and smooths the position\n    Can take a few minutes.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCSmoothInterpSelection\n    ---\n    -&gt; AnalysisNwbfile\n    dlc_smooth_interp_position_object_id : varchar(80)\n    dlc_smooth_interp_info_object_id : varchar(80)\n    \"\"\"\n\n    def make(self, key):\n        from .dlc_utils import OutputLogger, infer_output_dir\n\n        METERS_PER_CM = 0.01\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n            print_console=False,\n        ) as logger:\n            logger.logger.info(\"-----------------------\")\n            idx = pd.IndexSlice\n            # Get labels to smooth from Parameters table\n            params = (DLCSmoothInterpParams() &amp; key).fetch1(\"params\")\n            # Get DLC output dataframe\n            logger.logger.info(\"fetching Pose Estimation Dataframe\")\n            dlc_df = (DLCPoseEstimation.BodyPart() &amp; key).fetch1_dataframe()\n            dt = np.median(np.diff(dlc_df.index.to_numpy()))\n            sampling_rate = 1 / dt\n            logger.logger.info(\"Identifying indices to NaN\")\n            df_w_nans, bad_inds = nan_inds(\n                dlc_df.copy(),\n                params[\"max_cm_between_pts\"],\n                likelihood_thresh=params.pop(\"likelihood_thresh\"),\n                inds_to_span=params[\"num_inds_to_span\"],\n            )\n\n            nan_spans = get_span_start_stop(np.where(bad_inds)[0])\n            if params[\"interpolate\"]:\n                logger.logger.info(\"interpolating across low likelihood times\")\n                interp_df = interp_pos(\n                    df_w_nans.copy(), nan_spans, **params[\"interp_params\"]\n                )\n            else:\n                interp_df = df_w_nans.copy()\n                logger.logger.info(\"skipping interpolation\")\n            if params[\"smooth\"]:\n                if \"smoothing_duration\" in params[\"smoothing_params\"]:\n                    smoothing_duration = params[\"smoothing_params\"].pop(\n                        \"smoothing_duration\"\n                    )\n                dt = np.median(np.diff(dlc_df.index.to_numpy()))\n                sampling_rate = 1 / dt\n                logger.logger.info(\"smoothing position\")\n                smooth_func = _key_to_smooth_func_dict[\n                    params[\"smoothing_params\"][\"smooth_method\"]\n                ]\n                logger.logger.info(\n                    \"Smoothing using method: %s\",\n                    str(params[\"smoothing_params\"][\"smooth_method\"]),\n                )\n                smooth_df = smooth_func(\n                    interp_df,\n                    smoothing_duration=smoothing_duration,\n                    sampling_rate=sampling_rate,\n                    **params[\"smoothing_params\"],\n                )\n            else:\n                smooth_df = interp_df.copy()\n                logger.logger.info(\"skipping smoothing\")\n            final_df = smooth_df.drop([\"likelihood\"], axis=1)\n            final_df = final_df.rename_axis(\"time\").reset_index()\n            position_nwb_data = (\n                (DLCPoseEstimation.BodyPart() &amp; key)\n                .fetch_nwb()[0][\"dlc_pose_estimation_position\"]\n                .get_spatial_series()\n            )\n            key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                key[\"nwb_file_name\"]\n            )\n            # Add dataframe to AnalysisNwbfile\n            nwb_analysis_file = AnalysisNwbfile()\n            position = pynwb.behavior.Position()\n            video_frame_ind = pynwb.behavior.BehavioralTimeSeries()\n            logger.logger.info(\"Creating NWB objects\")\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=final_df.time.to_numpy(),\n                conversion=METERS_PER_CM,\n                data=final_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                reference_frame=position_nwb_data.reference_frame,\n                comments=position_nwb_data.comments,\n                description=\"x_position, y_position\",\n            )\n            video_frame_ind.create_timeseries(\n                name=\"video_frame_ind\",\n                timestamps=final_df.time.to_numpy(),\n                data=final_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                unit=\"index\",\n                comments=\"no comments\",\n                description=\"video_frame_ind\",\n            )\n            key[\n                \"dlc_smooth_interp_position_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=position,\n            )\n            key[\n                \"dlc_smooth_interp_info_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=video_frame_ind,\n            )\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n            self.insert1(key)\n            logger.logger.info(\"inserted entry into DLCSmoothInterp\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(\n                nwb_data[\"dlc_smooth_interp_position\"]\n                .get_spatial_series()\n                .timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"x\",\n            \"y\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"dlc_smooth_interp_info\"]\n                        .time_series[\"video_frame_ind\"]\n                        .data,\n                        dtype=int,\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"dlc_smooth_interp_position\"]\n                        .get_spatial_series()\n                        .data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/", "title": "position_dlc_model.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.DLCModelInput", "title": "<code>DLCModelInput</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Table to hold model path if model is being input from local disk instead of Spyglass</p> Source code in <code>src/spyglass/position/v1/position_dlc_model.py</code> <pre><code>@schema\nclass DLCModelInput(dj.Manual):\n\"\"\"Table to hold model path if model is being input\n    from local disk instead of Spyglass\n    \"\"\"\n\n    definition = \"\"\"\n    dlc_model_name : varchar(64)  # Different than dlc_model_name in DLCModelSource... not great\n    -&gt; DLCProject\n    ---\n    project_path         : varchar(255) # Path to project directory\n    \"\"\"\n\n    def insert1(self, key, **kwargs):\n        # expects key from DLCProject with config_path\n        project_path = Path(key[\"config_path\"]).parent\n        assert project_path.exists(), \"project path does not exist\"\n        key[\"dlc_model_name\"] = f'{project_path.name.split(\"model\")[0]}model'\n        key[\"project_path\"] = project_path.as_posix()\n        del key[\"config_path\"]\n        super().insert1(key, **kwargs)\n        DLCModelSource.insert_entry(\n            dlc_model_name=key[\"dlc_model_name\"],\n            project_name=key[\"project_name\"],\n            source=\"FromImport\",\n            key=key,\n            skip_duplicates=True,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.BodyPart", "title": "<code>BodyPart</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Holds bodyparts for use in DeepLabCut models</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@schema\nclass BodyPart(dj.Manual):\n\"\"\"Holds bodyparts for use in DeepLabCut models\"\"\"\n\n    definition = \"\"\"\n    bodypart                : varchar(32)\n    ---\n    bodypart_description='' : varchar(80)\n    \"\"\"\n\n    @classmethod\n    def add_from_config(cls, bodyparts: List, descriptions: List = None):\n\"\"\"Given a list of bodyparts from the config and\n        an optional list of descriptions, inserts into BodyPart table.\n\n        Parameters\n        ----------\n        bodyparts : List\n            list of bodyparts from config\n        descriptions : List, default None\n            optional list of descriptions for bodyparts.\n            If None, description is set to bodypart name\n        \"\"\"\n        if descriptions is not None:\n            bodyparts_dict = [\n                {\"bodypart\": bp, \"bodypart_description\": desc}\n                for (bp, desc) in zip(bodyparts, descriptions)\n            ]\n        else:\n            bodyparts_dict = [\n                {\"bodypart\": bp, \"bodypart_description\": bp} for bp in bodyparts\n            ]\n        cls.insert(bodyparts_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_project.BodyPart.add_from_config", "title": "<code>add_from_config(bodyparts, descriptions=None)</code>  <code>classmethod</code>", "text": "<p>Given a list of bodyparts from the config and an optional list of descriptions, inserts into BodyPart table.</p> <p>Parameters:</p> Name Type Description Default <code>bodyparts</code> <code>List</code> <p>list of bodyparts from config</p> required <code>descriptions</code> <code>List</code> <p>optional list of descriptions for bodyparts. If None, description is set to bodypart name</p> <code>None</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef add_from_config(cls, bodyparts: List, descriptions: List = None):\n\"\"\"Given a list of bodyparts from the config and\n    an optional list of descriptions, inserts into BodyPart table.\n\n    Parameters\n    ----------\n    bodyparts : List\n        list of bodyparts from config\n    descriptions : List, default None\n        optional list of descriptions for bodyparts.\n        If None, description is set to bodypart name\n    \"\"\"\n    if descriptions is not None:\n        bodyparts_dict = [\n            {\"bodypart\": bp, \"bodypart_description\": desc}\n            for (bp, desc) in zip(bodyparts, descriptions)\n        ]\n    else:\n        bodyparts_dict = [\n            {\"bodypart\": bp, \"bodypart_description\": bp} for bp in bodyparts\n        ]\n    cls.insert(bodyparts_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.DLCModelSource", "title": "<code>DLCModelSource</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Table to determine whether model originates from upstream DLCModelTraining table, or from local directory</p> Source code in <code>src/spyglass/position/v1/position_dlc_model.py</code> <pre><code>@schema\nclass DLCModelSource(dj.Manual):\n\"\"\"Table to determine whether model originates from\n    upstream DLCModelTraining table, or from local directory\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCProject\n    dlc_model_name : varchar(64)    # User-friendly model name\n    ---\n    source         : enum ('FromUpstream', 'FromImport')\n    \"\"\"\n\n    class FromImport(dj.Part):\n        definition = \"\"\"\n        -&gt; DLCModelSource\n        -&gt; DLCModelInput\n        ---\n        project_path : varchar(255)\n        \"\"\"\n\n    class FromUpstream(dj.Part):\n        definition = \"\"\"\n        -&gt; DLCModelSource\n        -&gt; DLCModelTraining\n        ---\n        project_path : varchar(255)\n        \"\"\"\n\n    @classmethod\n    @accepts(None, None, (\"FromUpstream\", \"FromImport\"), None)\n    def insert_entry(\n        cls,\n        dlc_model_name: str,\n        project_name: str,\n        source: str = \"FromUpstream\",\n        key: dict = None,\n        **kwargs,\n    ):\n        cls.insert1(\n            {\n                \"dlc_model_name\": dlc_model_name,\n                \"project_name\": project_name,\n                \"source\": source,\n            },\n            **kwargs,\n        )\n        part_table = getattr(cls, source)\n        table_query = dj.FreeTable(\n            dj.conn(), full_table_name=part_table.parents()[-1]\n        ) &amp; {\"project_name\": project_name}\n        project_path = table_query.fetch1(\"project_path\")\n        part_table.insert1(\n            {\n                \"dlc_model_name\": dlc_model_name,\n                \"project_name\": project_name,\n                \"project_path\": project_path,\n                **key,\n            },\n            **kwargs,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.DLCProject", "title": "<code>DLCProject</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Table to facilitate creation of a new DeepLabCut model. With ability to edit config, extract frames, label frames</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@schema\nclass DLCProject(dj.Manual):\n\"\"\"Table to facilitate creation of a new DeepLabCut model.\n    With ability to edit config, extract frames, label frames\n    \"\"\"\n\n    # Add more parameters as secondary keys...\n    # TODO: collapse params into blob dict\n    definition = \"\"\"\n    project_name     : varchar(100) # name of DLC project\n    ---\n    -&gt; LabTeam\n    bodyparts        : blob         # list of bodyparts to label\n    frames_per_video : int          # number of frames to extract from each video\n    config_path      : varchar(120) # path to config.yaml for model\n    \"\"\"\n\n    class BodyPart(dj.Part):\n\"\"\"Part table to hold bodyparts used in each project.\"\"\"\n\n        definition = \"\"\"\n        -&gt; DLCProject\n        -&gt; BodyPart\n        \"\"\"\n\n    class File(dj.Part):\n        definition = \"\"\"\n        # Paths of training files (e.g., labeled pngs, CSV or video)\n        -&gt; DLCProject\n        file_name: varchar(200) # Concise name to describe file\n        file_ext : enum(\"mp4\", \"csv\", \"h5\") # extension of file\n        ---\n        file_path: varchar(255)\n        \"\"\"\n\n    def insert1(self, key, **kwargs):\n        assert isinstance(\n            key[\"project_name\"], str\n        ), \"project_name must be a string\"\n        assert isinstance(\n            key[\"frames_per_video\"], int\n        ), \"frames_per_video must be of type `int`\"\n        super().insert1(key, **kwargs)\n\n    @classmethod\n    def insert_existing_project(\n        cls,\n        project_name: str,\n        lab_team: str,\n        config_path: str,\n        bodyparts: List = None,\n        frames_per_video: int = None,\n        add_to_files: bool = True,\n        **kwargs,\n    ):\n\"\"\"\n        insert an existing project into DLCProject table.\n        Parameters\n        ----------\n        project_name : str\n            user-friendly name of project\n        lab_team : str\n            name of lab team. Should match an entry in LabTeam table\n        config_path : str\n            path to project directory\n        bodyparts : list\n            optional list of bodyparts to label that\n            are not already in existing config\n        \"\"\"\n\n        # Read config\n        project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n        if project_name in project_names_in_use:\n            print(f\"project name: {project_name} is already in use.\")\n            return_key = {}\n            return_key[\"project_name\"], return_key[\"config_path\"] = (\n                cls &amp; {\"project_name\": project_name}\n            ).fetch1(\"project_name\", \"config_path\")\n            return return_key\n        from deeplabcut.utils.auxiliaryfunctions import read_config\n\n        cfg = read_config(config_path)\n        if bodyparts:\n            bodyparts_to_add = [\n                bodypart\n                for bodypart in bodyparts\n                if bodypart not in cfg[\"bodyparts\"]\n            ]\n            all_bodyparts = bodyparts_to_add + cfg[\"bodyparts\"]\n        else:\n            all_bodyparts = cfg[\"bodyparts\"]\n        BodyPart.add_from_config(cfg[\"bodyparts\"])\n        for bodypart in all_bodyparts:\n            if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n                raise ValueError(\n                    f\"bodypart: {bodypart} not found in BodyPart table\"\n                )\n        # check bodyparts are in config, if not add\n        if len(bodyparts_to_add) &gt; 0:\n            add_to_config(config_path, bodyparts=bodyparts_to_add)\n        # Get frames per video from config. If passed as arg, check match\n        if frames_per_video:\n            if frames_per_video != cfg[\"numframes2pick\"]:\n                add_to_config(\n                    config_path, **{\"numframes2pick\": frames_per_video}\n                )\n        config_path = Path(config_path)\n        project_path = config_path.parent\n        dlc_project_path = os.environ[\"DLC_PROJECT_PATH\"]\n        if dlc_project_path not in project_path.as_posix():\n            project_dirname = project_path.name\n            dest_folder = Path(f\"{dlc_project_path}/{project_dirname}/\")\n            if dest_folder.exists():\n                new_proj_dir = dest_folder.as_posix()\n            else:\n                new_proj_dir = shutil.copytree(\n                    src=project_path,\n                    dst=f\"{dlc_project_path}/{project_dirname}/\",\n                )\n            new_config_path = Path(f\"{new_proj_dir}/config.yaml\")\n            assert (\n                new_config_path.exists()\n            ), \"config.yaml does not exist in new project directory\"\n            config_path = new_config_path\n            add_to_config(config_path, **{\"project_path\": new_proj_dir})\n        # TODO still need to copy videos over to video dir\n        key = {\n            \"project_name\": project_name,\n            \"team_name\": lab_team,\n            \"bodyparts\": bodyparts,\n            \"config_path\": config_path.as_posix(),\n            \"frames_per_video\": frames_per_video,\n        }\n        cls.insert1(key, **kwargs)\n        cls.BodyPart.insert(\n            [\n                {\"project_name\": project_name, \"bodypart\": bp}\n                for bp in all_bodyparts\n            ],\n            **kwargs,\n        )\n        if add_to_files:\n            del key[\"bodyparts\"]\n            del key[\"team_name\"]\n            del key[\"config_path\"]\n            del key[\"frames_per_video\"]\n            # Check for training files to add\n            cls.add_training_files(key, **kwargs)\n        return {\n            \"project_name\": project_name,\n            \"config_path\": config_path.as_posix(),\n        }\n\n    @classmethod\n    def insert_new_project(\n        cls,\n        project_name: str,\n        bodyparts: List,\n        lab_team: str,\n        frames_per_video: int,\n        video_list: List,\n        project_directory: str = os.getenv(\"DLC_PROJECT_PATH\"),\n        output_path: str = os.getenv(\"DLC_VIDEO_PATH\"),\n        set_permissions=False,\n        **kwargs,\n    ):\n\"\"\"\n        insert a new project into DLCProject table.\n        Parameters\n        ----------\n        project_name : str\n            user-friendly name of project\n        bodyparts : list\n            list of bodyparts to label. Should match bodyparts in BodyPart table\n        lab_team : str\n            name of lab team. Should match an entry in LabTeam table\n        project_directory : str\n            directory where to create project.\n            (Default is '/cumulus/deeplabcut/')\n        frames_per_video : int\n            number of frames to extract from each video\n        video_list : list\n            list of dicts of form [{'nwb_file_name': nwb_file_name, 'epoch': epoch #},...]\n            to query VideoFile table for videos to train on.\n            Can also be list of absolute paths to import videos from\n        output_path : str\n            target path to output converted videos\n            (Default is '/nimbus/deeplabcut/videos/')\n        set_permissions : bool\n            if True, will set permissions for user and group to be read+write\n            (Default is False)\n        \"\"\"\n        project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n        if project_name in project_names_in_use:\n            print(f\"project name: {project_name} is already in use.\")\n            return_key = {}\n            return_key[\"project_name\"], return_key[\"config_path\"] = (\n                cls &amp; {\"project_name\": project_name}\n            ).fetch1(\"project_name\", \"config_path\")\n            return return_key\n\n        add_to_files = kwargs.pop(\"add_to_files\", True)\n        if not bool(LabTeam() &amp; {\"team_name\": lab_team}):\n            raise ValueError(f\"team_name: {lab_team} does not exist in LabTeam\")\n        skeleton_node = None\n        # If dict, assume of form {'nwb_file_name': nwb_file_name, 'epoch': epoch}\n        # and pass to get_video_path to reference VideoFile table for path\n\n        if all(isinstance(n, Dict) for n in video_list):\n            videos_to_convert = [\n                get_video_path(video_key) for video_key in video_list\n            ]\n            videos = [\n                check_videofile(\n                    video_path=video[0],\n                    output_path=output_path,\n                    video_filename=video[1],\n                )[0].as_posix()\n                for video in videos_to_convert\n            ]\n        # If not dict, assume list of video file paths that may or may not need to be converted\n        else:\n            videos = []\n            if not all([Path(video).exists() for video in video_list]):\n                raise OSError(\"at least one file in video_list does not exist\")\n            for video in video_list:\n                video_path = Path(video).parent\n                video_filename = video.rsplit(\n                    video_path.as_posix(), maxsplit=1\n                )[-1].split(\"/\")[-1]\n                videos.extend(\n                    [\n                        check_videofile(\n                            video_path=video_path,\n                            output_path=output_path,\n                            video_filename=video_filename,\n                        )[0].as_posix()\n                    ]\n                )\n            if len(videos) &lt; 1:\n                raise ValueError(f\"no .mp4 videos found in{video_path}\")\n        from deeplabcut import create_new_project\n\n        config_path = create_new_project(\n            project_name,\n            lab_team,\n            videos,\n            working_directory=project_directory,\n            copy_videos=True,\n            multianimal=False,\n        )\n        for bodypart in bodyparts:\n            if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n                raise ValueError(\n                    f\"bodypart: {bodypart} not found in BodyPart table\"\n                )\n        kwargs_copy = copy.deepcopy(kwargs)\n        kwargs_copy.update({\"numframes2pick\": frames_per_video, \"dotsize\": 3})\n        add_to_config(\n            config_path, bodyparts, skeleton_node=skeleton_node, **kwargs_copy\n        )\n        key = {\n            \"project_name\": project_name,\n            \"team_name\": lab_team,\n            \"bodyparts\": bodyparts,\n            \"config_path\": config_path,\n            \"frames_per_video\": frames_per_video,\n        }\n        # TODO: make permissions setting more flexible.\n        if set_permissions:\n            permissions = (\n                stat.S_IRUSR\n                | stat.S_IWUSR\n                | stat.S_IRGRP\n                | stat.S_IWGRP\n                | stat.S_IROTH\n            )\n            username = getpass.getuser()\n            if not groupname:\n                groupname = username\n            _set_permissions(\n                directory=project_directory,\n                mode=permissions,\n                username=username,\n                groupname=groupname,\n            )\n        cls.insert1(key, **kwargs)\n        cls.BodyPart.insert(\n            [\n                {\"project_name\": project_name, \"bodypart\": bp}\n                for bp in bodyparts\n            ],\n            **kwargs,\n        )\n        if add_to_files:\n            del key[\"bodyparts\"]\n            del key[\"team_name\"]\n            del key[\"config_path\"]\n            del key[\"frames_per_video\"]\n            # Add videos to training files\n            cls.add_training_files(key, **kwargs)\n        if isinstance(config_path, PosixPath):\n            config_path = config_path.as_posix()\n        return {\"project_name\": project_name, \"config_path\": config_path}\n\n    @classmethod\n    def add_training_files(cls, key, **kwargs):\n\"\"\"Add training videos and labeled frames .h5 and .csv to DLCProject.File\"\"\"\n        config_path = (cls &amp; {\"project_name\": key[\"project_name\"]}).fetch1(\n            \"config_path\"\n        )\n        from deeplabcut.utils.auxiliaryfunctions import read_config\n\n        if \"config_path\" in key:\n            del key[\"config_path\"]\n        cfg = read_config(config_path)\n        video_names = list(cfg[\"video_sets\"].keys())\n        training_files = []\n        for video in video_names:\n            video_name = os.path.splitext(\n                video.split(os.path.dirname(video) + \"/\")[-1]\n            )[0]\n            training_files.extend(\n                glob.glob(\n                    f\"{cfg['project_path']}/labeled-data/{video_name}/*Collected*\"\n                )\n            )\n        for video in video_names:\n            key[\"file_name\"] = f'{os.path.splitext(video.split(\"/\")[-1])[0]}'\n            key[\"file_ext\"] = os.path.splitext(video.split(\"/\")[-1])[-1].split(\n                \".\"\n            )[-1]\n            key[\"file_path\"] = video\n            cls.File.insert1(key, **kwargs)\n        if len(training_files) &gt; 0:\n            for file in training_files:\n                video_name = os.path.dirname(file).split(\"/\")[-1]\n                file_type = os.path.splitext(\n                    file.split(os.path.dirname(file) + \"/\")[-1]\n                )[-1].split(\".\")[-1]\n                key[\"file_name\"] = f\"{video_name}_labeled_data\"\n                key[\"file_ext\"] = file_type\n                key[\"file_path\"] = file\n                cls.File.insert1(key, **kwargs)\n        else:\n            Warning(\"No training files to add\")\n\n    @classmethod\n    def run_extract_frames(cls, key, **kwargs):\n\"\"\"Convenience function to launch DLC GUI for extracting frames.\n        Must be run on local machine to access GUI,\n        cannot be run through ssh tunnel\n        \"\"\"\n        config_path = (cls &amp; key).fetch1(\"config_path\")\n        from deeplabcut import extract_frames\n\n        extract_frames(config_path, **kwargs)\n\n    @classmethod\n    def run_label_frames(cls, key):\n\"\"\"Convenience function to launch DLC GUI for labeling frames.\n        Must be run on local machine to access GUI,\n        cannot be run through ssh tunnel\n        \"\"\"\n        config_path = (cls &amp; key).fetch1(\"config_path\")\n        from deeplabcut import label_frames\n\n        label_frames(config_path)\n\n    @classmethod\n    def check_labels(cls, key, **kwargs):\n\"\"\"Convenience function to check labels on\n        previously extracted and labeled frames\n        \"\"\"\n        config_path = (cls &amp; key).fetch1(\"config_path\")\n        from deeplabcut import check_labels\n\n        check_labels(config_path, **kwargs)\n\n    @classmethod\n    def import_labeled_frames(\n        cls,\n        key: Dict,\n        import_project_path: Union[str, PosixPath],\n        video_filenames: Union[str, List],\n        **kwargs,\n    ):\n\"\"\"Function to import pre-labeled frames from an existing project into a new project\n\n        Parameters\n        ----------\n        key : Dict\n            key to specify entry in DLCProject table to add labeled frames to\n        import_project_path : str\n            absolute path to project directory containing labeled frames to import\n        video_filenames : str or List\n            filename or list of filenames of video(s) from which to import frames.\n            without file extension\n        \"\"\"\n        project_entry = (cls &amp; key).fetch1()\n        team_name = project_entry[\"team_name\"]\n        current_project_path = Path(project_entry[\"config_path\"]).parent\n        current_labeled_data_path = Path(\n            f\"{current_project_path.as_posix()}/labeled-data\"\n        )\n        if isinstance(import_project_path, PosixPath):\n            assert (\n                import_project_path.exists()\n            ), f\"import_project_path: {import_project_path.as_posix()} does not exist\"\n            import_labeled_data_path = Path(\n                f\"{import_project_path.as_posix()}/labeled-data\"\n            )\n        else:\n            assert Path(\n                import_project_path\n            ).exists(), (\n                f\"import_project_path: {import_project_path} does not exist\"\n            )\n            import_labeled_data_path = Path(\n                f\"{import_project_path}/labeled-data\"\n            )\n        assert (\n            import_labeled_data_path.exists()\n        ), \"import_project has no directory 'labeled-data'\"\n        if not isinstance(video_filenames, List):\n            video_filenames = [video_filenames]\n        for video_file in video_filenames:\n            h5_file = glob.glob(\n                f\"{import_labeled_data_path.as_posix()}/{video_file}/*.h5\"\n            )[0]\n            dlc_df = pd.read_hdf(h5_file)\n            dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n            dlc_df.to_hdf(\n                Path(\n                    f\"{current_labeled_data_path.as_posix()}/{video_file}/CollectedData_{team_name}.h5\"\n                ).as_posix(),\n                \"df_with_missing\",\n            )\n        cls.add_training_files(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_project.DLCProject.BodyPart", "title": "<code>BodyPart</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Part table to hold bodyparts used in each project.</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>class BodyPart(dj.Part):\n\"\"\"Part table to hold bodyparts used in each project.\"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCProject\n    -&gt; BodyPart\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_project.DLCProject.insert_existing_project", "title": "<code>insert_existing_project(project_name, lab_team, config_path, bodyparts=None, frames_per_video=None, add_to_files=True, **kwargs)</code>  <code>classmethod</code>", "text": "<p>insert an existing project into DLCProject table.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>user-friendly name of project</p> required <code>lab_team</code> <code>str</code> <p>name of lab team. Should match an entry in LabTeam table</p> required <code>config_path</code> <code>str</code> <p>path to project directory</p> required <code>bodyparts</code> <code>list</code> <p>optional list of bodyparts to label that are not already in existing config</p> <code>None</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef insert_existing_project(\n    cls,\n    project_name: str,\n    lab_team: str,\n    config_path: str,\n    bodyparts: List = None,\n    frames_per_video: int = None,\n    add_to_files: bool = True,\n    **kwargs,\n):\n\"\"\"\n    insert an existing project into DLCProject table.\n    Parameters\n    ----------\n    project_name : str\n        user-friendly name of project\n    lab_team : str\n        name of lab team. Should match an entry in LabTeam table\n    config_path : str\n        path to project directory\n    bodyparts : list\n        optional list of bodyparts to label that\n        are not already in existing config\n    \"\"\"\n\n    # Read config\n    project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n    if project_name in project_names_in_use:\n        print(f\"project name: {project_name} is already in use.\")\n        return_key = {}\n        return_key[\"project_name\"], return_key[\"config_path\"] = (\n            cls &amp; {\"project_name\": project_name}\n        ).fetch1(\"project_name\", \"config_path\")\n        return return_key\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    cfg = read_config(config_path)\n    if bodyparts:\n        bodyparts_to_add = [\n            bodypart\n            for bodypart in bodyparts\n            if bodypart not in cfg[\"bodyparts\"]\n        ]\n        all_bodyparts = bodyparts_to_add + cfg[\"bodyparts\"]\n    else:\n        all_bodyparts = cfg[\"bodyparts\"]\n    BodyPart.add_from_config(cfg[\"bodyparts\"])\n    for bodypart in all_bodyparts:\n        if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n            raise ValueError(\n                f\"bodypart: {bodypart} not found in BodyPart table\"\n            )\n    # check bodyparts are in config, if not add\n    if len(bodyparts_to_add) &gt; 0:\n        add_to_config(config_path, bodyparts=bodyparts_to_add)\n    # Get frames per video from config. If passed as arg, check match\n    if frames_per_video:\n        if frames_per_video != cfg[\"numframes2pick\"]:\n            add_to_config(\n                config_path, **{\"numframes2pick\": frames_per_video}\n            )\n    config_path = Path(config_path)\n    project_path = config_path.parent\n    dlc_project_path = os.environ[\"DLC_PROJECT_PATH\"]\n    if dlc_project_path not in project_path.as_posix():\n        project_dirname = project_path.name\n        dest_folder = Path(f\"{dlc_project_path}/{project_dirname}/\")\n        if dest_folder.exists():\n            new_proj_dir = dest_folder.as_posix()\n        else:\n            new_proj_dir = shutil.copytree(\n                src=project_path,\n                dst=f\"{dlc_project_path}/{project_dirname}/\",\n            )\n        new_config_path = Path(f\"{new_proj_dir}/config.yaml\")\n        assert (\n            new_config_path.exists()\n        ), \"config.yaml does not exist in new project directory\"\n        config_path = new_config_path\n        add_to_config(config_path, **{\"project_path\": new_proj_dir})\n    # TODO still need to copy videos over to video dir\n    key = {\n        \"project_name\": project_name,\n        \"team_name\": lab_team,\n        \"bodyparts\": bodyparts,\n        \"config_path\": config_path.as_posix(),\n        \"frames_per_video\": frames_per_video,\n    }\n    cls.insert1(key, **kwargs)\n    cls.BodyPart.insert(\n        [\n            {\"project_name\": project_name, \"bodypart\": bp}\n            for bp in all_bodyparts\n        ],\n        **kwargs,\n    )\n    if add_to_files:\n        del key[\"bodyparts\"]\n        del key[\"team_name\"]\n        del key[\"config_path\"]\n        del key[\"frames_per_video\"]\n        # Check for training files to add\n        cls.add_training_files(key, **kwargs)\n    return {\n        \"project_name\": project_name,\n        \"config_path\": config_path.as_posix(),\n    }\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_project.DLCProject.insert_new_project", "title": "<code>insert_new_project(project_name, bodyparts, lab_team, frames_per_video, video_list, project_directory=os.getenv('DLC_PROJECT_PATH'), output_path=os.getenv('DLC_VIDEO_PATH'), set_permissions=False, **kwargs)</code>  <code>classmethod</code>", "text": "<p>insert a new project into DLCProject table.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>user-friendly name of project</p> required <code>bodyparts</code> <code>list</code> <p>list of bodyparts to label. Should match bodyparts in BodyPart table</p> required <code>lab_team</code> <code>str</code> <p>name of lab team. Should match an entry in LabTeam table</p> required <code>project_directory</code> <code>str</code> <p>directory where to create project. (Default is '/cumulus/deeplabcut/')</p> <code>os.getenv('DLC_PROJECT_PATH')</code> <code>frames_per_video</code> <code>int</code> <p>number of frames to extract from each video</p> required <code>video_list</code> <code>list</code> <p>list of dicts of form [{'nwb_file_name': nwb_file_name, 'epoch': epoch #},...] to query VideoFile table for videos to train on. Can also be list of absolute paths to import videos from</p> required <code>output_path</code> <code>str</code> <p>target path to output converted videos (Default is '/nimbus/deeplabcut/videos/')</p> <code>os.getenv('DLC_VIDEO_PATH')</code> <code>set_permissions</code> <code>bool</code> <p>if True, will set permissions for user and group to be read+write (Default is False)</p> <code>False</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef insert_new_project(\n    cls,\n    project_name: str,\n    bodyparts: List,\n    lab_team: str,\n    frames_per_video: int,\n    video_list: List,\n    project_directory: str = os.getenv(\"DLC_PROJECT_PATH\"),\n    output_path: str = os.getenv(\"DLC_VIDEO_PATH\"),\n    set_permissions=False,\n    **kwargs,\n):\n\"\"\"\n    insert a new project into DLCProject table.\n    Parameters\n    ----------\n    project_name : str\n        user-friendly name of project\n    bodyparts : list\n        list of bodyparts to label. Should match bodyparts in BodyPart table\n    lab_team : str\n        name of lab team. Should match an entry in LabTeam table\n    project_directory : str\n        directory where to create project.\n        (Default is '/cumulus/deeplabcut/')\n    frames_per_video : int\n        number of frames to extract from each video\n    video_list : list\n        list of dicts of form [{'nwb_file_name': nwb_file_name, 'epoch': epoch #},...]\n        to query VideoFile table for videos to train on.\n        Can also be list of absolute paths to import videos from\n    output_path : str\n        target path to output converted videos\n        (Default is '/nimbus/deeplabcut/videos/')\n    set_permissions : bool\n        if True, will set permissions for user and group to be read+write\n        (Default is False)\n    \"\"\"\n    project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n    if project_name in project_names_in_use:\n        print(f\"project name: {project_name} is already in use.\")\n        return_key = {}\n        return_key[\"project_name\"], return_key[\"config_path\"] = (\n            cls &amp; {\"project_name\": project_name}\n        ).fetch1(\"project_name\", \"config_path\")\n        return return_key\n\n    add_to_files = kwargs.pop(\"add_to_files\", True)\n    if not bool(LabTeam() &amp; {\"team_name\": lab_team}):\n        raise ValueError(f\"team_name: {lab_team} does not exist in LabTeam\")\n    skeleton_node = None\n    # If dict, assume of form {'nwb_file_name': nwb_file_name, 'epoch': epoch}\n    # and pass to get_video_path to reference VideoFile table for path\n\n    if all(isinstance(n, Dict) for n in video_list):\n        videos_to_convert = [\n            get_video_path(video_key) for video_key in video_list\n        ]\n        videos = [\n            check_videofile(\n                video_path=video[0],\n                output_path=output_path,\n                video_filename=video[1],\n            )[0].as_posix()\n            for video in videos_to_convert\n        ]\n    # If not dict, assume list of video file paths that may or may not need to be converted\n    else:\n        videos = []\n        if not all([Path(video).exists() for video in video_list]):\n            raise OSError(\"at least one file in video_list does not exist\")\n        for video in video_list:\n            video_path = Path(video).parent\n            video_filename = video.rsplit(\n                video_path.as_posix(), maxsplit=1\n            )[-1].split(\"/\")[-1]\n            videos.extend(\n                [\n                    check_videofile(\n                        video_path=video_path,\n                        output_path=output_path,\n                        video_filename=video_filename,\n                    )[0].as_posix()\n                ]\n            )\n        if len(videos) &lt; 1:\n            raise ValueError(f\"no .mp4 videos found in{video_path}\")\n    from deeplabcut import create_new_project\n\n    config_path = create_new_project(\n        project_name,\n        lab_team,\n        videos,\n        working_directory=project_directory,\n        copy_videos=True,\n        multianimal=False,\n    )\n    for bodypart in bodyparts:\n        if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n            raise ValueError(\n                f\"bodypart: {bodypart} not found in BodyPart table\"\n            )\n    kwargs_copy = copy.deepcopy(kwargs)\n    kwargs_copy.update({\"numframes2pick\": frames_per_video, \"dotsize\": 3})\n    add_to_config(\n        config_path, bodyparts, skeleton_node=skeleton_node, **kwargs_copy\n    )\n    key = {\n        \"project_name\": project_name,\n        \"team_name\": lab_team,\n        \"bodyparts\": bodyparts,\n        \"config_path\": config_path,\n        \"frames_per_video\": frames_per_video,\n    }\n    # TODO: make permissions setting more flexible.\n    if set_permissions:\n        permissions = (\n            stat.S_IRUSR\n            | stat.S_IWUSR\n            | stat.S_IRGRP\n            | stat.S_IWGRP\n            | stat.S_IROTH\n        )\n        username = getpass.getuser()\n        if not groupname:\n            groupname = username\n        _set_permissions(\n            directory=project_directory,\n            mode=permissions,\n            username=username,\n            groupname=groupname,\n        )\n    cls.insert1(key, **kwargs)\n    cls.BodyPart.insert(\n        [\n            {\"project_name\": project_name, \"bodypart\": bp}\n            for bp in bodyparts\n        ],\n        **kwargs,\n    )\n    if add_to_files:\n        del key[\"bodyparts\"]\n        del key[\"team_name\"]\n        del key[\"config_path\"]\n        del key[\"frames_per_video\"]\n        # Add videos to training files\n        cls.add_training_files(key, **kwargs)\n    if isinstance(config_path, PosixPath):\n        config_path = config_path.as_posix()\n    return {\"project_name\": project_name, \"config_path\": config_path}\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_project.DLCProject.add_training_files", "title": "<code>add_training_files(key, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Add training videos and labeled frames .h5 and .csv to DLCProject.File</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef add_training_files(cls, key, **kwargs):\n\"\"\"Add training videos and labeled frames .h5 and .csv to DLCProject.File\"\"\"\n    config_path = (cls &amp; {\"project_name\": key[\"project_name\"]}).fetch1(\n        \"config_path\"\n    )\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    if \"config_path\" in key:\n        del key[\"config_path\"]\n    cfg = read_config(config_path)\n    video_names = list(cfg[\"video_sets\"].keys())\n    training_files = []\n    for video in video_names:\n        video_name = os.path.splitext(\n            video.split(os.path.dirname(video) + \"/\")[-1]\n        )[0]\n        training_files.extend(\n            glob.glob(\n                f\"{cfg['project_path']}/labeled-data/{video_name}/*Collected*\"\n            )\n        )\n    for video in video_names:\n        key[\"file_name\"] = f'{os.path.splitext(video.split(\"/\")[-1])[0]}'\n        key[\"file_ext\"] = os.path.splitext(video.split(\"/\")[-1])[-1].split(\n            \".\"\n        )[-1]\n        key[\"file_path\"] = video\n        cls.File.insert1(key, **kwargs)\n    if len(training_files) &gt; 0:\n        for file in training_files:\n            video_name = os.path.dirname(file).split(\"/\")[-1]\n            file_type = os.path.splitext(\n                file.split(os.path.dirname(file) + \"/\")[-1]\n            )[-1].split(\".\")[-1]\n            key[\"file_name\"] = f\"{video_name}_labeled_data\"\n            key[\"file_ext\"] = file_type\n            key[\"file_path\"] = file\n            cls.File.insert1(key, **kwargs)\n    else:\n        Warning(\"No training files to add\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_project.DLCProject.run_extract_frames", "title": "<code>run_extract_frames(key, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Convenience function to launch DLC GUI for extracting frames. Must be run on local machine to access GUI, cannot be run through ssh tunnel</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef run_extract_frames(cls, key, **kwargs):\n\"\"\"Convenience function to launch DLC GUI for extracting frames.\n    Must be run on local machine to access GUI,\n    cannot be run through ssh tunnel\n    \"\"\"\n    config_path = (cls &amp; key).fetch1(\"config_path\")\n    from deeplabcut import extract_frames\n\n    extract_frames(config_path, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_project.DLCProject.run_label_frames", "title": "<code>run_label_frames(key)</code>  <code>classmethod</code>", "text": "<p>Convenience function to launch DLC GUI for labeling frames. Must be run on local machine to access GUI, cannot be run through ssh tunnel</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef run_label_frames(cls, key):\n\"\"\"Convenience function to launch DLC GUI for labeling frames.\n    Must be run on local machine to access GUI,\n    cannot be run through ssh tunnel\n    \"\"\"\n    config_path = (cls &amp; key).fetch1(\"config_path\")\n    from deeplabcut import label_frames\n\n    label_frames(config_path)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_project.DLCProject.check_labels", "title": "<code>check_labels(key, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Convenience function to check labels on previously extracted and labeled frames</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef check_labels(cls, key, **kwargs):\n\"\"\"Convenience function to check labels on\n    previously extracted and labeled frames\n    \"\"\"\n    config_path = (cls &amp; key).fetch1(\"config_path\")\n    from deeplabcut import check_labels\n\n    check_labels(config_path, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_project.DLCProject.import_labeled_frames", "title": "<code>import_labeled_frames(key, import_project_path, video_filenames, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Function to import pre-labeled frames from an existing project into a new project</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Dict</code> <p>key to specify entry in DLCProject table to add labeled frames to</p> required <code>import_project_path</code> <code>str</code> <p>absolute path to project directory containing labeled frames to import</p> required <code>video_filenames</code> <code>str or List</code> <p>filename or list of filenames of video(s) from which to import frames. without file extension</p> required Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef import_labeled_frames(\n    cls,\n    key: Dict,\n    import_project_path: Union[str, PosixPath],\n    video_filenames: Union[str, List],\n    **kwargs,\n):\n\"\"\"Function to import pre-labeled frames from an existing project into a new project\n\n    Parameters\n    ----------\n    key : Dict\n        key to specify entry in DLCProject table to add labeled frames to\n    import_project_path : str\n        absolute path to project directory containing labeled frames to import\n    video_filenames : str or List\n        filename or list of filenames of video(s) from which to import frames.\n        without file extension\n    \"\"\"\n    project_entry = (cls &amp; key).fetch1()\n    team_name = project_entry[\"team_name\"]\n    current_project_path = Path(project_entry[\"config_path\"]).parent\n    current_labeled_data_path = Path(\n        f\"{current_project_path.as_posix()}/labeled-data\"\n    )\n    if isinstance(import_project_path, PosixPath):\n        assert (\n            import_project_path.exists()\n        ), f\"import_project_path: {import_project_path.as_posix()} does not exist\"\n        import_labeled_data_path = Path(\n            f\"{import_project_path.as_posix()}/labeled-data\"\n        )\n    else:\n        assert Path(\n            import_project_path\n        ).exists(), (\n            f\"import_project_path: {import_project_path} does not exist\"\n        )\n        import_labeled_data_path = Path(\n            f\"{import_project_path}/labeled-data\"\n        )\n    assert (\n        import_labeled_data_path.exists()\n    ), \"import_project has no directory 'labeled-data'\"\n    if not isinstance(video_filenames, List):\n        video_filenames = [video_filenames]\n    for video_file in video_filenames:\n        h5_file = glob.glob(\n            f\"{import_labeled_data_path.as_posix()}/{video_file}/*.h5\"\n        )[0]\n        dlc_df = pd.read_hdf(h5_file)\n        dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n        dlc_df.to_hdf(\n            Path(\n                f\"{current_labeled_data_path.as_posix()}/{video_file}/CollectedData_{team_name}.h5\"\n            ).as_posix(),\n            \"df_with_missing\",\n        )\n    cls.add_training_files(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.DLCModelTraining", "title": "<code>DLCModelTraining</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_training.py</code> <pre><code>@schema\nclass DLCModelTraining(dj.Computed):\n    definition = \"\"\"\n    -&gt; DLCModelTrainingSelection\n    ---\n    project_path         : varchar(255) # Path to project directory\n    latest_snapshot: int unsigned # latest exact snapshot index (i.e., never -1)\n    config_template: longblob     # stored full config file\n    \"\"\"\n\n    # To continue from previous training snapshot, devs suggest editing pose_cfg.yml\n    # https://github.com/DeepLabCut/DeepLabCut/issues/70\n\n    def make(self, key):\n\"\"\"Launch training for each entry in DLCModelTrainingSelection via `.populate()`.\"\"\"\n        model_prefix = (DLCModelTrainingSelection &amp; key).fetch1(\"model_prefix\")\n        from deeplabcut import create_training_dataset, train_network\n        from deeplabcut.utils.auxiliaryfunctions import read_config\n\n        from . import dlc_reader\n\n        try:\n            from deeplabcut.utils.auxiliaryfunctions import get_model_folder\n        except ImportError:\n            from deeplabcut.utils.auxiliaryfunctions import (\n                GetModelFolder as get_model_folder,\n            )\n        config_path, project_name = (DLCProject() &amp; key).fetch1(\n            \"config_path\", \"project_name\"\n        )\n        with OutputLogger(\n            name=\"DLC_project_{project_name}_training\",\n            path=f\"{os.path.dirname(config_path)}/log.log\",\n            print_console=True,\n        ) as logger:\n            dlc_config = read_config(config_path)\n            project_path = dlc_config[\"project_path\"]\n            key[\"project_path\"] = project_path\n            # ---- Build and save DLC configuration (yaml) file ----\n            _, dlc_config = dlc_reader.read_yaml(project_path)\n            if not dlc_config:\n                dlc_config = read_config(config_path)\n            dlc_config.update((DLCModelTrainingParams &amp; key).fetch1(\"params\"))\n            dlc_config.update(\n                {\n                    \"project_path\": Path(project_path).as_posix(),\n                    \"modelprefix\": model_prefix,\n                    \"train_fraction\": dlc_config[\"TrainingFraction\"][\n                        int(dlc_config[\"trainingsetindex\"])\n                    ],\n                    \"training_filelist_datajoint\": [  # don't overwrite origin video_sets\n                        Path(fp).as_posix()\n                        for fp in (DLCProject.File &amp; key).fetch(\"file_path\")\n                    ],\n                }\n            )\n            # Write dlc config file to base project folder\n            # TODO: need to make sure this will work\n            dlc_cfg_filepath = dlc_reader.save_yaml(project_path, dlc_config)\n            # ---- create training dataset ----\n            training_dataset_input_args = list(\n                inspect.signature(create_training_dataset).parameters\n            )\n            training_dataset_kwargs = {\n                k: v\n                for k, v in dlc_config.items()\n                if k in training_dataset_input_args\n            }\n            logger.logger.info(\"creating training dataset\")\n            create_training_dataset(dlc_cfg_filepath, **training_dataset_kwargs)\n            # ---- Trigger DLC model training job ----\n            train_network_input_args = list(\n                inspect.signature(train_network).parameters\n            )\n            train_network_kwargs = {\n                k: v\n                for k, v in dlc_config.items()\n                if k in train_network_input_args\n            }\n            for k in [\"shuffle\", \"trainingsetindex\", \"maxiters\"]:\n                if k in train_network_kwargs:\n                    train_network_kwargs[k] = int(train_network_kwargs[k])\n            try:\n                train_network(dlc_cfg_filepath, **train_network_kwargs)\n            except (\n                KeyboardInterrupt\n            ):  # Instructions indicate to train until interrupt\n                logger.logger.info(\n                    \"DLC training stopped via Keyboard Interrupt\"\n                )\n\n            snapshots = list(\n                (\n                    project_path\n                    / get_model_folder(\n                        trainFraction=dlc_config[\"train_fraction\"],\n                        shuffle=dlc_config[\"shuffle\"],\n                        cfg=dlc_config,\n                        modelprefix=dlc_config[\"modelprefix\"],\n                    )\n                    / \"train\"\n                ).glob(\"*index*\")\n            )\n            max_modified_time = 0\n            # DLC goes by snapshot magnitude when judging 'latest' for evaluation\n            # Here, we mean most recently generated\n            for snapshot in snapshots:\n                modified_time = os.path.getmtime(snapshot)\n                if modified_time &gt; max_modified_time:\n                    latest_snapshot = int(snapshot.stem[9:])\n                    max_modified_time = modified_time\n\n            self.insert1(\n                {\n                    **key,\n                    \"latest_snapshot\": latest_snapshot,\n                    \"config_template\": dlc_config,\n                }\n            )\n            from .position_dlc_model import DLCModelSource\n\n            dlc_model_name = f\"{key['project_name']}_{key['dlc_training_params_name']}_{key['training_id']:02d}\"\n            DLCModelSource.insert_entry(\n                dlc_model_name=dlc_model_name,\n                project_name=key[\"project_name\"],\n                source=\"FromUpstream\",\n                key=key,\n                skip_duplicates=True,\n            )\n        print(\n            f\"Inserted {dlc_model_name} from {key['project_name']} into DLCModelSource\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_training.DLCModelTraining.make", "title": "<code>make(key)</code>", "text": "<p>Launch training for each entry in DLCModelTrainingSelection via <code>.populate()</code>.</p> Source code in <code>src/spyglass/position/v1/position_dlc_training.py</code> <pre><code>def make(self, key):\n\"\"\"Launch training for each entry in DLCModelTrainingSelection via `.populate()`.\"\"\"\n    model_prefix = (DLCModelTrainingSelection &amp; key).fetch1(\"model_prefix\")\n    from deeplabcut import create_training_dataset, train_network\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    from . import dlc_reader\n\n    try:\n        from deeplabcut.utils.auxiliaryfunctions import get_model_folder\n    except ImportError:\n        from deeplabcut.utils.auxiliaryfunctions import (\n            GetModelFolder as get_model_folder,\n        )\n    config_path, project_name = (DLCProject() &amp; key).fetch1(\n        \"config_path\", \"project_name\"\n    )\n    with OutputLogger(\n        name=\"DLC_project_{project_name}_training\",\n        path=f\"{os.path.dirname(config_path)}/log.log\",\n        print_console=True,\n    ) as logger:\n        dlc_config = read_config(config_path)\n        project_path = dlc_config[\"project_path\"]\n        key[\"project_path\"] = project_path\n        # ---- Build and save DLC configuration (yaml) file ----\n        _, dlc_config = dlc_reader.read_yaml(project_path)\n        if not dlc_config:\n            dlc_config = read_config(config_path)\n        dlc_config.update((DLCModelTrainingParams &amp; key).fetch1(\"params\"))\n        dlc_config.update(\n            {\n                \"project_path\": Path(project_path).as_posix(),\n                \"modelprefix\": model_prefix,\n                \"train_fraction\": dlc_config[\"TrainingFraction\"][\n                    int(dlc_config[\"trainingsetindex\"])\n                ],\n                \"training_filelist_datajoint\": [  # don't overwrite origin video_sets\n                    Path(fp).as_posix()\n                    for fp in (DLCProject.File &amp; key).fetch(\"file_path\")\n                ],\n            }\n        )\n        # Write dlc config file to base project folder\n        # TODO: need to make sure this will work\n        dlc_cfg_filepath = dlc_reader.save_yaml(project_path, dlc_config)\n        # ---- create training dataset ----\n        training_dataset_input_args = list(\n            inspect.signature(create_training_dataset).parameters\n        )\n        training_dataset_kwargs = {\n            k: v\n            for k, v in dlc_config.items()\n            if k in training_dataset_input_args\n        }\n        logger.logger.info(\"creating training dataset\")\n        create_training_dataset(dlc_cfg_filepath, **training_dataset_kwargs)\n        # ---- Trigger DLC model training job ----\n        train_network_input_args = list(\n            inspect.signature(train_network).parameters\n        )\n        train_network_kwargs = {\n            k: v\n            for k, v in dlc_config.items()\n            if k in train_network_input_args\n        }\n        for k in [\"shuffle\", \"trainingsetindex\", \"maxiters\"]:\n            if k in train_network_kwargs:\n                train_network_kwargs[k] = int(train_network_kwargs[k])\n        try:\n            train_network(dlc_cfg_filepath, **train_network_kwargs)\n        except (\n            KeyboardInterrupt\n        ):  # Instructions indicate to train until interrupt\n            logger.logger.info(\n                \"DLC training stopped via Keyboard Interrupt\"\n            )\n\n        snapshots = list(\n            (\n                project_path\n                / get_model_folder(\n                    trainFraction=dlc_config[\"train_fraction\"],\n                    shuffle=dlc_config[\"shuffle\"],\n                    cfg=dlc_config,\n                    modelprefix=dlc_config[\"modelprefix\"],\n                )\n                / \"train\"\n            ).glob(\"*index*\")\n        )\n        max_modified_time = 0\n        # DLC goes by snapshot magnitude when judging 'latest' for evaluation\n        # Here, we mean most recently generated\n        for snapshot in snapshots:\n            modified_time = os.path.getmtime(snapshot)\n            if modified_time &gt; max_modified_time:\n                latest_snapshot = int(snapshot.stem[9:])\n                max_modified_time = modified_time\n\n        self.insert1(\n            {\n                **key,\n                \"latest_snapshot\": latest_snapshot,\n                \"config_template\": dlc_config,\n            }\n        )\n        from .position_dlc_model import DLCModelSource\n\n        dlc_model_name = f\"{key['project_name']}_{key['dlc_training_params_name']}_{key['training_id']:02d}\"\n        DLCModelSource.insert_entry(\n            dlc_model_name=dlc_model_name,\n            project_name=key[\"project_name\"],\n            source=\"FromUpstream\",\n            key=key,\n            skip_duplicates=True,\n        )\n    print(\n        f\"Inserted {dlc_model_name} from {key['project_name']} into DLCModelSource\"\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.DLCModelEvaluation", "title": "<code>DLCModelEvaluation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_model.py</code> <pre><code>@schema\nclass DLCModelEvaluation(dj.Computed):\n    definition = \"\"\"\n    -&gt; DLCModel\n    ---\n    train_iterations   : int   # Training iterations\n    train_error=null   : float # Train error (px)\n    test_error=null    : float # Test error (px)\n    p_cutoff=null      : float # p-cutoff used\n    train_error_p=null : float # Train error with p-cutoff\n    test_error_p=null  : float # Test error with p-cutoff\n    \"\"\"\n\n    def make(self, key):\n\"\"\".populate() method will launch evaluation for each unique entry in Model.\"\"\"\n        import csv\n\n        from deeplabcut import evaluate_network\n        from deeplabcut.utils.auxiliaryfunctions import get_evaluation_folder\n\n        dlc_config, project_path, model_prefix, shuffle, trainingsetindex = (\n            DLCModel &amp; key\n        ).fetch1(\n            \"config_template\",\n            \"project_path\",\n            \"model_prefix\",\n            \"shuffle\",\n            \"trainingsetindex\",\n        )\n\n        yml_path, _ = dlc_reader.read_yaml(project_path)\n\n        evaluate_network(\n            yml_path,\n            Shuffles=[shuffle],  # this needs to be a list\n            trainingsetindex=trainingsetindex,\n            comparisonbodyparts=\"all\",\n        )\n\n        eval_folder = get_evaluation_folder(\n            trainFraction=dlc_config[\"TrainingFraction\"][trainingsetindex],\n            shuffle=shuffle,\n            cfg=dlc_config,\n            modelprefix=model_prefix,\n        )\n        eval_path = project_path / eval_folder\n        assert (\n            eval_path.exists()\n        ), f\"Couldn't find evaluation folder:\\n{eval_path}\"\n\n        eval_csvs = list(eval_path.glob(\"*csv\"))\n        max_modified_time = 0\n        for eval_csv in eval_csvs:\n            modified_time = os.path.getmtime(eval_csv)\n            if modified_time &gt; max_modified_time:\n                eval_csv_latest = eval_csv\n        with open(eval_csv_latest, newline=\"\") as f:\n            results = list(csv.DictReader(f, delimiter=\",\"))[0]\n        # in testing, test_error_p returned empty string\n        self.insert1(\n            dict(\n                key,\n                train_iterations=results[\"Training iterations:\"],\n                train_error=results[\" Train error(px)\"],\n                test_error=results[\" Test error(px)\"],\n                p_cutoff=results[\"p-cutoff used\"],\n                train_error_p=results[\"Train error with p-cutoff\"],\n                test_error_p=results[\"Test error with p-cutoff\"],\n            )\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.DLCModelEvaluation.make", "title": "<code>make(key)</code>", "text": "<p>.populate() method will launch evaluation for each unique entry in Model.</p> Source code in <code>src/spyglass/position/v1/position_dlc_model.py</code> <pre><code>def make(self, key):\n\"\"\".populate() method will launch evaluation for each unique entry in Model.\"\"\"\n    import csv\n\n    from deeplabcut import evaluate_network\n    from deeplabcut.utils.auxiliaryfunctions import get_evaluation_folder\n\n    dlc_config, project_path, model_prefix, shuffle, trainingsetindex = (\n        DLCModel &amp; key\n    ).fetch1(\n        \"config_template\",\n        \"project_path\",\n        \"model_prefix\",\n        \"shuffle\",\n        \"trainingsetindex\",\n    )\n\n    yml_path, _ = dlc_reader.read_yaml(project_path)\n\n    evaluate_network(\n        yml_path,\n        Shuffles=[shuffle],  # this needs to be a list\n        trainingsetindex=trainingsetindex,\n        comparisonbodyparts=\"all\",\n    )\n\n    eval_folder = get_evaluation_folder(\n        trainFraction=dlc_config[\"TrainingFraction\"][trainingsetindex],\n        shuffle=shuffle,\n        cfg=dlc_config,\n        modelprefix=model_prefix,\n    )\n    eval_path = project_path / eval_folder\n    assert (\n        eval_path.exists()\n    ), f\"Couldn't find evaluation folder:\\n{eval_path}\"\n\n    eval_csvs = list(eval_path.glob(\"*csv\"))\n    max_modified_time = 0\n    for eval_csv in eval_csvs:\n        modified_time = os.path.getmtime(eval_csv)\n        if modified_time &gt; max_modified_time:\n            eval_csv_latest = eval_csv\n    with open(eval_csv_latest, newline=\"\") as f:\n        results = list(csv.DictReader(f, delimiter=\",\"))[0]\n    # in testing, test_error_p returned empty string\n    self.insert1(\n        dict(\n            key,\n            train_iterations=results[\"Training iterations:\"],\n            train_error=results[\" Train error(px)\"],\n            test_error=results[\" Test error(px)\"],\n            p_cutoff=results[\"p-cutoff used\"],\n            train_error_p=results[\"Train error with p-cutoff\"],\n            test_error_p=results[\"Test error with p-cutoff\"],\n        )\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_model/#src.spyglass.position.v1.position_dlc_model.str_to_bool", "title": "<code>str_to_bool(value)</code>", "text": "<p>Return whether the provided string represents true. Otherwise false.</p> Source code in <code>src/spyglass/position/v1/position_dlc_model.py</code> <pre><code>def str_to_bool(value) -&gt; bool:\n\"\"\"Return whether the provided string represents true. Otherwise false.\"\"\"\n    # Due to distutils equivalent depreciation in 3.10\n    # Adopted from github.com/PostHog/posthog/blob/master/posthog/utils.py\n    if not value:\n        return False\n    return str(value).lower() in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/", "title": "position_dlc_orient.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.DLCOrientationParams", "title": "<code>DLCOrientationParams</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Parameters for determining and smoothing the orientation of a set of BodyParts</p> Source code in <code>src/spyglass/position/v1/position_dlc_orient.py</code> <pre><code>@schema\nclass DLCOrientationParams(dj.Manual):\n\"\"\"\n    Parameters for determining and smoothing the orientation of a set of BodyParts\n    \"\"\"\n\n    definition = \"\"\"\n    dlc_orientation_params_name: varchar(80) # name for this set of parameters\n    ---\n    params: longblob\n    \"\"\"\n\n    @classmethod\n    def insert_params(cls, params_name: str, params: dict, **kwargs):\n        cls.insert1(\n            {\"dlc_orientation_params_name\": params_name, \"params\": params},\n            **kwargs,\n        )\n\n    @classmethod\n    def insert_default(cls, **kwargs):\n        params = {\n            \"orient_method\": \"red_green_orientation\",\n            \"bodypart1\": \"greenLED\",\n            \"bodypart2\": \"redLED_C\",\n            \"orientation_smoothing_std_dev\": 0.001,\n        }\n        cls.insert1(\n            {\"dlc_orientation_params_name\": \"default\", \"params\": params},\n            **kwargs,\n        )\n\n    @classmethod\n    def get_default(cls):\n        query = cls &amp; {\"dlc_orientation_params_name\": \"default\"}\n        if not len(query) &gt; 0:\n            cls().insert_default(skip_duplicates=True)\n            default = (\n                cls &amp; {\"dlc_orientation_params_name\": \"default\"}\n            ).fetch1()\n        else:\n            default = query.fetch1()\n        return default\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.DLCSmoothInterpCohort", "title": "<code>DLCSmoothInterpCohort</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Table to combine multiple bodyparts from DLCSmoothInterp to enable centroid/orientation calculations</p> Source code in <code>src/spyglass/position/v1/position_dlc_cohort.py</code> <pre><code>@schema\nclass DLCSmoothInterpCohort(dj.Computed):\n\"\"\"\n    Table to combine multiple bodyparts from DLCSmoothInterp\n    to enable centroid/orientation calculations\n    \"\"\"\n\n    # Need to ensure that nwb_file_name/epoch/interval list name endure as primary keys\n    definition = \"\"\"\n    -&gt; DLCSmoothInterpCohortSelection\n    ---\n    \"\"\"\n\n    class BodyPart(dj.Part):\n        definition = \"\"\"\n        -&gt; DLCSmoothInterpCohort\n        -&gt; DLCSmoothInterp\n        ---\n        -&gt; AnalysisNwbfile\n        dlc_smooth_interp_position_object_id : varchar(80)\n        dlc_smooth_interp_info_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n        def fetch1_dataframe(self):\n            nwb_data = self.fetch_nwb()[0]\n            index = pd.Index(\n                np.asarray(\n                    nwb_data[\"dlc_smooth_interp_position\"]\n                    .get_spatial_series()\n                    .timestamps\n                ),\n                name=\"time\",\n            )\n            COLUMNS = [\n                \"video_frame_ind\",\n                \"x\",\n                \"y\",\n            ]\n            return pd.DataFrame(\n                np.concatenate(\n                    (\n                        np.asarray(\n                            nwb_data[\"dlc_smooth_interp_info\"]\n                            .time_series[\"video_frame_ind\"]\n                            .data,\n                            dtype=int,\n                        )[:, np.newaxis],\n                        np.asarray(\n                            nwb_data[\"dlc_smooth_interp_position\"]\n                            .get_spatial_series()\n                            .data\n                        ),\n                    ),\n                    axis=1,\n                ),\n                columns=COLUMNS,\n                index=index,\n            )\n\n    def make(self, key):\n        from .dlc_utils import OutputLogger, infer_output_dir\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n            print_console=False,\n        ) as logger:\n            logger.logger.info(\"-----------------------\")\n            logger.logger.info(\"Bodypart Cohort\")\n            # from Jen Guidera\n            self.insert1(key)\n            cohort_selection = (DLCSmoothInterpCohortSelection &amp; key).fetch1()\n            table_entries = []\n            bodyparts_params_dict = cohort_selection.pop(\n                \"bodyparts_params_dict\"\n            )\n            temp_key = cohort_selection.copy()\n            for bodypart, params in bodyparts_params_dict.items():\n                temp_key[\"bodypart\"] = bodypart\n                temp_key[\"dlc_si_params_name\"] = params\n                table_entries.append((DLCSmoothInterp &amp; temp_key).fetch())\n            assert len(table_entries) == len(\n                bodyparts_params_dict\n            ), \"more entries found in DLCSmoothInterp than specified in bodyparts_params_dict\"\n            table_column_names = list(table_entries[0].dtype.fields.keys())\n            for table_entry in table_entries:\n                entry_key = {\n                    **{\n                        k: v for k, v in zip(table_column_names, table_entry[0])\n                    },\n                    **key,\n                }\n                DLCSmoothInterpCohort.BodyPart.insert1(\n                    entry_key, skip_duplicates=True\n                )\n        logger.logger.info(\"Inserted entry into DLCSmoothInterpCohort\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.DLCOrientation", "title": "<code>DLCOrientation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Determines and smooths orientation of a set of bodyparts given a specified method</p> Source code in <code>src/spyglass/position/v1/position_dlc_orient.py</code> <pre><code>@schema\nclass DLCOrientation(dj.Computed):\n\"\"\"\n    Determines and smooths orientation of a set of bodyparts given a specified method\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCOrientationSelection\n    ---\n    -&gt; AnalysisNwbfile\n    dlc_orientation_object_id : varchar(80)\n    \"\"\"\n\n    def make(self, key):\n        # Get labels to smooth from Parameters table\n        cohort_entries = DLCSmoothInterpCohort.BodyPart &amp; key\n        pos_df = pd.concat(\n            {\n                bodypart: (\n                    DLCSmoothInterpCohort.BodyPart\n                    &amp; {**key, **{\"bodypart\": bodypart}}\n                ).fetch1_dataframe()\n                for bodypart in cohort_entries.fetch(\"bodypart\")\n            },\n            axis=1,\n        )\n        params = (DLCOrientationParams() &amp; key).fetch1(\"params\")\n        orientation_smoothing_std_dev = params.pop(\n            \"orientation_smoothing_std_dev\", None\n        )\n        dt = np.median(np.diff(pos_df.index.to_numpy()))\n        sampling_rate = 1 / dt\n        orient_func = _key_to_func_dict[params[\"orient_method\"]]\n        orientation = orient_func(pos_df, **params)\n        if not params[\"orient_method\"] == \"none\":\n            # Smooth orientation\n            is_nan = np.isnan(orientation)\n            unwrap_orientation = orientation.copy()\n            # Only unwrap non nan values, while keeping nans in dataset for interpolation\n            unwrap_orientation[~is_nan] = np.unwrap(orientation[~is_nan])\n            unwrap_df = pd.DataFrame(\n                unwrap_orientation, columns=[\"orientation\"], index=pos_df.index\n            )\n            nan_spans = get_span_start_stop(np.where(is_nan)[0])\n            orient_df = interp_orientation(\n                unwrap_df,\n                nan_spans,\n            )\n            orientation = gaussian_smooth(\n                orient_df[\"orientation\"].to_numpy(),\n                orientation_smoothing_std_dev,\n                sampling_rate,\n                axis=0,\n                truncate=8,\n            )\n            # convert back to between -pi and pi\n            orientation = np.angle(np.exp(1j * orientation))\n        final_df = pd.DataFrame(\n            orientation, columns=[\"orientation\"], index=pos_df.index\n        )\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        spatial_series = (RawPosition() &amp; key).fetch_nwb()[0][\"raw_position\"]\n        orientation = pynwb.behavior.CompassDirection()\n        orientation.create_spatial_series(\n            name=\"orientation\",\n            timestamps=final_df.index.to_numpy(),\n            conversion=1.0,\n            data=final_df[\"orientation\"].to_numpy(),\n            reference_frame=spatial_series.reference_frame,\n            comments=spatial_series.comments,\n            description=\"orientation\",\n        )\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"dlc_orientation_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], orientation\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(\n                nwb_data[\"dlc_orientation\"].get_spatial_series().timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"orientation\",\n        ]\n        return pd.DataFrame(\n            np.asarray(nwb_data[\"dlc_orientation\"].get_spatial_series().data)[\n                :, np.newaxis\n            ],\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.RawPosition", "title": "<code>RawPosition</code>", "text": "<p>         Bases: <code>dj.Imported</code></p>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.RawPosition--notes", "title": "Notes", "text": "<p>The position timestamps come from: .pos_cameraHWSync.dat. If PTP is not used, the position timestamps are inferred by finding the closest timestamps from the neural recording via the trodes time.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass RawPosition(dj.Imported):\n\"\"\"\n\n    Notes\n    -----\n    The position timestamps come from: .pos_cameraHWSync.dat.\n    If PTP is not used, the position timestamps are inferred by finding the\n    closest timestamps from the neural recording via the trodes time.\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionSource\n    ---\n    raw_position_object_id: varchar(40)    # the object id of the spatial series for this epoch in the NWB file\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        # TODO refactor this. this calculates sampling rate (unused here) and is expensive to do twice\n        pos_dict = get_all_spatial_series(nwbf)\n        for epoch in pos_dict:\n            if key[\n                \"interval_list_name\"\n            ] == PositionSource.get_pos_interval_name(epoch):\n                pdict = pos_dict[epoch]\n                key[\"raw_position_object_id\"] = pdict[\"raw_position_object_id\"]\n                self.insert1(key)\n                break\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    def fetch1_dataframe(self):\n        raw_position_nwb = self.fetch_nwb()[0][\"raw_position\"]\n        return pd.DataFrame(\n            data=raw_position_nwb.data,\n            index=pd.Index(raw_position_nwb.timestamps, name=\"time\"),\n            columns=raw_position_nwb.description.split(\", \"),\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.two_pt_head_orientation", "title": "<code>two_pt_head_orientation(pos_df, **params)</code>", "text": "<p>Determines orientation based on vector between two points</p> Source code in <code>src/spyglass/position/v1/position_dlc_orient.py</code> <pre><code>def two_pt_head_orientation(pos_df: pd.DataFrame, **params):\n\"\"\"Determines orientation based on vector between two points\"\"\"\n    BP1 = params.pop(\"bodypart1\", None)\n    BP2 = params.pop(\"bodypart2\", None)\n    orientation = np.arctan2(\n        (pos_df[BP1][\"y\"] - pos_df[BP2][\"y\"]),\n        (pos_df[BP1][\"x\"] - pos_df[BP2][\"x\"]),\n    )\n    return orientation\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.red_led_bisector_orientation", "title": "<code>red_led_bisector_orientation(pos_df, **params)</code>", "text": "<p>Determines orientation based on 2 equally-spaced identifiers that are assumed to be perpendicular to the orientation direction. A third object is needed to determine forward/backward</p> Source code in <code>src/spyglass/position/v1/position_dlc_orient.py</code> <pre><code>def red_led_bisector_orientation(pos_df: pd.DataFrame, **params):\n\"\"\"Determines orientation based on 2 equally-spaced identifiers\n    that are assumed to be perpendicular to the orientation direction.\n    A third object is needed to determine forward/backward\n    \"\"\"\n    LED1 = params.pop(\"led1\", None)\n    LED2 = params.pop(\"led2\", None)\n    LED3 = params.pop(\"led3\", None)\n    orientation = []\n    for index, row in pos_df.iterrows():\n        x_vec = row[LED1][\"x\"] - row[LED2][\"x\"]\n        y_vec = row[LED1][\"y\"] - row[LED2][\"y\"]\n        if y_vec == 0:\n            if (row[LED3][\"y\"] &gt; row[LED1][\"y\"]) &amp; (\n                row[LED3][\"y\"] &gt; row[LED2][\"y\"]\n            ):\n                orientation.append(np.pi / 2)\n            elif (row[LED3][\"y\"] &lt; row[LED1][\"y\"]) &amp; (\n                row[LED3][\"y\"] &lt; row[LED2][\"y\"]\n            ):\n                orientation.append(-(np.pi / 2))\n            else:\n                raise Exception(\"Cannot determine head direction from bisector\")\n        else:\n            length = np.sqrt(y_vec * y_vec + x_vec * x_vec)\n            norm = np.array([-y_vec / length, x_vec / length])\n            orientation.append(np.arctan2(norm[1], norm[0]))\n        if index + 1 == len(pos_df):\n            break\n    return np.array(orientation)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_orient/#src.spyglass.position.v1.position_dlc_orient.get_span_start_stop", "title": "<code>get_span_start_stop(indices)</code>", "text": "<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Type Description <code>_type_</code> <p>description</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_span_start_stop(indices):\n\"\"\"_summary_\n\n    Parameters\n    ----------\n    indices : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    span_inds = []\n    # Get start and stop index of spans of consecutive indices\n    for k, g in groupby(enumerate(indices), lambda x: x[1] - x[0]):\n        group = list(map(itemgetter(1), g))\n        span_inds.append((group[0], group[-1]))\n    return span_inds\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/", "title": "position_dlc_pose_estimation.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimationSelection", "title": "<code>DLCPoseEstimationSelection</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@schema\nclass DLCPoseEstimationSelection(dj.Manual):\n    definition = \"\"\"\n    -&gt; VideoFile                           # Session -&gt; Recording + File part table\n    -&gt; DLCModel                                    # Must specify a DLC project_path\n    ---\n    task_mode='load' : enum('load', 'trigger')  # load results or trigger computation\n    video_path : varchar(120)                   # path to video file\n    pose_estimation_output_dir='': varchar(255) # output dir relative to the root dir\n    pose_estimation_params=null  : longblob     # analyze_videos params, if not default\n    \"\"\"\n\n    @classmethod\n    def get_video_crop(cls, video_path):\n\"\"\"\n        Queries the user to determine the cropping parameters for a given video\n\n        Parameters\n        ----------\n        video_path : str\n            path to the video file\n\n        Returns\n        -------\n        crop_ints : list\n            list of 4 integers [x min, x max, y min, y max]\n        \"\"\"\n\n        cap = cv2.VideoCapture(video_path)\n        _, frame = cap.read()\n        fig, ax = plt.subplots(figsize=(20, 10))\n        ax.imshow(frame)\n        xlims = ax.get_xlim()\n        ylims = ax.get_ylim()\n        ax.set_xticks(np.arange(xlims[0], xlims[-1], 50))\n        ax.set_yticks(np.arange(ylims[0], ylims[-1], -50))\n        ax.grid(visible=True, color=\"white\", lw=0.5, alpha=0.5)\n        display(fig)\n        crop_input = input(\n            \"Please enter the crop parameters for your video in format xmin, xmax, ymin, ymax, or 'none'\\n\"\n        )\n        plt.close()\n        if crop_input.lower() == \"none\":\n            return None\n        crop_ints = [int(val) for val in crop_input.split(\",\")]\n        assert all(isinstance(val, int) for val in crop_ints)\n        return crop_ints\n\n    @classmethod\n    def insert_estimation_task(\n        cls,\n        key,\n        task_mode=\"trigger\",\n        params: dict = None,\n        check_crop=None,\n        skip_duplicates=True,\n    ):\n\"\"\"\n        Insert PoseEstimationTask in inferred output dir.\n        From Datajoint Elements\n\n        Parameters\n        ----------\n        key: DataJoint key specifying a pairing of VideoRecording and Model.\n        task_mode (bool): Default 'trigger' computation. Or 'load' existing results.\n        params (dict): Optional. Parameters passed to DLC's analyze_videos:\n            videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference,\n            dynamic, robust_nframes, allow_growth, use_shelve\n        \"\"\"\n        from .dlc_utils import check_videofile, get_video_path\n\n        video_path, video_filename, _, _ = get_video_path(key)\n        output_dir = infer_output_dir(key)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n        ) as logger:\n            logger.logger.info(\"Pose Estimation Selection\")\n            video_dir = os.path.dirname(video_path) + \"/\"\n            logger.logger.info(\"video_dir: %s\", video_dir)\n            video_path = check_videofile(\n                video_path=video_dir, video_filename=video_filename\n            )[0]\n            if check_crop is not None:\n                params[\"cropping\"] = cls.get_video_crop(\n                    video_path=video_path.as_posix()\n                )\n            cls.insert1(\n                {\n                    **key,\n                    \"task_mode\": task_mode,\n                    \"pose_estimation_params\": params,\n                    \"video_path\": video_path,\n                    \"pose_estimation_output_dir\": output_dir,\n                },\n                skip_duplicates=skip_duplicates,\n            )\n        logger.logger.info(\"inserted entry into Pose Estimation Selection\")\n        return {**key, \"task_mode\": task_mode}\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimationSelection.get_video_crop", "title": "<code>get_video_crop(video_path)</code>  <code>classmethod</code>", "text": "<p>Queries the user to determine the cropping parameters for a given video</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>path to the video file</p> required <p>Returns:</p> Name Type Description <code>crop_ints</code> <code>list</code> <p>list of 4 integers [x min, x max, y min, y max]</p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@classmethod\ndef get_video_crop(cls, video_path):\n\"\"\"\n    Queries the user to determine the cropping parameters for a given video\n\n    Parameters\n    ----------\n    video_path : str\n        path to the video file\n\n    Returns\n    -------\n    crop_ints : list\n        list of 4 integers [x min, x max, y min, y max]\n    \"\"\"\n\n    cap = cv2.VideoCapture(video_path)\n    _, frame = cap.read()\n    fig, ax = plt.subplots(figsize=(20, 10))\n    ax.imshow(frame)\n    xlims = ax.get_xlim()\n    ylims = ax.get_ylim()\n    ax.set_xticks(np.arange(xlims[0], xlims[-1], 50))\n    ax.set_yticks(np.arange(ylims[0], ylims[-1], -50))\n    ax.grid(visible=True, color=\"white\", lw=0.5, alpha=0.5)\n    display(fig)\n    crop_input = input(\n        \"Please enter the crop parameters for your video in format xmin, xmax, ymin, ymax, or 'none'\\n\"\n    )\n    plt.close()\n    if crop_input.lower() == \"none\":\n        return None\n    crop_ints = [int(val) for val in crop_input.split(\",\")]\n    assert all(isinstance(val, int) for val in crop_ints)\n    return crop_ints\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimationSelection.insert_estimation_task", "title": "<code>insert_estimation_task(key, task_mode='trigger', params=None, check_crop=None, skip_duplicates=True)</code>  <code>classmethod</code>", "text": "<p>Insert PoseEstimationTask in inferred output dir. From Datajoint Elements</p> <p>Parameters:</p> Name Type Description Default <code>key</code> required <code>task_mode</code> <code>'trigger'</code> <code>params</code> <code>dict</code> <p>videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference, dynamic, robust_nframes, allow_growth, use_shelve</p> <code>None</code> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@classmethod\ndef insert_estimation_task(\n    cls,\n    key,\n    task_mode=\"trigger\",\n    params: dict = None,\n    check_crop=None,\n    skip_duplicates=True,\n):\n\"\"\"\n    Insert PoseEstimationTask in inferred output dir.\n    From Datajoint Elements\n\n    Parameters\n    ----------\n    key: DataJoint key specifying a pairing of VideoRecording and Model.\n    task_mode (bool): Default 'trigger' computation. Or 'load' existing results.\n    params (dict): Optional. Parameters passed to DLC's analyze_videos:\n        videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference,\n        dynamic, robust_nframes, allow_growth, use_shelve\n    \"\"\"\n    from .dlc_utils import check_videofile, get_video_path\n\n    video_path, video_filename, _, _ = get_video_path(key)\n    output_dir = infer_output_dir(key)\n    with OutputLogger(\n        name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n        path=f\"{output_dir.as_posix()}/log.log\",\n    ) as logger:\n        logger.logger.info(\"Pose Estimation Selection\")\n        video_dir = os.path.dirname(video_path) + \"/\"\n        logger.logger.info(\"video_dir: %s\", video_dir)\n        video_path = check_videofile(\n            video_path=video_dir, video_filename=video_filename\n        )[0]\n        if check_crop is not None:\n            params[\"cropping\"] = cls.get_video_crop(\n                video_path=video_path.as_posix()\n            )\n        cls.insert1(\n            {\n                **key,\n                \"task_mode\": task_mode,\n                \"pose_estimation_params\": params,\n                \"video_path\": video_path,\n                \"pose_estimation_output_dir\": output_dir,\n            },\n            skip_duplicates=skip_duplicates,\n        )\n    logger.logger.info(\"inserted entry into Pose Estimation Selection\")\n    return {**key, \"task_mode\": task_mode}\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.OutputLogger", "title": "<code>OutputLogger</code>", "text": "<p>A class to wrap a logging.Logger object in order to provide context manager capabilities.</p> <p>This class uses contextlib.redirect_stdout to temporarily redirect sys.stdout and thus print statements to the log file instead of, or as well as the console.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <code>logging.Logger</code> <p>logger object</p> <code>name</code> <code>str</code> <p>name of logger</p> <code>level</code> <code>int</code> <p>level of logging that the logger is set to handle</p>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.OutputLogger--methods", "title": "Methods", "text": "<p>setup_logger(name_logfile, path_logfile, print_console=False)     initialize or get logger object with name_logfile     that writes to path_logfile</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with OutputLogger(name, path, print_console=True) as logger:\n...    print(\"this will print to logfile\")\n...    logger.logger.info(\"this will log to the logfile\")\n... print(\"this will print to the console\")\n... logger.logger.info(\"this will log to the logfile\")\n</code></pre> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>class OutputLogger:\n\"\"\"\n    A class to wrap a logging.Logger object in order to provide context manager capabilities.\n\n    This class uses contextlib.redirect_stdout to temporarily redirect sys.stdout and thus\n    print statements to the log file instead of, or as well as the console.\n\n    Attributes\n    ----------\n    logger : logging.Logger\n        logger object\n    name : str\n        name of logger\n    level : int\n        level of logging that the logger is set to handle\n\n    Methods\n    -------\n    setup_logger(name_logfile, path_logfile, print_console=False)\n        initialize or get logger object with name_logfile\n        that writes to path_logfile\n\n    Examples\n    --------\n    &gt;&gt;&gt; with OutputLogger(name, path, print_console=True) as logger:\n    ...    print(\"this will print to logfile\")\n    ...    logger.logger.info(\"this will log to the logfile\")\n    ... print(\"this will print to the console\")\n    ... logger.logger.info(\"this will log to the logfile\")\n\n    \"\"\"\n\n    def __init__(self, name, path, level=\"INFO\", **kwargs):\n        self.logger = self.setup_logger(name, path, **kwargs)\n        self.name = self.logger.name\n        self.level = getattr(logging, level)\n\n    def setup_logger(\n        self, name_logfile, path_logfile, print_console=False\n    ) -&gt; logging.Logger:\n\"\"\"\n        Sets up a logger for that outputs to a file, and optionally, the console\n\n        Parameters\n        ----------\n        name_logfile : str\n            name of the logfile to use\n        path_logfile : str\n            path to the file that should be used as the file handler\n        print_console : bool, default-False\n            if True, prints to console as well as log file.\n\n        Returns\n        -------\n        logger : logging.Logger\n            the logger object with specified handlers\n        \"\"\"\n\n        logger = logging.getLogger(name_logfile)\n        # check to see if handlers already exist for this logger\n        if logger.handlers:\n            for handler in logger.handlers:\n                # if it's a file handler\n                # type is used instead of isinstance,\n                # which doesn't work properly with logging.StreamHandler\n                if type(handler) == logging.FileHandler:\n                    # if paths don't match, change file handler path\n                    if not os.path.samefile(handler.baseFilename, path_logfile):\n                        handler.close()\n                        logger.removeHandler(handler)\n                        file_handler = self._get_file_handler(path_logfile)\n                        logger.addHandler(file_handler)\n                # if a stream handler exists and\n                # if print_console is False remove streamHandler\n                if type(handler) == logging.StreamHandler:\n                    if not print_console:\n                        handler.close()\n                        logger.removeHandler(handler)\n            if print_console and not any(\n                type(handler) == logging.StreamHandler\n                for handler in logger.handlers\n            ):\n                logger.addHandler(self._get_stream_handler())\n\n        else:\n            file_handler = self._get_file_handler(path_logfile)\n            logger.addHandler(file_handler)\n            if print_console:\n                logger.addHandler(self._get_stream_handler())\n        logger.setLevel(logging.INFO)\n        return logger\n\n    def _get_file_handler(self, path):\n        output_dir = pathlib.Path(os.path.dirname(path))\n        if not os.path.exists(output_dir):\n            output_dir.mkdir(parents=True, exist_ok=True)\n        file_handler = logging.FileHandler(path, mode=\"a\")\n        file_handler.setFormatter(self._get_formatter())\n        return file_handler\n\n    def _get_stream_handler(self):\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(self._get_formatter())\n        return stream_handler\n\n    def _get_formatter(self):\n        return logging.Formatter(\n            \"[%(asctime)s] in %(pathname)s, line %(lineno)d: %(message)s\",\n            datefmt=\"%d-%b-%y %H:%M:%S\",\n        )\n\n    def write(self, msg):\n        if msg and not msg.isspace():\n            self.logger.log(self.level, msg)\n\n    def flush(self):\n        pass\n\n    def __enter__(self):\n        self._redirector = redirect_stdout(self)\n        self._redirector.__enter__()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        # let contextlib do any exception handling here\n        self._redirector.__exit__(exc_type, exc_value, traceback)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.dlc_utils.OutputLogger.setup_logger", "title": "<code>setup_logger(name_logfile, path_logfile, print_console=False)</code>", "text": "<p>Sets up a logger for that outputs to a file, and optionally, the console</p> <p>Parameters:</p> Name Type Description Default <code>name_logfile</code> <code>str</code> <p>name of the logfile to use</p> required <code>path_logfile</code> <code>str</code> <p>path to the file that should be used as the file handler</p> required <code>print_console</code> <code>bool, default-False</code> <p>if True, prints to console as well as log file.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>logger</code> <code>logging.Logger</code> <p>the logger object with specified handlers</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def setup_logger(\n    self, name_logfile, path_logfile, print_console=False\n) -&gt; logging.Logger:\n\"\"\"\n    Sets up a logger for that outputs to a file, and optionally, the console\n\n    Parameters\n    ----------\n    name_logfile : str\n        name of the logfile to use\n    path_logfile : str\n        path to the file that should be used as the file handler\n    print_console : bool, default-False\n        if True, prints to console as well as log file.\n\n    Returns\n    -------\n    logger : logging.Logger\n        the logger object with specified handlers\n    \"\"\"\n\n    logger = logging.getLogger(name_logfile)\n    # check to see if handlers already exist for this logger\n    if logger.handlers:\n        for handler in logger.handlers:\n            # if it's a file handler\n            # type is used instead of isinstance,\n            # which doesn't work properly with logging.StreamHandler\n            if type(handler) == logging.FileHandler:\n                # if paths don't match, change file handler path\n                if not os.path.samefile(handler.baseFilename, path_logfile):\n                    handler.close()\n                    logger.removeHandler(handler)\n                    file_handler = self._get_file_handler(path_logfile)\n                    logger.addHandler(file_handler)\n            # if a stream handler exists and\n            # if print_console is False remove streamHandler\n            if type(handler) == logging.StreamHandler:\n                if not print_console:\n                    handler.close()\n                    logger.removeHandler(handler)\n        if print_console and not any(\n            type(handler) == logging.StreamHandler\n            for handler in logger.handlers\n        ):\n            logger.addHandler(self._get_stream_handler())\n\n    else:\n        file_handler = self._get_file_handler(path_logfile)\n        logger.addHandler(file_handler)\n        if print_console:\n            logger.addHandler(self._get_stream_handler())\n    logger.setLevel(logging.INFO)\n    return logger\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.RawPosition", "title": "<code>RawPosition</code>", "text": "<p>         Bases: <code>dj.Imported</code></p>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.RawPosition--notes", "title": "Notes", "text": "<p>The position timestamps come from: .pos_cameraHWSync.dat. If PTP is not used, the position timestamps are inferred by finding the closest timestamps from the neural recording via the trodes time.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass RawPosition(dj.Imported):\n\"\"\"\n\n    Notes\n    -----\n    The position timestamps come from: .pos_cameraHWSync.dat.\n    If PTP is not used, the position timestamps are inferred by finding the\n    closest timestamps from the neural recording via the trodes time.\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionSource\n    ---\n    raw_position_object_id: varchar(40)    # the object id of the spatial series for this epoch in the NWB file\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        # TODO refactor this. this calculates sampling rate (unused here) and is expensive to do twice\n        pos_dict = get_all_spatial_series(nwbf)\n        for epoch in pos_dict:\n            if key[\n                \"interval_list_name\"\n            ] == PositionSource.get_pos_interval_name(epoch):\n                pdict = pos_dict[epoch]\n                key[\"raw_position_object_id\"] = pdict[\"raw_position_object_id\"]\n                self.insert1(key)\n                break\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    def fetch1_dataframe(self):\n        raw_position_nwb = self.fetch_nwb()[0][\"raw_position\"]\n        return pd.DataFrame(\n            data=raw_position_nwb.data,\n            index=pd.Index(raw_position_nwb.timestamps, name=\"time\"),\n            columns=raw_position_nwb.description.split(\", \"),\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimation", "title": "<code>DLCPoseEstimation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@schema\nclass DLCPoseEstimation(dj.Computed):\n    definition = \"\"\"\n    -&gt; DLCPoseEstimationSelection\n    ---\n    pose_estimation_time: datetime  # time of generation of this set of DLC results\n    meters_per_pixel : double       # conversion of meters per pixel for analyzed video\n    \"\"\"\n\n    class BodyPart(dj.Part):\n        definition = \"\"\" # uses DeepLabCut h5 output for body part position\n        -&gt; DLCPoseEstimation\n        -&gt; DLCModel.BodyPart\n        ---\n        -&gt; AnalysisNwbfile\n        dlc_pose_estimation_position_object_id : varchar(80)\n        dlc_pose_estimation_likelihood_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n        def fetch1_dataframe(self):\n            nwb_data = self.fetch_nwb()[0]\n            index = pd.Index(\n                np.asarray(\n                    nwb_data[\"dlc_pose_estimation_position\"]\n                    .get_spatial_series()\n                    .timestamps\n                ),\n                name=\"time\",\n            )\n            COLUMNS = [\n                \"video_frame_ind\",\n                \"x\",\n                \"y\",\n                \"likelihood\",\n            ]\n            return pd.DataFrame(\n                np.concatenate(\n                    (\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_likelihood\"]\n                            .time_series[\"video_frame_ind\"]\n                            .data,\n                            dtype=int,\n                        )[:, np.newaxis],\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_position\"]\n                            .get_spatial_series()\n                            .data\n                        ),\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_likelihood\"]\n                            .time_series[\"likelihood\"]\n                            .data\n                        )[:, np.newaxis],\n                    ),\n                    axis=1,\n                ),\n                columns=COLUMNS,\n                index=index,\n            )\n\n    def make(self, key):\n\"\"\".populate() method will launch training for each PoseEstimationTask\"\"\"\n        from . import dlc_reader\n        from .dlc_utils import get_video_path\n\n        METERS_PER_CM = 0.01\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n        ) as logger:\n            logger.logger.info(\"----------------------\")\n            logger.logger.info(\"Pose Estimation\")\n            # ID model and directories\n            dlc_model = (DLCModel &amp; key).fetch1()\n            bodyparts = (DLCModel.BodyPart &amp; key).fetch(\"bodypart\")\n            task_mode, analyze_video_params, video_path, output_dir = (\n                DLCPoseEstimationSelection &amp; key\n            ).fetch1(\n                \"task_mode\",\n                \"pose_estimation_params\",\n                \"video_path\",\n                \"pose_estimation_output_dir\",\n            )\n            analyze_video_params = analyze_video_params or {}\n\n            project_path = dlc_model[\"project_path\"]\n\n            # Trigger PoseEstimation\n            if task_mode == \"trigger\":\n                dlc_reader.do_pose_estimation(\n                    video_path,\n                    dlc_model,\n                    project_path,\n                    output_dir,\n                    **analyze_video_params,\n                )\n            dlc_result = dlc_reader.PoseEstimation(output_dir)\n            creation_time = datetime.fromtimestamp(\n                dlc_result.creation_time\n            ).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n            logger.logger.info(\"getting raw position\")\n            interval_list_name = f\"pos {key['epoch']-1} valid times\"\n            spatial_series = (\n                RawPosition()\n                &amp; {**key, \"interval_list_name\": interval_list_name}\n            ).fetch_nwb()[0][\"raw_position\"]\n            _, _, _, video_time = get_video_path(key)\n            pos_time = spatial_series.timestamps\n            # TODO: should get timestamps from VideoFile, but need the video_frame_ind from RawPosition,\n            # which also has timestamps\n            key[\"meters_per_pixel\"] = spatial_series.conversion\n\n            # Insert entry into DLCPoseEstimation\n            logger.logger.info(\n                \"Inserting %s, epoch %02d into DLCPoseEsimation\",\n                key[\"nwb_file_name\"],\n                key[\"epoch\"],\n            )\n            self.insert1({**key, \"pose_estimation_time\": creation_time})\n            meters_per_pixel = key[\"meters_per_pixel\"]\n            del key[\"meters_per_pixel\"]\n            body_parts = dlc_result.df.columns.levels[0]\n            body_parts_df = {}\n            # Insert dlc pose estimation into analysis NWB file for each body part.\n            for body_part in bodyparts:\n                if body_part in body_parts:\n                    body_parts_df[body_part] = pd.DataFrame.from_dict(\n                        {\n                            c: dlc_result.df.get(body_part).get(c).values\n                            for c in dlc_result.df.get(body_part).columns\n                        }\n                    )\n            idx = pd.IndexSlice\n            for body_part, part_df in body_parts_df.items():\n                logger.logger.info(\"converting to cm\")\n                part_df = convert_to_cm(part_df, meters_per_pixel)\n                logger.logger.info(\"adding timestamps to DataFrame\")\n                part_df = add_timestamps(\n                    part_df, pos_time=pos_time, video_time=video_time\n                )\n                key[\"bodypart\"] = body_part\n                key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                    key[\"nwb_file_name\"]\n                )\n                position = pynwb.behavior.Position()\n                likelihood = pynwb.behavior.BehavioralTimeSeries()\n                position.create_spatial_series(\n                    name=\"position\",\n                    timestamps=part_df.time.to_numpy(),\n                    conversion=METERS_PER_CM,\n                    data=part_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                    reference_frame=spatial_series.reference_frame,\n                    comments=spatial_series.comments,\n                    description=\"x_position, y_position\",\n                )\n                likelihood.create_timeseries(\n                    name=\"likelihood\",\n                    timestamps=part_df.time.to_numpy(),\n                    data=part_df.loc[:, idx[\"likelihood\"]].to_numpy(),\n                    unit=\"likelihood\",\n                    comments=\"no comments\",\n                    description=\"likelihood\",\n                )\n                likelihood.create_timeseries(\n                    name=\"video_frame_ind\",\n                    timestamps=part_df.time.to_numpy(),\n                    data=part_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                    unit=\"index\",\n                    comments=\"no comments\",\n                    description=\"video_frame_ind\",\n                )\n                nwb_analysis_file = AnalysisNwbfile()\n                key[\n                    \"dlc_pose_estimation_position_object_id\"\n                ] = nwb_analysis_file.add_nwb_object(\n                    analysis_file_name=key[\"analysis_file_name\"],\n                    nwb_object=position,\n                )\n                key[\n                    \"dlc_pose_estimation_likelihood_object_id\"\n                ] = nwb_analysis_file.add_nwb_object(\n                    analysis_file_name=key[\"analysis_file_name\"],\n                    nwb_object=likelihood,\n                )\n                nwb_analysis_file.add(\n                    nwb_file_name=key[\"nwb_file_name\"],\n                    analysis_file_name=key[\"analysis_file_name\"],\n                )\n                self.BodyPart.insert1(key)\n\n    def fetch_dataframe(self, *attrs, **kwargs):\n        entries = (self.BodyPart &amp; self).fetch(\"KEY\")\n        nwb_data_dict = {\n            entry[\"bodypart\"]: (self.BodyPart() &amp; entry).fetch_nwb()[0]\n            for entry in entries\n        }\n        index = pd.Index(\n            np.asarray(\n                nwb_data_dict[entries[0][\"bodypart\"]][\n                    \"dlc_pose_estimation_position\"\n                ]\n                .get_spatial_series()\n                .timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"x\",\n            \"y\",\n            \"likelihood\",\n        ]\n        return pd.concat(\n            {\n                entry[\"bodypart\"]: pd.DataFrame(\n                    np.concatenate(\n                        (\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_likelihood\"\n                                ]\n                                .time_series[\"video_frame_ind\"]\n                                .data,\n                                dtype=int,\n                            )[:, np.newaxis],\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_position\"\n                                ]\n                                .get_spatial_series()\n                                .data\n                            ),\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_likelihood\"\n                                ]\n                                .time_series[\"likelihood\"]\n                                .data\n                            )[:, np.newaxis],\n                        ),\n                        axis=1,\n                    ),\n                    columns=COLUMNS,\n                    index=index,\n                )\n                for entry in entries\n            },\n            axis=1,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimation.make", "title": "<code>make(key)</code>", "text": "<p>.populate() method will launch training for each PoseEstimationTask</p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>def make(self, key):\n\"\"\".populate() method will launch training for each PoseEstimationTask\"\"\"\n    from . import dlc_reader\n    from .dlc_utils import get_video_path\n\n    METERS_PER_CM = 0.01\n\n    output_dir = infer_output_dir(key=key, makedir=False)\n    with OutputLogger(\n        name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n        path=f\"{output_dir.as_posix()}/log.log\",\n    ) as logger:\n        logger.logger.info(\"----------------------\")\n        logger.logger.info(\"Pose Estimation\")\n        # ID model and directories\n        dlc_model = (DLCModel &amp; key).fetch1()\n        bodyparts = (DLCModel.BodyPart &amp; key).fetch(\"bodypart\")\n        task_mode, analyze_video_params, video_path, output_dir = (\n            DLCPoseEstimationSelection &amp; key\n        ).fetch1(\n            \"task_mode\",\n            \"pose_estimation_params\",\n            \"video_path\",\n            \"pose_estimation_output_dir\",\n        )\n        analyze_video_params = analyze_video_params or {}\n\n        project_path = dlc_model[\"project_path\"]\n\n        # Trigger PoseEstimation\n        if task_mode == \"trigger\":\n            dlc_reader.do_pose_estimation(\n                video_path,\n                dlc_model,\n                project_path,\n                output_dir,\n                **analyze_video_params,\n            )\n        dlc_result = dlc_reader.PoseEstimation(output_dir)\n        creation_time = datetime.fromtimestamp(\n            dlc_result.creation_time\n        ).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        logger.logger.info(\"getting raw position\")\n        interval_list_name = f\"pos {key['epoch']-1} valid times\"\n        spatial_series = (\n            RawPosition()\n            &amp; {**key, \"interval_list_name\": interval_list_name}\n        ).fetch_nwb()[0][\"raw_position\"]\n        _, _, _, video_time = get_video_path(key)\n        pos_time = spatial_series.timestamps\n        # TODO: should get timestamps from VideoFile, but need the video_frame_ind from RawPosition,\n        # which also has timestamps\n        key[\"meters_per_pixel\"] = spatial_series.conversion\n\n        # Insert entry into DLCPoseEstimation\n        logger.logger.info(\n            \"Inserting %s, epoch %02d into DLCPoseEsimation\",\n            key[\"nwb_file_name\"],\n            key[\"epoch\"],\n        )\n        self.insert1({**key, \"pose_estimation_time\": creation_time})\n        meters_per_pixel = key[\"meters_per_pixel\"]\n        del key[\"meters_per_pixel\"]\n        body_parts = dlc_result.df.columns.levels[0]\n        body_parts_df = {}\n        # Insert dlc pose estimation into analysis NWB file for each body part.\n        for body_part in bodyparts:\n            if body_part in body_parts:\n                body_parts_df[body_part] = pd.DataFrame.from_dict(\n                    {\n                        c: dlc_result.df.get(body_part).get(c).values\n                        for c in dlc_result.df.get(body_part).columns\n                    }\n                )\n        idx = pd.IndexSlice\n        for body_part, part_df in body_parts_df.items():\n            logger.logger.info(\"converting to cm\")\n            part_df = convert_to_cm(part_df, meters_per_pixel)\n            logger.logger.info(\"adding timestamps to DataFrame\")\n            part_df = add_timestamps(\n                part_df, pos_time=pos_time, video_time=video_time\n            )\n            key[\"bodypart\"] = body_part\n            key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                key[\"nwb_file_name\"]\n            )\n            position = pynwb.behavior.Position()\n            likelihood = pynwb.behavior.BehavioralTimeSeries()\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=part_df.time.to_numpy(),\n                conversion=METERS_PER_CM,\n                data=part_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"x_position, y_position\",\n            )\n            likelihood.create_timeseries(\n                name=\"likelihood\",\n                timestamps=part_df.time.to_numpy(),\n                data=part_df.loc[:, idx[\"likelihood\"]].to_numpy(),\n                unit=\"likelihood\",\n                comments=\"no comments\",\n                description=\"likelihood\",\n            )\n            likelihood.create_timeseries(\n                name=\"video_frame_ind\",\n                timestamps=part_df.time.to_numpy(),\n                data=part_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                unit=\"index\",\n                comments=\"no comments\",\n                description=\"video_frame_ind\",\n            )\n            nwb_analysis_file = AnalysisNwbfile()\n            key[\n                \"dlc_pose_estimation_position_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=position,\n            )\n            key[\n                \"dlc_pose_estimation_likelihood_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=likelihood,\n            )\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n            self.BodyPart.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.VideoFile", "title": "<code>VideoFile</code>", "text": "<p>         Bases: <code>dj.Imported</code></p>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.VideoFile--notes", "title": "Notes", "text": "<p>The video timestamps come from: videoTimeStamps.cameraHWSync if PTP is used. If PTP is not used, the video timestamps come from videoTimeStamps.cameraHWFrameCount .</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass VideoFile(dj.Imported):\n\"\"\"\n\n    Notes\n    -----\n    The video timestamps come from: videoTimeStamps.cameraHWSync if PTP is used.\n    If PTP is not used, the video timestamps come from videoTimeStamps.cameraHWFrameCount .\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; TaskEpoch\n    video_file_num = 0: int\n    ---\n    camera_name: varchar(80)\n    video_file_object_id: varchar(40)  # the object id of the file object\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        videos = get_data_interface(\n            nwbf, \"video\", pynwb.behavior.BehavioralEvents\n        )\n\n        if videos is None:\n            print(f\"No video data interface found in {nwb_file_name}\\n\")\n            return\n        else:\n            videos = videos.time_series\n\n        # get the interval for the current TaskEpoch\n        interval_list_name = (TaskEpoch() &amp; key).fetch1(\"interval_list_name\")\n        valid_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n\n        is_found = False\n        for ind, video in enumerate(videos.values()):\n            if isinstance(video, pynwb.image.ImageSeries):\n                video = [video]\n            for video_obj in video:\n                # check to see if the times for this video_object are largely overlapping with the task epoch times\n                if len(\n                    interval_list_contains(valid_times, video_obj.timestamps)\n                    &gt; 0.9 * len(video_obj.timestamps)\n                ):\n                    key[\"video_file_num\"] = ind\n                    camera_name = video_obj.device.camera_name\n                    if CameraDevice &amp; {\"camera_name\": camera_name}:\n                        key[\"camera_name\"] = video_obj.device.camera_name\n                    else:\n                        raise KeyError(\n                            f\"No camera with camera_name: {camera_name} found in CameraDevice table.\"\n                        )\n                    key[\"video_file_object_id\"] = video_obj.object_id\n                    self.insert1(key)\n                    is_found = True\n\n        if not is_found:\n            print(f\"No video found corresponding to epoch {interval_list_name}\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    @classmethod\n    def update_entries(cls, restrict={}):\n        existing_entries = (cls &amp; restrict).fetch(\"KEY\")\n        for row in existing_entries:\n            if (cls &amp; row).fetch1(\"camera_name\"):\n                continue\n            video_nwb = (cls &amp; row).fetch_nwb()[0]\n            if len(video_nwb) != 1:\n                raise ValueError(\n                    f\"expecting 1 video file per entry, but {len(video_nwb)} files found\"\n                )\n            row[\"camera_name\"] = video_nwb[0][\"video_file\"].device.camera_name\n            cls.update1(row=row)\n\n    @classmethod\n    def get_abs_path(cls, key: Dict):\n\"\"\"Return the absolute path for a stored video file given a key with the nwb_file_name and epoch number\n\n        The SPYGLASS_VIDEO_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        key : dict\n            dictionary with nwb_file_name and epoch as keys\n\n        Returns\n        -------\n        nwb_video_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        video_dir = pathlib.Path(os.getenv(\"SPYGLASS_VIDEO_DIR\", None))\n        assert video_dir is not None, \"You must set SPYGLASS_VIDEO_DIR\"\n        if not video_dir.exists():\n            raise OSError(\"SPYGLASS_VIDEO_DIR does not exist\")\n        video_info = (cls &amp; key).fetch1()\n        nwb_path = Nwbfile.get_abs_path(key[\"nwb_file_name\"])\n        nwbf = get_nwb_file(nwb_path)\n        nwb_video = nwbf.objects[video_info[\"video_file_object_id\"]]\n        video_filename = nwb_video.name\n        # see if the file exists and is stored in the base analysis dir\n        nwb_video_file_abspath = pathlib.Path(\n            f\"{video_dir}/{pathlib.Path(video_filename)}\"\n        )\n        if nwb_video_file_abspath.exists():\n            return nwb_video_file_abspath.as_posix()\n        else:\n            raise FileNotFoundError(\n                f\"video file with filename: {video_filename} \"\n                f\"does not exist in {video_dir}/\"\n            )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.common.common_behav.VideoFile.get_abs_path", "title": "<code>get_abs_path(key)</code>  <code>classmethod</code>", "text": "<p>Return the absolute path for a stored video file given a key with the nwb_file_name and epoch number</p> <p>The SPYGLASS_VIDEO_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>dictionary with nwb_file_name and epoch as keys</p> required <p>Returns:</p> Name Type Description <code>nwb_video_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@classmethod\ndef get_abs_path(cls, key: Dict):\n\"\"\"Return the absolute path for a stored video file given a key with the nwb_file_name and epoch number\n\n    The SPYGLASS_VIDEO_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    key : dict\n        dictionary with nwb_file_name and epoch as keys\n\n    Returns\n    -------\n    nwb_video_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    video_dir = pathlib.Path(os.getenv(\"SPYGLASS_VIDEO_DIR\", None))\n    assert video_dir is not None, \"You must set SPYGLASS_VIDEO_DIR\"\n    if not video_dir.exists():\n        raise OSError(\"SPYGLASS_VIDEO_DIR does not exist\")\n    video_info = (cls &amp; key).fetch1()\n    nwb_path = Nwbfile.get_abs_path(key[\"nwb_file_name\"])\n    nwbf = get_nwb_file(nwb_path)\n    nwb_video = nwbf.objects[video_info[\"video_file_object_id\"]]\n    video_filename = nwb_video.name\n    # see if the file exists and is stored in the base analysis dir\n    nwb_video_file_abspath = pathlib.Path(\n        f\"{video_dir}/{pathlib.Path(video_filename)}\"\n    )\n    if nwb_video_file_abspath.exists():\n        return nwb_video_file_abspath.as_posix()\n    else:\n        raise FileNotFoundError(\n            f\"video file with filename: {video_filename} \"\n            f\"does not exist in {video_dir}/\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.infer_output_dir", "title": "<code>infer_output_dir(key, makedir=True)</code>", "text": "<p>Return the expected pose_estimation_output_dir.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> required Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def infer_output_dir(key, makedir=True):\n\"\"\"Return the expected pose_estimation_output_dir.\n\n    Parameters\n    ----------\n    key: DataJoint key specifying a pairing of VideoFile and Model.\n    \"\"\"\n    # TODO: add check to make sure interval_list_name refers to a single epoch\n    # Or make key include epoch in and of itself instead of interval_list_name\n    nwb_file_name = key[\"nwb_file_name\"].split(\"_.\")[0]\n    output_dir = pathlib.Path(os.getenv(\"DLC_OUTPUT_PATH\")) / pathlib.Path(\n        f\"{nwb_file_name}/{nwb_file_name}_{key['epoch']:02}\"\n        f\"_model_\" + key[\"dlc_model_name\"].replace(\" \", \"-\")\n    )\n    if makedir is True:\n        if not os.path.exists(output_dir):\n            output_dir.mkdir(parents=True, exist_ok=True)\n    return output_dir\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_pose_estimation/#src.spyglass.position.v1.position_dlc_pose_estimation.add_timestamps", "title": "<code>add_timestamps(df, pos_time, video_time)</code>", "text": "<p>Takes timestamps from raw_pos_df and adds to df, which is returned with timestamps and their matching video frame index</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>pose estimation dataframe to add timestamps</p> required <code>pos_time</code> <code>np.ndarray</code> <p>numpy array containing timestamps from the raw position object</p> required <code>video_time</code> <code>np.ndarray</code> <p>numpy array containing timestamps from the video file</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>original df with timestamps and video_frame_ind as new columns</p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>def add_timestamps(\n    df: pd.DataFrame, pos_time: np.ndarray, video_time: np.ndarray\n) -&gt; pd.DataFrame:\n\"\"\"\n    Takes timestamps from raw_pos_df and adds to df,\n    which is returned with timestamps and their matching video frame index\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        pose estimation dataframe to add timestamps\n    pos_time : np.ndarray\n        numpy array containing timestamps from the raw position object\n    video_time: np.ndarray\n        numpy array containing timestamps from the video file\n\n    Returns\n    -------\n    pd.DataFrame\n        original df with timestamps and video_frame_ind as new columns\n    \"\"\"\n    first_video_frame = np.searchsorted(video_time, pos_time[0])\n    video_frame_ind = np.arange(first_video_frame, len(video_time))\n    time_df = pd.DataFrame(\n        index=video_frame_ind,\n        data=video_time[first_video_frame:],\n        columns=[\"time\"],\n    )\n    df = df.join(time_df)\n    # Drop indices where time is NaN\n    df = df.dropna(subset=[\"time\"])\n    # Add video_frame_ind as column\n    df = df.rename_axis(\"video_frame_ind\").reset_index()\n    return df\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/", "title": "position_dlc_position.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.position.v1.position_dlc_position.DLCSmoothInterpParams", "title": "<code>DLCSmoothInterpParams</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Parameters for extracting the smoothed head position.</p> <p>Attributes:</p> Name Type Description <code>interpolate</code> <code>bool, default True</code> <p>whether to interpolate over NaN spans</p> <code>smooth</code> <code>bool, default True</code> <p>whether to smooth the dataset</p> <code>smoothing_params</code> <code>dict</code> <p>smoothing_duration : float, default 0.05 number of frames to smooth over: sampling_rate*smoothing_duration = num_frames</p> <code>interp_params</code> <code>dict</code> <p>max_cm_to_interp : int, default 20 maximum distance between high likelihood points on either side of a NaN span to interpolate over</p> <code>likelihood_thresh</code> <code>float, default 0.95</code> <p>likelihood below which to NaN and interpolate over</p> Source code in <code>src/spyglass/position/v1/position_dlc_position.py</code> <pre><code>@schema\nclass DLCSmoothInterpParams(dj.Manual):\n\"\"\"\n    Parameters for extracting the smoothed head position.\n\n    Attributes\n    ----------\n    interpolate : bool, default True\n        whether to interpolate over NaN spans\n    smooth : bool, default True\n        whether to smooth the dataset\n    smoothing_params : dict\n        smoothing_duration : float, default 0.05\n            number of frames to smooth over: sampling_rate*smoothing_duration = num_frames\n    interp_params : dict\n        max_cm_to_interp : int, default 20\n            maximum distance between high likelihood points on either side of a NaN span\n            to interpolate over\n    likelihood_thresh : float, default 0.95\n        likelihood below which to NaN and interpolate over\n    \"\"\"\n\n    definition = \"\"\"\n    dlc_si_params_name : varchar(80) # name for this set of parameters\n    ---\n    params: longblob # dictionary of parameters\n    \"\"\"\n\n    @classmethod\n    def insert_params(cls, params_name: str, params: dict, **kwargs):\n        cls.insert1(\n            {\"dlc_si_params_name\": params_name, \"params\": params},\n            **kwargs,\n        )\n\n    @classmethod\n    def insert_default(cls, **kwargs):\n        default_params = {\n            \"smooth\": True,\n            \"smoothing_params\": {\n                \"smoothing_duration\": 0.05,\n                \"smooth_method\": \"moving_avg\",\n            },\n            \"interpolate\": True,\n            \"likelihood_thresh\": 0.95,\n            \"interp_params\": {\"max_cm_to_interp\": 15},\n            \"max_cm_between_pts\": 20,\n            # This is for use when finding \"good spans\" and is how many indices to bridge in between good spans\n            # see inds_to_span in get_good_spans\n            \"num_inds_to_span\": 20,\n        }\n        cls.insert1(\n            {\"dlc_si_params_name\": \"default\", \"params\": default_params},\n            **kwargs,\n        )\n\n    @classmethod\n    def insert_nan_params(cls, **kwargs):\n        nan_params = {\n            \"smooth\": False,\n            \"interpolate\": False,\n            \"likelihood_thresh\": 0.95,\n            \"max_cm_between_pts\": 20,\n            \"num_inds_to_span\": 20,\n        }\n        cls.insert1(\n            {\"dlc_si_params_name\": \"just_nan\", \"params\": nan_params}, **kwargs\n        )\n\n    @classmethod\n    def get_default(cls):\n        query = cls &amp; {\"dlc_si_params_name\": \"default\"}\n        if not len(query) &gt; 0:\n            cls().insert_default(skip_duplicates=True)\n            default = (cls &amp; {\"dlc_si_params_name\": \"default\"}).fetch1()\n        else:\n            default = query.fetch1()\n        return default\n\n    @classmethod\n    def get_nan_params(cls):\n        query = cls &amp; {\"dlc_si_params_name\": \"just_nan\"}\n        if not len(query) &gt; 0:\n            cls().insert_nan_params(skip_duplicates=True)\n            nan_params = (cls &amp; {\"dlc_si_params_name\": \"just_nan\"}).fetch1()\n        else:\n            nan_params = query.fetch1()\n        return nan_params\n\n    @staticmethod\n    def get_available_methods():\n        return _key_to_smooth_func_dict.keys()\n\n    def insert1(self, key, **kwargs):\n        if \"params\" in key:\n            if not \"max_cm_between_pts\" in key[\"params\"]:\n                raise KeyError(\"max_cm_between_pts is a required parameter\")\n            if \"smooth\" in key[\"params\"]:\n                if key[\"params\"][\"smooth\"]:\n                    if \"smoothing_params\" in key[\"params\"]:\n                        if \"smooth_method\" in key[\"params\"][\"smoothing_params\"]:\n                            smooth_method = key[\"params\"][\"smoothing_params\"][\n                                \"smooth_method\"\n                            ]\n                            if smooth_method not in _key_to_smooth_func_dict:\n                                raise KeyError(\n                                    f\"smooth_method: {smooth_method} not an available method.\"\n                                )\n                        if (\n                            not \"smoothing_duration\"\n                            in key[\"params\"][\"smoothing_params\"]\n                        ):\n                            raise KeyError(\n                                \"smoothing_duration must be passed as a smoothing_params within key['params']\"\n                            )\n                        else:\n                            assert isinstance(\n                                key[\"params\"][\"smoothing_params\"][\n                                    \"smoothing_duration\"\n                                ],\n                                (float, int),\n                            ), \"smoothing_duration must be a float or int\"\n                    else:\n                        raise ValueError(\n                            \"smoothing_params not in key['params']\"\n                        )\n            if \"likelihood_thresh\" in key[\"params\"]:\n                assert isinstance(\n                    key[\"params\"][\"likelihood_thresh\"],\n                    float,\n                ), \"likelihood_thresh must be a float\"\n                assert (\n                    0 &lt; key[\"params\"][\"likelihood_thresh\"] &lt; 1\n                ), \"likelihood_thresh must be between 0 and 1\"\n            else:\n                raise ValueError(\n                    \"likelihood_thresh must be passed within key['params']\"\n                )\n        else:\n            raise KeyError(\"'params' must be in key\")\n        super().insert1(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.position.v1.position_dlc_position.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.position.v1.position_dlc_position.DLCPoseEstimation", "title": "<code>DLCPoseEstimation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@schema\nclass DLCPoseEstimation(dj.Computed):\n    definition = \"\"\"\n    -&gt; DLCPoseEstimationSelection\n    ---\n    pose_estimation_time: datetime  # time of generation of this set of DLC results\n    meters_per_pixel : double       # conversion of meters per pixel for analyzed video\n    \"\"\"\n\n    class BodyPart(dj.Part):\n        definition = \"\"\" # uses DeepLabCut h5 output for body part position\n        -&gt; DLCPoseEstimation\n        -&gt; DLCModel.BodyPart\n        ---\n        -&gt; AnalysisNwbfile\n        dlc_pose_estimation_position_object_id : varchar(80)\n        dlc_pose_estimation_likelihood_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n        def fetch1_dataframe(self):\n            nwb_data = self.fetch_nwb()[0]\n            index = pd.Index(\n                np.asarray(\n                    nwb_data[\"dlc_pose_estimation_position\"]\n                    .get_spatial_series()\n                    .timestamps\n                ),\n                name=\"time\",\n            )\n            COLUMNS = [\n                \"video_frame_ind\",\n                \"x\",\n                \"y\",\n                \"likelihood\",\n            ]\n            return pd.DataFrame(\n                np.concatenate(\n                    (\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_likelihood\"]\n                            .time_series[\"video_frame_ind\"]\n                            .data,\n                            dtype=int,\n                        )[:, np.newaxis],\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_position\"]\n                            .get_spatial_series()\n                            .data\n                        ),\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_likelihood\"]\n                            .time_series[\"likelihood\"]\n                            .data\n                        )[:, np.newaxis],\n                    ),\n                    axis=1,\n                ),\n                columns=COLUMNS,\n                index=index,\n            )\n\n    def make(self, key):\n\"\"\".populate() method will launch training for each PoseEstimationTask\"\"\"\n        from . import dlc_reader\n        from .dlc_utils import get_video_path\n\n        METERS_PER_CM = 0.01\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n        ) as logger:\n            logger.logger.info(\"----------------------\")\n            logger.logger.info(\"Pose Estimation\")\n            # ID model and directories\n            dlc_model = (DLCModel &amp; key).fetch1()\n            bodyparts = (DLCModel.BodyPart &amp; key).fetch(\"bodypart\")\n            task_mode, analyze_video_params, video_path, output_dir = (\n                DLCPoseEstimationSelection &amp; key\n            ).fetch1(\n                \"task_mode\",\n                \"pose_estimation_params\",\n                \"video_path\",\n                \"pose_estimation_output_dir\",\n            )\n            analyze_video_params = analyze_video_params or {}\n\n            project_path = dlc_model[\"project_path\"]\n\n            # Trigger PoseEstimation\n            if task_mode == \"trigger\":\n                dlc_reader.do_pose_estimation(\n                    video_path,\n                    dlc_model,\n                    project_path,\n                    output_dir,\n                    **analyze_video_params,\n                )\n            dlc_result = dlc_reader.PoseEstimation(output_dir)\n            creation_time = datetime.fromtimestamp(\n                dlc_result.creation_time\n            ).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n            logger.logger.info(\"getting raw position\")\n            interval_list_name = f\"pos {key['epoch']-1} valid times\"\n            spatial_series = (\n                RawPosition()\n                &amp; {**key, \"interval_list_name\": interval_list_name}\n            ).fetch_nwb()[0][\"raw_position\"]\n            _, _, _, video_time = get_video_path(key)\n            pos_time = spatial_series.timestamps\n            # TODO: should get timestamps from VideoFile, but need the video_frame_ind from RawPosition,\n            # which also has timestamps\n            key[\"meters_per_pixel\"] = spatial_series.conversion\n\n            # Insert entry into DLCPoseEstimation\n            logger.logger.info(\n                \"Inserting %s, epoch %02d into DLCPoseEsimation\",\n                key[\"nwb_file_name\"],\n                key[\"epoch\"],\n            )\n            self.insert1({**key, \"pose_estimation_time\": creation_time})\n            meters_per_pixel = key[\"meters_per_pixel\"]\n            del key[\"meters_per_pixel\"]\n            body_parts = dlc_result.df.columns.levels[0]\n            body_parts_df = {}\n            # Insert dlc pose estimation into analysis NWB file for each body part.\n            for body_part in bodyparts:\n                if body_part in body_parts:\n                    body_parts_df[body_part] = pd.DataFrame.from_dict(\n                        {\n                            c: dlc_result.df.get(body_part).get(c).values\n                            for c in dlc_result.df.get(body_part).columns\n                        }\n                    )\n            idx = pd.IndexSlice\n            for body_part, part_df in body_parts_df.items():\n                logger.logger.info(\"converting to cm\")\n                part_df = convert_to_cm(part_df, meters_per_pixel)\n                logger.logger.info(\"adding timestamps to DataFrame\")\n                part_df = add_timestamps(\n                    part_df, pos_time=pos_time, video_time=video_time\n                )\n                key[\"bodypart\"] = body_part\n                key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                    key[\"nwb_file_name\"]\n                )\n                position = pynwb.behavior.Position()\n                likelihood = pynwb.behavior.BehavioralTimeSeries()\n                position.create_spatial_series(\n                    name=\"position\",\n                    timestamps=part_df.time.to_numpy(),\n                    conversion=METERS_PER_CM,\n                    data=part_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                    reference_frame=spatial_series.reference_frame,\n                    comments=spatial_series.comments,\n                    description=\"x_position, y_position\",\n                )\n                likelihood.create_timeseries(\n                    name=\"likelihood\",\n                    timestamps=part_df.time.to_numpy(),\n                    data=part_df.loc[:, idx[\"likelihood\"]].to_numpy(),\n                    unit=\"likelihood\",\n                    comments=\"no comments\",\n                    description=\"likelihood\",\n                )\n                likelihood.create_timeseries(\n                    name=\"video_frame_ind\",\n                    timestamps=part_df.time.to_numpy(),\n                    data=part_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                    unit=\"index\",\n                    comments=\"no comments\",\n                    description=\"video_frame_ind\",\n                )\n                nwb_analysis_file = AnalysisNwbfile()\n                key[\n                    \"dlc_pose_estimation_position_object_id\"\n                ] = nwb_analysis_file.add_nwb_object(\n                    analysis_file_name=key[\"analysis_file_name\"],\n                    nwb_object=position,\n                )\n                key[\n                    \"dlc_pose_estimation_likelihood_object_id\"\n                ] = nwb_analysis_file.add_nwb_object(\n                    analysis_file_name=key[\"analysis_file_name\"],\n                    nwb_object=likelihood,\n                )\n                nwb_analysis_file.add(\n                    nwb_file_name=key[\"nwb_file_name\"],\n                    analysis_file_name=key[\"analysis_file_name\"],\n                )\n                self.BodyPart.insert1(key)\n\n    def fetch_dataframe(self, *attrs, **kwargs):\n        entries = (self.BodyPart &amp; self).fetch(\"KEY\")\n        nwb_data_dict = {\n            entry[\"bodypart\"]: (self.BodyPart() &amp; entry).fetch_nwb()[0]\n            for entry in entries\n        }\n        index = pd.Index(\n            np.asarray(\n                nwb_data_dict[entries[0][\"bodypart\"]][\n                    \"dlc_pose_estimation_position\"\n                ]\n                .get_spatial_series()\n                .timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"x\",\n            \"y\",\n            \"likelihood\",\n        ]\n        return pd.concat(\n            {\n                entry[\"bodypart\"]: pd.DataFrame(\n                    np.concatenate(\n                        (\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_likelihood\"\n                                ]\n                                .time_series[\"video_frame_ind\"]\n                                .data,\n                                dtype=int,\n                            )[:, np.newaxis],\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_position\"\n                                ]\n                                .get_spatial_series()\n                                .data\n                            ),\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_likelihood\"\n                                ]\n                                .time_series[\"likelihood\"]\n                                .data\n                            )[:, np.newaxis],\n                        ),\n                        axis=1,\n                    ),\n                    columns=COLUMNS,\n                    index=index,\n                )\n                for entry in entries\n            },\n            axis=1,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimation.make", "title": "<code>make(key)</code>", "text": "<p>.populate() method will launch training for each PoseEstimationTask</p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>def make(self, key):\n\"\"\".populate() method will launch training for each PoseEstimationTask\"\"\"\n    from . import dlc_reader\n    from .dlc_utils import get_video_path\n\n    METERS_PER_CM = 0.01\n\n    output_dir = infer_output_dir(key=key, makedir=False)\n    with OutputLogger(\n        name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n        path=f\"{output_dir.as_posix()}/log.log\",\n    ) as logger:\n        logger.logger.info(\"----------------------\")\n        logger.logger.info(\"Pose Estimation\")\n        # ID model and directories\n        dlc_model = (DLCModel &amp; key).fetch1()\n        bodyparts = (DLCModel.BodyPart &amp; key).fetch(\"bodypart\")\n        task_mode, analyze_video_params, video_path, output_dir = (\n            DLCPoseEstimationSelection &amp; key\n        ).fetch1(\n            \"task_mode\",\n            \"pose_estimation_params\",\n            \"video_path\",\n            \"pose_estimation_output_dir\",\n        )\n        analyze_video_params = analyze_video_params or {}\n\n        project_path = dlc_model[\"project_path\"]\n\n        # Trigger PoseEstimation\n        if task_mode == \"trigger\":\n            dlc_reader.do_pose_estimation(\n                video_path,\n                dlc_model,\n                project_path,\n                output_dir,\n                **analyze_video_params,\n            )\n        dlc_result = dlc_reader.PoseEstimation(output_dir)\n        creation_time = datetime.fromtimestamp(\n            dlc_result.creation_time\n        ).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        logger.logger.info(\"getting raw position\")\n        interval_list_name = f\"pos {key['epoch']-1} valid times\"\n        spatial_series = (\n            RawPosition()\n            &amp; {**key, \"interval_list_name\": interval_list_name}\n        ).fetch_nwb()[0][\"raw_position\"]\n        _, _, _, video_time = get_video_path(key)\n        pos_time = spatial_series.timestamps\n        # TODO: should get timestamps from VideoFile, but need the video_frame_ind from RawPosition,\n        # which also has timestamps\n        key[\"meters_per_pixel\"] = spatial_series.conversion\n\n        # Insert entry into DLCPoseEstimation\n        logger.logger.info(\n            \"Inserting %s, epoch %02d into DLCPoseEsimation\",\n            key[\"nwb_file_name\"],\n            key[\"epoch\"],\n        )\n        self.insert1({**key, \"pose_estimation_time\": creation_time})\n        meters_per_pixel = key[\"meters_per_pixel\"]\n        del key[\"meters_per_pixel\"]\n        body_parts = dlc_result.df.columns.levels[0]\n        body_parts_df = {}\n        # Insert dlc pose estimation into analysis NWB file for each body part.\n        for body_part in bodyparts:\n            if body_part in body_parts:\n                body_parts_df[body_part] = pd.DataFrame.from_dict(\n                    {\n                        c: dlc_result.df.get(body_part).get(c).values\n                        for c in dlc_result.df.get(body_part).columns\n                    }\n                )\n        idx = pd.IndexSlice\n        for body_part, part_df in body_parts_df.items():\n            logger.logger.info(\"converting to cm\")\n            part_df = convert_to_cm(part_df, meters_per_pixel)\n            logger.logger.info(\"adding timestamps to DataFrame\")\n            part_df = add_timestamps(\n                part_df, pos_time=pos_time, video_time=video_time\n            )\n            key[\"bodypart\"] = body_part\n            key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                key[\"nwb_file_name\"]\n            )\n            position = pynwb.behavior.Position()\n            likelihood = pynwb.behavior.BehavioralTimeSeries()\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=part_df.time.to_numpy(),\n                conversion=METERS_PER_CM,\n                data=part_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"x_position, y_position\",\n            )\n            likelihood.create_timeseries(\n                name=\"likelihood\",\n                timestamps=part_df.time.to_numpy(),\n                data=part_df.loc[:, idx[\"likelihood\"]].to_numpy(),\n                unit=\"likelihood\",\n                comments=\"no comments\",\n                description=\"likelihood\",\n            )\n            likelihood.create_timeseries(\n                name=\"video_frame_ind\",\n                timestamps=part_df.time.to_numpy(),\n                data=part_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                unit=\"index\",\n                comments=\"no comments\",\n                description=\"video_frame_ind\",\n            )\n            nwb_analysis_file = AnalysisNwbfile()\n            key[\n                \"dlc_pose_estimation_position_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=position,\n            )\n            key[\n                \"dlc_pose_estimation_likelihood_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=likelihood,\n            )\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n            self.BodyPart.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.position.v1.position_dlc_position.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.position.v1.position_dlc_position.DLCSmoothInterp", "title": "<code>DLCSmoothInterp</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Interpolates across low likelihood periods and smooths the position Can take a few minutes.</p> Source code in <code>src/spyglass/position/v1/position_dlc_position.py</code> <pre><code>@schema\nclass DLCSmoothInterp(dj.Computed):\n\"\"\"\n    Interpolates across low likelihood periods and smooths the position\n    Can take a few minutes.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCSmoothInterpSelection\n    ---\n    -&gt; AnalysisNwbfile\n    dlc_smooth_interp_position_object_id : varchar(80)\n    dlc_smooth_interp_info_object_id : varchar(80)\n    \"\"\"\n\n    def make(self, key):\n        from .dlc_utils import OutputLogger, infer_output_dir\n\n        METERS_PER_CM = 0.01\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n            print_console=False,\n        ) as logger:\n            logger.logger.info(\"-----------------------\")\n            idx = pd.IndexSlice\n            # Get labels to smooth from Parameters table\n            params = (DLCSmoothInterpParams() &amp; key).fetch1(\"params\")\n            # Get DLC output dataframe\n            logger.logger.info(\"fetching Pose Estimation Dataframe\")\n            dlc_df = (DLCPoseEstimation.BodyPart() &amp; key).fetch1_dataframe()\n            dt = np.median(np.diff(dlc_df.index.to_numpy()))\n            sampling_rate = 1 / dt\n            logger.logger.info(\"Identifying indices to NaN\")\n            df_w_nans, bad_inds = nan_inds(\n                dlc_df.copy(),\n                params[\"max_cm_between_pts\"],\n                likelihood_thresh=params.pop(\"likelihood_thresh\"),\n                inds_to_span=params[\"num_inds_to_span\"],\n            )\n\n            nan_spans = get_span_start_stop(np.where(bad_inds)[0])\n            if params[\"interpolate\"]:\n                logger.logger.info(\"interpolating across low likelihood times\")\n                interp_df = interp_pos(\n                    df_w_nans.copy(), nan_spans, **params[\"interp_params\"]\n                )\n            else:\n                interp_df = df_w_nans.copy()\n                logger.logger.info(\"skipping interpolation\")\n            if params[\"smooth\"]:\n                if \"smoothing_duration\" in params[\"smoothing_params\"]:\n                    smoothing_duration = params[\"smoothing_params\"].pop(\n                        \"smoothing_duration\"\n                    )\n                dt = np.median(np.diff(dlc_df.index.to_numpy()))\n                sampling_rate = 1 / dt\n                logger.logger.info(\"smoothing position\")\n                smooth_func = _key_to_smooth_func_dict[\n                    params[\"smoothing_params\"][\"smooth_method\"]\n                ]\n                logger.logger.info(\n                    \"Smoothing using method: %s\",\n                    str(params[\"smoothing_params\"][\"smooth_method\"]),\n                )\n                smooth_df = smooth_func(\n                    interp_df,\n                    smoothing_duration=smoothing_duration,\n                    sampling_rate=sampling_rate,\n                    **params[\"smoothing_params\"],\n                )\n            else:\n                smooth_df = interp_df.copy()\n                logger.logger.info(\"skipping smoothing\")\n            final_df = smooth_df.drop([\"likelihood\"], axis=1)\n            final_df = final_df.rename_axis(\"time\").reset_index()\n            position_nwb_data = (\n                (DLCPoseEstimation.BodyPart() &amp; key)\n                .fetch_nwb()[0][\"dlc_pose_estimation_position\"]\n                .get_spatial_series()\n            )\n            key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                key[\"nwb_file_name\"]\n            )\n            # Add dataframe to AnalysisNwbfile\n            nwb_analysis_file = AnalysisNwbfile()\n            position = pynwb.behavior.Position()\n            video_frame_ind = pynwb.behavior.BehavioralTimeSeries()\n            logger.logger.info(\"Creating NWB objects\")\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=final_df.time.to_numpy(),\n                conversion=METERS_PER_CM,\n                data=final_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                reference_frame=position_nwb_data.reference_frame,\n                comments=position_nwb_data.comments,\n                description=\"x_position, y_position\",\n            )\n            video_frame_ind.create_timeseries(\n                name=\"video_frame_ind\",\n                timestamps=final_df.time.to_numpy(),\n                data=final_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                unit=\"index\",\n                comments=\"no comments\",\n                description=\"video_frame_ind\",\n            )\n            key[\n                \"dlc_smooth_interp_position_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=position,\n            )\n            key[\n                \"dlc_smooth_interp_info_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=video_frame_ind,\n            )\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n            self.insert1(key)\n            logger.logger.info(\"inserted entry into DLCSmoothInterp\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(\n                nwb_data[\"dlc_smooth_interp_position\"]\n                .get_spatial_series()\n                .timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"x\",\n            \"y\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"dlc_smooth_interp_info\"]\n                        .time_series[\"video_frame_ind\"]\n                        .data,\n                        dtype=int,\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"dlc_smooth_interp_position\"]\n                        .get_spatial_series()\n                        .data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.position.v1.position_dlc_position.get_good_spans", "title": "<code>get_good_spans(bad_inds_mask, inds_to_span=50)</code>", "text": "<p>This function takes in a boolean mask of good and bad indices and determines spans of consecutive good indices. It combines two neighboring spans with a separation of less than inds_to_span and treats them as a single good span.</p> <p>Parameters:</p> Name Type Description Default <code>bad_inds_mask</code> <code>boolean mask</code> <p>A boolean mask where True is a bad index and False is a good index.</p> required <code>inds_to_span</code> <code>int</code> <p>This indicates how many indices between two good spans should be bridged to form a single good span. For instance if span A is (1500, 2350) and span B is (2370, 3700), then span A and span B would be combined into span A (1500, 3700) since one would want to identify potential jumps in the space in between the original A and B.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>good_spans</code> <code>list</code> <p>List of spans of good indices, unmodified.</p> <code>modified_spans</code> <code>list</code> <p>spans that are amended to bridge up to inds_to_span consecutive bad indices</p> Source code in <code>src/spyglass/position/v1/position_dlc_position.py</code> <pre><code>def get_good_spans(bad_inds_mask, inds_to_span: int = 50):\n\"\"\"\n    This function takes in a boolean mask of good and bad indices and\n    determines spans of consecutive good indices. It combines two neighboring spans\n    with a separation of less than inds_to_span and treats them as a single good span.\n\n    Parameters\n    ----------\n    bad_inds_mask : boolean mask\n        A boolean mask where True is a bad index and False is a good index.\n    inds_to_span : int, default 50\n        This indicates how many indices between two good spans should\n        be bridged to form a single good span.\n        For instance if span A is (1500, 2350) and span B is (2370, 3700),\n        then span A and span B would be combined into span A (1500, 3700)\n        since one would want to identify potential jumps in the space in between the original A and B.\n\n    Returns\n    -------\n    good_spans : list\n        List of spans of good indices, unmodified.\n    modified_spans : list\n        spans that are amended to bridge up to inds_to_span consecutive bad indices\n    \"\"\"\n    good_spans = get_span_start_stop(\n        np.arange(len(bad_inds_mask))[~bad_inds_mask]\n    )\n    if len(good_spans) &gt; 1:\n        modified_spans = []\n        for (start1, stop1), (start2, stop2) in zip(\n            good_spans[:-1], good_spans[1:]\n        ):\n            check_existing = [\n                entry\n                for entry in modified_spans\n                if start1\n                in range(entry[0] - inds_to_span, entry[1] + inds_to_span)\n            ]\n            if len(check_existing) &gt; 0:\n                modify_ind = modified_spans.index(check_existing[0])\n                if (start2 - stop1) &lt;= inds_to_span:\n                    modified_spans[modify_ind] = (check_existing[0][0], stop2)\n                else:\n                    modified_spans[modify_ind] = (check_existing[0][0], stop1)\n                    modified_spans.append((start2, stop2))\n                continue\n            if (start2 - stop1) &lt;= inds_to_span:\n                modified_spans.append((start1, stop2))\n            else:\n                modified_spans.append((start1, stop1))\n                modified_spans.append((start2, stop2))\n        return good_spans, modified_spans\n    else:\n        return None, good_spans\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_position/#src.spyglass.position.v1.position_dlc_position.get_span_start_stop", "title": "<code>get_span_start_stop(indices)</code>", "text": "<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Type Description <code>_type_</code> <p>description</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_span_start_stop(indices):\n\"\"\"_summary_\n\n    Parameters\n    ----------\n    indices : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    span_inds = []\n    # Get start and stop index of spans of consecutive indices\n    for k, g in groupby(enumerate(indices), lambda x: x[1] - x[0]):\n        group = list(map(itemgetter(1), g))\n        span_inds.append((group[0], group[-1]))\n    return span_inds\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/", "title": "position_dlc_project.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.BodyPart", "title": "<code>BodyPart</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Holds bodyparts for use in DeepLabCut models</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@schema\nclass BodyPart(dj.Manual):\n\"\"\"Holds bodyparts for use in DeepLabCut models\"\"\"\n\n    definition = \"\"\"\n    bodypart                : varchar(32)\n    ---\n    bodypart_description='' : varchar(80)\n    \"\"\"\n\n    @classmethod\n    def add_from_config(cls, bodyparts: List, descriptions: List = None):\n\"\"\"Given a list of bodyparts from the config and\n        an optional list of descriptions, inserts into BodyPart table.\n\n        Parameters\n        ----------\n        bodyparts : List\n            list of bodyparts from config\n        descriptions : List, default None\n            optional list of descriptions for bodyparts.\n            If None, description is set to bodypart name\n        \"\"\"\n        if descriptions is not None:\n            bodyparts_dict = [\n                {\"bodypart\": bp, \"bodypart_description\": desc}\n                for (bp, desc) in zip(bodyparts, descriptions)\n            ]\n        else:\n            bodyparts_dict = [\n                {\"bodypart\": bp, \"bodypart_description\": bp} for bp in bodyparts\n            ]\n        cls.insert(bodyparts_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.BodyPart.add_from_config", "title": "<code>add_from_config(bodyparts, descriptions=None)</code>  <code>classmethod</code>", "text": "<p>Given a list of bodyparts from the config and an optional list of descriptions, inserts into BodyPart table.</p> <p>Parameters:</p> Name Type Description Default <code>bodyparts</code> <code>List</code> <p>list of bodyparts from config</p> required <code>descriptions</code> <code>List</code> <p>optional list of descriptions for bodyparts. If None, description is set to bodypart name</p> <code>None</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef add_from_config(cls, bodyparts: List, descriptions: List = None):\n\"\"\"Given a list of bodyparts from the config and\n    an optional list of descriptions, inserts into BodyPart table.\n\n    Parameters\n    ----------\n    bodyparts : List\n        list of bodyparts from config\n    descriptions : List, default None\n        optional list of descriptions for bodyparts.\n        If None, description is set to bodypart name\n    \"\"\"\n    if descriptions is not None:\n        bodyparts_dict = [\n            {\"bodypart\": bp, \"bodypart_description\": desc}\n            for (bp, desc) in zip(bodyparts, descriptions)\n        ]\n    else:\n        bodyparts_dict = [\n            {\"bodypart\": bp, \"bodypart_description\": bp} for bp in bodyparts\n        ]\n    cls.insert(bodyparts_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject", "title": "<code>DLCProject</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Table to facilitate creation of a new DeepLabCut model. With ability to edit config, extract frames, label frames</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@schema\nclass DLCProject(dj.Manual):\n\"\"\"Table to facilitate creation of a new DeepLabCut model.\n    With ability to edit config, extract frames, label frames\n    \"\"\"\n\n    # Add more parameters as secondary keys...\n    # TODO: collapse params into blob dict\n    definition = \"\"\"\n    project_name     : varchar(100) # name of DLC project\n    ---\n    -&gt; LabTeam\n    bodyparts        : blob         # list of bodyparts to label\n    frames_per_video : int          # number of frames to extract from each video\n    config_path      : varchar(120) # path to config.yaml for model\n    \"\"\"\n\n    class BodyPart(dj.Part):\n\"\"\"Part table to hold bodyparts used in each project.\"\"\"\n\n        definition = \"\"\"\n        -&gt; DLCProject\n        -&gt; BodyPart\n        \"\"\"\n\n    class File(dj.Part):\n        definition = \"\"\"\n        # Paths of training files (e.g., labeled pngs, CSV or video)\n        -&gt; DLCProject\n        file_name: varchar(200) # Concise name to describe file\n        file_ext : enum(\"mp4\", \"csv\", \"h5\") # extension of file\n        ---\n        file_path: varchar(255)\n        \"\"\"\n\n    def insert1(self, key, **kwargs):\n        assert isinstance(\n            key[\"project_name\"], str\n        ), \"project_name must be a string\"\n        assert isinstance(\n            key[\"frames_per_video\"], int\n        ), \"frames_per_video must be of type `int`\"\n        super().insert1(key, **kwargs)\n\n    @classmethod\n    def insert_existing_project(\n        cls,\n        project_name: str,\n        lab_team: str,\n        config_path: str,\n        bodyparts: List = None,\n        frames_per_video: int = None,\n        add_to_files: bool = True,\n        **kwargs,\n    ):\n\"\"\"\n        insert an existing project into DLCProject table.\n        Parameters\n        ----------\n        project_name : str\n            user-friendly name of project\n        lab_team : str\n            name of lab team. Should match an entry in LabTeam table\n        config_path : str\n            path to project directory\n        bodyparts : list\n            optional list of bodyparts to label that\n            are not already in existing config\n        \"\"\"\n\n        # Read config\n        project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n        if project_name in project_names_in_use:\n            print(f\"project name: {project_name} is already in use.\")\n            return_key = {}\n            return_key[\"project_name\"], return_key[\"config_path\"] = (\n                cls &amp; {\"project_name\": project_name}\n            ).fetch1(\"project_name\", \"config_path\")\n            return return_key\n        from deeplabcut.utils.auxiliaryfunctions import read_config\n\n        cfg = read_config(config_path)\n        if bodyparts:\n            bodyparts_to_add = [\n                bodypart\n                for bodypart in bodyparts\n                if bodypart not in cfg[\"bodyparts\"]\n            ]\n            all_bodyparts = bodyparts_to_add + cfg[\"bodyparts\"]\n        else:\n            all_bodyparts = cfg[\"bodyparts\"]\n        BodyPart.add_from_config(cfg[\"bodyparts\"])\n        for bodypart in all_bodyparts:\n            if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n                raise ValueError(\n                    f\"bodypart: {bodypart} not found in BodyPart table\"\n                )\n        # check bodyparts are in config, if not add\n        if len(bodyparts_to_add) &gt; 0:\n            add_to_config(config_path, bodyparts=bodyparts_to_add)\n        # Get frames per video from config. If passed as arg, check match\n        if frames_per_video:\n            if frames_per_video != cfg[\"numframes2pick\"]:\n                add_to_config(\n                    config_path, **{\"numframes2pick\": frames_per_video}\n                )\n        config_path = Path(config_path)\n        project_path = config_path.parent\n        dlc_project_path = os.environ[\"DLC_PROJECT_PATH\"]\n        if dlc_project_path not in project_path.as_posix():\n            project_dirname = project_path.name\n            dest_folder = Path(f\"{dlc_project_path}/{project_dirname}/\")\n            if dest_folder.exists():\n                new_proj_dir = dest_folder.as_posix()\n            else:\n                new_proj_dir = shutil.copytree(\n                    src=project_path,\n                    dst=f\"{dlc_project_path}/{project_dirname}/\",\n                )\n            new_config_path = Path(f\"{new_proj_dir}/config.yaml\")\n            assert (\n                new_config_path.exists()\n            ), \"config.yaml does not exist in new project directory\"\n            config_path = new_config_path\n            add_to_config(config_path, **{\"project_path\": new_proj_dir})\n        # TODO still need to copy videos over to video dir\n        key = {\n            \"project_name\": project_name,\n            \"team_name\": lab_team,\n            \"bodyparts\": bodyparts,\n            \"config_path\": config_path.as_posix(),\n            \"frames_per_video\": frames_per_video,\n        }\n        cls.insert1(key, **kwargs)\n        cls.BodyPart.insert(\n            [\n                {\"project_name\": project_name, \"bodypart\": bp}\n                for bp in all_bodyparts\n            ],\n            **kwargs,\n        )\n        if add_to_files:\n            del key[\"bodyparts\"]\n            del key[\"team_name\"]\n            del key[\"config_path\"]\n            del key[\"frames_per_video\"]\n            # Check for training files to add\n            cls.add_training_files(key, **kwargs)\n        return {\n            \"project_name\": project_name,\n            \"config_path\": config_path.as_posix(),\n        }\n\n    @classmethod\n    def insert_new_project(\n        cls,\n        project_name: str,\n        bodyparts: List,\n        lab_team: str,\n        frames_per_video: int,\n        video_list: List,\n        project_directory: str = os.getenv(\"DLC_PROJECT_PATH\"),\n        output_path: str = os.getenv(\"DLC_VIDEO_PATH\"),\n        set_permissions=False,\n        **kwargs,\n    ):\n\"\"\"\n        insert a new project into DLCProject table.\n        Parameters\n        ----------\n        project_name : str\n            user-friendly name of project\n        bodyparts : list\n            list of bodyparts to label. Should match bodyparts in BodyPart table\n        lab_team : str\n            name of lab team. Should match an entry in LabTeam table\n        project_directory : str\n            directory where to create project.\n            (Default is '/cumulus/deeplabcut/')\n        frames_per_video : int\n            number of frames to extract from each video\n        video_list : list\n            list of dicts of form [{'nwb_file_name': nwb_file_name, 'epoch': epoch #},...]\n            to query VideoFile table for videos to train on.\n            Can also be list of absolute paths to import videos from\n        output_path : str\n            target path to output converted videos\n            (Default is '/nimbus/deeplabcut/videos/')\n        set_permissions : bool\n            if True, will set permissions for user and group to be read+write\n            (Default is False)\n        \"\"\"\n        project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n        if project_name in project_names_in_use:\n            print(f\"project name: {project_name} is already in use.\")\n            return_key = {}\n            return_key[\"project_name\"], return_key[\"config_path\"] = (\n                cls &amp; {\"project_name\": project_name}\n            ).fetch1(\"project_name\", \"config_path\")\n            return return_key\n\n        add_to_files = kwargs.pop(\"add_to_files\", True)\n        if not bool(LabTeam() &amp; {\"team_name\": lab_team}):\n            raise ValueError(f\"team_name: {lab_team} does not exist in LabTeam\")\n        skeleton_node = None\n        # If dict, assume of form {'nwb_file_name': nwb_file_name, 'epoch': epoch}\n        # and pass to get_video_path to reference VideoFile table for path\n\n        if all(isinstance(n, Dict) for n in video_list):\n            videos_to_convert = [\n                get_video_path(video_key) for video_key in video_list\n            ]\n            videos = [\n                check_videofile(\n                    video_path=video[0],\n                    output_path=output_path,\n                    video_filename=video[1],\n                )[0].as_posix()\n                for video in videos_to_convert\n            ]\n        # If not dict, assume list of video file paths that may or may not need to be converted\n        else:\n            videos = []\n            if not all([Path(video).exists() for video in video_list]):\n                raise OSError(\"at least one file in video_list does not exist\")\n            for video in video_list:\n                video_path = Path(video).parent\n                video_filename = video.rsplit(\n                    video_path.as_posix(), maxsplit=1\n                )[-1].split(\"/\")[-1]\n                videos.extend(\n                    [\n                        check_videofile(\n                            video_path=video_path,\n                            output_path=output_path,\n                            video_filename=video_filename,\n                        )[0].as_posix()\n                    ]\n                )\n            if len(videos) &lt; 1:\n                raise ValueError(f\"no .mp4 videos found in{video_path}\")\n        from deeplabcut import create_new_project\n\n        config_path = create_new_project(\n            project_name,\n            lab_team,\n            videos,\n            working_directory=project_directory,\n            copy_videos=True,\n            multianimal=False,\n        )\n        for bodypart in bodyparts:\n            if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n                raise ValueError(\n                    f\"bodypart: {bodypart} not found in BodyPart table\"\n                )\n        kwargs_copy = copy.deepcopy(kwargs)\n        kwargs_copy.update({\"numframes2pick\": frames_per_video, \"dotsize\": 3})\n        add_to_config(\n            config_path, bodyparts, skeleton_node=skeleton_node, **kwargs_copy\n        )\n        key = {\n            \"project_name\": project_name,\n            \"team_name\": lab_team,\n            \"bodyparts\": bodyparts,\n            \"config_path\": config_path,\n            \"frames_per_video\": frames_per_video,\n        }\n        # TODO: make permissions setting more flexible.\n        if set_permissions:\n            permissions = (\n                stat.S_IRUSR\n                | stat.S_IWUSR\n                | stat.S_IRGRP\n                | stat.S_IWGRP\n                | stat.S_IROTH\n            )\n            username = getpass.getuser()\n            if not groupname:\n                groupname = username\n            _set_permissions(\n                directory=project_directory,\n                mode=permissions,\n                username=username,\n                groupname=groupname,\n            )\n        cls.insert1(key, **kwargs)\n        cls.BodyPart.insert(\n            [\n                {\"project_name\": project_name, \"bodypart\": bp}\n                for bp in bodyparts\n            ],\n            **kwargs,\n        )\n        if add_to_files:\n            del key[\"bodyparts\"]\n            del key[\"team_name\"]\n            del key[\"config_path\"]\n            del key[\"frames_per_video\"]\n            # Add videos to training files\n            cls.add_training_files(key, **kwargs)\n        if isinstance(config_path, PosixPath):\n            config_path = config_path.as_posix()\n        return {\"project_name\": project_name, \"config_path\": config_path}\n\n    @classmethod\n    def add_training_files(cls, key, **kwargs):\n\"\"\"Add training videos and labeled frames .h5 and .csv to DLCProject.File\"\"\"\n        config_path = (cls &amp; {\"project_name\": key[\"project_name\"]}).fetch1(\n            \"config_path\"\n        )\n        from deeplabcut.utils.auxiliaryfunctions import read_config\n\n        if \"config_path\" in key:\n            del key[\"config_path\"]\n        cfg = read_config(config_path)\n        video_names = list(cfg[\"video_sets\"].keys())\n        training_files = []\n        for video in video_names:\n            video_name = os.path.splitext(\n                video.split(os.path.dirname(video) + \"/\")[-1]\n            )[0]\n            training_files.extend(\n                glob.glob(\n                    f\"{cfg['project_path']}/labeled-data/{video_name}/*Collected*\"\n                )\n            )\n        for video in video_names:\n            key[\"file_name\"] = f'{os.path.splitext(video.split(\"/\")[-1])[0]}'\n            key[\"file_ext\"] = os.path.splitext(video.split(\"/\")[-1])[-1].split(\n                \".\"\n            )[-1]\n            key[\"file_path\"] = video\n            cls.File.insert1(key, **kwargs)\n        if len(training_files) &gt; 0:\n            for file in training_files:\n                video_name = os.path.dirname(file).split(\"/\")[-1]\n                file_type = os.path.splitext(\n                    file.split(os.path.dirname(file) + \"/\")[-1]\n                )[-1].split(\".\")[-1]\n                key[\"file_name\"] = f\"{video_name}_labeled_data\"\n                key[\"file_ext\"] = file_type\n                key[\"file_path\"] = file\n                cls.File.insert1(key, **kwargs)\n        else:\n            Warning(\"No training files to add\")\n\n    @classmethod\n    def run_extract_frames(cls, key, **kwargs):\n\"\"\"Convenience function to launch DLC GUI for extracting frames.\n        Must be run on local machine to access GUI,\n        cannot be run through ssh tunnel\n        \"\"\"\n        config_path = (cls &amp; key).fetch1(\"config_path\")\n        from deeplabcut import extract_frames\n\n        extract_frames(config_path, **kwargs)\n\n    @classmethod\n    def run_label_frames(cls, key):\n\"\"\"Convenience function to launch DLC GUI for labeling frames.\n        Must be run on local machine to access GUI,\n        cannot be run through ssh tunnel\n        \"\"\"\n        config_path = (cls &amp; key).fetch1(\"config_path\")\n        from deeplabcut import label_frames\n\n        label_frames(config_path)\n\n    @classmethod\n    def check_labels(cls, key, **kwargs):\n\"\"\"Convenience function to check labels on\n        previously extracted and labeled frames\n        \"\"\"\n        config_path = (cls &amp; key).fetch1(\"config_path\")\n        from deeplabcut import check_labels\n\n        check_labels(config_path, **kwargs)\n\n    @classmethod\n    def import_labeled_frames(\n        cls,\n        key: Dict,\n        import_project_path: Union[str, PosixPath],\n        video_filenames: Union[str, List],\n        **kwargs,\n    ):\n\"\"\"Function to import pre-labeled frames from an existing project into a new project\n\n        Parameters\n        ----------\n        key : Dict\n            key to specify entry in DLCProject table to add labeled frames to\n        import_project_path : str\n            absolute path to project directory containing labeled frames to import\n        video_filenames : str or List\n            filename or list of filenames of video(s) from which to import frames.\n            without file extension\n        \"\"\"\n        project_entry = (cls &amp; key).fetch1()\n        team_name = project_entry[\"team_name\"]\n        current_project_path = Path(project_entry[\"config_path\"]).parent\n        current_labeled_data_path = Path(\n            f\"{current_project_path.as_posix()}/labeled-data\"\n        )\n        if isinstance(import_project_path, PosixPath):\n            assert (\n                import_project_path.exists()\n            ), f\"import_project_path: {import_project_path.as_posix()} does not exist\"\n            import_labeled_data_path = Path(\n                f\"{import_project_path.as_posix()}/labeled-data\"\n            )\n        else:\n            assert Path(\n                import_project_path\n            ).exists(), (\n                f\"import_project_path: {import_project_path} does not exist\"\n            )\n            import_labeled_data_path = Path(\n                f\"{import_project_path}/labeled-data\"\n            )\n        assert (\n            import_labeled_data_path.exists()\n        ), \"import_project has no directory 'labeled-data'\"\n        if not isinstance(video_filenames, List):\n            video_filenames = [video_filenames]\n        for video_file in video_filenames:\n            h5_file = glob.glob(\n                f\"{import_labeled_data_path.as_posix()}/{video_file}/*.h5\"\n            )[0]\n            dlc_df = pd.read_hdf(h5_file)\n            dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n            dlc_df.to_hdf(\n                Path(\n                    f\"{current_labeled_data_path.as_posix()}/{video_file}/CollectedData_{team_name}.h5\"\n                ).as_posix(),\n                \"df_with_missing\",\n            )\n        cls.add_training_files(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.BodyPart", "title": "<code>BodyPart</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Part table to hold bodyparts used in each project.</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>class BodyPart(dj.Part):\n\"\"\"Part table to hold bodyparts used in each project.\"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCProject\n    -&gt; BodyPart\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.insert_existing_project", "title": "<code>insert_existing_project(project_name, lab_team, config_path, bodyparts=None, frames_per_video=None, add_to_files=True, **kwargs)</code>  <code>classmethod</code>", "text": "<p>insert an existing project into DLCProject table.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>user-friendly name of project</p> required <code>lab_team</code> <code>str</code> <p>name of lab team. Should match an entry in LabTeam table</p> required <code>config_path</code> <code>str</code> <p>path to project directory</p> required <code>bodyparts</code> <code>list</code> <p>optional list of bodyparts to label that are not already in existing config</p> <code>None</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef insert_existing_project(\n    cls,\n    project_name: str,\n    lab_team: str,\n    config_path: str,\n    bodyparts: List = None,\n    frames_per_video: int = None,\n    add_to_files: bool = True,\n    **kwargs,\n):\n\"\"\"\n    insert an existing project into DLCProject table.\n    Parameters\n    ----------\n    project_name : str\n        user-friendly name of project\n    lab_team : str\n        name of lab team. Should match an entry in LabTeam table\n    config_path : str\n        path to project directory\n    bodyparts : list\n        optional list of bodyparts to label that\n        are not already in existing config\n    \"\"\"\n\n    # Read config\n    project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n    if project_name in project_names_in_use:\n        print(f\"project name: {project_name} is already in use.\")\n        return_key = {}\n        return_key[\"project_name\"], return_key[\"config_path\"] = (\n            cls &amp; {\"project_name\": project_name}\n        ).fetch1(\"project_name\", \"config_path\")\n        return return_key\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    cfg = read_config(config_path)\n    if bodyparts:\n        bodyparts_to_add = [\n            bodypart\n            for bodypart in bodyparts\n            if bodypart not in cfg[\"bodyparts\"]\n        ]\n        all_bodyparts = bodyparts_to_add + cfg[\"bodyparts\"]\n    else:\n        all_bodyparts = cfg[\"bodyparts\"]\n    BodyPart.add_from_config(cfg[\"bodyparts\"])\n    for bodypart in all_bodyparts:\n        if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n            raise ValueError(\n                f\"bodypart: {bodypart} not found in BodyPart table\"\n            )\n    # check bodyparts are in config, if not add\n    if len(bodyparts_to_add) &gt; 0:\n        add_to_config(config_path, bodyparts=bodyparts_to_add)\n    # Get frames per video from config. If passed as arg, check match\n    if frames_per_video:\n        if frames_per_video != cfg[\"numframes2pick\"]:\n            add_to_config(\n                config_path, **{\"numframes2pick\": frames_per_video}\n            )\n    config_path = Path(config_path)\n    project_path = config_path.parent\n    dlc_project_path = os.environ[\"DLC_PROJECT_PATH\"]\n    if dlc_project_path not in project_path.as_posix():\n        project_dirname = project_path.name\n        dest_folder = Path(f\"{dlc_project_path}/{project_dirname}/\")\n        if dest_folder.exists():\n            new_proj_dir = dest_folder.as_posix()\n        else:\n            new_proj_dir = shutil.copytree(\n                src=project_path,\n                dst=f\"{dlc_project_path}/{project_dirname}/\",\n            )\n        new_config_path = Path(f\"{new_proj_dir}/config.yaml\")\n        assert (\n            new_config_path.exists()\n        ), \"config.yaml does not exist in new project directory\"\n        config_path = new_config_path\n        add_to_config(config_path, **{\"project_path\": new_proj_dir})\n    # TODO still need to copy videos over to video dir\n    key = {\n        \"project_name\": project_name,\n        \"team_name\": lab_team,\n        \"bodyparts\": bodyparts,\n        \"config_path\": config_path.as_posix(),\n        \"frames_per_video\": frames_per_video,\n    }\n    cls.insert1(key, **kwargs)\n    cls.BodyPart.insert(\n        [\n            {\"project_name\": project_name, \"bodypart\": bp}\n            for bp in all_bodyparts\n        ],\n        **kwargs,\n    )\n    if add_to_files:\n        del key[\"bodyparts\"]\n        del key[\"team_name\"]\n        del key[\"config_path\"]\n        del key[\"frames_per_video\"]\n        # Check for training files to add\n        cls.add_training_files(key, **kwargs)\n    return {\n        \"project_name\": project_name,\n        \"config_path\": config_path.as_posix(),\n    }\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.insert_new_project", "title": "<code>insert_new_project(project_name, bodyparts, lab_team, frames_per_video, video_list, project_directory=os.getenv('DLC_PROJECT_PATH'), output_path=os.getenv('DLC_VIDEO_PATH'), set_permissions=False, **kwargs)</code>  <code>classmethod</code>", "text": "<p>insert a new project into DLCProject table.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>user-friendly name of project</p> required <code>bodyparts</code> <code>list</code> <p>list of bodyparts to label. Should match bodyparts in BodyPart table</p> required <code>lab_team</code> <code>str</code> <p>name of lab team. Should match an entry in LabTeam table</p> required <code>project_directory</code> <code>str</code> <p>directory where to create project. (Default is '/cumulus/deeplabcut/')</p> <code>os.getenv('DLC_PROJECT_PATH')</code> <code>frames_per_video</code> <code>int</code> <p>number of frames to extract from each video</p> required <code>video_list</code> <code>list</code> <p>list of dicts of form [{'nwb_file_name': nwb_file_name, 'epoch': epoch #},...] to query VideoFile table for videos to train on. Can also be list of absolute paths to import videos from</p> required <code>output_path</code> <code>str</code> <p>target path to output converted videos (Default is '/nimbus/deeplabcut/videos/')</p> <code>os.getenv('DLC_VIDEO_PATH')</code> <code>set_permissions</code> <code>bool</code> <p>if True, will set permissions for user and group to be read+write (Default is False)</p> <code>False</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef insert_new_project(\n    cls,\n    project_name: str,\n    bodyparts: List,\n    lab_team: str,\n    frames_per_video: int,\n    video_list: List,\n    project_directory: str = os.getenv(\"DLC_PROJECT_PATH\"),\n    output_path: str = os.getenv(\"DLC_VIDEO_PATH\"),\n    set_permissions=False,\n    **kwargs,\n):\n\"\"\"\n    insert a new project into DLCProject table.\n    Parameters\n    ----------\n    project_name : str\n        user-friendly name of project\n    bodyparts : list\n        list of bodyparts to label. Should match bodyparts in BodyPart table\n    lab_team : str\n        name of lab team. Should match an entry in LabTeam table\n    project_directory : str\n        directory where to create project.\n        (Default is '/cumulus/deeplabcut/')\n    frames_per_video : int\n        number of frames to extract from each video\n    video_list : list\n        list of dicts of form [{'nwb_file_name': nwb_file_name, 'epoch': epoch #},...]\n        to query VideoFile table for videos to train on.\n        Can also be list of absolute paths to import videos from\n    output_path : str\n        target path to output converted videos\n        (Default is '/nimbus/deeplabcut/videos/')\n    set_permissions : bool\n        if True, will set permissions for user and group to be read+write\n        (Default is False)\n    \"\"\"\n    project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n    if project_name in project_names_in_use:\n        print(f\"project name: {project_name} is already in use.\")\n        return_key = {}\n        return_key[\"project_name\"], return_key[\"config_path\"] = (\n            cls &amp; {\"project_name\": project_name}\n        ).fetch1(\"project_name\", \"config_path\")\n        return return_key\n\n    add_to_files = kwargs.pop(\"add_to_files\", True)\n    if not bool(LabTeam() &amp; {\"team_name\": lab_team}):\n        raise ValueError(f\"team_name: {lab_team} does not exist in LabTeam\")\n    skeleton_node = None\n    # If dict, assume of form {'nwb_file_name': nwb_file_name, 'epoch': epoch}\n    # and pass to get_video_path to reference VideoFile table for path\n\n    if all(isinstance(n, Dict) for n in video_list):\n        videos_to_convert = [\n            get_video_path(video_key) for video_key in video_list\n        ]\n        videos = [\n            check_videofile(\n                video_path=video[0],\n                output_path=output_path,\n                video_filename=video[1],\n            )[0].as_posix()\n            for video in videos_to_convert\n        ]\n    # If not dict, assume list of video file paths that may or may not need to be converted\n    else:\n        videos = []\n        if not all([Path(video).exists() for video in video_list]):\n            raise OSError(\"at least one file in video_list does not exist\")\n        for video in video_list:\n            video_path = Path(video).parent\n            video_filename = video.rsplit(\n                video_path.as_posix(), maxsplit=1\n            )[-1].split(\"/\")[-1]\n            videos.extend(\n                [\n                    check_videofile(\n                        video_path=video_path,\n                        output_path=output_path,\n                        video_filename=video_filename,\n                    )[0].as_posix()\n                ]\n            )\n        if len(videos) &lt; 1:\n            raise ValueError(f\"no .mp4 videos found in{video_path}\")\n    from deeplabcut import create_new_project\n\n    config_path = create_new_project(\n        project_name,\n        lab_team,\n        videos,\n        working_directory=project_directory,\n        copy_videos=True,\n        multianimal=False,\n    )\n    for bodypart in bodyparts:\n        if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n            raise ValueError(\n                f\"bodypart: {bodypart} not found in BodyPart table\"\n            )\n    kwargs_copy = copy.deepcopy(kwargs)\n    kwargs_copy.update({\"numframes2pick\": frames_per_video, \"dotsize\": 3})\n    add_to_config(\n        config_path, bodyparts, skeleton_node=skeleton_node, **kwargs_copy\n    )\n    key = {\n        \"project_name\": project_name,\n        \"team_name\": lab_team,\n        \"bodyparts\": bodyparts,\n        \"config_path\": config_path,\n        \"frames_per_video\": frames_per_video,\n    }\n    # TODO: make permissions setting more flexible.\n    if set_permissions:\n        permissions = (\n            stat.S_IRUSR\n            | stat.S_IWUSR\n            | stat.S_IRGRP\n            | stat.S_IWGRP\n            | stat.S_IROTH\n        )\n        username = getpass.getuser()\n        if not groupname:\n            groupname = username\n        _set_permissions(\n            directory=project_directory,\n            mode=permissions,\n            username=username,\n            groupname=groupname,\n        )\n    cls.insert1(key, **kwargs)\n    cls.BodyPart.insert(\n        [\n            {\"project_name\": project_name, \"bodypart\": bp}\n            for bp in bodyparts\n        ],\n        **kwargs,\n    )\n    if add_to_files:\n        del key[\"bodyparts\"]\n        del key[\"team_name\"]\n        del key[\"config_path\"]\n        del key[\"frames_per_video\"]\n        # Add videos to training files\n        cls.add_training_files(key, **kwargs)\n    if isinstance(config_path, PosixPath):\n        config_path = config_path.as_posix()\n    return {\"project_name\": project_name, \"config_path\": config_path}\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.add_training_files", "title": "<code>add_training_files(key, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Add training videos and labeled frames .h5 and .csv to DLCProject.File</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef add_training_files(cls, key, **kwargs):\n\"\"\"Add training videos and labeled frames .h5 and .csv to DLCProject.File\"\"\"\n    config_path = (cls &amp; {\"project_name\": key[\"project_name\"]}).fetch1(\n        \"config_path\"\n    )\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    if \"config_path\" in key:\n        del key[\"config_path\"]\n    cfg = read_config(config_path)\n    video_names = list(cfg[\"video_sets\"].keys())\n    training_files = []\n    for video in video_names:\n        video_name = os.path.splitext(\n            video.split(os.path.dirname(video) + \"/\")[-1]\n        )[0]\n        training_files.extend(\n            glob.glob(\n                f\"{cfg['project_path']}/labeled-data/{video_name}/*Collected*\"\n            )\n        )\n    for video in video_names:\n        key[\"file_name\"] = f'{os.path.splitext(video.split(\"/\")[-1])[0]}'\n        key[\"file_ext\"] = os.path.splitext(video.split(\"/\")[-1])[-1].split(\n            \".\"\n        )[-1]\n        key[\"file_path\"] = video\n        cls.File.insert1(key, **kwargs)\n    if len(training_files) &gt; 0:\n        for file in training_files:\n            video_name = os.path.dirname(file).split(\"/\")[-1]\n            file_type = os.path.splitext(\n                file.split(os.path.dirname(file) + \"/\")[-1]\n            )[-1].split(\".\")[-1]\n            key[\"file_name\"] = f\"{video_name}_labeled_data\"\n            key[\"file_ext\"] = file_type\n            key[\"file_path\"] = file\n            cls.File.insert1(key, **kwargs)\n    else:\n        Warning(\"No training files to add\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.run_extract_frames", "title": "<code>run_extract_frames(key, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Convenience function to launch DLC GUI for extracting frames. Must be run on local machine to access GUI, cannot be run through ssh tunnel</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef run_extract_frames(cls, key, **kwargs):\n\"\"\"Convenience function to launch DLC GUI for extracting frames.\n    Must be run on local machine to access GUI,\n    cannot be run through ssh tunnel\n    \"\"\"\n    config_path = (cls &amp; key).fetch1(\"config_path\")\n    from deeplabcut import extract_frames\n\n    extract_frames(config_path, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.run_label_frames", "title": "<code>run_label_frames(key)</code>  <code>classmethod</code>", "text": "<p>Convenience function to launch DLC GUI for labeling frames. Must be run on local machine to access GUI, cannot be run through ssh tunnel</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef run_label_frames(cls, key):\n\"\"\"Convenience function to launch DLC GUI for labeling frames.\n    Must be run on local machine to access GUI,\n    cannot be run through ssh tunnel\n    \"\"\"\n    config_path = (cls &amp; key).fetch1(\"config_path\")\n    from deeplabcut import label_frames\n\n    label_frames(config_path)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.check_labels", "title": "<code>check_labels(key, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Convenience function to check labels on previously extracted and labeled frames</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef check_labels(cls, key, **kwargs):\n\"\"\"Convenience function to check labels on\n    previously extracted and labeled frames\n    \"\"\"\n    config_path = (cls &amp; key).fetch1(\"config_path\")\n    from deeplabcut import check_labels\n\n    check_labels(config_path, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.DLCProject.import_labeled_frames", "title": "<code>import_labeled_frames(key, import_project_path, video_filenames, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Function to import pre-labeled frames from an existing project into a new project</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Dict</code> <p>key to specify entry in DLCProject table to add labeled frames to</p> required <code>import_project_path</code> <code>str</code> <p>absolute path to project directory containing labeled frames to import</p> required <code>video_filenames</code> <code>str or List</code> <p>filename or list of filenames of video(s) from which to import frames. without file extension</p> required Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef import_labeled_frames(\n    cls,\n    key: Dict,\n    import_project_path: Union[str, PosixPath],\n    video_filenames: Union[str, List],\n    **kwargs,\n):\n\"\"\"Function to import pre-labeled frames from an existing project into a new project\n\n    Parameters\n    ----------\n    key : Dict\n        key to specify entry in DLCProject table to add labeled frames to\n    import_project_path : str\n        absolute path to project directory containing labeled frames to import\n    video_filenames : str or List\n        filename or list of filenames of video(s) from which to import frames.\n        without file extension\n    \"\"\"\n    project_entry = (cls &amp; key).fetch1()\n    team_name = project_entry[\"team_name\"]\n    current_project_path = Path(project_entry[\"config_path\"]).parent\n    current_labeled_data_path = Path(\n        f\"{current_project_path.as_posix()}/labeled-data\"\n    )\n    if isinstance(import_project_path, PosixPath):\n        assert (\n            import_project_path.exists()\n        ), f\"import_project_path: {import_project_path.as_posix()} does not exist\"\n        import_labeled_data_path = Path(\n            f\"{import_project_path.as_posix()}/labeled-data\"\n        )\n    else:\n        assert Path(\n            import_project_path\n        ).exists(), (\n            f\"import_project_path: {import_project_path} does not exist\"\n        )\n        import_labeled_data_path = Path(\n            f\"{import_project_path}/labeled-data\"\n        )\n    assert (\n        import_labeled_data_path.exists()\n    ), \"import_project has no directory 'labeled-data'\"\n    if not isinstance(video_filenames, List):\n        video_filenames = [video_filenames]\n    for video_file in video_filenames:\n        h5_file = glob.glob(\n            f\"{import_labeled_data_path.as_posix()}/{video_file}/*.h5\"\n        )[0]\n        dlc_df = pd.read_hdf(h5_file)\n        dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n        dlc_df.to_hdf(\n            Path(\n                f\"{current_labeled_data_path.as_posix()}/{video_file}/CollectedData_{team_name}.h5\"\n            ).as_posix(),\n            \"df_with_missing\",\n        )\n    cls.add_training_files(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.LabTeam", "title": "<code>LabTeam</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass LabTeam(dj.Manual):\n    definition = \"\"\"\n    team_name: varchar(80)\n    ---\n    team_description = \"\": varchar(2000)\n    \"\"\"\n\n    class LabTeamMember(dj.Part):\n        definition = \"\"\"\n        -&gt; LabTeam\n        -&gt; LabMember\n        \"\"\"\n\n    @classmethod\n    def create_new_team(\n        cls, team_name: str, team_members: list, team_description: str = \"\"\n    ):\n\"\"\"Create a new team with a list of team members.\n\n        If the lab member does not exist in the database, they will be added.\n\n        Parameters\n        ----------\n        team_name : str\n            The name of the team.\n        team_members : str\n            The full names of the lab members that are part of the team.\n        team_description: str\n            The description of the team.\n        \"\"\"\n        labteam_dict = dict()\n        labteam_dict[\"team_name\"] = team_name\n        labteam_dict[\"team_description\"] = team_description\n        cls.insert1(labteam_dict, skip_duplicates=True)\n\n        for team_member in team_members:\n            LabMember.insert_from_name(team_member)\n            query = (\n                LabMember.LabMemberInfo() &amp; {\"lab_member_name\": team_member}\n            ).fetch(\"google_user_name\")\n            if not len(query):\n                print(\n                    f\"Please add the Google user ID for {team_member} in the LabMember.LabMemberInfo table \"\n                    \"if you want to give them permission to manually curate sorting by this team.\"\n                )\n            labteammember_dict = dict()\n            labteammember_dict[\"team_name\"] = team_name\n            labteammember_dict[\"lab_member_name\"] = team_member\n            cls.LabTeamMember.insert1(labteammember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.common.common_lab.LabTeam.create_new_team", "title": "<code>create_new_team(team_name, team_members, team_description='')</code>  <code>classmethod</code>", "text": "<p>Create a new team with a list of team members.</p> <p>If the lab member does not exist in the database, they will be added.</p> <p>Parameters:</p> Name Type Description Default <code>team_name</code> <code>str</code> <p>The name of the team.</p> required <code>team_members</code> <code>str</code> <p>The full names of the lab members that are part of the team.</p> required <code>team_description</code> <code>str</code> <p>The description of the team.</p> <code>''</code> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef create_new_team(\n    cls, team_name: str, team_members: list, team_description: str = \"\"\n):\n\"\"\"Create a new team with a list of team members.\n\n    If the lab member does not exist in the database, they will be added.\n\n    Parameters\n    ----------\n    team_name : str\n        The name of the team.\n    team_members : str\n        The full names of the lab members that are part of the team.\n    team_description: str\n        The description of the team.\n    \"\"\"\n    labteam_dict = dict()\n    labteam_dict[\"team_name\"] = team_name\n    labteam_dict[\"team_description\"] = team_description\n    cls.insert1(labteam_dict, skip_duplicates=True)\n\n    for team_member in team_members:\n        LabMember.insert_from_name(team_member)\n        query = (\n            LabMember.LabMemberInfo() &amp; {\"lab_member_name\": team_member}\n        ).fetch(\"google_user_name\")\n        if not len(query):\n            print(\n                f\"Please add the Google user ID for {team_member} in the LabMember.LabMemberInfo table \"\n                \"if you want to give them permission to manually curate sorting by this team.\"\n            )\n        labteammember_dict = dict()\n        labteammember_dict[\"team_name\"] = team_name\n        labteammember_dict[\"lab_member_name\"] = team_member\n        cls.LabTeamMember.insert1(labteammember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.get_video_path", "title": "<code>get_video_path(key)</code>", "text": "<p>Given nwb_file_name and interval_list_name returns specified video file filename and path</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Dictionary containing nwb_file_name and interval_list_name as keys</p> required <p>Returns:</p> Name Type Description <code>video_filepath</code> <code>str</code> <p>path to the video file, including video filename</p> <code>video_filename</code> <code>str</code> <p>filename of the video</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_video_path(key):\n\"\"\"\n    Given nwb_file_name and interval_list_name returns specified\n    video file filename and path\n\n    Parameters\n    ----------\n    key : dict\n        Dictionary containing nwb_file_name and interval_list_name as keys\n\n    Returns\n    -------\n    video_filepath : str\n        path to the video file, including video filename\n    video_filename : str\n        filename of the video\n    \"\"\"\n    import pynwb\n\n    from ...common.common_behav import VideoFile\n\n    video_info = (\n        VideoFile()\n        &amp; {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": key[\"epoch\"]}\n    ).fetch1()\n    nwb_path = (\n        f\"{os.getenv('SPYGLASS_BASE_DIR')}/raw/{video_info['nwb_file_name']}\"\n    )\n    with pynwb.NWBHDF5IO(path=nwb_path, mode=\"r\") as in_out:\n        nwb_file = in_out.read()\n        nwb_video = nwb_file.objects[video_info[\"video_file_object_id\"]]\n        video_filepath = VideoFile.get_abs_path(\n            {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": key[\"epoch\"]}\n        )\n        video_dir = os.path.dirname(video_filepath) + \"/\"\n        video_filename = video_filepath.split(video_dir)[-1]\n        meters_per_pixel = nwb_video.device.meters_per_pixel\n        timestamps = np.asarray(nwb_video.timestamps)\n    return video_dir, video_filename, meters_per_pixel, timestamps\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.check_videofile", "title": "<code>check_videofile(video_path, output_path=os.getenv('DLC_VIDEO_PATH'), video_filename=None, video_filetype='h264')</code>", "text": "<p>Checks the file extension of a video file to make sure it is .mp4 for DeepLabCut processes. Converts to MP4 if not already.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str or PosixPath object</code> <p>path to directory of the existing video file without filename</p> required <code>output_path</code> <code>str or PosixPath object</code> <p>path to directory where converted video will be saved</p> <code>os.getenv('DLC_VIDEO_PATH')</code> <code>video_filename</code> <code>str, Optional</code> <p>filename of the video to convert, if not provided, video_filetype must be and all video files of video_filetype in the directory will be converted</p> <code>None</code> <code>video_filetype</code> <code>str or List, Default 'h264', Optional</code> <p>If video_filename is not provided, all videos of this filetype will be converted to .mp4</p> <code>'h264'</code> <p>Returns:</p> Name Type Description <code>output_files</code> <code>List of PosixPath objects</code> <p>paths to converted video file(s)</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def check_videofile(\n    video_path: Union[str, pathlib.PosixPath],\n    output_path: Union[str, pathlib.PosixPath] = os.getenv(\"DLC_VIDEO_PATH\"),\n    video_filename: str = None,\n    video_filetype: str = \"h264\",\n):\n\"\"\"\n    Checks the file extension of a video file to make sure it is .mp4 for\n    DeepLabCut processes. Converts to MP4 if not already.\n\n    Parameters\n    ----------\n    video_path : str or PosixPath object\n        path to directory of the existing video file without filename\n    output_path : str or PosixPath object\n        path to directory where converted video will be saved\n    video_filename : str, Optional\n        filename of the video to convert, if not provided, video_filetype must be\n        and all video files of video_filetype in the directory will be converted\n    video_filetype : str or List, Default 'h264', Optional\n        If video_filename is not provided,\n        all videos of this filetype will be converted to .mp4\n\n    Returns\n    -------\n    output_files : List of PosixPath objects\n        paths to converted video file(s)\n    \"\"\"\n\n    if not video_filename:\n        video_files = pathlib.Path(video_path).glob(f\"*.{video_filetype}\")\n    else:\n        video_files = [pathlib.Path(f\"{video_path}/{video_filename}\")]\n    output_files = []\n    for video_filepath in video_files:\n        if video_filepath.exists():\n            if video_filepath.suffix == \".mp4\":\n                output_files.append(video_filepath)\n                continue\n        video_file = (\n            video_filepath.as_posix()\n            .rsplit(video_filepath.parent.as_posix(), maxsplit=1)[-1]\n            .split(\"/\")[-1]\n        )\n        output_files.append(\n            _convert_mp4(video_file, video_path, output_path, videotype=\"mp4\")\n        )\n    return output_files\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_project/#src.spyglass.position.v1.position_dlc_project.add_to_config", "title": "<code>add_to_config(config, bodyparts=None, skeleton_node=None, **kwargs)</code>", "text": "<p>Add necessary items to the config.yaml for the model</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>Path to config.yaml</p> required <code>bodyparts</code> <code>list</code> <p>list of bodyparts to add to model</p> <code>None</code> <code>skeleton_node</code> <code>str</code> <p>(default is None) node to link LEDs in skeleton</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Other parameters of config to modify in key:value pairs</p> <code>{}</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>def add_to_config(\n    config, bodyparts: List = None, skeleton_node: str = None, **kwargs\n):\n\"\"\"\n    Add necessary items to the config.yaml for the model\n    Parameters\n    ----------\n    config : str\n        Path to config.yaml\n    bodyparts : list\n        list of bodyparts to add to model\n    skeleton_node : str\n        (default is None) node to link LEDs in skeleton\n    kwargs : dict\n        Other parameters of config to modify in key:value pairs\n    \"\"\"\n\n    yaml = ruamel.yaml.YAML()\n    with open(config) as fp:\n        data = yaml.load(fp)\n    if bodyparts:\n        data[\"bodyparts\"] = bodyparts\n        led_parts = [element for element in bodyparts if \"LED\" in element]\n        if skeleton_node is not None:\n            bodypart_skeleton = [\n                list(link)\n                for link in combinations(led_parts, 2)\n                if skeleton_node in link\n            ]\n        else:\n            bodypart_skeleton = list(combinations(led_parts, 2))\n        other_parts = list(set(bodyparts) - set(led_parts))\n        for ind, part in enumerate(other_parts):\n            other_parts[ind] = [part, part]\n        bodypart_skeleton.append(other_parts)\n        data[\"skeleton\"] = bodypart_skeleton\n    for kwarg, val in kwargs.items():\n        if not isinstance(kwarg, str):\n            kwarg = str(kwarg)\n        data[kwarg] = val\n    with open(config, \"w\") as fw:\n        yaml.dump(data, fw)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/", "title": "position_dlc_selection.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.DLCSmoothInterpParams", "title": "<code>DLCSmoothInterpParams</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Parameters for extracting the smoothed head position.</p> <p>Attributes:</p> Name Type Description <code>interpolate</code> <code>bool, default True</code> <p>whether to interpolate over NaN spans</p> <code>smooth</code> <code>bool, default True</code> <p>whether to smooth the dataset</p> <code>smoothing_params</code> <code>dict</code> <p>smoothing_duration : float, default 0.05 number of frames to smooth over: sampling_rate*smoothing_duration = num_frames</p> <code>interp_params</code> <code>dict</code> <p>max_cm_to_interp : int, default 20 maximum distance between high likelihood points on either side of a NaN span to interpolate over</p> <code>likelihood_thresh</code> <code>float, default 0.95</code> <p>likelihood below which to NaN and interpolate over</p> Source code in <code>src/spyglass/position/v1/position_dlc_position.py</code> <pre><code>@schema\nclass DLCSmoothInterpParams(dj.Manual):\n\"\"\"\n    Parameters for extracting the smoothed head position.\n\n    Attributes\n    ----------\n    interpolate : bool, default True\n        whether to interpolate over NaN spans\n    smooth : bool, default True\n        whether to smooth the dataset\n    smoothing_params : dict\n        smoothing_duration : float, default 0.05\n            number of frames to smooth over: sampling_rate*smoothing_duration = num_frames\n    interp_params : dict\n        max_cm_to_interp : int, default 20\n            maximum distance between high likelihood points on either side of a NaN span\n            to interpolate over\n    likelihood_thresh : float, default 0.95\n        likelihood below which to NaN and interpolate over\n    \"\"\"\n\n    definition = \"\"\"\n    dlc_si_params_name : varchar(80) # name for this set of parameters\n    ---\n    params: longblob # dictionary of parameters\n    \"\"\"\n\n    @classmethod\n    def insert_params(cls, params_name: str, params: dict, **kwargs):\n        cls.insert1(\n            {\"dlc_si_params_name\": params_name, \"params\": params},\n            **kwargs,\n        )\n\n    @classmethod\n    def insert_default(cls, **kwargs):\n        default_params = {\n            \"smooth\": True,\n            \"smoothing_params\": {\n                \"smoothing_duration\": 0.05,\n                \"smooth_method\": \"moving_avg\",\n            },\n            \"interpolate\": True,\n            \"likelihood_thresh\": 0.95,\n            \"interp_params\": {\"max_cm_to_interp\": 15},\n            \"max_cm_between_pts\": 20,\n            # This is for use when finding \"good spans\" and is how many indices to bridge in between good spans\n            # see inds_to_span in get_good_spans\n            \"num_inds_to_span\": 20,\n        }\n        cls.insert1(\n            {\"dlc_si_params_name\": \"default\", \"params\": default_params},\n            **kwargs,\n        )\n\n    @classmethod\n    def insert_nan_params(cls, **kwargs):\n        nan_params = {\n            \"smooth\": False,\n            \"interpolate\": False,\n            \"likelihood_thresh\": 0.95,\n            \"max_cm_between_pts\": 20,\n            \"num_inds_to_span\": 20,\n        }\n        cls.insert1(\n            {\"dlc_si_params_name\": \"just_nan\", \"params\": nan_params}, **kwargs\n        )\n\n    @classmethod\n    def get_default(cls):\n        query = cls &amp; {\"dlc_si_params_name\": \"default\"}\n        if not len(query) &gt; 0:\n            cls().insert_default(skip_duplicates=True)\n            default = (cls &amp; {\"dlc_si_params_name\": \"default\"}).fetch1()\n        else:\n            default = query.fetch1()\n        return default\n\n    @classmethod\n    def get_nan_params(cls):\n        query = cls &amp; {\"dlc_si_params_name\": \"just_nan\"}\n        if not len(query) &gt; 0:\n            cls().insert_nan_params(skip_duplicates=True)\n            nan_params = (cls &amp; {\"dlc_si_params_name\": \"just_nan\"}).fetch1()\n        else:\n            nan_params = query.fetch1()\n        return nan_params\n\n    @staticmethod\n    def get_available_methods():\n        return _key_to_smooth_func_dict.keys()\n\n    def insert1(self, key, **kwargs):\n        if \"params\" in key:\n            if not \"max_cm_between_pts\" in key[\"params\"]:\n                raise KeyError(\"max_cm_between_pts is a required parameter\")\n            if \"smooth\" in key[\"params\"]:\n                if key[\"params\"][\"smooth\"]:\n                    if \"smoothing_params\" in key[\"params\"]:\n                        if \"smooth_method\" in key[\"params\"][\"smoothing_params\"]:\n                            smooth_method = key[\"params\"][\"smoothing_params\"][\n                                \"smooth_method\"\n                            ]\n                            if smooth_method not in _key_to_smooth_func_dict:\n                                raise KeyError(\n                                    f\"smooth_method: {smooth_method} not an available method.\"\n                                )\n                        if (\n                            not \"smoothing_duration\"\n                            in key[\"params\"][\"smoothing_params\"]\n                        ):\n                            raise KeyError(\n                                \"smoothing_duration must be passed as a smoothing_params within key['params']\"\n                            )\n                        else:\n                            assert isinstance(\n                                key[\"params\"][\"smoothing_params\"][\n                                    \"smoothing_duration\"\n                                ],\n                                (float, int),\n                            ), \"smoothing_duration must be a float or int\"\n                    else:\n                        raise ValueError(\n                            \"smoothing_params not in key['params']\"\n                        )\n            if \"likelihood_thresh\" in key[\"params\"]:\n                assert isinstance(\n                    key[\"params\"][\"likelihood_thresh\"],\n                    float,\n                ), \"likelihood_thresh must be a float\"\n                assert (\n                    0 &lt; key[\"params\"][\"likelihood_thresh\"] &lt; 1\n                ), \"likelihood_thresh must be between 0 and 1\"\n            else:\n                raise ValueError(\n                    \"likelihood_thresh must be passed within key['params']\"\n                )\n        else:\n            raise KeyError(\"'params' must be in key\")\n        super().insert1(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.DLCPoseEstimationSelection", "title": "<code>DLCPoseEstimationSelection</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@schema\nclass DLCPoseEstimationSelection(dj.Manual):\n    definition = \"\"\"\n    -&gt; VideoFile                           # Session -&gt; Recording + File part table\n    -&gt; DLCModel                                    # Must specify a DLC project_path\n    ---\n    task_mode='load' : enum('load', 'trigger')  # load results or trigger computation\n    video_path : varchar(120)                   # path to video file\n    pose_estimation_output_dir='': varchar(255) # output dir relative to the root dir\n    pose_estimation_params=null  : longblob     # analyze_videos params, if not default\n    \"\"\"\n\n    @classmethod\n    def get_video_crop(cls, video_path):\n\"\"\"\n        Queries the user to determine the cropping parameters for a given video\n\n        Parameters\n        ----------\n        video_path : str\n            path to the video file\n\n        Returns\n        -------\n        crop_ints : list\n            list of 4 integers [x min, x max, y min, y max]\n        \"\"\"\n\n        cap = cv2.VideoCapture(video_path)\n        _, frame = cap.read()\n        fig, ax = plt.subplots(figsize=(20, 10))\n        ax.imshow(frame)\n        xlims = ax.get_xlim()\n        ylims = ax.get_ylim()\n        ax.set_xticks(np.arange(xlims[0], xlims[-1], 50))\n        ax.set_yticks(np.arange(ylims[0], ylims[-1], -50))\n        ax.grid(visible=True, color=\"white\", lw=0.5, alpha=0.5)\n        display(fig)\n        crop_input = input(\n            \"Please enter the crop parameters for your video in format xmin, xmax, ymin, ymax, or 'none'\\n\"\n        )\n        plt.close()\n        if crop_input.lower() == \"none\":\n            return None\n        crop_ints = [int(val) for val in crop_input.split(\",\")]\n        assert all(isinstance(val, int) for val in crop_ints)\n        return crop_ints\n\n    @classmethod\n    def insert_estimation_task(\n        cls,\n        key,\n        task_mode=\"trigger\",\n        params: dict = None,\n        check_crop=None,\n        skip_duplicates=True,\n    ):\n\"\"\"\n        Insert PoseEstimationTask in inferred output dir.\n        From Datajoint Elements\n\n        Parameters\n        ----------\n        key: DataJoint key specifying a pairing of VideoRecording and Model.\n        task_mode (bool): Default 'trigger' computation. Or 'load' existing results.\n        params (dict): Optional. Parameters passed to DLC's analyze_videos:\n            videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference,\n            dynamic, robust_nframes, allow_growth, use_shelve\n        \"\"\"\n        from .dlc_utils import check_videofile, get_video_path\n\n        video_path, video_filename, _, _ = get_video_path(key)\n        output_dir = infer_output_dir(key)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n        ) as logger:\n            logger.logger.info(\"Pose Estimation Selection\")\n            video_dir = os.path.dirname(video_path) + \"/\"\n            logger.logger.info(\"video_dir: %s\", video_dir)\n            video_path = check_videofile(\n                video_path=video_dir, video_filename=video_filename\n            )[0]\n            if check_crop is not None:\n                params[\"cropping\"] = cls.get_video_crop(\n                    video_path=video_path.as_posix()\n                )\n            cls.insert1(\n                {\n                    **key,\n                    \"task_mode\": task_mode,\n                    \"pose_estimation_params\": params,\n                    \"video_path\": video_path,\n                    \"pose_estimation_output_dir\": output_dir,\n                },\n                skip_duplicates=skip_duplicates,\n            )\n        logger.logger.info(\"inserted entry into Pose Estimation Selection\")\n        return {**key, \"task_mode\": task_mode}\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimationSelection.get_video_crop", "title": "<code>get_video_crop(video_path)</code>  <code>classmethod</code>", "text": "<p>Queries the user to determine the cropping parameters for a given video</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>path to the video file</p> required <p>Returns:</p> Name Type Description <code>crop_ints</code> <code>list</code> <p>list of 4 integers [x min, x max, y min, y max]</p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@classmethod\ndef get_video_crop(cls, video_path):\n\"\"\"\n    Queries the user to determine the cropping parameters for a given video\n\n    Parameters\n    ----------\n    video_path : str\n        path to the video file\n\n    Returns\n    -------\n    crop_ints : list\n        list of 4 integers [x min, x max, y min, y max]\n    \"\"\"\n\n    cap = cv2.VideoCapture(video_path)\n    _, frame = cap.read()\n    fig, ax = plt.subplots(figsize=(20, 10))\n    ax.imshow(frame)\n    xlims = ax.get_xlim()\n    ylims = ax.get_ylim()\n    ax.set_xticks(np.arange(xlims[0], xlims[-1], 50))\n    ax.set_yticks(np.arange(ylims[0], ylims[-1], -50))\n    ax.grid(visible=True, color=\"white\", lw=0.5, alpha=0.5)\n    display(fig)\n    crop_input = input(\n        \"Please enter the crop parameters for your video in format xmin, xmax, ymin, ymax, or 'none'\\n\"\n    )\n    plt.close()\n    if crop_input.lower() == \"none\":\n        return None\n    crop_ints = [int(val) for val in crop_input.split(\",\")]\n    assert all(isinstance(val, int) for val in crop_ints)\n    return crop_ints\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimationSelection.insert_estimation_task", "title": "<code>insert_estimation_task(key, task_mode='trigger', params=None, check_crop=None, skip_duplicates=True)</code>  <code>classmethod</code>", "text": "<p>Insert PoseEstimationTask in inferred output dir. From Datajoint Elements</p> <p>Parameters:</p> Name Type Description Default <code>key</code> required <code>task_mode</code> <code>'trigger'</code> <code>params</code> <code>dict</code> <p>videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference, dynamic, robust_nframes, allow_growth, use_shelve</p> <code>None</code> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@classmethod\ndef insert_estimation_task(\n    cls,\n    key,\n    task_mode=\"trigger\",\n    params: dict = None,\n    check_crop=None,\n    skip_duplicates=True,\n):\n\"\"\"\n    Insert PoseEstimationTask in inferred output dir.\n    From Datajoint Elements\n\n    Parameters\n    ----------\n    key: DataJoint key specifying a pairing of VideoRecording and Model.\n    task_mode (bool): Default 'trigger' computation. Or 'load' existing results.\n    params (dict): Optional. Parameters passed to DLC's analyze_videos:\n        videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference,\n        dynamic, robust_nframes, allow_growth, use_shelve\n    \"\"\"\n    from .dlc_utils import check_videofile, get_video_path\n\n    video_path, video_filename, _, _ = get_video_path(key)\n    output_dir = infer_output_dir(key)\n    with OutputLogger(\n        name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n        path=f\"{output_dir.as_posix()}/log.log\",\n    ) as logger:\n        logger.logger.info(\"Pose Estimation Selection\")\n        video_dir = os.path.dirname(video_path) + \"/\"\n        logger.logger.info(\"video_dir: %s\", video_dir)\n        video_path = check_videofile(\n            video_path=video_dir, video_filename=video_filename\n        )[0]\n        if check_crop is not None:\n            params[\"cropping\"] = cls.get_video_crop(\n                video_path=video_path.as_posix()\n            )\n        cls.insert1(\n            {\n                **key,\n                \"task_mode\": task_mode,\n                \"pose_estimation_params\": params,\n                \"video_path\": video_path,\n                \"pose_estimation_output_dir\": output_dir,\n            },\n            skip_duplicates=skip_duplicates,\n        )\n    logger.logger.info(\"inserted entry into Pose Estimation Selection\")\n    return {**key, \"task_mode\": task_mode}\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.DLCPosSelection", "title": "<code>DLCPosSelection</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Specify collection of upstream DLCCentroid and DLCOrientation entries to combine into a set of position information</p> Source code in <code>src/spyglass/position/v1/position_dlc_selection.py</code> <pre><code>@schema\nclass DLCPosSelection(dj.Manual):\n\"\"\"\n    Specify collection of upstream DLCCentroid and DLCOrientation entries\n    to combine into a set of position information\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCCentroid.proj(dlc_si_cohort_centroid='dlc_si_cohort_selection_name', centroid_analysis_file_name='analysis_file_name')\n    -&gt; DLCOrientation.proj(dlc_si_cohort_orientation='dlc_si_cohort_selection_name', orientation_analysis_file_name='analysis_file_name')\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.DLCSmoothInterpCohort", "title": "<code>DLCSmoothInterpCohort</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Table to combine multiple bodyparts from DLCSmoothInterp to enable centroid/orientation calculations</p> Source code in <code>src/spyglass/position/v1/position_dlc_cohort.py</code> <pre><code>@schema\nclass DLCSmoothInterpCohort(dj.Computed):\n\"\"\"\n    Table to combine multiple bodyparts from DLCSmoothInterp\n    to enable centroid/orientation calculations\n    \"\"\"\n\n    # Need to ensure that nwb_file_name/epoch/interval list name endure as primary keys\n    definition = \"\"\"\n    -&gt; DLCSmoothInterpCohortSelection\n    ---\n    \"\"\"\n\n    class BodyPart(dj.Part):\n        definition = \"\"\"\n        -&gt; DLCSmoothInterpCohort\n        -&gt; DLCSmoothInterp\n        ---\n        -&gt; AnalysisNwbfile\n        dlc_smooth_interp_position_object_id : varchar(80)\n        dlc_smooth_interp_info_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n        def fetch1_dataframe(self):\n            nwb_data = self.fetch_nwb()[0]\n            index = pd.Index(\n                np.asarray(\n                    nwb_data[\"dlc_smooth_interp_position\"]\n                    .get_spatial_series()\n                    .timestamps\n                ),\n                name=\"time\",\n            )\n            COLUMNS = [\n                \"video_frame_ind\",\n                \"x\",\n                \"y\",\n            ]\n            return pd.DataFrame(\n                np.concatenate(\n                    (\n                        np.asarray(\n                            nwb_data[\"dlc_smooth_interp_info\"]\n                            .time_series[\"video_frame_ind\"]\n                            .data,\n                            dtype=int,\n                        )[:, np.newaxis],\n                        np.asarray(\n                            nwb_data[\"dlc_smooth_interp_position\"]\n                            .get_spatial_series()\n                            .data\n                        ),\n                    ),\n                    axis=1,\n                ),\n                columns=COLUMNS,\n                index=index,\n            )\n\n    def make(self, key):\n        from .dlc_utils import OutputLogger, infer_output_dir\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n            print_console=False,\n        ) as logger:\n            logger.logger.info(\"-----------------------\")\n            logger.logger.info(\"Bodypart Cohort\")\n            # from Jen Guidera\n            self.insert1(key)\n            cohort_selection = (DLCSmoothInterpCohortSelection &amp; key).fetch1()\n            table_entries = []\n            bodyparts_params_dict = cohort_selection.pop(\n                \"bodyparts_params_dict\"\n            )\n            temp_key = cohort_selection.copy()\n            for bodypart, params in bodyparts_params_dict.items():\n                temp_key[\"bodypart\"] = bodypart\n                temp_key[\"dlc_si_params_name\"] = params\n                table_entries.append((DLCSmoothInterp &amp; temp_key).fetch())\n            assert len(table_entries) == len(\n                bodyparts_params_dict\n            ), \"more entries found in DLCSmoothInterp than specified in bodyparts_params_dict\"\n            table_column_names = list(table_entries[0].dtype.fields.keys())\n            for table_entry in table_entries:\n                entry_key = {\n                    **{\n                        k: v for k, v in zip(table_column_names, table_entry[0])\n                    },\n                    **key,\n                }\n                DLCSmoothInterpCohort.BodyPart.insert1(\n                    entry_key, skip_duplicates=True\n                )\n        logger.logger.info(\"Inserted entry into DLCSmoothInterpCohort\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.DLCPosV1", "title": "<code>DLCPosV1</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Combines upstream DLCCentroid and DLCOrientation entries into a single entry with a single Analysis NWB file</p> Source code in <code>src/spyglass/position/v1/position_dlc_selection.py</code> <pre><code>@schema\nclass DLCPosV1(dj.Computed):\n\"\"\"\n    Combines upstream DLCCentroid and DLCOrientation\n    entries into a single entry with a single Analysis NWB file\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCPosSelection\n    ---\n    -&gt; AnalysisNwbfile\n    position_object_id      : varchar(80)\n    orientation_object_id   : varchar(80)\n    velocity_object_id      : varchar(80)\n    pose_eval_result        : longblob\n    \"\"\"\n\n    def make(self, key):\n        key[\"pose_eval_result\"] = self.evaluate_pose_estimation(key)\n        position_nwb_data = (DLCCentroid &amp; key).fetch_nwb()[0]\n        orientation_nwb_data = (DLCOrientation &amp; key).fetch_nwb()[0]\n        position_object = position_nwb_data[\"dlc_position\"].spatial_series[\n            \"position\"\n        ]\n        velocity_object = position_nwb_data[\"dlc_velocity\"].time_series[\n            \"velocity\"\n        ]\n        video_frame_object = position_nwb_data[\"dlc_velocity\"].time_series[\n            \"video_frame_ind\"\n        ]\n        orientation_object = orientation_nwb_data[\n            \"dlc_orientation\"\n        ].spatial_series[\"orientation\"]\n        position = pynwb.behavior.Position()\n        orientation = pynwb.behavior.CompassDirection()\n        velocity = pynwb.behavior.BehavioralTimeSeries()\n        position.create_spatial_series(\n            name=position_object.name,\n            timestamps=np.asarray(position_object.timestamps),\n            conversion=position_object.conversion,\n            data=np.asarray(position_object.data),\n            reference_frame=position_object.reference_frame,\n            comments=position_object.comments,\n            description=position_object.description,\n        )\n        orientation.create_spatial_series(\n            name=orientation_object.name,\n            timestamps=np.asarray(orientation_object.timestamps),\n            conversion=orientation_object.conversion,\n            data=np.asarray(orientation_object.data),\n            reference_frame=orientation_object.reference_frame,\n            comments=orientation_object.comments,\n            description=orientation_object.description,\n        )\n        velocity.create_timeseries(\n            name=velocity_object.name,\n            timestamps=np.asarray(velocity_object.timestamps),\n            conversion=velocity_object.conversion,\n            unit=velocity_object.unit,\n            data=np.asarray(velocity_object.data),\n            comments=velocity_object.comments,\n            description=velocity_object.description,\n        )\n        velocity.create_timeseries(\n            name=video_frame_object.name,\n            unit=video_frame_object.unit,\n            timestamps=np.asarray(video_frame_object.timestamps),\n            data=np.asarray(video_frame_object.data),\n            description=video_frame_object.description,\n            comments=video_frame_object.comments,\n        )\n        # Add to Analysis NWB file\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"orientation_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], orientation\n        )\n        key[\"position_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], position\n        )\n        key[\"velocity_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], velocity\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n        from ..position_merge import PositionOutput\n\n        key[\"source\"] = \"DLC\"\n        key[\"version\"] = 1\n        dlc_key = key.copy()\n        del dlc_key[\"pose_eval_result\"]\n        key[\"interval_list_name\"] = f\"pos {key['epoch']-1} valid times\"\n        valid_fields = PositionOutput().fetch().dtype.fields.keys()\n        entries_to_delete = [\n            entry for entry in key.keys() if entry not in valid_fields\n        ]\n        for entry in entries_to_delete:\n            del key[entry]\n\n        PositionOutput().insert1(key=key, params=dlc_key, skip_duplicates=True)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(nwb_data[\"position\"].get_spatial_series().timestamps),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"position_x\",\n            \"position_y\",\n            \"orientation\",\n            \"velocity_x\",\n            \"velocity_y\",\n            \"speed\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"velocity\"]\n                        .time_series[\"video_frame_ind\"]\n                        .data,\n                        dtype=int,\n                    )[:, np.newaxis],\n                    np.asarray(nwb_data[\"position\"].get_spatial_series().data),\n                    np.asarray(\n                        nwb_data[\"orientation\"].get_spatial_series().data\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"velocity\"].time_series[\"velocity\"].data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n\n    @classmethod\n    def evaluate_pose_estimation(cls, key):\n        likelihood_thresh = []\n        valid_fields = (\n            DLCSmoothInterpCohort.BodyPart().fetch().dtype.fields.keys()\n        )\n        centroid_key = {k: val for k, val in key.items() if k in valid_fields}\n        centroid_key[\"dlc_si_cohort_selection_name\"] = key[\n            \"dlc_si_cohort_centroid\"\n        ]\n        orientation_key = centroid_key.copy()\n        orientation_key[\"dlc_si_cohort_selection_name\"] = key[\n            \"dlc_si_cohort_orientation\"\n        ]\n        centroid_bodyparts, centroid_si_params = (\n            DLCSmoothInterpCohort.BodyPart &amp; centroid_key\n        ).fetch(\"bodypart\", \"dlc_si_params_name\")\n        orientation_bodyparts, orientation_si_params = (\n            DLCSmoothInterpCohort.BodyPart &amp; orientation_key\n        ).fetch(\"bodypart\", \"dlc_si_params_name\")\n        for param in np.unique(\n            np.concatenate((centroid_si_params, orientation_si_params))\n        ):\n            likelihood_thresh.append(\n                (\n                    DLCSmoothInterpParams() &amp; {\"dlc_si_params_name\": param}\n                ).fetch1(\"params\")[\"likelihood_thresh\"]\n            )\n\n        if len(np.unique(likelihood_thresh)) &gt; 1:\n            raise ValueError(\"more than one likelihood threshold used\")\n        like_thresh = likelihood_thresh[0]\n        bodyparts = np.unique([*centroid_bodyparts, *orientation_bodyparts])\n        fields = list(DLCPoseEstimation.BodyPart.fetch().dtype.fields.keys())\n        pose_estimation_key = {k: v for k, v in key.items() if k in fields}\n        pose_estimation_df = pd.concat(\n            {\n                bodypart: (\n                    DLCPoseEstimation.BodyPart()\n                    &amp; {**pose_estimation_key, **{\"bodypart\": bodypart}}\n                ).fetch1_dataframe()\n                for bodypart in bodyparts.tolist()\n            },\n            axis=1,\n        )\n        df_filter = {\n            bodypart: pose_estimation_df[bodypart][\"likelihood\"] &lt; like_thresh\n            for bodypart in bodyparts\n            if bodypart in pose_estimation_df.columns\n        }\n        sub_thresh_ind_dict = {\n            bodypart: {\n                \"inds\": np.where(\n                    ~np.isnan(\n                        pose_estimation_df[bodypart][\"likelihood\"].where(\n                            df_filter[bodypart]\n                        )\n                    )\n                )[0],\n            }\n            for bodypart in bodyparts\n        }\n        sub_thresh_percent_dict = {\n            bodypart: (\n                len(\n                    np.where(\n                        ~np.isnan(\n                            pose_estimation_df[bodypart][\"likelihood\"].where(\n                                df_filter[bodypart]\n                            )\n                        )\n                    )[0]\n                )\n                / len(pose_estimation_df)\n            )\n            * 100\n            for bodypart in bodyparts\n        }\n        return sub_thresh_percent_dict\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.DLCOrientation", "title": "<code>DLCOrientation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Determines and smooths orientation of a set of bodyparts given a specified method</p> Source code in <code>src/spyglass/position/v1/position_dlc_orient.py</code> <pre><code>@schema\nclass DLCOrientation(dj.Computed):\n\"\"\"\n    Determines and smooths orientation of a set of bodyparts given a specified method\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCOrientationSelection\n    ---\n    -&gt; AnalysisNwbfile\n    dlc_orientation_object_id : varchar(80)\n    \"\"\"\n\n    def make(self, key):\n        # Get labels to smooth from Parameters table\n        cohort_entries = DLCSmoothInterpCohort.BodyPart &amp; key\n        pos_df = pd.concat(\n            {\n                bodypart: (\n                    DLCSmoothInterpCohort.BodyPart\n                    &amp; {**key, **{\"bodypart\": bodypart}}\n                ).fetch1_dataframe()\n                for bodypart in cohort_entries.fetch(\"bodypart\")\n            },\n            axis=1,\n        )\n        params = (DLCOrientationParams() &amp; key).fetch1(\"params\")\n        orientation_smoothing_std_dev = params.pop(\n            \"orientation_smoothing_std_dev\", None\n        )\n        dt = np.median(np.diff(pos_df.index.to_numpy()))\n        sampling_rate = 1 / dt\n        orient_func = _key_to_func_dict[params[\"orient_method\"]]\n        orientation = orient_func(pos_df, **params)\n        if not params[\"orient_method\"] == \"none\":\n            # Smooth orientation\n            is_nan = np.isnan(orientation)\n            unwrap_orientation = orientation.copy()\n            # Only unwrap non nan values, while keeping nans in dataset for interpolation\n            unwrap_orientation[~is_nan] = np.unwrap(orientation[~is_nan])\n            unwrap_df = pd.DataFrame(\n                unwrap_orientation, columns=[\"orientation\"], index=pos_df.index\n            )\n            nan_spans = get_span_start_stop(np.where(is_nan)[0])\n            orient_df = interp_orientation(\n                unwrap_df,\n                nan_spans,\n            )\n            orientation = gaussian_smooth(\n                orient_df[\"orientation\"].to_numpy(),\n                orientation_smoothing_std_dev,\n                sampling_rate,\n                axis=0,\n                truncate=8,\n            )\n            # convert back to between -pi and pi\n            orientation = np.angle(np.exp(1j * orientation))\n        final_df = pd.DataFrame(\n            orientation, columns=[\"orientation\"], index=pos_df.index\n        )\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        spatial_series = (RawPosition() &amp; key).fetch_nwb()[0][\"raw_position\"]\n        orientation = pynwb.behavior.CompassDirection()\n        orientation.create_spatial_series(\n            name=\"orientation\",\n            timestamps=final_df.index.to_numpy(),\n            conversion=1.0,\n            data=final_df[\"orientation\"].to_numpy(),\n            reference_frame=spatial_series.reference_frame,\n            comments=spatial_series.comments,\n            description=\"orientation\",\n        )\n        nwb_analysis_file = AnalysisNwbfile()\n        key[\"dlc_orientation_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], orientation\n        )\n\n        nwb_analysis_file.add(\n            nwb_file_name=key[\"nwb_file_name\"],\n            analysis_file_name=key[\"analysis_file_name\"],\n        )\n\n        self.insert1(key)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(\n                nwb_data[\"dlc_orientation\"].get_spatial_series().timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"orientation\",\n        ]\n        return pd.DataFrame(\n            np.asarray(nwb_data[\"dlc_orientation\"].get_spatial_series().data)[\n                :, np.newaxis\n            ],\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.DLCPoseEstimation", "title": "<code>DLCPoseEstimation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>@schema\nclass DLCPoseEstimation(dj.Computed):\n    definition = \"\"\"\n    -&gt; DLCPoseEstimationSelection\n    ---\n    pose_estimation_time: datetime  # time of generation of this set of DLC results\n    meters_per_pixel : double       # conversion of meters per pixel for analyzed video\n    \"\"\"\n\n    class BodyPart(dj.Part):\n        definition = \"\"\" # uses DeepLabCut h5 output for body part position\n        -&gt; DLCPoseEstimation\n        -&gt; DLCModel.BodyPart\n        ---\n        -&gt; AnalysisNwbfile\n        dlc_pose_estimation_position_object_id : varchar(80)\n        dlc_pose_estimation_likelihood_object_id : varchar(80)\n        \"\"\"\n\n        def fetch_nwb(self, *attrs, **kwargs):\n            return fetch_nwb(\n                self,\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n\n        def fetch1_dataframe(self):\n            nwb_data = self.fetch_nwb()[0]\n            index = pd.Index(\n                np.asarray(\n                    nwb_data[\"dlc_pose_estimation_position\"]\n                    .get_spatial_series()\n                    .timestamps\n                ),\n                name=\"time\",\n            )\n            COLUMNS = [\n                \"video_frame_ind\",\n                \"x\",\n                \"y\",\n                \"likelihood\",\n            ]\n            return pd.DataFrame(\n                np.concatenate(\n                    (\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_likelihood\"]\n                            .time_series[\"video_frame_ind\"]\n                            .data,\n                            dtype=int,\n                        )[:, np.newaxis],\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_position\"]\n                            .get_spatial_series()\n                            .data\n                        ),\n                        np.asarray(\n                            nwb_data[\"dlc_pose_estimation_likelihood\"]\n                            .time_series[\"likelihood\"]\n                            .data\n                        )[:, np.newaxis],\n                    ),\n                    axis=1,\n                ),\n                columns=COLUMNS,\n                index=index,\n            )\n\n    def make(self, key):\n\"\"\".populate() method will launch training for each PoseEstimationTask\"\"\"\n        from . import dlc_reader\n        from .dlc_utils import get_video_path\n\n        METERS_PER_CM = 0.01\n\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n        ) as logger:\n            logger.logger.info(\"----------------------\")\n            logger.logger.info(\"Pose Estimation\")\n            # ID model and directories\n            dlc_model = (DLCModel &amp; key).fetch1()\n            bodyparts = (DLCModel.BodyPart &amp; key).fetch(\"bodypart\")\n            task_mode, analyze_video_params, video_path, output_dir = (\n                DLCPoseEstimationSelection &amp; key\n            ).fetch1(\n                \"task_mode\",\n                \"pose_estimation_params\",\n                \"video_path\",\n                \"pose_estimation_output_dir\",\n            )\n            analyze_video_params = analyze_video_params or {}\n\n            project_path = dlc_model[\"project_path\"]\n\n            # Trigger PoseEstimation\n            if task_mode == \"trigger\":\n                dlc_reader.do_pose_estimation(\n                    video_path,\n                    dlc_model,\n                    project_path,\n                    output_dir,\n                    **analyze_video_params,\n                )\n            dlc_result = dlc_reader.PoseEstimation(output_dir)\n            creation_time = datetime.fromtimestamp(\n                dlc_result.creation_time\n            ).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n            logger.logger.info(\"getting raw position\")\n            interval_list_name = f\"pos {key['epoch']-1} valid times\"\n            spatial_series = (\n                RawPosition()\n                &amp; {**key, \"interval_list_name\": interval_list_name}\n            ).fetch_nwb()[0][\"raw_position\"]\n            _, _, _, video_time = get_video_path(key)\n            pos_time = spatial_series.timestamps\n            # TODO: should get timestamps from VideoFile, but need the video_frame_ind from RawPosition,\n            # which also has timestamps\n            key[\"meters_per_pixel\"] = spatial_series.conversion\n\n            # Insert entry into DLCPoseEstimation\n            logger.logger.info(\n                \"Inserting %s, epoch %02d into DLCPoseEsimation\",\n                key[\"nwb_file_name\"],\n                key[\"epoch\"],\n            )\n            self.insert1({**key, \"pose_estimation_time\": creation_time})\n            meters_per_pixel = key[\"meters_per_pixel\"]\n            del key[\"meters_per_pixel\"]\n            body_parts = dlc_result.df.columns.levels[0]\n            body_parts_df = {}\n            # Insert dlc pose estimation into analysis NWB file for each body part.\n            for body_part in bodyparts:\n                if body_part in body_parts:\n                    body_parts_df[body_part] = pd.DataFrame.from_dict(\n                        {\n                            c: dlc_result.df.get(body_part).get(c).values\n                            for c in dlc_result.df.get(body_part).columns\n                        }\n                    )\n            idx = pd.IndexSlice\n            for body_part, part_df in body_parts_df.items():\n                logger.logger.info(\"converting to cm\")\n                part_df = convert_to_cm(part_df, meters_per_pixel)\n                logger.logger.info(\"adding timestamps to DataFrame\")\n                part_df = add_timestamps(\n                    part_df, pos_time=pos_time, video_time=video_time\n                )\n                key[\"bodypart\"] = body_part\n                key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                    key[\"nwb_file_name\"]\n                )\n                position = pynwb.behavior.Position()\n                likelihood = pynwb.behavior.BehavioralTimeSeries()\n                position.create_spatial_series(\n                    name=\"position\",\n                    timestamps=part_df.time.to_numpy(),\n                    conversion=METERS_PER_CM,\n                    data=part_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                    reference_frame=spatial_series.reference_frame,\n                    comments=spatial_series.comments,\n                    description=\"x_position, y_position\",\n                )\n                likelihood.create_timeseries(\n                    name=\"likelihood\",\n                    timestamps=part_df.time.to_numpy(),\n                    data=part_df.loc[:, idx[\"likelihood\"]].to_numpy(),\n                    unit=\"likelihood\",\n                    comments=\"no comments\",\n                    description=\"likelihood\",\n                )\n                likelihood.create_timeseries(\n                    name=\"video_frame_ind\",\n                    timestamps=part_df.time.to_numpy(),\n                    data=part_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                    unit=\"index\",\n                    comments=\"no comments\",\n                    description=\"video_frame_ind\",\n                )\n                nwb_analysis_file = AnalysisNwbfile()\n                key[\n                    \"dlc_pose_estimation_position_object_id\"\n                ] = nwb_analysis_file.add_nwb_object(\n                    analysis_file_name=key[\"analysis_file_name\"],\n                    nwb_object=position,\n                )\n                key[\n                    \"dlc_pose_estimation_likelihood_object_id\"\n                ] = nwb_analysis_file.add_nwb_object(\n                    analysis_file_name=key[\"analysis_file_name\"],\n                    nwb_object=likelihood,\n                )\n                nwb_analysis_file.add(\n                    nwb_file_name=key[\"nwb_file_name\"],\n                    analysis_file_name=key[\"analysis_file_name\"],\n                )\n                self.BodyPart.insert1(key)\n\n    def fetch_dataframe(self, *attrs, **kwargs):\n        entries = (self.BodyPart &amp; self).fetch(\"KEY\")\n        nwb_data_dict = {\n            entry[\"bodypart\"]: (self.BodyPart() &amp; entry).fetch_nwb()[0]\n            for entry in entries\n        }\n        index = pd.Index(\n            np.asarray(\n                nwb_data_dict[entries[0][\"bodypart\"]][\n                    \"dlc_pose_estimation_position\"\n                ]\n                .get_spatial_series()\n                .timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"x\",\n            \"y\",\n            \"likelihood\",\n        ]\n        return pd.concat(\n            {\n                entry[\"bodypart\"]: pd.DataFrame(\n                    np.concatenate(\n                        (\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_likelihood\"\n                                ]\n                                .time_series[\"video_frame_ind\"]\n                                .data,\n                                dtype=int,\n                            )[:, np.newaxis],\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_position\"\n                                ]\n                                .get_spatial_series()\n                                .data\n                            ),\n                            np.asarray(\n                                nwb_data_dict[entry[\"bodypart\"]][\n                                    \"dlc_pose_estimation_likelihood\"\n                                ]\n                                .time_series[\"likelihood\"]\n                                .data\n                            )[:, np.newaxis],\n                        ),\n                        axis=1,\n                    ),\n                    columns=COLUMNS,\n                    index=index,\n                )\n                for entry in entries\n            },\n            axis=1,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_pose_estimation.DLCPoseEstimation.make", "title": "<code>make(key)</code>", "text": "<p>.populate() method will launch training for each PoseEstimationTask</p> Source code in <code>src/spyglass/position/v1/position_dlc_pose_estimation.py</code> <pre><code>def make(self, key):\n\"\"\".populate() method will launch training for each PoseEstimationTask\"\"\"\n    from . import dlc_reader\n    from .dlc_utils import get_video_path\n\n    METERS_PER_CM = 0.01\n\n    output_dir = infer_output_dir(key=key, makedir=False)\n    with OutputLogger(\n        name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n        path=f\"{output_dir.as_posix()}/log.log\",\n    ) as logger:\n        logger.logger.info(\"----------------------\")\n        logger.logger.info(\"Pose Estimation\")\n        # ID model and directories\n        dlc_model = (DLCModel &amp; key).fetch1()\n        bodyparts = (DLCModel.BodyPart &amp; key).fetch(\"bodypart\")\n        task_mode, analyze_video_params, video_path, output_dir = (\n            DLCPoseEstimationSelection &amp; key\n        ).fetch1(\n            \"task_mode\",\n            \"pose_estimation_params\",\n            \"video_path\",\n            \"pose_estimation_output_dir\",\n        )\n        analyze_video_params = analyze_video_params or {}\n\n        project_path = dlc_model[\"project_path\"]\n\n        # Trigger PoseEstimation\n        if task_mode == \"trigger\":\n            dlc_reader.do_pose_estimation(\n                video_path,\n                dlc_model,\n                project_path,\n                output_dir,\n                **analyze_video_params,\n            )\n        dlc_result = dlc_reader.PoseEstimation(output_dir)\n        creation_time = datetime.fromtimestamp(\n            dlc_result.creation_time\n        ).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        logger.logger.info(\"getting raw position\")\n        interval_list_name = f\"pos {key['epoch']-1} valid times\"\n        spatial_series = (\n            RawPosition()\n            &amp; {**key, \"interval_list_name\": interval_list_name}\n        ).fetch_nwb()[0][\"raw_position\"]\n        _, _, _, video_time = get_video_path(key)\n        pos_time = spatial_series.timestamps\n        # TODO: should get timestamps from VideoFile, but need the video_frame_ind from RawPosition,\n        # which also has timestamps\n        key[\"meters_per_pixel\"] = spatial_series.conversion\n\n        # Insert entry into DLCPoseEstimation\n        logger.logger.info(\n            \"Inserting %s, epoch %02d into DLCPoseEsimation\",\n            key[\"nwb_file_name\"],\n            key[\"epoch\"],\n        )\n        self.insert1({**key, \"pose_estimation_time\": creation_time})\n        meters_per_pixel = key[\"meters_per_pixel\"]\n        del key[\"meters_per_pixel\"]\n        body_parts = dlc_result.df.columns.levels[0]\n        body_parts_df = {}\n        # Insert dlc pose estimation into analysis NWB file for each body part.\n        for body_part in bodyparts:\n            if body_part in body_parts:\n                body_parts_df[body_part] = pd.DataFrame.from_dict(\n                    {\n                        c: dlc_result.df.get(body_part).get(c).values\n                        for c in dlc_result.df.get(body_part).columns\n                    }\n                )\n        idx = pd.IndexSlice\n        for body_part, part_df in body_parts_df.items():\n            logger.logger.info(\"converting to cm\")\n            part_df = convert_to_cm(part_df, meters_per_pixel)\n            logger.logger.info(\"adding timestamps to DataFrame\")\n            part_df = add_timestamps(\n                part_df, pos_time=pos_time, video_time=video_time\n            )\n            key[\"bodypart\"] = body_part\n            key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                key[\"nwb_file_name\"]\n            )\n            position = pynwb.behavior.Position()\n            likelihood = pynwb.behavior.BehavioralTimeSeries()\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=part_df.time.to_numpy(),\n                conversion=METERS_PER_CM,\n                data=part_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"x_position, y_position\",\n            )\n            likelihood.create_timeseries(\n                name=\"likelihood\",\n                timestamps=part_df.time.to_numpy(),\n                data=part_df.loc[:, idx[\"likelihood\"]].to_numpy(),\n                unit=\"likelihood\",\n                comments=\"no comments\",\n                description=\"likelihood\",\n            )\n            likelihood.create_timeseries(\n                name=\"video_frame_ind\",\n                timestamps=part_df.time.to_numpy(),\n                data=part_df.loc[:, idx[\"video_frame_ind\"]].to_numpy(),\n                unit=\"index\",\n                comments=\"no comments\",\n                description=\"video_frame_ind\",\n            )\n            nwb_analysis_file = AnalysisNwbfile()\n            key[\n                \"dlc_pose_estimation_position_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=position,\n            )\n            key[\n                \"dlc_pose_estimation_likelihood_object_id\"\n            ] = nwb_analysis_file.add_nwb_object(\n                analysis_file_name=key[\"analysis_file_name\"],\n                nwb_object=likelihood,\n            )\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n            self.BodyPart.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.DLCCentroid", "title": "<code>DLCCentroid</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Table to calculate the centroid of a group of bodyparts</p> Source code in <code>src/spyglass/position/v1/position_dlc_centroid.py</code> <pre><code>@schema\nclass DLCCentroid(dj.Computed):\n\"\"\"\n    Table to calculate the centroid of a group of bodyparts\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCCentroidSelection\n    ---\n    -&gt; AnalysisNwbfile\n    dlc_position_object_id : varchar(80)\n    dlc_velocity_object_id : varchar(80)\n    \"\"\"\n\n    def make(self, key):\n        from .dlc_utils import OutputLogger, infer_output_dir\n\n        idx = pd.IndexSlice\n        output_dir = infer_output_dir(key=key, makedir=False)\n        with OutputLogger(\n            name=f\"{key['nwb_file_name']}_{key['epoch']}_{key['dlc_model_name']}_log\",\n            path=f\"{output_dir.as_posix()}/log.log\",\n            print_console=False,\n        ) as logger:\n            logger.logger.info(\"-----------------------\")\n            logger.logger.info(\"Centroid Calculation\")\n            # Get labels to smooth from Parameters table\n            cohort_entries = DLCSmoothInterpCohort.BodyPart &amp; key\n            params = (DLCCentroidParams() &amp; key).fetch1(\"params\")\n            centroid_method = params.pop(\"centroid_method\")\n            bodyparts_avail = cohort_entries.fetch(\"bodypart\")\n            speed_smoothing_std_dev = params.pop(\"speed_smoothing_std_dev\")\n            # TODO, generalize key naming\n            if centroid_method == \"four_led_centroid\":\n                centroid_func = _key_to_func_dict[centroid_method]\n                if \"greenLED\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"greenLED\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"greenLED\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"A green led needs to be specified for the 4 led centroid method\"\n                    )\n                if \"redLED_L\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"redLED_L\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"redLED_L\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"A left red led needs to be specified for the 4 led centroid method\"\n                    )\n                if \"redLED_C\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"redLED_C\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"redLED_C\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"A center red led needs to be specified for the 4 led centroid method\"\n                    )\n                if \"redLED_R\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"redLED_R\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"redLED_R\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"A right red led needs to be specified for the 4 led centroid method\"\n                    )\n                bodyparts_to_use = [\n                    params[\"points\"][\"greenLED\"],\n                    params[\"points\"][\"redLED_L\"],\n                    params[\"points\"][\"redLED_C\"],\n                    params[\"points\"][\"redLED_R\"],\n                ]\n\n            elif centroid_method == \"two_pt_centroid\":\n                centroid_func = _key_to_func_dict[centroid_method]\n                if \"point1\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"point1\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"point1\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"point1 needs to be specified for the 2 pt centroid method\"\n                    )\n                if \"point2\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"point2\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"point2\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"point2 needs to be specified for the 2 pt centroid method\"\n                    )\n                bodyparts_to_use = [\n                    params[\"points\"][\"point1\"],\n                    params[\"points\"][\"point2\"],\n                ]\n\n            elif centroid_method == \"one_pt_centroid\":\n                centroid_func = _key_to_func_dict[centroid_method]\n                if \"point1\" in params[\"points\"]:\n                    assert (\n                        params[\"points\"][\"point1\"] in bodyparts_avail\n                    ), f'{params[\"points\"][\"point1\"]} not a bodypart used in this model'\n                else:\n                    raise ValueError(\n                        \"point1 needs to be specified for the 1 pt centroid method\"\n                    )\n                bodyparts_to_use = [params[\"points\"][\"point1\"]]\n\n            else:\n                raise ValueError(\"Please specify a centroid method to use.\")\n            pos_df = pd.concat(\n                {\n                    bodypart: (\n                        DLCSmoothInterpCohort.BodyPart\n                        &amp; {**key, **{\"bodypart\": bodypart}}\n                    ).fetch1_dataframe()\n                    for bodypart in bodyparts_to_use\n                },\n                axis=1,\n            )\n            dt = np.median(np.diff(pos_df.index.to_numpy()))\n            sampling_rate = 1 / dt\n            logger.logger.info(\n                \"Calculating centroid with %s\", str(centroid_method)\n            )\n            centroid = centroid_func(pos_df, **params)\n            centroid_df = pd.DataFrame(\n                centroid,\n                columns=[\"x\", \"y\"],\n                index=pos_df.index.to_numpy(),\n            )\n            if params[\"interpolate\"]:\n                if np.any(np.isnan(centroid)):\n                    logger.logger.info(\"interpolating over NaNs\")\n                    nan_inds = (\n                        pd.isnull(centroid_df.loc[:, idx[(\"x\", \"y\")]])\n                        .any(axis=1)\n                        .to_numpy()\n                        .nonzero()[0]\n                    )\n                    nan_spans = get_span_start_stop(nan_inds)\n                    interp_df = interp_pos(\n                        centroid_df.copy(), nan_spans, **params[\"interp_params\"]\n                    )\n                else:\n                    logger.logger.info(\"no NaNs to interpolate over\")\n                    interp_df = centroid_df.copy()\n            else:\n                interp_df = centroid_df.copy()\n            if params[\"smooth\"]:\n                if \"smoothing_duration\" in params[\"smoothing_params\"]:\n                    smoothing_duration = params[\"smoothing_params\"].pop(\n                        \"smoothing_duration\"\n                    )\n                    dt = np.median(np.diff(pos_df.index.to_numpy()))\n                    sampling_rate = 1 / dt\n                    logger.logger.info(\"smoothing position\")\n                    smooth_func = _key_to_smooth_func_dict[\n                        params[\"smoothing_params\"][\"smooth_method\"]\n                    ]\n                    logger.logger.info(\n                        \"Smoothing using method: %s\",\n                        str(params[\"smoothing_params\"][\"smooth_method\"]),\n                    )\n                    final_df = smooth_func(\n                        interp_df,\n                        smoothing_duration=smoothing_duration,\n                        sampling_rate=sampling_rate,\n                        **params[\"smoothing_params\"],\n                    )\n                else:\n                    raise KeyError(\n                        \"smoothing_duration needs to be passed within smoothing_params\"\n                    )\n            else:\n                final_df = interp_df.copy()\n            logger.logger.info(\"getting velocity\")\n            velocity = get_velocity(\n                final_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                time=pos_df.index.to_numpy(),\n                sigma=speed_smoothing_std_dev,\n                sampling_frequency=sampling_rate,\n            )  # cm/s\n            speed = np.sqrt(np.sum(velocity**2, axis=1))  # cm/s\n            # Create dataframe\n            velocity_df = pd.DataFrame(\n                np.concatenate((velocity, speed[:, np.newaxis]), axis=1),\n                columns=[\"velocity_x\", \"velocity_y\", \"speed\"],\n                index=pos_df.index.to_numpy(),\n            )\n            total_nan = np.sum(\n                final_df.loc[:, idx[(\"x\", \"y\")]].isna().any(axis=1)\n            )\n            pretrack_nan = np.sum(\n                final_df.iloc[:1000].loc[:, idx[(\"x\", \"y\")]].isna().any(axis=1)\n            )\n            logger.logger.info(\"total NaNs in centroid dataset: %d\", total_nan)\n            logger.logger.info(\n                \"NaNs in centroid dataset before ind 1000: %d\", pretrack_nan\n            )\n            position = pynwb.behavior.Position()\n            velocity = pynwb.behavior.BehavioralTimeSeries()\n            spatial_series = (RawPosition() &amp; key).fetch_nwb()[0][\n                \"raw_position\"\n            ]\n            METERS_PER_CM = 0.01\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=final_df.index.to_numpy(),\n                conversion=METERS_PER_CM,\n                data=final_df.loc[:, idx[(\"x\", \"y\")]].to_numpy(),\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"x_position, y_position\",\n            )\n            velocity.create_timeseries(\n                name=\"velocity\",\n                timestamps=velocity_df.index.to_numpy(),\n                conversion=METERS_PER_CM,\n                unit=\"m/s\",\n                data=velocity_df.loc[\n                    :, idx[(\"velocity_x\", \"velocity_y\", \"speed\")]\n                ].to_numpy(),\n                comments=spatial_series.comments,\n                description=\"x_velocity, y_velocity, speed\",\n            )\n            velocity.create_timeseries(\n                name=\"video_frame_ind\",\n                unit=\"index\",\n                timestamps=final_df.index.to_numpy(),\n                data=pos_df[\n                    pos_df.columns.levels[0][0]\n                ].video_frame_ind.to_numpy(),\n                description=\"video_frame_ind\",\n                comments=\"no comments\",\n            )\n            # Add to Analysis NWB file\n            key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n                key[\"nwb_file_name\"]\n            )\n            nwb_analysis_file = AnalysisNwbfile()\n            key[\"dlc_position_object_id\"] = nwb_analysis_file.add_nwb_object(\n                key[\"analysis_file_name\"], position\n            )\n            key[\"dlc_velocity_object_id\"] = nwb_analysis_file.add_nwb_object(\n                key[\"analysis_file_name\"], velocity\n            )\n\n            nwb_analysis_file.add(\n                nwb_file_name=key[\"nwb_file_name\"],\n                analysis_file_name=key[\"analysis_file_name\"],\n            )\n            self.insert1(key)\n            logger.logger.info(\"inserted entry into DLCCentroid\")\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(\n                nwb_data[\"dlc_position\"].get_spatial_series().timestamps\n            ),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"position_x\",\n            \"position_y\",\n            \"velocity_x\",\n            \"velocity_y\",\n            \"speed\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"dlc_velocity\"]\n                        .time_series[\"video_frame_ind\"]\n                        .data,\n                        dtype=int,\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"dlc_position\"].get_spatial_series().data\n                    ),\n                    np.asarray(\n                        nwb_data[\"dlc_velocity\"].time_series[\"velocity\"].data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.get_video_path", "title": "<code>get_video_path(key)</code>", "text": "<p>Given nwb_file_name and interval_list_name returns specified video file filename and path</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Dictionary containing nwb_file_name and interval_list_name as keys</p> required <p>Returns:</p> Name Type Description <code>video_filepath</code> <code>str</code> <p>path to the video file, including video filename</p> <code>video_filename</code> <code>str</code> <p>filename of the video</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_video_path(key):\n\"\"\"\n    Given nwb_file_name and interval_list_name returns specified\n    video file filename and path\n\n    Parameters\n    ----------\n    key : dict\n        Dictionary containing nwb_file_name and interval_list_name as keys\n\n    Returns\n    -------\n    video_filepath : str\n        path to the video file, including video filename\n    video_filename : str\n        filename of the video\n    \"\"\"\n    import pynwb\n\n    from ...common.common_behav import VideoFile\n\n    video_info = (\n        VideoFile()\n        &amp; {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": key[\"epoch\"]}\n    ).fetch1()\n    nwb_path = (\n        f\"{os.getenv('SPYGLASS_BASE_DIR')}/raw/{video_info['nwb_file_name']}\"\n    )\n    with pynwb.NWBHDF5IO(path=nwb_path, mode=\"r\") as in_out:\n        nwb_file = in_out.read()\n        nwb_video = nwb_file.objects[video_info[\"video_file_object_id\"]]\n        video_filepath = VideoFile.get_abs_path(\n            {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": key[\"epoch\"]}\n        )\n        video_dir = os.path.dirname(video_filepath) + \"/\"\n        video_filename = video_filepath.split(video_dir)[-1]\n        meters_per_pixel = nwb_video.device.meters_per_pixel\n        timestamps = np.asarray(nwb_video.timestamps)\n    return video_dir, video_filename, meters_per_pixel, timestamps\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_selection/#src.spyglass.position.v1.position_dlc_selection.DLCPosVideo", "title": "<code>DLCPosVideo</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Creates a video of the computed head position and orientation as well as the original LED positions overlaid on the video of the animal.</p> <p>Use for debugging the effect of position extraction parameters.</p> Source code in <code>src/spyglass/position/v1/position_dlc_selection.py</code> <pre><code>@schema\nclass DLCPosVideo(dj.Computed):\n\"\"\"Creates a video of the computed head position and orientation as well as\n    the original LED positions overlaid on the video of the animal.\n\n    Use for debugging the effect of position extraction parameters.\"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCPosVideoSelection\n    ---\n    \"\"\"\n\n    def make(self, key):\n        from tqdm import tqdm as tqdm\n\n        params = (DLCPosVideoParams &amp; key).fetch1(\"params\")\n        if \"video_params\" not in params:\n            params[\"video_params\"] = {}\n        M_TO_CM = 100\n        key[\"interval_list_name\"] = f\"pos {key['epoch']-1} valid times\"\n        epoch = (\n            int(\n                key[\"interval_list_name\"]\n                .replace(\"pos \", \"\")\n                .replace(\" valid times\", \"\")\n            )\n            + 1\n        )\n        pose_estimation_key = {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"epoch\": epoch,\n            \"dlc_model_name\": key[\"dlc_model_name\"],\n            \"dlc_model_params_name\": key[\"dlc_model_params_name\"],\n        }\n        pose_estimation_params, video_filename, output_dir = (\n            DLCPoseEstimationSelection() &amp; pose_estimation_key\n        ).fetch1(\n            \"pose_estimation_params\", \"video_path\", \"pose_estimation_output_dir\"\n        )\n        print(f\"video filename: {video_filename}\")\n        meters_per_pixel = (DLCPoseEstimation() &amp; pose_estimation_key).fetch1(\n            \"meters_per_pixel\"\n        )\n        crop = None\n        if \"cropping\" in pose_estimation_params:\n            crop = pose_estimation_params[\"cropping\"]\n        print(\"Loading position data...\")\n        position_info_df = (\n            DLCPosV1()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"epoch\": epoch,\n                \"dlc_si_cohort_centroid\": key[\"dlc_si_cohort_centroid\"],\n                \"dlc_centroid_params_name\": key[\"dlc_centroid_params_name\"],\n                \"dlc_si_cohort_orientation\": key[\"dlc_si_cohort_orientation\"],\n                \"dlc_orientation_params_name\": key[\n                    \"dlc_orientation_params_name\"\n                ],\n            }\n        ).fetch1_dataframe()\n        pose_estimation_df = pd.concat(\n            {\n                bodypart: (\n                    DLCPoseEstimation.BodyPart()\n                    &amp; {**pose_estimation_key, **{\"bodypart\": bodypart}}\n                ).fetch1_dataframe()\n                for bodypart in (\n                    DLCSmoothInterpCohort.BodyPart &amp; pose_estimation_key\n                )\n                .fetch(\"bodypart\")\n                .tolist()\n            },\n            axis=1,\n        )\n        assert len(pose_estimation_df) == len(position_info_df), (\n            f\"length of pose_estimation_df: {len(pose_estimation_df)} \"\n            f\"does not match the length of position_info_df: {len(position_info_df)}.\"\n        )\n\n        nwb_base_filename = key[\"nwb_file_name\"].replace(\".nwb\", \"\")\n        if Path(output_dir).exists():\n            output_video_filename = (\n                f\"{Path(output_dir).as_posix()}/\"\n                f\"{nwb_base_filename}_{epoch:02d}_\"\n                f'{key[\"dlc_si_cohort_centroid\"]}_'\n                f'{key[\"dlc_centroid_params_name\"]}'\n                f'{key[\"dlc_orientation_params_name\"]}.mp4'\n            )\n        else:\n            output_video_filename = (\n                f\"{nwb_base_filename}_{epoch:02d}_\"\n                f'{key[\"dlc_si_cohort_centroid\"]}_'\n                f'{key[\"dlc_centroid_params_name\"]}'\n                f'{key[\"dlc_orientation_params_name\"]}.mp4'\n            )\n        idx = pd.IndexSlice\n        video_frame_inds = (\n            position_info_df[\"video_frame_ind\"].astype(int).to_numpy()\n        )\n        centroids = {\n            bodypart: pose_estimation_df.loc[\n                :, idx[bodypart, (\"x\", \"y\")]\n            ].to_numpy()\n            for bodypart in pose_estimation_df.columns.levels[0]\n        }\n        if params.get(\"incl_likelihood\", None):\n            likelihoods = {\n                bodypart: pose_estimation_df.loc[\n                    :, idx[bodypart, (\"likelihood\")]\n                ].to_numpy()\n                for bodypart in pose_estimation_df.columns.levels[0]\n            }\n        else:\n            likelihoods = None\n        position_mean = {\n            \"DLC\": np.asarray(position_info_df[[\"position_x\", \"position_y\"]])\n        }\n        orientation_mean = {\n            \"DLC\": np.asarray(position_info_df[[\"orientation\"]])\n        }\n        position_time = np.asarray(position_info_df.index)\n        cm_per_pixel = meters_per_pixel * M_TO_CM\n        percent_frames = params.get(\"percent_frames\", None)\n        frames = params.get(\"frames\", None)\n        if frames is not None:\n            frames_arr = np.arange(frames[0], frames[1])\n        else:\n            frames_arr = frames\n\n        print(\"Making video...\")\n        make_video(\n            video_filename=video_filename,\n            video_frame_inds=video_frame_inds,\n            position_mean=position_mean,\n            orientation_mean=orientation_mean,\n            centroids=centroids,\n            likelihoods=likelihoods,\n            position_time=position_time,\n            video_time=None,\n            processor=params.get(\"processor\", \"matplotlib\"),\n            frames=frames_arr,\n            percent_frames=percent_frames,\n            output_video_filename=output_video_filename,\n            cm_to_pixels=cm_per_pixel,\n            disable_progressbar=False,\n            crop=crop,\n            **params[\"video_params\"],\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/", "title": "position_dlc_training.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_training.DLCModelTrainingParams", "title": "<code>DLCModelTrainingParams</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_training.py</code> <pre><code>@schema\nclass DLCModelTrainingParams(dj.Lookup):\n    definition = \"\"\"\n    # Parameters to specify a DLC model training instance\n    # For DLC \u2264 v2.0, include scorer_lecacy = True in params\n    dlc_training_params_name      : varchar(50) # descriptive name of parameter set\n    ---\n    params                        : longblob    # dictionary of all applicable parameters\n    \"\"\"\n\n    required_parameters = (\n        \"shuffle\",\n        \"trainingsetindex\",\n        \"net_type\",\n        \"gputouse\",\n    )\n    skipped_parameters = (\"project_path\", \"video_sets\")\n\n    @classmethod\n    def insert_new_params(cls, paramset_name: str, params: dict, **kwargs):\n\"\"\"\n        Insert a new set of training parameters into dlc.TrainingParamSet.\n\n        Parameters\n        ----------\n        paramset_name : str\n            Description of parameter set to be inserted\n        params : dict\n            Dictionary including all settings to specify model training.\n            Must include shuffle &amp; trainingsetindex b/c not in config.yaml.\n            project_path and video_sets will be overwritten by config.yaml.\n            Note that trainingsetindex is 0-indexed\n        \"\"\"\n\n        for required_param in cls.required_parameters:\n            assert required_param in params, (\n                \"Missing required parameter: \" + required_param\n            )\n        for skipped_param in cls.skipped_parameters:\n            if skipped_param in params:\n                params.pop(skipped_param)\n\n        param_dict = {\n            \"dlc_training_params_name\": paramset_name,\n            \"params\": params,\n        }\n        param_query = cls &amp; {\n            \"dlc_training_params_name\": param_dict[\"dlc_training_params_name\"]\n        }\n        # If the specified param-set already exists\n        # Not sure we need this part, as much just a check if the name is the same\n        if param_query:\n            existing_paramset_name = param_query.fetch1(\n                \"dlc_training_params_name\"\n            )\n            if (\n                existing_paramset_name == paramset_name\n            ):  # If existing name same:\n                return print(\n                    f\"New param set not added\\n\"\n                    f\"A param set with name: {paramset_name} already exists\"\n                )\n        else:\n            cls.insert1(\n                param_dict, **kwargs\n            )  # if duplicate, will raise duplicate error\n            # if this will raise duplicate error, why is above check needed? @datajoint\n\n    @classmethod\n    def get_accepted_params(cls):\n        from deeplabcut import create_training_dataset, train_network\n\n        return list(\n            set(\n                [\n                    *list(inspect.signature(train_network).parameters),\n                    *list(\n                        inspect.signature(create_training_dataset).parameters\n                    ),\n                ]\n            )\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_training.DLCModelTrainingParams.insert_new_params", "title": "<code>insert_new_params(paramset_name, params, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Insert a new set of training parameters into dlc.TrainingParamSet.</p> <p>Parameters:</p> Name Type Description Default <code>paramset_name</code> <code>str</code> <p>Description of parameter set to be inserted</p> required <code>params</code> <code>dict</code> <p>Dictionary including all settings to specify model training. Must include shuffle &amp; trainingsetindex b/c not in config.yaml. project_path and video_sets will be overwritten by config.yaml. Note that trainingsetindex is 0-indexed</p> required Source code in <code>src/spyglass/position/v1/position_dlc_training.py</code> <pre><code>@classmethod\ndef insert_new_params(cls, paramset_name: str, params: dict, **kwargs):\n\"\"\"\n    Insert a new set of training parameters into dlc.TrainingParamSet.\n\n    Parameters\n    ----------\n    paramset_name : str\n        Description of parameter set to be inserted\n    params : dict\n        Dictionary including all settings to specify model training.\n        Must include shuffle &amp; trainingsetindex b/c not in config.yaml.\n        project_path and video_sets will be overwritten by config.yaml.\n        Note that trainingsetindex is 0-indexed\n    \"\"\"\n\n    for required_param in cls.required_parameters:\n        assert required_param in params, (\n            \"Missing required parameter: \" + required_param\n        )\n    for skipped_param in cls.skipped_parameters:\n        if skipped_param in params:\n            params.pop(skipped_param)\n\n    param_dict = {\n        \"dlc_training_params_name\": paramset_name,\n        \"params\": params,\n    }\n    param_query = cls &amp; {\n        \"dlc_training_params_name\": param_dict[\"dlc_training_params_name\"]\n    }\n    # If the specified param-set already exists\n    # Not sure we need this part, as much just a check if the name is the same\n    if param_query:\n        existing_paramset_name = param_query.fetch1(\n            \"dlc_training_params_name\"\n        )\n        if (\n            existing_paramset_name == paramset_name\n        ):  # If existing name same:\n            return print(\n                f\"New param set not added\\n\"\n                f\"A param set with name: {paramset_name} already exists\"\n            )\n    else:\n        cls.insert1(\n            param_dict, **kwargs\n        )  # if duplicate, will raise duplicate error\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_training.DLCProject", "title": "<code>DLCProject</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Table to facilitate creation of a new DeepLabCut model. With ability to edit config, extract frames, label frames</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@schema\nclass DLCProject(dj.Manual):\n\"\"\"Table to facilitate creation of a new DeepLabCut model.\n    With ability to edit config, extract frames, label frames\n    \"\"\"\n\n    # Add more parameters as secondary keys...\n    # TODO: collapse params into blob dict\n    definition = \"\"\"\n    project_name     : varchar(100) # name of DLC project\n    ---\n    -&gt; LabTeam\n    bodyparts        : blob         # list of bodyparts to label\n    frames_per_video : int          # number of frames to extract from each video\n    config_path      : varchar(120) # path to config.yaml for model\n    \"\"\"\n\n    class BodyPart(dj.Part):\n\"\"\"Part table to hold bodyparts used in each project.\"\"\"\n\n        definition = \"\"\"\n        -&gt; DLCProject\n        -&gt; BodyPart\n        \"\"\"\n\n    class File(dj.Part):\n        definition = \"\"\"\n        # Paths of training files (e.g., labeled pngs, CSV or video)\n        -&gt; DLCProject\n        file_name: varchar(200) # Concise name to describe file\n        file_ext : enum(\"mp4\", \"csv\", \"h5\") # extension of file\n        ---\n        file_path: varchar(255)\n        \"\"\"\n\n    def insert1(self, key, **kwargs):\n        assert isinstance(\n            key[\"project_name\"], str\n        ), \"project_name must be a string\"\n        assert isinstance(\n            key[\"frames_per_video\"], int\n        ), \"frames_per_video must be of type `int`\"\n        super().insert1(key, **kwargs)\n\n    @classmethod\n    def insert_existing_project(\n        cls,\n        project_name: str,\n        lab_team: str,\n        config_path: str,\n        bodyparts: List = None,\n        frames_per_video: int = None,\n        add_to_files: bool = True,\n        **kwargs,\n    ):\n\"\"\"\n        insert an existing project into DLCProject table.\n        Parameters\n        ----------\n        project_name : str\n            user-friendly name of project\n        lab_team : str\n            name of lab team. Should match an entry in LabTeam table\n        config_path : str\n            path to project directory\n        bodyparts : list\n            optional list of bodyparts to label that\n            are not already in existing config\n        \"\"\"\n\n        # Read config\n        project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n        if project_name in project_names_in_use:\n            print(f\"project name: {project_name} is already in use.\")\n            return_key = {}\n            return_key[\"project_name\"], return_key[\"config_path\"] = (\n                cls &amp; {\"project_name\": project_name}\n            ).fetch1(\"project_name\", \"config_path\")\n            return return_key\n        from deeplabcut.utils.auxiliaryfunctions import read_config\n\n        cfg = read_config(config_path)\n        if bodyparts:\n            bodyparts_to_add = [\n                bodypart\n                for bodypart in bodyparts\n                if bodypart not in cfg[\"bodyparts\"]\n            ]\n            all_bodyparts = bodyparts_to_add + cfg[\"bodyparts\"]\n        else:\n            all_bodyparts = cfg[\"bodyparts\"]\n        BodyPart.add_from_config(cfg[\"bodyparts\"])\n        for bodypart in all_bodyparts:\n            if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n                raise ValueError(\n                    f\"bodypart: {bodypart} not found in BodyPart table\"\n                )\n        # check bodyparts are in config, if not add\n        if len(bodyparts_to_add) &gt; 0:\n            add_to_config(config_path, bodyparts=bodyparts_to_add)\n        # Get frames per video from config. If passed as arg, check match\n        if frames_per_video:\n            if frames_per_video != cfg[\"numframes2pick\"]:\n                add_to_config(\n                    config_path, **{\"numframes2pick\": frames_per_video}\n                )\n        config_path = Path(config_path)\n        project_path = config_path.parent\n        dlc_project_path = os.environ[\"DLC_PROJECT_PATH\"]\n        if dlc_project_path not in project_path.as_posix():\n            project_dirname = project_path.name\n            dest_folder = Path(f\"{dlc_project_path}/{project_dirname}/\")\n            if dest_folder.exists():\n                new_proj_dir = dest_folder.as_posix()\n            else:\n                new_proj_dir = shutil.copytree(\n                    src=project_path,\n                    dst=f\"{dlc_project_path}/{project_dirname}/\",\n                )\n            new_config_path = Path(f\"{new_proj_dir}/config.yaml\")\n            assert (\n                new_config_path.exists()\n            ), \"config.yaml does not exist in new project directory\"\n            config_path = new_config_path\n            add_to_config(config_path, **{\"project_path\": new_proj_dir})\n        # TODO still need to copy videos over to video dir\n        key = {\n            \"project_name\": project_name,\n            \"team_name\": lab_team,\n            \"bodyparts\": bodyparts,\n            \"config_path\": config_path.as_posix(),\n            \"frames_per_video\": frames_per_video,\n        }\n        cls.insert1(key, **kwargs)\n        cls.BodyPart.insert(\n            [\n                {\"project_name\": project_name, \"bodypart\": bp}\n                for bp in all_bodyparts\n            ],\n            **kwargs,\n        )\n        if add_to_files:\n            del key[\"bodyparts\"]\n            del key[\"team_name\"]\n            del key[\"config_path\"]\n            del key[\"frames_per_video\"]\n            # Check for training files to add\n            cls.add_training_files(key, **kwargs)\n        return {\n            \"project_name\": project_name,\n            \"config_path\": config_path.as_posix(),\n        }\n\n    @classmethod\n    def insert_new_project(\n        cls,\n        project_name: str,\n        bodyparts: List,\n        lab_team: str,\n        frames_per_video: int,\n        video_list: List,\n        project_directory: str = os.getenv(\"DLC_PROJECT_PATH\"),\n        output_path: str = os.getenv(\"DLC_VIDEO_PATH\"),\n        set_permissions=False,\n        **kwargs,\n    ):\n\"\"\"\n        insert a new project into DLCProject table.\n        Parameters\n        ----------\n        project_name : str\n            user-friendly name of project\n        bodyparts : list\n            list of bodyparts to label. Should match bodyparts in BodyPart table\n        lab_team : str\n            name of lab team. Should match an entry in LabTeam table\n        project_directory : str\n            directory where to create project.\n            (Default is '/cumulus/deeplabcut/')\n        frames_per_video : int\n            number of frames to extract from each video\n        video_list : list\n            list of dicts of form [{'nwb_file_name': nwb_file_name, 'epoch': epoch #},...]\n            to query VideoFile table for videos to train on.\n            Can also be list of absolute paths to import videos from\n        output_path : str\n            target path to output converted videos\n            (Default is '/nimbus/deeplabcut/videos/')\n        set_permissions : bool\n            if True, will set permissions for user and group to be read+write\n            (Default is False)\n        \"\"\"\n        project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n        if project_name in project_names_in_use:\n            print(f\"project name: {project_name} is already in use.\")\n            return_key = {}\n            return_key[\"project_name\"], return_key[\"config_path\"] = (\n                cls &amp; {\"project_name\": project_name}\n            ).fetch1(\"project_name\", \"config_path\")\n            return return_key\n\n        add_to_files = kwargs.pop(\"add_to_files\", True)\n        if not bool(LabTeam() &amp; {\"team_name\": lab_team}):\n            raise ValueError(f\"team_name: {lab_team} does not exist in LabTeam\")\n        skeleton_node = None\n        # If dict, assume of form {'nwb_file_name': nwb_file_name, 'epoch': epoch}\n        # and pass to get_video_path to reference VideoFile table for path\n\n        if all(isinstance(n, Dict) for n in video_list):\n            videos_to_convert = [\n                get_video_path(video_key) for video_key in video_list\n            ]\n            videos = [\n                check_videofile(\n                    video_path=video[0],\n                    output_path=output_path,\n                    video_filename=video[1],\n                )[0].as_posix()\n                for video in videos_to_convert\n            ]\n        # If not dict, assume list of video file paths that may or may not need to be converted\n        else:\n            videos = []\n            if not all([Path(video).exists() for video in video_list]):\n                raise OSError(\"at least one file in video_list does not exist\")\n            for video in video_list:\n                video_path = Path(video).parent\n                video_filename = video.rsplit(\n                    video_path.as_posix(), maxsplit=1\n                )[-1].split(\"/\")[-1]\n                videos.extend(\n                    [\n                        check_videofile(\n                            video_path=video_path,\n                            output_path=output_path,\n                            video_filename=video_filename,\n                        )[0].as_posix()\n                    ]\n                )\n            if len(videos) &lt; 1:\n                raise ValueError(f\"no .mp4 videos found in{video_path}\")\n        from deeplabcut import create_new_project\n\n        config_path = create_new_project(\n            project_name,\n            lab_team,\n            videos,\n            working_directory=project_directory,\n            copy_videos=True,\n            multianimal=False,\n        )\n        for bodypart in bodyparts:\n            if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n                raise ValueError(\n                    f\"bodypart: {bodypart} not found in BodyPart table\"\n                )\n        kwargs_copy = copy.deepcopy(kwargs)\n        kwargs_copy.update({\"numframes2pick\": frames_per_video, \"dotsize\": 3})\n        add_to_config(\n            config_path, bodyparts, skeleton_node=skeleton_node, **kwargs_copy\n        )\n        key = {\n            \"project_name\": project_name,\n            \"team_name\": lab_team,\n            \"bodyparts\": bodyparts,\n            \"config_path\": config_path,\n            \"frames_per_video\": frames_per_video,\n        }\n        # TODO: make permissions setting more flexible.\n        if set_permissions:\n            permissions = (\n                stat.S_IRUSR\n                | stat.S_IWUSR\n                | stat.S_IRGRP\n                | stat.S_IWGRP\n                | stat.S_IROTH\n            )\n            username = getpass.getuser()\n            if not groupname:\n                groupname = username\n            _set_permissions(\n                directory=project_directory,\n                mode=permissions,\n                username=username,\n                groupname=groupname,\n            )\n        cls.insert1(key, **kwargs)\n        cls.BodyPart.insert(\n            [\n                {\"project_name\": project_name, \"bodypart\": bp}\n                for bp in bodyparts\n            ],\n            **kwargs,\n        )\n        if add_to_files:\n            del key[\"bodyparts\"]\n            del key[\"team_name\"]\n            del key[\"config_path\"]\n            del key[\"frames_per_video\"]\n            # Add videos to training files\n            cls.add_training_files(key, **kwargs)\n        if isinstance(config_path, PosixPath):\n            config_path = config_path.as_posix()\n        return {\"project_name\": project_name, \"config_path\": config_path}\n\n    @classmethod\n    def add_training_files(cls, key, **kwargs):\n\"\"\"Add training videos and labeled frames .h5 and .csv to DLCProject.File\"\"\"\n        config_path = (cls &amp; {\"project_name\": key[\"project_name\"]}).fetch1(\n            \"config_path\"\n        )\n        from deeplabcut.utils.auxiliaryfunctions import read_config\n\n        if \"config_path\" in key:\n            del key[\"config_path\"]\n        cfg = read_config(config_path)\n        video_names = list(cfg[\"video_sets\"].keys())\n        training_files = []\n        for video in video_names:\n            video_name = os.path.splitext(\n                video.split(os.path.dirname(video) + \"/\")[-1]\n            )[0]\n            training_files.extend(\n                glob.glob(\n                    f\"{cfg['project_path']}/labeled-data/{video_name}/*Collected*\"\n                )\n            )\n        for video in video_names:\n            key[\"file_name\"] = f'{os.path.splitext(video.split(\"/\")[-1])[0]}'\n            key[\"file_ext\"] = os.path.splitext(video.split(\"/\")[-1])[-1].split(\n                \".\"\n            )[-1]\n            key[\"file_path\"] = video\n            cls.File.insert1(key, **kwargs)\n        if len(training_files) &gt; 0:\n            for file in training_files:\n                video_name = os.path.dirname(file).split(\"/\")[-1]\n                file_type = os.path.splitext(\n                    file.split(os.path.dirname(file) + \"/\")[-1]\n                )[-1].split(\".\")[-1]\n                key[\"file_name\"] = f\"{video_name}_labeled_data\"\n                key[\"file_ext\"] = file_type\n                key[\"file_path\"] = file\n                cls.File.insert1(key, **kwargs)\n        else:\n            Warning(\"No training files to add\")\n\n    @classmethod\n    def run_extract_frames(cls, key, **kwargs):\n\"\"\"Convenience function to launch DLC GUI for extracting frames.\n        Must be run on local machine to access GUI,\n        cannot be run through ssh tunnel\n        \"\"\"\n        config_path = (cls &amp; key).fetch1(\"config_path\")\n        from deeplabcut import extract_frames\n\n        extract_frames(config_path, **kwargs)\n\n    @classmethod\n    def run_label_frames(cls, key):\n\"\"\"Convenience function to launch DLC GUI for labeling frames.\n        Must be run on local machine to access GUI,\n        cannot be run through ssh tunnel\n        \"\"\"\n        config_path = (cls &amp; key).fetch1(\"config_path\")\n        from deeplabcut import label_frames\n\n        label_frames(config_path)\n\n    @classmethod\n    def check_labels(cls, key, **kwargs):\n\"\"\"Convenience function to check labels on\n        previously extracted and labeled frames\n        \"\"\"\n        config_path = (cls &amp; key).fetch1(\"config_path\")\n        from deeplabcut import check_labels\n\n        check_labels(config_path, **kwargs)\n\n    @classmethod\n    def import_labeled_frames(\n        cls,\n        key: Dict,\n        import_project_path: Union[str, PosixPath],\n        video_filenames: Union[str, List],\n        **kwargs,\n    ):\n\"\"\"Function to import pre-labeled frames from an existing project into a new project\n\n        Parameters\n        ----------\n        key : Dict\n            key to specify entry in DLCProject table to add labeled frames to\n        import_project_path : str\n            absolute path to project directory containing labeled frames to import\n        video_filenames : str or List\n            filename or list of filenames of video(s) from which to import frames.\n            without file extension\n        \"\"\"\n        project_entry = (cls &amp; key).fetch1()\n        team_name = project_entry[\"team_name\"]\n        current_project_path = Path(project_entry[\"config_path\"]).parent\n        current_labeled_data_path = Path(\n            f\"{current_project_path.as_posix()}/labeled-data\"\n        )\n        if isinstance(import_project_path, PosixPath):\n            assert (\n                import_project_path.exists()\n            ), f\"import_project_path: {import_project_path.as_posix()} does not exist\"\n            import_labeled_data_path = Path(\n                f\"{import_project_path.as_posix()}/labeled-data\"\n            )\n        else:\n            assert Path(\n                import_project_path\n            ).exists(), (\n                f\"import_project_path: {import_project_path} does not exist\"\n            )\n            import_labeled_data_path = Path(\n                f\"{import_project_path}/labeled-data\"\n            )\n        assert (\n            import_labeled_data_path.exists()\n        ), \"import_project has no directory 'labeled-data'\"\n        if not isinstance(video_filenames, List):\n            video_filenames = [video_filenames]\n        for video_file in video_filenames:\n            h5_file = glob.glob(\n                f\"{import_labeled_data_path.as_posix()}/{video_file}/*.h5\"\n            )[0]\n            dlc_df = pd.read_hdf(h5_file)\n            dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n            dlc_df.to_hdf(\n                Path(\n                    f\"{current_labeled_data_path.as_posix()}/{video_file}/CollectedData_{team_name}.h5\"\n                ).as_posix(),\n                \"df_with_missing\",\n            )\n        cls.add_training_files(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_project.DLCProject.BodyPart", "title": "<code>BodyPart</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Part table to hold bodyparts used in each project.</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>class BodyPart(dj.Part):\n\"\"\"Part table to hold bodyparts used in each project.\"\"\"\n\n    definition = \"\"\"\n    -&gt; DLCProject\n    -&gt; BodyPart\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_project.DLCProject.insert_existing_project", "title": "<code>insert_existing_project(project_name, lab_team, config_path, bodyparts=None, frames_per_video=None, add_to_files=True, **kwargs)</code>  <code>classmethod</code>", "text": "<p>insert an existing project into DLCProject table.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>user-friendly name of project</p> required <code>lab_team</code> <code>str</code> <p>name of lab team. Should match an entry in LabTeam table</p> required <code>config_path</code> <code>str</code> <p>path to project directory</p> required <code>bodyparts</code> <code>list</code> <p>optional list of bodyparts to label that are not already in existing config</p> <code>None</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef insert_existing_project(\n    cls,\n    project_name: str,\n    lab_team: str,\n    config_path: str,\n    bodyparts: List = None,\n    frames_per_video: int = None,\n    add_to_files: bool = True,\n    **kwargs,\n):\n\"\"\"\n    insert an existing project into DLCProject table.\n    Parameters\n    ----------\n    project_name : str\n        user-friendly name of project\n    lab_team : str\n        name of lab team. Should match an entry in LabTeam table\n    config_path : str\n        path to project directory\n    bodyparts : list\n        optional list of bodyparts to label that\n        are not already in existing config\n    \"\"\"\n\n    # Read config\n    project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n    if project_name in project_names_in_use:\n        print(f\"project name: {project_name} is already in use.\")\n        return_key = {}\n        return_key[\"project_name\"], return_key[\"config_path\"] = (\n            cls &amp; {\"project_name\": project_name}\n        ).fetch1(\"project_name\", \"config_path\")\n        return return_key\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    cfg = read_config(config_path)\n    if bodyparts:\n        bodyparts_to_add = [\n            bodypart\n            for bodypart in bodyparts\n            if bodypart not in cfg[\"bodyparts\"]\n        ]\n        all_bodyparts = bodyparts_to_add + cfg[\"bodyparts\"]\n    else:\n        all_bodyparts = cfg[\"bodyparts\"]\n    BodyPart.add_from_config(cfg[\"bodyparts\"])\n    for bodypart in all_bodyparts:\n        if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n            raise ValueError(\n                f\"bodypart: {bodypart} not found in BodyPart table\"\n            )\n    # check bodyparts are in config, if not add\n    if len(bodyparts_to_add) &gt; 0:\n        add_to_config(config_path, bodyparts=bodyparts_to_add)\n    # Get frames per video from config. If passed as arg, check match\n    if frames_per_video:\n        if frames_per_video != cfg[\"numframes2pick\"]:\n            add_to_config(\n                config_path, **{\"numframes2pick\": frames_per_video}\n            )\n    config_path = Path(config_path)\n    project_path = config_path.parent\n    dlc_project_path = os.environ[\"DLC_PROJECT_PATH\"]\n    if dlc_project_path not in project_path.as_posix():\n        project_dirname = project_path.name\n        dest_folder = Path(f\"{dlc_project_path}/{project_dirname}/\")\n        if dest_folder.exists():\n            new_proj_dir = dest_folder.as_posix()\n        else:\n            new_proj_dir = shutil.copytree(\n                src=project_path,\n                dst=f\"{dlc_project_path}/{project_dirname}/\",\n            )\n        new_config_path = Path(f\"{new_proj_dir}/config.yaml\")\n        assert (\n            new_config_path.exists()\n        ), \"config.yaml does not exist in new project directory\"\n        config_path = new_config_path\n        add_to_config(config_path, **{\"project_path\": new_proj_dir})\n    # TODO still need to copy videos over to video dir\n    key = {\n        \"project_name\": project_name,\n        \"team_name\": lab_team,\n        \"bodyparts\": bodyparts,\n        \"config_path\": config_path.as_posix(),\n        \"frames_per_video\": frames_per_video,\n    }\n    cls.insert1(key, **kwargs)\n    cls.BodyPart.insert(\n        [\n            {\"project_name\": project_name, \"bodypart\": bp}\n            for bp in all_bodyparts\n        ],\n        **kwargs,\n    )\n    if add_to_files:\n        del key[\"bodyparts\"]\n        del key[\"team_name\"]\n        del key[\"config_path\"]\n        del key[\"frames_per_video\"]\n        # Check for training files to add\n        cls.add_training_files(key, **kwargs)\n    return {\n        \"project_name\": project_name,\n        \"config_path\": config_path.as_posix(),\n    }\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_project.DLCProject.insert_new_project", "title": "<code>insert_new_project(project_name, bodyparts, lab_team, frames_per_video, video_list, project_directory=os.getenv('DLC_PROJECT_PATH'), output_path=os.getenv('DLC_VIDEO_PATH'), set_permissions=False, **kwargs)</code>  <code>classmethod</code>", "text": "<p>insert a new project into DLCProject table.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>user-friendly name of project</p> required <code>bodyparts</code> <code>list</code> <p>list of bodyparts to label. Should match bodyparts in BodyPart table</p> required <code>lab_team</code> <code>str</code> <p>name of lab team. Should match an entry in LabTeam table</p> required <code>project_directory</code> <code>str</code> <p>directory where to create project. (Default is '/cumulus/deeplabcut/')</p> <code>os.getenv('DLC_PROJECT_PATH')</code> <code>frames_per_video</code> <code>int</code> <p>number of frames to extract from each video</p> required <code>video_list</code> <code>list</code> <p>list of dicts of form [{'nwb_file_name': nwb_file_name, 'epoch': epoch #},...] to query VideoFile table for videos to train on. Can also be list of absolute paths to import videos from</p> required <code>output_path</code> <code>str</code> <p>target path to output converted videos (Default is '/nimbus/deeplabcut/videos/')</p> <code>os.getenv('DLC_VIDEO_PATH')</code> <code>set_permissions</code> <code>bool</code> <p>if True, will set permissions for user and group to be read+write (Default is False)</p> <code>False</code> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef insert_new_project(\n    cls,\n    project_name: str,\n    bodyparts: List,\n    lab_team: str,\n    frames_per_video: int,\n    video_list: List,\n    project_directory: str = os.getenv(\"DLC_PROJECT_PATH\"),\n    output_path: str = os.getenv(\"DLC_VIDEO_PATH\"),\n    set_permissions=False,\n    **kwargs,\n):\n\"\"\"\n    insert a new project into DLCProject table.\n    Parameters\n    ----------\n    project_name : str\n        user-friendly name of project\n    bodyparts : list\n        list of bodyparts to label. Should match bodyparts in BodyPart table\n    lab_team : str\n        name of lab team. Should match an entry in LabTeam table\n    project_directory : str\n        directory where to create project.\n        (Default is '/cumulus/deeplabcut/')\n    frames_per_video : int\n        number of frames to extract from each video\n    video_list : list\n        list of dicts of form [{'nwb_file_name': nwb_file_name, 'epoch': epoch #},...]\n        to query VideoFile table for videos to train on.\n        Can also be list of absolute paths to import videos from\n    output_path : str\n        target path to output converted videos\n        (Default is '/nimbus/deeplabcut/videos/')\n    set_permissions : bool\n        if True, will set permissions for user and group to be read+write\n        (Default is False)\n    \"\"\"\n    project_names_in_use = np.unique(cls.fetch(\"project_name\"))\n    if project_name in project_names_in_use:\n        print(f\"project name: {project_name} is already in use.\")\n        return_key = {}\n        return_key[\"project_name\"], return_key[\"config_path\"] = (\n            cls &amp; {\"project_name\": project_name}\n        ).fetch1(\"project_name\", \"config_path\")\n        return return_key\n\n    add_to_files = kwargs.pop(\"add_to_files\", True)\n    if not bool(LabTeam() &amp; {\"team_name\": lab_team}):\n        raise ValueError(f\"team_name: {lab_team} does not exist in LabTeam\")\n    skeleton_node = None\n    # If dict, assume of form {'nwb_file_name': nwb_file_name, 'epoch': epoch}\n    # and pass to get_video_path to reference VideoFile table for path\n\n    if all(isinstance(n, Dict) for n in video_list):\n        videos_to_convert = [\n            get_video_path(video_key) for video_key in video_list\n        ]\n        videos = [\n            check_videofile(\n                video_path=video[0],\n                output_path=output_path,\n                video_filename=video[1],\n            )[0].as_posix()\n            for video in videos_to_convert\n        ]\n    # If not dict, assume list of video file paths that may or may not need to be converted\n    else:\n        videos = []\n        if not all([Path(video).exists() for video in video_list]):\n            raise OSError(\"at least one file in video_list does not exist\")\n        for video in video_list:\n            video_path = Path(video).parent\n            video_filename = video.rsplit(\n                video_path.as_posix(), maxsplit=1\n            )[-1].split(\"/\")[-1]\n            videos.extend(\n                [\n                    check_videofile(\n                        video_path=video_path,\n                        output_path=output_path,\n                        video_filename=video_filename,\n                    )[0].as_posix()\n                ]\n            )\n        if len(videos) &lt; 1:\n            raise ValueError(f\"no .mp4 videos found in{video_path}\")\n    from deeplabcut import create_new_project\n\n    config_path = create_new_project(\n        project_name,\n        lab_team,\n        videos,\n        working_directory=project_directory,\n        copy_videos=True,\n        multianimal=False,\n    )\n    for bodypart in bodyparts:\n        if not bool(BodyPart() &amp; {\"bodypart\": bodypart}):\n            raise ValueError(\n                f\"bodypart: {bodypart} not found in BodyPart table\"\n            )\n    kwargs_copy = copy.deepcopy(kwargs)\n    kwargs_copy.update({\"numframes2pick\": frames_per_video, \"dotsize\": 3})\n    add_to_config(\n        config_path, bodyparts, skeleton_node=skeleton_node, **kwargs_copy\n    )\n    key = {\n        \"project_name\": project_name,\n        \"team_name\": lab_team,\n        \"bodyparts\": bodyparts,\n        \"config_path\": config_path,\n        \"frames_per_video\": frames_per_video,\n    }\n    # TODO: make permissions setting more flexible.\n    if set_permissions:\n        permissions = (\n            stat.S_IRUSR\n            | stat.S_IWUSR\n            | stat.S_IRGRP\n            | stat.S_IWGRP\n            | stat.S_IROTH\n        )\n        username = getpass.getuser()\n        if not groupname:\n            groupname = username\n        _set_permissions(\n            directory=project_directory,\n            mode=permissions,\n            username=username,\n            groupname=groupname,\n        )\n    cls.insert1(key, **kwargs)\n    cls.BodyPart.insert(\n        [\n            {\"project_name\": project_name, \"bodypart\": bp}\n            for bp in bodyparts\n        ],\n        **kwargs,\n    )\n    if add_to_files:\n        del key[\"bodyparts\"]\n        del key[\"team_name\"]\n        del key[\"config_path\"]\n        del key[\"frames_per_video\"]\n        # Add videos to training files\n        cls.add_training_files(key, **kwargs)\n    if isinstance(config_path, PosixPath):\n        config_path = config_path.as_posix()\n    return {\"project_name\": project_name, \"config_path\": config_path}\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_project.DLCProject.add_training_files", "title": "<code>add_training_files(key, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Add training videos and labeled frames .h5 and .csv to DLCProject.File</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef add_training_files(cls, key, **kwargs):\n\"\"\"Add training videos and labeled frames .h5 and .csv to DLCProject.File\"\"\"\n    config_path = (cls &amp; {\"project_name\": key[\"project_name\"]}).fetch1(\n        \"config_path\"\n    )\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    if \"config_path\" in key:\n        del key[\"config_path\"]\n    cfg = read_config(config_path)\n    video_names = list(cfg[\"video_sets\"].keys())\n    training_files = []\n    for video in video_names:\n        video_name = os.path.splitext(\n            video.split(os.path.dirname(video) + \"/\")[-1]\n        )[0]\n        training_files.extend(\n            glob.glob(\n                f\"{cfg['project_path']}/labeled-data/{video_name}/*Collected*\"\n            )\n        )\n    for video in video_names:\n        key[\"file_name\"] = f'{os.path.splitext(video.split(\"/\")[-1])[0]}'\n        key[\"file_ext\"] = os.path.splitext(video.split(\"/\")[-1])[-1].split(\n            \".\"\n        )[-1]\n        key[\"file_path\"] = video\n        cls.File.insert1(key, **kwargs)\n    if len(training_files) &gt; 0:\n        for file in training_files:\n            video_name = os.path.dirname(file).split(\"/\")[-1]\n            file_type = os.path.splitext(\n                file.split(os.path.dirname(file) + \"/\")[-1]\n            )[-1].split(\".\")[-1]\n            key[\"file_name\"] = f\"{video_name}_labeled_data\"\n            key[\"file_ext\"] = file_type\n            key[\"file_path\"] = file\n            cls.File.insert1(key, **kwargs)\n    else:\n        Warning(\"No training files to add\")\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_project.DLCProject.run_extract_frames", "title": "<code>run_extract_frames(key, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Convenience function to launch DLC GUI for extracting frames. Must be run on local machine to access GUI, cannot be run through ssh tunnel</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef run_extract_frames(cls, key, **kwargs):\n\"\"\"Convenience function to launch DLC GUI for extracting frames.\n    Must be run on local machine to access GUI,\n    cannot be run through ssh tunnel\n    \"\"\"\n    config_path = (cls &amp; key).fetch1(\"config_path\")\n    from deeplabcut import extract_frames\n\n    extract_frames(config_path, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_project.DLCProject.run_label_frames", "title": "<code>run_label_frames(key)</code>  <code>classmethod</code>", "text": "<p>Convenience function to launch DLC GUI for labeling frames. Must be run on local machine to access GUI, cannot be run through ssh tunnel</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef run_label_frames(cls, key):\n\"\"\"Convenience function to launch DLC GUI for labeling frames.\n    Must be run on local machine to access GUI,\n    cannot be run through ssh tunnel\n    \"\"\"\n    config_path = (cls &amp; key).fetch1(\"config_path\")\n    from deeplabcut import label_frames\n\n    label_frames(config_path)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_project.DLCProject.check_labels", "title": "<code>check_labels(key, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Convenience function to check labels on previously extracted and labeled frames</p> Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef check_labels(cls, key, **kwargs):\n\"\"\"Convenience function to check labels on\n    previously extracted and labeled frames\n    \"\"\"\n    config_path = (cls &amp; key).fetch1(\"config_path\")\n    from deeplabcut import check_labels\n\n    check_labels(config_path, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_project.DLCProject.import_labeled_frames", "title": "<code>import_labeled_frames(key, import_project_path, video_filenames, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Function to import pre-labeled frames from an existing project into a new project</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Dict</code> <p>key to specify entry in DLCProject table to add labeled frames to</p> required <code>import_project_path</code> <code>str</code> <p>absolute path to project directory containing labeled frames to import</p> required <code>video_filenames</code> <code>str or List</code> <p>filename or list of filenames of video(s) from which to import frames. without file extension</p> required Source code in <code>src/spyglass/position/v1/position_dlc_project.py</code> <pre><code>@classmethod\ndef import_labeled_frames(\n    cls,\n    key: Dict,\n    import_project_path: Union[str, PosixPath],\n    video_filenames: Union[str, List],\n    **kwargs,\n):\n\"\"\"Function to import pre-labeled frames from an existing project into a new project\n\n    Parameters\n    ----------\n    key : Dict\n        key to specify entry in DLCProject table to add labeled frames to\n    import_project_path : str\n        absolute path to project directory containing labeled frames to import\n    video_filenames : str or List\n        filename or list of filenames of video(s) from which to import frames.\n        without file extension\n    \"\"\"\n    project_entry = (cls &amp; key).fetch1()\n    team_name = project_entry[\"team_name\"]\n    current_project_path = Path(project_entry[\"config_path\"]).parent\n    current_labeled_data_path = Path(\n        f\"{current_project_path.as_posix()}/labeled-data\"\n    )\n    if isinstance(import_project_path, PosixPath):\n        assert (\n            import_project_path.exists()\n        ), f\"import_project_path: {import_project_path.as_posix()} does not exist\"\n        import_labeled_data_path = Path(\n            f\"{import_project_path.as_posix()}/labeled-data\"\n        )\n    else:\n        assert Path(\n            import_project_path\n        ).exists(), (\n            f\"import_project_path: {import_project_path} does not exist\"\n        )\n        import_labeled_data_path = Path(\n            f\"{import_project_path}/labeled-data\"\n        )\n    assert (\n        import_labeled_data_path.exists()\n    ), \"import_project has no directory 'labeled-data'\"\n    if not isinstance(video_filenames, List):\n        video_filenames = [video_filenames]\n    for video_file in video_filenames:\n        h5_file = glob.glob(\n            f\"{import_labeled_data_path.as_posix()}/{video_file}/*.h5\"\n        )[0]\n        dlc_df = pd.read_hdf(h5_file)\n        dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n        dlc_df.to_hdf(\n            Path(\n                f\"{current_labeled_data_path.as_posix()}/{video_file}/CollectedData_{team_name}.h5\"\n            ).as_posix(),\n            \"df_with_missing\",\n        )\n    cls.add_training_files(key, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_training.OutputLogger", "title": "<code>OutputLogger</code>", "text": "<p>A class to wrap a logging.Logger object in order to provide context manager capabilities.</p> <p>This class uses contextlib.redirect_stdout to temporarily redirect sys.stdout and thus print statements to the log file instead of, or as well as the console.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <code>logging.Logger</code> <p>logger object</p> <code>name</code> <code>str</code> <p>name of logger</p> <code>level</code> <code>int</code> <p>level of logging that the logger is set to handle</p>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_training.OutputLogger--methods", "title": "Methods", "text": "<p>setup_logger(name_logfile, path_logfile, print_console=False)     initialize or get logger object with name_logfile     that writes to path_logfile</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with OutputLogger(name, path, print_console=True) as logger:\n...    print(\"this will print to logfile\")\n...    logger.logger.info(\"this will log to the logfile\")\n... print(\"this will print to the console\")\n... logger.logger.info(\"this will log to the logfile\")\n</code></pre> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>class OutputLogger:\n\"\"\"\n    A class to wrap a logging.Logger object in order to provide context manager capabilities.\n\n    This class uses contextlib.redirect_stdout to temporarily redirect sys.stdout and thus\n    print statements to the log file instead of, or as well as the console.\n\n    Attributes\n    ----------\n    logger : logging.Logger\n        logger object\n    name : str\n        name of logger\n    level : int\n        level of logging that the logger is set to handle\n\n    Methods\n    -------\n    setup_logger(name_logfile, path_logfile, print_console=False)\n        initialize or get logger object with name_logfile\n        that writes to path_logfile\n\n    Examples\n    --------\n    &gt;&gt;&gt; with OutputLogger(name, path, print_console=True) as logger:\n    ...    print(\"this will print to logfile\")\n    ...    logger.logger.info(\"this will log to the logfile\")\n    ... print(\"this will print to the console\")\n    ... logger.logger.info(\"this will log to the logfile\")\n\n    \"\"\"\n\n    def __init__(self, name, path, level=\"INFO\", **kwargs):\n        self.logger = self.setup_logger(name, path, **kwargs)\n        self.name = self.logger.name\n        self.level = getattr(logging, level)\n\n    def setup_logger(\n        self, name_logfile, path_logfile, print_console=False\n    ) -&gt; logging.Logger:\n\"\"\"\n        Sets up a logger for that outputs to a file, and optionally, the console\n\n        Parameters\n        ----------\n        name_logfile : str\n            name of the logfile to use\n        path_logfile : str\n            path to the file that should be used as the file handler\n        print_console : bool, default-False\n            if True, prints to console as well as log file.\n\n        Returns\n        -------\n        logger : logging.Logger\n            the logger object with specified handlers\n        \"\"\"\n\n        logger = logging.getLogger(name_logfile)\n        # check to see if handlers already exist for this logger\n        if logger.handlers:\n            for handler in logger.handlers:\n                # if it's a file handler\n                # type is used instead of isinstance,\n                # which doesn't work properly with logging.StreamHandler\n                if type(handler) == logging.FileHandler:\n                    # if paths don't match, change file handler path\n                    if not os.path.samefile(handler.baseFilename, path_logfile):\n                        handler.close()\n                        logger.removeHandler(handler)\n                        file_handler = self._get_file_handler(path_logfile)\n                        logger.addHandler(file_handler)\n                # if a stream handler exists and\n                # if print_console is False remove streamHandler\n                if type(handler) == logging.StreamHandler:\n                    if not print_console:\n                        handler.close()\n                        logger.removeHandler(handler)\n            if print_console and not any(\n                type(handler) == logging.StreamHandler\n                for handler in logger.handlers\n            ):\n                logger.addHandler(self._get_stream_handler())\n\n        else:\n            file_handler = self._get_file_handler(path_logfile)\n            logger.addHandler(file_handler)\n            if print_console:\n                logger.addHandler(self._get_stream_handler())\n        logger.setLevel(logging.INFO)\n        return logger\n\n    def _get_file_handler(self, path):\n        output_dir = pathlib.Path(os.path.dirname(path))\n        if not os.path.exists(output_dir):\n            output_dir.mkdir(parents=True, exist_ok=True)\n        file_handler = logging.FileHandler(path, mode=\"a\")\n        file_handler.setFormatter(self._get_formatter())\n        return file_handler\n\n    def _get_stream_handler(self):\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(self._get_formatter())\n        return stream_handler\n\n    def _get_formatter(self):\n        return logging.Formatter(\n            \"[%(asctime)s] in %(pathname)s, line %(lineno)d: %(message)s\",\n            datefmt=\"%d-%b-%y %H:%M:%S\",\n        )\n\n    def write(self, msg):\n        if msg and not msg.isspace():\n            self.logger.log(self.level, msg)\n\n    def flush(self):\n        pass\n\n    def __enter__(self):\n        self._redirector = redirect_stdout(self)\n        self._redirector.__enter__()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        # let contextlib do any exception handling here\n        self._redirector.__exit__(exc_type, exc_value, traceback)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.dlc_utils.OutputLogger.setup_logger", "title": "<code>setup_logger(name_logfile, path_logfile, print_console=False)</code>", "text": "<p>Sets up a logger for that outputs to a file, and optionally, the console</p> <p>Parameters:</p> Name Type Description Default <code>name_logfile</code> <code>str</code> <p>name of the logfile to use</p> required <code>path_logfile</code> <code>str</code> <p>path to the file that should be used as the file handler</p> required <code>print_console</code> <code>bool, default-False</code> <p>if True, prints to console as well as log file.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>logger</code> <code>logging.Logger</code> <p>the logger object with specified handlers</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def setup_logger(\n    self, name_logfile, path_logfile, print_console=False\n) -&gt; logging.Logger:\n\"\"\"\n    Sets up a logger for that outputs to a file, and optionally, the console\n\n    Parameters\n    ----------\n    name_logfile : str\n        name of the logfile to use\n    path_logfile : str\n        path to the file that should be used as the file handler\n    print_console : bool, default-False\n        if True, prints to console as well as log file.\n\n    Returns\n    -------\n    logger : logging.Logger\n        the logger object with specified handlers\n    \"\"\"\n\n    logger = logging.getLogger(name_logfile)\n    # check to see if handlers already exist for this logger\n    if logger.handlers:\n        for handler in logger.handlers:\n            # if it's a file handler\n            # type is used instead of isinstance,\n            # which doesn't work properly with logging.StreamHandler\n            if type(handler) == logging.FileHandler:\n                # if paths don't match, change file handler path\n                if not os.path.samefile(handler.baseFilename, path_logfile):\n                    handler.close()\n                    logger.removeHandler(handler)\n                    file_handler = self._get_file_handler(path_logfile)\n                    logger.addHandler(file_handler)\n            # if a stream handler exists and\n            # if print_console is False remove streamHandler\n            if type(handler) == logging.StreamHandler:\n                if not print_console:\n                    handler.close()\n                    logger.removeHandler(handler)\n        if print_console and not any(\n            type(handler) == logging.StreamHandler\n            for handler in logger.handlers\n        ):\n            logger.addHandler(self._get_stream_handler())\n\n    else:\n        file_handler = self._get_file_handler(path_logfile)\n        logger.addHandler(file_handler)\n        if print_console:\n            logger.addHandler(self._get_stream_handler())\n    logger.setLevel(logging.INFO)\n    return logger\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_training.DLCModelTraining", "title": "<code>DLCModelTraining</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/position/v1/position_dlc_training.py</code> <pre><code>@schema\nclass DLCModelTraining(dj.Computed):\n    definition = \"\"\"\n    -&gt; DLCModelTrainingSelection\n    ---\n    project_path         : varchar(255) # Path to project directory\n    latest_snapshot: int unsigned # latest exact snapshot index (i.e., never -1)\n    config_template: longblob     # stored full config file\n    \"\"\"\n\n    # To continue from previous training snapshot, devs suggest editing pose_cfg.yml\n    # https://github.com/DeepLabCut/DeepLabCut/issues/70\n\n    def make(self, key):\n\"\"\"Launch training for each entry in DLCModelTrainingSelection via `.populate()`.\"\"\"\n        model_prefix = (DLCModelTrainingSelection &amp; key).fetch1(\"model_prefix\")\n        from deeplabcut import create_training_dataset, train_network\n        from deeplabcut.utils.auxiliaryfunctions import read_config\n\n        from . import dlc_reader\n\n        try:\n            from deeplabcut.utils.auxiliaryfunctions import get_model_folder\n        except ImportError:\n            from deeplabcut.utils.auxiliaryfunctions import (\n                GetModelFolder as get_model_folder,\n            )\n        config_path, project_name = (DLCProject() &amp; key).fetch1(\n            \"config_path\", \"project_name\"\n        )\n        with OutputLogger(\n            name=\"DLC_project_{project_name}_training\",\n            path=f\"{os.path.dirname(config_path)}/log.log\",\n            print_console=True,\n        ) as logger:\n            dlc_config = read_config(config_path)\n            project_path = dlc_config[\"project_path\"]\n            key[\"project_path\"] = project_path\n            # ---- Build and save DLC configuration (yaml) file ----\n            _, dlc_config = dlc_reader.read_yaml(project_path)\n            if not dlc_config:\n                dlc_config = read_config(config_path)\n            dlc_config.update((DLCModelTrainingParams &amp; key).fetch1(\"params\"))\n            dlc_config.update(\n                {\n                    \"project_path\": Path(project_path).as_posix(),\n                    \"modelprefix\": model_prefix,\n                    \"train_fraction\": dlc_config[\"TrainingFraction\"][\n                        int(dlc_config[\"trainingsetindex\"])\n                    ],\n                    \"training_filelist_datajoint\": [  # don't overwrite origin video_sets\n                        Path(fp).as_posix()\n                        for fp in (DLCProject.File &amp; key).fetch(\"file_path\")\n                    ],\n                }\n            )\n            # Write dlc config file to base project folder\n            # TODO: need to make sure this will work\n            dlc_cfg_filepath = dlc_reader.save_yaml(project_path, dlc_config)\n            # ---- create training dataset ----\n            training_dataset_input_args = list(\n                inspect.signature(create_training_dataset).parameters\n            )\n            training_dataset_kwargs = {\n                k: v\n                for k, v in dlc_config.items()\n                if k in training_dataset_input_args\n            }\n            logger.logger.info(\"creating training dataset\")\n            create_training_dataset(dlc_cfg_filepath, **training_dataset_kwargs)\n            # ---- Trigger DLC model training job ----\n            train_network_input_args = list(\n                inspect.signature(train_network).parameters\n            )\n            train_network_kwargs = {\n                k: v\n                for k, v in dlc_config.items()\n                if k in train_network_input_args\n            }\n            for k in [\"shuffle\", \"trainingsetindex\", \"maxiters\"]:\n                if k in train_network_kwargs:\n                    train_network_kwargs[k] = int(train_network_kwargs[k])\n            try:\n                train_network(dlc_cfg_filepath, **train_network_kwargs)\n            except (\n                KeyboardInterrupt\n            ):  # Instructions indicate to train until interrupt\n                logger.logger.info(\n                    \"DLC training stopped via Keyboard Interrupt\"\n                )\n\n            snapshots = list(\n                (\n                    project_path\n                    / get_model_folder(\n                        trainFraction=dlc_config[\"train_fraction\"],\n                        shuffle=dlc_config[\"shuffle\"],\n                        cfg=dlc_config,\n                        modelprefix=dlc_config[\"modelprefix\"],\n                    )\n                    / \"train\"\n                ).glob(\"*index*\")\n            )\n            max_modified_time = 0\n            # DLC goes by snapshot magnitude when judging 'latest' for evaluation\n            # Here, we mean most recently generated\n            for snapshot in snapshots:\n                modified_time = os.path.getmtime(snapshot)\n                if modified_time &gt; max_modified_time:\n                    latest_snapshot = int(snapshot.stem[9:])\n                    max_modified_time = modified_time\n\n            self.insert1(\n                {\n                    **key,\n                    \"latest_snapshot\": latest_snapshot,\n                    \"config_template\": dlc_config,\n                }\n            )\n            from .position_dlc_model import DLCModelSource\n\n            dlc_model_name = f\"{key['project_name']}_{key['dlc_training_params_name']}_{key['training_id']:02d}\"\n            DLCModelSource.insert_entry(\n                dlc_model_name=dlc_model_name,\n                project_name=key[\"project_name\"],\n                source=\"FromUpstream\",\n                key=key,\n                skip_duplicates=True,\n            )\n        print(\n            f\"Inserted {dlc_model_name} from {key['project_name']} into DLCModelSource\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_dlc_training/#src.spyglass.position.v1.position_dlc_training.DLCModelTraining.make", "title": "<code>make(key)</code>", "text": "<p>Launch training for each entry in DLCModelTrainingSelection via <code>.populate()</code>.</p> Source code in <code>src/spyglass/position/v1/position_dlc_training.py</code> <pre><code>def make(self, key):\n\"\"\"Launch training for each entry in DLCModelTrainingSelection via `.populate()`.\"\"\"\n    model_prefix = (DLCModelTrainingSelection &amp; key).fetch1(\"model_prefix\")\n    from deeplabcut import create_training_dataset, train_network\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    from . import dlc_reader\n\n    try:\n        from deeplabcut.utils.auxiliaryfunctions import get_model_folder\n    except ImportError:\n        from deeplabcut.utils.auxiliaryfunctions import (\n            GetModelFolder as get_model_folder,\n        )\n    config_path, project_name = (DLCProject() &amp; key).fetch1(\n        \"config_path\", \"project_name\"\n    )\n    with OutputLogger(\n        name=\"DLC_project_{project_name}_training\",\n        path=f\"{os.path.dirname(config_path)}/log.log\",\n        print_console=True,\n    ) as logger:\n        dlc_config = read_config(config_path)\n        project_path = dlc_config[\"project_path\"]\n        key[\"project_path\"] = project_path\n        # ---- Build and save DLC configuration (yaml) file ----\n        _, dlc_config = dlc_reader.read_yaml(project_path)\n        if not dlc_config:\n            dlc_config = read_config(config_path)\n        dlc_config.update((DLCModelTrainingParams &amp; key).fetch1(\"params\"))\n        dlc_config.update(\n            {\n                \"project_path\": Path(project_path).as_posix(),\n                \"modelprefix\": model_prefix,\n                \"train_fraction\": dlc_config[\"TrainingFraction\"][\n                    int(dlc_config[\"trainingsetindex\"])\n                ],\n                \"training_filelist_datajoint\": [  # don't overwrite origin video_sets\n                    Path(fp).as_posix()\n                    for fp in (DLCProject.File &amp; key).fetch(\"file_path\")\n                ],\n            }\n        )\n        # Write dlc config file to base project folder\n        # TODO: need to make sure this will work\n        dlc_cfg_filepath = dlc_reader.save_yaml(project_path, dlc_config)\n        # ---- create training dataset ----\n        training_dataset_input_args = list(\n            inspect.signature(create_training_dataset).parameters\n        )\n        training_dataset_kwargs = {\n            k: v\n            for k, v in dlc_config.items()\n            if k in training_dataset_input_args\n        }\n        logger.logger.info(\"creating training dataset\")\n        create_training_dataset(dlc_cfg_filepath, **training_dataset_kwargs)\n        # ---- Trigger DLC model training job ----\n        train_network_input_args = list(\n            inspect.signature(train_network).parameters\n        )\n        train_network_kwargs = {\n            k: v\n            for k, v in dlc_config.items()\n            if k in train_network_input_args\n        }\n        for k in [\"shuffle\", \"trainingsetindex\", \"maxiters\"]:\n            if k in train_network_kwargs:\n                train_network_kwargs[k] = int(train_network_kwargs[k])\n        try:\n            train_network(dlc_cfg_filepath, **train_network_kwargs)\n        except (\n            KeyboardInterrupt\n        ):  # Instructions indicate to train until interrupt\n            logger.logger.info(\n                \"DLC training stopped via Keyboard Interrupt\"\n            )\n\n        snapshots = list(\n            (\n                project_path\n                / get_model_folder(\n                    trainFraction=dlc_config[\"train_fraction\"],\n                    shuffle=dlc_config[\"shuffle\"],\n                    cfg=dlc_config,\n                    modelprefix=dlc_config[\"modelprefix\"],\n                )\n                / \"train\"\n            ).glob(\"*index*\")\n        )\n        max_modified_time = 0\n        # DLC goes by snapshot magnitude when judging 'latest' for evaluation\n        # Here, we mean most recently generated\n        for snapshot in snapshots:\n            modified_time = os.path.getmtime(snapshot)\n            if modified_time &gt; max_modified_time:\n                latest_snapshot = int(snapshot.stem[9:])\n                max_modified_time = modified_time\n\n        self.insert1(\n            {\n                **key,\n                \"latest_snapshot\": latest_snapshot,\n                \"config_template\": dlc_config,\n            }\n        )\n        from .position_dlc_model import DLCModelSource\n\n        dlc_model_name = f\"{key['project_name']}_{key['dlc_training_params_name']}_{key['training_id']:02d}\"\n        DLCModelSource.insert_entry(\n            dlc_model_name=dlc_model_name,\n            project_name=key[\"project_name\"],\n            source=\"FromUpstream\",\n            key=key,\n            skip_duplicates=True,\n        )\n    print(\n        f\"Inserted {dlc_model_name} from {key['project_name']} into DLCModelSource\"\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/", "title": "position_trodes_position.py", "text": ""}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosParams", "title": "<code>TrodesPosParams</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Parameters for calculating the position (centroid, velocity, orientation)</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@schema\nclass TrodesPosParams(dj.Manual):\n\"\"\"\n    Parameters for calculating the position (centroid, velocity, orientation)\n    \"\"\"\n\n    definition = \"\"\"\n    trodes_pos_params_name: varchar(80) # name for this set of parameters\n    ---\n    params: longblob\n    \"\"\"\n\n    @classmethod\n    def insert_default(cls, **kwargs):\n\"\"\"\n        Insert default parameter set for position determination\n        \"\"\"\n        params = {\n            \"max_separation\": 9.0,\n            \"max_speed\": 300.0,\n            \"position_smoothing_duration\": 0.125,\n            \"speed_smoothing_std_dev\": 0.100,\n            \"orient_smoothing_std_dev\": 0.001,\n            \"led1_is_front\": 1,\n            \"is_upsampled\": 0,\n            \"upsampling_sampling_rate\": None,\n            \"upsampling_interpolation_method\": \"linear\",\n        }\n        cls.insert1(\n            {\"trodes_pos_params_name\": \"default\", \"params\": params},\n            skip_duplicates=True,\n        )\n\n    @classmethod\n    def get_default(cls):\n        query = cls &amp; {\"trodes_pos_params_name\": \"default\"}\n        if not len(query) &gt; 0:\n            cls().insert_default(skip_duplicates=True)\n            default = (cls &amp; {\"trodes_pos_params_name\": \"default\"}).fetch1()\n        else:\n            default = query.fetch1()\n        return default\n\n    @classmethod\n    def get_accepted_params(cls):\n        default = cls.get_default()\n        return list(default[\"params\"].keys())\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosParams.insert_default", "title": "<code>insert_default(**kwargs)</code>  <code>classmethod</code>", "text": "<p>Insert default parameter set for position determination</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@classmethod\ndef insert_default(cls, **kwargs):\n\"\"\"\n    Insert default parameter set for position determination\n    \"\"\"\n    params = {\n        \"max_separation\": 9.0,\n        \"max_speed\": 300.0,\n        \"position_smoothing_duration\": 0.125,\n        \"speed_smoothing_std_dev\": 0.100,\n        \"orient_smoothing_std_dev\": 0.001,\n        \"led1_is_front\": 1,\n        \"is_upsampled\": 0,\n        \"upsampling_sampling_rate\": None,\n        \"upsampling_interpolation_method\": \"linear\",\n    }\n    cls.insert1(\n        {\"trodes_pos_params_name\": \"default\", \"params\": params},\n        skip_duplicates=True,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.RawPosition", "title": "<code>RawPosition</code>", "text": "<p>         Bases: <code>dj.Imported</code></p>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.RawPosition--notes", "title": "Notes", "text": "<p>The position timestamps come from: .pos_cameraHWSync.dat. If PTP is not used, the position timestamps are inferred by finding the closest timestamps from the neural recording via the trodes time.</p> Source code in <code>src/spyglass/common/common_behav.py</code> <pre><code>@schema\nclass RawPosition(dj.Imported):\n\"\"\"\n\n    Notes\n    -----\n    The position timestamps come from: .pos_cameraHWSync.dat.\n    If PTP is not used, the position timestamps are inferred by finding the\n    closest timestamps from the neural recording via the trodes time.\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PositionSource\n    ---\n    raw_position_object_id: varchar(40)    # the object id of the spatial series for this epoch in the NWB file\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n\n        # TODO refactor this. this calculates sampling rate (unused here) and is expensive to do twice\n        pos_dict = get_all_spatial_series(nwbf)\n        for epoch in pos_dict:\n            if key[\n                \"interval_list_name\"\n            ] == PositionSource.get_pos_interval_name(epoch):\n                pdict = pos_dict[epoch]\n                key[\"raw_position_object_id\"] = pdict[\"raw_position_object_id\"]\n                self.insert1(key)\n                break\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(self, (Nwbfile, \"nwb_file_abs_path\"), *attrs, **kwargs)\n\n    def fetch1_dataframe(self):\n        raw_position_nwb = self.fetch_nwb()[0][\"raw_position\"]\n        return pd.DataFrame(\n            data=raw_position_nwb.data,\n            index=pd.Index(raw_position_nwb.timestamps, name=\"time\"),\n            columns=raw_position_nwb.description.split(\", \"),\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosSelection", "title": "<code>TrodesPosSelection</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Table to pair an interval with position data and position determination parameters</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@schema\nclass TrodesPosSelection(dj.Manual):\n\"\"\"\n    Table to pair an interval with position data\n    and position determination parameters\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; RawPosition\n    -&gt; TrodesPosParams\n    ---\n    \"\"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosV1", "title": "<code>TrodesPosV1</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Table to calculate the position based on Trodes tracking</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@schema\nclass TrodesPosV1(dj.Computed):\n\"\"\"\n    Table to calculate the position based on Trodes tracking\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; TrodesPosSelection\n    ---\n    -&gt; AnalysisNwbfile\n    position_object_id : varchar(80)\n    orientation_object_id : varchar(80)\n    velocity_object_id : varchar(80)\n    \"\"\"\n\n    def make(self, key):\n        print(f\"Computing position for: {key}\")\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        raw_position = (RawPosition() &amp; key).fetch_nwb()[0]\n        position_info_parameters = (TrodesPosParams() &amp; key).fetch1(\"params\")\n        position = pynwb.behavior.Position()\n        orientation = pynwb.behavior.CompassDirection()\n        velocity = pynwb.behavior.BehavioralTimeSeries()\n\n        METERS_PER_CM = 0.01\n        raw_pos_df = pd.DataFrame(\n            data=raw_position[\"raw_position\"].data,\n            index=pd.Index(\n                raw_position[\"raw_position\"].timestamps, name=\"time\"\n            ),\n            columns=raw_position[\"raw_position\"].description.split(\", \"),\n        )\n        try:\n            # calculate the processed position\n            spatial_series = raw_position[\"raw_position\"]\n            position_info = self.calculate_position_info_from_spatial_series(\n                spatial_series,\n                position_info_parameters[\"max_separation\"],\n                position_info_parameters[\"max_speed\"],\n                position_info_parameters[\"speed_smoothing_std_dev\"],\n                position_info_parameters[\"position_smoothing_duration\"],\n                position_info_parameters[\"orient_smoothing_std_dev\"],\n                position_info_parameters[\"led1_is_front\"],\n                position_info_parameters[\"is_upsampled\"],\n                position_info_parameters[\"upsampling_sampling_rate\"],\n                position_info_parameters[\"upsampling_interpolation_method\"],\n            )\n            # create nwb objects for insertion into analysis nwb file\n            position.create_spatial_series(\n                name=\"position\",\n                timestamps=position_info[\"time\"],\n                conversion=METERS_PER_CM,\n                data=position_info[\"position\"],\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"x_position, y_position\",\n            )\n\n            orientation.create_spatial_series(\n                name=\"orientation\",\n                timestamps=position_info[\"time\"],\n                conversion=1.0,\n                data=position_info[\"orientation\"],\n                reference_frame=spatial_series.reference_frame,\n                comments=spatial_series.comments,\n                description=\"orientation\",\n            )\n\n            velocity.create_timeseries(\n                name=\"velocity\",\n                timestamps=position_info[\"time\"],\n                conversion=METERS_PER_CM,\n                unit=\"m/s\",\n                data=np.concatenate(\n                    (\n                        position_info[\"velocity\"],\n                        position_info[\"speed\"][:, np.newaxis],\n                    ),\n                    axis=1,\n                ),\n                comments=spatial_series.comments,\n                description=\"x_velocity, y_velocity, speed\",\n            )\n            try:\n                velocity.create_timeseries(\n                    name=\"video_frame_ind\",\n                    unit=\"index\",\n                    timestamps=position_info[\"time\"],\n                    data=raw_pos_df.video_frame_ind.to_numpy(),\n                    description=\"video_frame_ind\",\n                    comments=spatial_series.comments,\n                )\n            except AttributeError:\n                print(\n                    \"No video frame index found. Assuming all camera frames are present.\"\n                )\n                velocity.create_timeseries(\n                    name=\"video_frame_ind\",\n                    unit=\"index\",\n                    timestamps=position_info[\"time\"],\n                    data=np.arange(len(position_info[\"time\"])),\n                    description=\"video_frame_ind\",\n                    comments=spatial_series.comments,\n                )\n        except ValueError:\n            pass\n\n        # Insert into analysis nwb file\n        nwb_analysis_file = AnalysisNwbfile()\n\n        key[\"position_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], position\n        )\n        key[\"orientation_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], orientation\n        )\n        key[\"velocity_object_id\"] = nwb_analysis_file.add_nwb_object(\n            key[\"analysis_file_name\"], velocity\n        )\n\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], key[\"analysis_file_name\"])\n\n        self.insert1(key)\n        from ..position_merge import PositionOutput\n\n        key[\"source\"] = \"Trodes\"\n        key[\"version\"] = 1\n        trodes_key = key.copy()\n        valid_fields = PositionOutput().fetch().dtype.fields.keys()\n        entries_to_delete = [\n            entry for entry in key.keys() if entry not in valid_fields\n        ]\n        for entry in entries_to_delete:\n            del key[entry]\n        PositionOutput().insert1(\n            key=key, params=trodes_key, skip_duplicates=True\n        )\n\n    @staticmethod\n    def calculate_position_info_from_spatial_series(\n        spatial_series,\n        max_LED_separation,\n        max_plausible_speed,\n        speed_smoothing_std_dev,\n        position_smoothing_duration,\n        orient_smoothing_std_dev,\n        led1_is_front,\n        is_upsampled,\n        upsampling_sampling_rate,\n        upsampling_interpolation_method,\n    ):\n        CM_TO_METERS = 100\n\n        # Get spatial series properties\n        time = np.asarray(spatial_series.timestamps)  # seconds\n        position = np.asarray(\n            pd.DataFrame(\n                spatial_series.data,\n                columns=spatial_series.description.split(\", \"),\n            ).loc[:, [\"xloc\", \"yloc\", \"xloc2\", \"yloc2\"]]\n        )  # meters\n\n        # remove NaN times\n        is_nan_time = np.isnan(time)\n        position = position[~is_nan_time]\n        time = time[~is_nan_time]\n\n        dt = np.median(np.diff(time))\n        sampling_rate = 1 / dt\n        meters_to_pixels = spatial_series.conversion\n\n        # Define LEDs\n        if led1_is_front:\n            front_LED = position[:, [0, 1]].astype(float)\n            back_LED = position[:, [2, 3]].astype(float)\n        else:\n            back_LED = position[:, [0, 1]].astype(float)\n            front_LED = position[:, [2, 3]].astype(float)\n\n        # Convert to cm\n        back_LED *= meters_to_pixels * CM_TO_METERS\n        front_LED *= meters_to_pixels * CM_TO_METERS\n\n        # Set points to NaN where the front and back LEDs are too separated\n        dist_between_LEDs = get_distance(back_LED, front_LED)\n        is_too_separated = dist_between_LEDs &gt;= max_LED_separation\n\n        back_LED[is_too_separated] = np.nan\n        front_LED[is_too_separated] = np.nan\n\n        # Calculate speed\n        front_LED_speed = get_speed(\n            front_LED,\n            time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )\n        back_LED_speed = get_speed(\n            back_LED,\n            time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )\n\n        # Set to points to NaN where the speed is too fast\n        is_too_fast = (front_LED_speed &gt; max_plausible_speed) | (\n            back_LED_speed &gt; max_plausible_speed\n        )\n        back_LED[is_too_fast] = np.nan\n        front_LED[is_too_fast] = np.nan\n\n        # Interpolate the NaN points\n        back_LED = interpolate_nan(back_LED)\n        front_LED = interpolate_nan(front_LED)\n\n        # Smooth\n        moving_average_window = int(position_smoothing_duration * sampling_rate)\n        back_LED = bottleneck.move_mean(\n            back_LED, window=moving_average_window, axis=0, min_count=1\n        )\n        front_LED = bottleneck.move_mean(\n            front_LED, window=moving_average_window, axis=0, min_count=1\n        )\n\n        if is_upsampled:\n            position_df = pd.DataFrame(\n                {\n                    \"time\": time,\n                    \"back_LED_x\": back_LED[:, 0],\n                    \"back_LED_y\": back_LED[:, 1],\n                    \"front_LED_x\": front_LED[:, 0],\n                    \"front_LED_y\": front_LED[:, 1],\n                }\n            ).set_index(\"time\")\n\n            upsampling_start_time = time[0]\n            upsampling_end_time = time[-1]\n\n            n_samples = (\n                int(\n                    np.ceil(\n                        (upsampling_end_time - upsampling_start_time)\n                        * upsampling_sampling_rate\n                    )\n                )\n                + 1\n            )\n            new_time = np.linspace(\n                upsampling_start_time, upsampling_end_time, n_samples\n            )\n            new_index = pd.Index(\n                np.unique(np.concatenate((position_df.index, new_time))),\n                name=\"time\",\n            )\n            position_df = (\n                position_df.reindex(index=new_index)\n                .interpolate(method=upsampling_interpolation_method)\n                .reindex(index=new_time)\n            )\n\n            time = np.asarray(position_df.index)\n            back_LED = np.asarray(\n                position_df.loc[:, [\"back_LED_x\", \"back_LED_y\"]]\n            )\n            front_LED = np.asarray(\n                position_df.loc[:, [\"front_LED_x\", \"front_LED_y\"]]\n            )\n\n            sampling_rate = upsampling_sampling_rate\n\n        # Calculate position, orientation, velocity, speed\n        position = get_centriod(back_LED, front_LED)  # cm\n\n        orientation = get_angle(back_LED, front_LED)  # radians\n        is_nan = np.isnan(orientation)\n\n        # Unwrap orientation before smoothing\n        orientation[~is_nan] = np.unwrap(orientation[~is_nan])\n        orientation[~is_nan] = gaussian_smooth(\n            orientation[~is_nan],\n            orient_smoothing_std_dev,\n            sampling_rate,\n            axis=0,\n            truncate=8,\n        )\n        # convert back to between -pi and pi\n        orientation[~is_nan] = np.angle(np.exp(1j * orientation[~is_nan]))\n\n        velocity = get_velocity(\n            position,\n            time=time,\n            sigma=speed_smoothing_std_dev,\n            sampling_frequency=sampling_rate,\n        )  # cm/s\n        speed = np.sqrt(np.sum(velocity**2, axis=1))  # cm/s\n\n        return {\n            \"time\": time,\n            \"position\": position,\n            \"orientation\": orientation,\n            \"velocity\": velocity,\n            \"speed\": speed,\n        }\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n\n    def fetch1_dataframe(self):\n        nwb_data = self.fetch_nwb()[0]\n        index = pd.Index(\n            np.asarray(nwb_data[\"position\"].get_spatial_series().timestamps),\n            name=\"time\",\n        )\n        COLUMNS = [\n            \"video_frame_ind\",\n            \"position_x\",\n            \"position_y\",\n            \"orientation\",\n            \"velocity_x\",\n            \"velocity_y\",\n            \"speed\",\n        ]\n        return pd.DataFrame(\n            np.concatenate(\n                (\n                    np.asarray(\n                        nwb_data[\"velocity\"]\n                        .time_series[\"video_frame_ind\"]\n                        .data,\n                        dtype=int,\n                    )[:, np.newaxis],\n                    np.asarray(nwb_data[\"position\"].get_spatial_series().data),\n                    np.asarray(\n                        nwb_data[\"orientation\"].get_spatial_series().data\n                    )[:, np.newaxis],\n                    np.asarray(\n                        nwb_data[\"velocity\"].time_series[\"velocity\"].data\n                    ),\n                ),\n                axis=1,\n            ),\n            columns=COLUMNS,\n            index=index,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.get_video_path", "title": "<code>get_video_path(key)</code>", "text": "<p>Given nwb_file_name and interval_list_name returns specified video file filename and path</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Dictionary containing nwb_file_name and interval_list_name as keys</p> required <p>Returns:</p> Name Type Description <code>video_filepath</code> <code>str</code> <p>path to the video file, including video filename</p> <code>video_filename</code> <code>str</code> <p>filename of the video</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def get_video_path(key):\n\"\"\"\n    Given nwb_file_name and interval_list_name returns specified\n    video file filename and path\n\n    Parameters\n    ----------\n    key : dict\n        Dictionary containing nwb_file_name and interval_list_name as keys\n\n    Returns\n    -------\n    video_filepath : str\n        path to the video file, including video filename\n    video_filename : str\n        filename of the video\n    \"\"\"\n    import pynwb\n\n    from ...common.common_behav import VideoFile\n\n    video_info = (\n        VideoFile()\n        &amp; {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": key[\"epoch\"]}\n    ).fetch1()\n    nwb_path = (\n        f\"{os.getenv('SPYGLASS_BASE_DIR')}/raw/{video_info['nwb_file_name']}\"\n    )\n    with pynwb.NWBHDF5IO(path=nwb_path, mode=\"r\") as in_out:\n        nwb_file = in_out.read()\n        nwb_video = nwb_file.objects[video_info[\"video_file_object_id\"]]\n        video_filepath = VideoFile.get_abs_path(\n            {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": key[\"epoch\"]}\n        )\n        video_dir = os.path.dirname(video_filepath) + \"/\"\n        video_filename = video_filepath.split(video_dir)[-1]\n        meters_per_pixel = nwb_video.device.meters_per_pixel\n        timestamps = np.asarray(nwb_video.timestamps)\n    return video_dir, video_filename, meters_per_pixel, timestamps\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.check_videofile", "title": "<code>check_videofile(video_path, output_path=os.getenv('DLC_VIDEO_PATH'), video_filename=None, video_filetype='h264')</code>", "text": "<p>Checks the file extension of a video file to make sure it is .mp4 for DeepLabCut processes. Converts to MP4 if not already.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str or PosixPath object</code> <p>path to directory of the existing video file without filename</p> required <code>output_path</code> <code>str or PosixPath object</code> <p>path to directory where converted video will be saved</p> <code>os.getenv('DLC_VIDEO_PATH')</code> <code>video_filename</code> <code>str, Optional</code> <p>filename of the video to convert, if not provided, video_filetype must be and all video files of video_filetype in the directory will be converted</p> <code>None</code> <code>video_filetype</code> <code>str or List, Default 'h264', Optional</code> <p>If video_filename is not provided, all videos of this filetype will be converted to .mp4</p> <code>'h264'</code> <p>Returns:</p> Name Type Description <code>output_files</code> <code>List of PosixPath objects</code> <p>paths to converted video file(s)</p> Source code in <code>src/spyglass/position/v1/dlc_utils.py</code> <pre><code>def check_videofile(\n    video_path: Union[str, pathlib.PosixPath],\n    output_path: Union[str, pathlib.PosixPath] = os.getenv(\"DLC_VIDEO_PATH\"),\n    video_filename: str = None,\n    video_filetype: str = \"h264\",\n):\n\"\"\"\n    Checks the file extension of a video file to make sure it is .mp4 for\n    DeepLabCut processes. Converts to MP4 if not already.\n\n    Parameters\n    ----------\n    video_path : str or PosixPath object\n        path to directory of the existing video file without filename\n    output_path : str or PosixPath object\n        path to directory where converted video will be saved\n    video_filename : str, Optional\n        filename of the video to convert, if not provided, video_filetype must be\n        and all video files of video_filetype in the directory will be converted\n    video_filetype : str or List, Default 'h264', Optional\n        If video_filename is not provided,\n        all videos of this filetype will be converted to .mp4\n\n    Returns\n    -------\n    output_files : List of PosixPath objects\n        paths to converted video file(s)\n    \"\"\"\n\n    if not video_filename:\n        video_files = pathlib.Path(video_path).glob(f\"*.{video_filetype}\")\n    else:\n        video_files = [pathlib.Path(f\"{video_path}/{video_filename}\")]\n    output_files = []\n    for video_filepath in video_files:\n        if video_filepath.exists():\n            if video_filepath.suffix == \".mp4\":\n                output_files.append(video_filepath)\n                continue\n        video_file = (\n            video_filepath.as_posix()\n            .rsplit(video_filepath.parent.as_posix(), maxsplit=1)[-1]\n            .split(\"/\")[-1]\n        )\n        output_files.append(\n            _convert_mp4(video_file, video_path, output_path, videotype=\"mp4\")\n        )\n    return output_files\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosVideo", "title": "<code>TrodesPosVideo</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Creates a video of the computed head position and orientation as well as the original LED positions overlaid on the video of the animal.</p> <p>Use for debugging the effect of position extraction parameters.</p> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@schema\nclass TrodesPosVideo(dj.Computed):\n\"\"\"Creates a video of the computed head position and orientation as well as\n    the original LED positions overlaid on the video of the animal.\n\n    Use for debugging the effect of position extraction parameters.\"\"\"\n\n    definition = \"\"\"\n    -&gt; TrodesPosV1\n    ---\n    \"\"\"\n\n    def make(self, key):\n        M_TO_CM = 100\n\n        print(\"Loading position data...\")\n        raw_position_df = (\n            RawPosition()\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": key[\"interval_list_name\"],\n            }\n        ).fetch1_dataframe()\n        position_info_df = (TrodesPosV1() &amp; key).fetch1_dataframe()\n\n        print(\"Loading video data...\")\n        epoch = (\n            int(\n                key[\"interval_list_name\"]\n                .replace(\"pos \", \"\")\n                .replace(\" valid times\", \"\")\n            )\n            + 1\n        )\n\n        (\n            video_path,\n            video_filename,\n            meters_per_pixel,\n            video_time,\n        ) = get_video_path(\n            {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": epoch}\n        )\n        video_dir = os.path.dirname(video_path) + \"/\"\n        video_path = check_videofile(\n            video_path=video_dir, video_filename=video_filename\n        )[0].as_posix()\n        nwb_base_filename = key[\"nwb_file_name\"].replace(\".nwb\", \"\")\n        current_dir = Path(os.getcwd())\n        output_video_filename = (\n            f\"{current_dir.as_posix()}/{nwb_base_filename}_\"\n            f\"{epoch:02d}_{key['trodes_pos_params_name']}.mp4\"\n        )\n        centroids = {\n            \"red\": np.asarray(raw_position_df[[\"xloc\", \"yloc\"]]),\n            \"green\": np.asarray(raw_position_df[[\"xloc2\", \"yloc2\"]]),\n        }\n        position_mean = np.asarray(\n            position_info_df[[\"position_x\", \"position_y\"]]\n        )\n        orientation_mean = np.asarray(position_info_df[[\"orientation\"]])\n        position_time = np.asarray(position_info_df.index)\n        cm_per_pixel = meters_per_pixel * M_TO_CM\n\n        print(\"Making video...\")\n        self.make_video(\n            video_path,\n            centroids,\n            position_mean,\n            orientation_mean,\n            video_time,\n            position_time,\n            output_video_filename=output_video_filename,\n            cm_to_pixels=cm_per_pixel,\n            disable_progressbar=False,\n        )\n\n    @staticmethod\n    def convert_to_pixels(data, frame_size, cm_to_pixels=1.0):\n\"\"\"Converts from cm to pixels and flips the y-axis.\n        Parameters\n        ----------\n        data : ndarray, shape (n_time, 2)\n        frame_size : array_like, shape (2,)\n        cm_to_pixels : float\n\n        Returns\n        -------\n        converted_data : ndarray, shape (n_time, 2)\n        \"\"\"\n        return data / cm_to_pixels\n\n    @staticmethod\n    def fill_nan(variable, video_time, variable_time):\n        video_ind = np.digitize(variable_time, video_time[1:])\n\n        n_video_time = len(video_time)\n        try:\n            n_variable_dims = variable.shape[1]\n            filled_variable = np.full((n_video_time, n_variable_dims), np.nan)\n        except IndexError:\n            filled_variable = np.full((n_video_time,), np.nan)\n        filled_variable[video_ind] = variable\n\n        return filled_variable\n\n    def make_video(\n        self,\n        video_filename,\n        centroids,\n        position_mean,\n        orientation_mean,\n        video_time,\n        position_time,\n        output_video_filename=\"output.mp4\",\n        cm_to_pixels=1.0,\n        disable_progressbar=False,\n        arrow_radius=15,\n        circle_radius=8,\n    ):\n        RGB_PINK = (234, 82, 111)\n        RGB_YELLOW = (253, 231, 76)\n        RGB_WHITE = (255, 255, 255)\n\n        video = cv2.VideoCapture(video_filename)\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n        frame_size = (int(video.get(3)), int(video.get(4)))\n        frame_rate = video.get(5)\n        n_frames = int(orientation_mean.shape[0])\n        print(f\"video filepath: {output_video_filename}\")\n        out = cv2.VideoWriter(\n            output_video_filename, fourcc, frame_rate, frame_size, True\n        )\n\n        centroids = {\n            color: self.fill_nan(data, video_time, position_time)\n            for color, data in centroids.items()\n        }\n        position_mean = self.fill_nan(position_mean, video_time, position_time)\n        orientation_mean = self.fill_nan(\n            orientation_mean, video_time, position_time\n        )\n\n        for time_ind in tqdm(\n            range(n_frames - 1), desc=\"frames\", disable=disable_progressbar\n        ):\n            is_grabbed, frame = video.read()\n            if is_grabbed:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n                red_centroid = centroids[\"red\"][time_ind]\n                green_centroid = centroids[\"green\"][time_ind]\n\n                position = position_mean[time_ind]\n                position = self.convert_to_pixels(\n                    position, frame_size, cm_to_pixels\n                )\n                orientation = orientation_mean[time_ind]\n\n                if np.all(~np.isnan(red_centroid)):\n                    cv2.circle(\n                        img=frame,\n                        center=tuple(red_centroid.astype(int)),\n                        radius=circle_radius,\n                        color=RGB_YELLOW,\n                        thickness=-1,\n                        shift=cv2.CV_8U,\n                    )\n\n                if np.all(~np.isnan(green_centroid)):\n                    cv2.circle(\n                        img=frame,\n                        center=tuple(green_centroid.astype(int)),\n                        radius=circle_radius,\n                        color=RGB_PINK,\n                        thickness=-1,\n                        shift=cv2.CV_8U,\n                    )\n\n                if np.all(~np.isnan(position)) &amp; np.all(~np.isnan(orientation)):\n                    arrow_tip = (\n                        int(position[0] + arrow_radius * np.cos(orientation)),\n                        int(position[1] + arrow_radius * np.sin(orientation)),\n                    )\n                    cv2.arrowedLine(\n                        img=frame,\n                        pt1=tuple(position.astype(int)),\n                        pt2=arrow_tip,\n                        color=RGB_WHITE,\n                        thickness=4,\n                        line_type=8,\n                        shift=cv2.CV_8U,\n                        tipLength=0.25,\n                    )\n\n                if np.all(~np.isnan(position)):\n                    cv2.circle(\n                        img=frame,\n                        center=tuple(position.astype(int)),\n                        radius=circle_radius,\n                        color=RGB_WHITE,\n                        thickness=-1,\n                        shift=cv2.CV_8U,\n                    )\n\n                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n                out.write(frame)\n            else:\n                break\n\n        video.release()\n        out.release()\n        cv2.destroyAllWindows()\n</code></pre>"}, {"location": "api/src/spyglass/position/v1/position_trodes_position/#src.spyglass.position.v1.position_trodes_position.TrodesPosVideo.convert_to_pixels", "title": "<code>convert_to_pixels(data, frame_size, cm_to_pixels=1.0)</code>  <code>staticmethod</code>", "text": "<p>Converts from cm to pixels and flips the y-axis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray, shape(n_time, 2)</code> required <code>frame_size</code> <code>array_like, shape(2)</code> required <code>cm_to_pixels</code> <code>float</code> <code>1.0</code> <p>Returns:</p> Name Type Description <code>converted_data</code> <code>ndarray, shape(n_time, 2)</code> Source code in <code>src/spyglass/position/v1/position_trodes_position.py</code> <pre><code>@staticmethod\ndef convert_to_pixels(data, frame_size, cm_to_pixels=1.0):\n\"\"\"Converts from cm to pixels and flips the y-axis.\n    Parameters\n    ----------\n    data : ndarray, shape (n_time, 2)\n    frame_size : array_like, shape (2,)\n    cm_to_pixels : float\n\n    Returns\n    -------\n    converted_data : ndarray, shape (n_time, 2)\n    \"\"\"\n    return data / cm_to_pixels\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/", "title": "sharing_kachery.py", "text": ""}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.kachery_download_file", "title": "<code>kachery_download_file(uri, dest, kachery_zone_name)</code>", "text": "<p>set the kachery resource url and attempt to down load the uri into the destination path</p> Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>def kachery_download_file(uri: str, dest: str, kachery_zone_name: str) -&gt; str:\n\"\"\"set the kachery resource url and attempt to down load the uri into the destination path\"\"\"\n    KacheryZone.set_resource_url({\"kachery_zone_name\": kachery_zone_name})\n    return kcl.load_file(uri, dest=dest)\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.KacheryZone", "title": "<code>KacheryZone</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>@schema\nclass KacheryZone(dj.Manual):\n    definition = \"\"\"\n    kachery_zone_name: varchar(200) # the name of the kachery zone. Note that this is the same as the name of the kachery resource.\n    ---\n    description: varchar(200) # description of this zone\n    kachery_cloud_dir: varchar(200) # kachery cloud directory on local machine where files are linked\n    kachery_proxy: varchar(200) # kachery sharing proxy\n    -&gt; Lab\n    \"\"\"\n\n    @staticmethod\n    def set_zone(key: dict):\n\"\"\"Set the kachery zone based on the key to KacheryZone\n\n        Parameters\n        ----------\n        key : dict\n            key defining a single KacheryZone\n\n        \"\"\"\n        try:\n            kachery_zone_name, kachery_cloud_dir = (KacheryZone &amp; key).fetch1(\n                \"kachery_zone_name\", \"kachery_cloud_dir\"\n            )\n        except:\n            raise Exception(\n                f\"{key} does not correspond to a single entry in KacheryZone.\"\n            )\n            return None\n        # set the new zone and cloud directory\n        os.environ[kachery_zone_envar] = kachery_zone_name\n        os.environ[kachery_cloud_dir_envar] = kachery_cloud_dir\n\n    @staticmethod\n    def reset_zone():\n\"\"\"Resets the kachery zone environment variable to the default values.\"\"\"\n        if default_kachery_zone is not None:\n            os.environ[kachery_zone_envar] = default_kachery_zone\n        if default_kachery_cloud_dir is not None:\n            os.environ[kachery_cloud_dir_envar] = default_kachery_cloud_dir\n\n    @staticmethod\n    def set_resource_url(key: dict):\n\"\"\"Sets the KACHERY_RESOURCE_URL based on the key corresponding to a single Kachery Zone\n\n        Parameters\n        ----------\n        key : dict\n            key to retrieve a single kachery zone\n        \"\"\"\n        try:\n            kachery_zone_name, kachery_proxy = (KacheryZone &amp; key).fetch1(\n                \"kachery_zone_name\", \"kachery_proxy\"\n            )\n        except:\n            raise Exception(\n                f\"{key} does not correspond to a single entry in KacheryZone.\"\n            )\n        # set the new zone and cloud directory\n        os.environ[kachery_zone_envar] = kachery_zone_name\n        os.environ[kachery_resource_url_envar] = (\n            kachery_proxy + \"/r/\" + kachery_zone_name\n        )\n\n    @staticmethod\n    def reset_resource_url():\n        KacheryZone.reset_zone()\n        if default_kachery_resource_url is not None:\n            os.environ[\n                kachery_resource_url_envar\n            ] = default_kachery_resource_url\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.KacheryZone.set_zone", "title": "<code>set_zone(key)</code>  <code>staticmethod</code>", "text": "<p>Set the kachery zone based on the key to KacheryZone</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>key defining a single KacheryZone</p> required Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>@staticmethod\ndef set_zone(key: dict):\n\"\"\"Set the kachery zone based on the key to KacheryZone\n\n    Parameters\n    ----------\n    key : dict\n        key defining a single KacheryZone\n\n    \"\"\"\n    try:\n        kachery_zone_name, kachery_cloud_dir = (KacheryZone &amp; key).fetch1(\n            \"kachery_zone_name\", \"kachery_cloud_dir\"\n        )\n    except:\n        raise Exception(\n            f\"{key} does not correspond to a single entry in KacheryZone.\"\n        )\n        return None\n    # set the new zone and cloud directory\n    os.environ[kachery_zone_envar] = kachery_zone_name\n    os.environ[kachery_cloud_dir_envar] = kachery_cloud_dir\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.KacheryZone.reset_zone", "title": "<code>reset_zone()</code>  <code>staticmethod</code>", "text": "<p>Resets the kachery zone environment variable to the default values.</p> Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>@staticmethod\ndef reset_zone():\n\"\"\"Resets the kachery zone environment variable to the default values.\"\"\"\n    if default_kachery_zone is not None:\n        os.environ[kachery_zone_envar] = default_kachery_zone\n    if default_kachery_cloud_dir is not None:\n        os.environ[kachery_cloud_dir_envar] = default_kachery_cloud_dir\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.KacheryZone.set_resource_url", "title": "<code>set_resource_url(key)</code>  <code>staticmethod</code>", "text": "<p>Sets the KACHERY_RESOURCE_URL based on the key corresponding to a single Kachery Zone</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>key to retrieve a single kachery zone</p> required Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>@staticmethod\ndef set_resource_url(key: dict):\n\"\"\"Sets the KACHERY_RESOURCE_URL based on the key corresponding to a single Kachery Zone\n\n    Parameters\n    ----------\n    key : dict\n        key to retrieve a single kachery zone\n    \"\"\"\n    try:\n        kachery_zone_name, kachery_proxy = (KacheryZone &amp; key).fetch1(\n            \"kachery_zone_name\", \"kachery_proxy\"\n        )\n    except:\n        raise Exception(\n            f\"{key} does not correspond to a single entry in KacheryZone.\"\n        )\n    # set the new zone and cloud directory\n    os.environ[kachery_zone_envar] = kachery_zone_name\n    os.environ[kachery_resource_url_envar] = (\n        kachery_proxy + \"/r/\" + kachery_zone_name\n    )\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.AnalysisNwbfileKachery", "title": "<code>AnalysisNwbfileKachery</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>@schema\nclass AnalysisNwbfileKachery(dj.Computed):\n    definition = \"\"\"\n    -&gt; AnalysisNwbfileKacherySelection\n    ---\n    analysis_file_uri='': varchar(200)  # the uri of the file\n    \"\"\"\n\n    class LinkedFile(dj.Part):\n        definition = \"\"\"\n        -&gt; AnalysisNwbfileKachery\n        linked_file_rel_path: varchar(200) # the path for the linked file relative to the SPYGLASS_BASE_DIR environment variable\n        ---\n        linked_file_uri='': varchar(200) # the uri for the linked file\n        \"\"\"\n\n    def make(self, key):\n        # note that we're assuming that the user has initialized a kachery-cloud client with kachery-cloud-init\n        # uncomment the line below once we are sharing linked files as well.\n        # linked_key = copy.deepcopy(key)\n        print(f'Linking {key[\"analysis_file_name\"]} in kachery-cloud...')\n        # set the kachery zone\n        KacheryZone.set_zone(key)\n        key[\"analysis_file_uri\"] = kcl.link_file(\n            AnalysisNwbfile().get_abs_path(key[\"analysis_file_name\"])\n        )\n        print(\n            os.environ[kachery_zone_envar], os.environ[kachery_cloud_dir_envar]\n        )\n        print(AnalysisNwbfile().get_abs_path(key[\"analysis_file_name\"]))\n        print(kcl.load_file(key[\"analysis_file_uri\"]))\n        self.insert1(key)\n\n        # we also need to insert any linked files\n        # TODO: change this to automatically detect all linked files\n        # self.LinkedFile.insert1(key)\n\n        # reset the Kachery zone and cloud_dir to the defaults\n        KacheryZone.reset_zone()\n\n    @staticmethod\n    def download_file(analysis_file_name: str) -&gt; bool:\n\"\"\"Download the specified analysis file and associated linked files from kachery-cloud if possible\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis file\n\n        Returns\n        ----------\n        is_success : bool\n            True if the file was successfully downloaded, False otherwise\n        \"\"\"\n        fetched_list = (\n            AnalysisNwbfileKachery &amp; {\"analysis_file_name\": analysis_file_name}\n        ).fetch(\"analysis_file_uri\", \"kachery_zone_name\")\n        downloaded = False\n        for uri, kachery_zone_name in zip(fetched_list[0], fetched_list[1]):\n            if len(uri) == 0:\n                return False\n            print(\"uri:\", uri)\n            if kachery_download_file(\n                uri=uri,\n                dest=AnalysisNwbfile.get_abs_path(analysis_file_name),\n                kachery_zone_name=kachery_zone_name,\n            ):\n                downloaded = True\n                # now download the linked file(s)\n                linked_files = (\n                    AnalysisNwbfileKachery.LinkedFile\n                    &amp; {\"analysis_file_name\": analysis_file_name}\n                ).fetch(as_dict=True)\n                for file in linked_files:\n                    uri = file[\"linked_file_uri\"]\n                    print(f\"attempting to download linked file uri {uri}\")\n                    linked_file_path = (\n                        os.environ[\"SPYGLASS_BASE_DIR\"]\n                        + file[\"linked_file_rel_path\"]\n                    )\n                    if not kachery_download_file(\n                        uri=uri,\n                        dest=linked_file_path,\n                        kachery_zone_name=kachery_zone_name,\n                    ):\n                        raise Exception(\n                            f\"Linked file {linked_file_path} cannot be downloaded\"\n                        )\n        if not downloaded:\n            raise Exception(f\"{analysis_file_name} cannot be downloaded\")\n\n        return True\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.AnalysisNwbfileKachery.download_file", "title": "<code>download_file(analysis_file_name)</code>  <code>staticmethod</code>", "text": "<p>Download the specified analysis file and associated linked files from kachery-cloud if possible</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis file</p> required <p>Returns:</p> Name Type Description <code>is_success</code> <code>bool</code> <p>True if the file was successfully downloaded, False otherwise</p> Source code in <code>src/spyglass/sharing/sharing_kachery.py</code> <pre><code>@staticmethod\ndef download_file(analysis_file_name: str) -&gt; bool:\n\"\"\"Download the specified analysis file and associated linked files from kachery-cloud if possible\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis file\n\n    Returns\n    ----------\n    is_success : bool\n        True if the file was successfully downloaded, False otherwise\n    \"\"\"\n    fetched_list = (\n        AnalysisNwbfileKachery &amp; {\"analysis_file_name\": analysis_file_name}\n    ).fetch(\"analysis_file_uri\", \"kachery_zone_name\")\n    downloaded = False\n    for uri, kachery_zone_name in zip(fetched_list[0], fetched_list[1]):\n        if len(uri) == 0:\n            return False\n        print(\"uri:\", uri)\n        if kachery_download_file(\n            uri=uri,\n            dest=AnalysisNwbfile.get_abs_path(analysis_file_name),\n            kachery_zone_name=kachery_zone_name,\n        ):\n            downloaded = True\n            # now download the linked file(s)\n            linked_files = (\n                AnalysisNwbfileKachery.LinkedFile\n                &amp; {\"analysis_file_name\": analysis_file_name}\n            ).fetch(as_dict=True)\n            for file in linked_files:\n                uri = file[\"linked_file_uri\"]\n                print(f\"attempting to download linked file uri {uri}\")\n                linked_file_path = (\n                    os.environ[\"SPYGLASS_BASE_DIR\"]\n                    + file[\"linked_file_rel_path\"]\n                )\n                if not kachery_download_file(\n                    uri=uri,\n                    dest=linked_file_path,\n                    kachery_zone_name=kachery_zone_name,\n                ):\n                    raise Exception(\n                        f\"Linked file {linked_file_path} cannot be downloaded\"\n                    )\n    if not downloaded:\n        raise Exception(f\"{analysis_file_name} cannot be downloaded\")\n\n    return True\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.sharing.sharing_kachery.Lab", "title": "<code>Lab</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass Lab(dj.Manual):\n    definition = \"\"\"\n    lab_name: varchar(80)\n    ---\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab name information from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The NWB file with lab name information.\n        \"\"\"\n        if nwbf.lab is None:\n            print(\"No lab metadata found.\\n\")\n            return\n        cls.insert1(dict(lab_name=nwbf.lab), skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/sharing/sharing_kachery/#src.spyglass.common.common_lab.Lab.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert lab name information from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The NWB file with lab name information.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab name information from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The NWB file with lab name information.\n    \"\"\"\n    if nwbf.lab is None:\n        print(\"No lab metadata found.\\n\")\n        return\n    cls.insert1(dict(lab_name=nwbf.lab), skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/", "title": "curation_figurl.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.curation_figurl.CurationFigurl", "title": "<code>CurationFigurl</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/curation_figurl.py</code> <pre><code>@schema\nclass CurationFigurl(dj.Computed):\n    definition = \"\"\"\n    -&gt; CurationFigurlSelection\n    ---\n    url: varchar(2000)\n    initial_curation_uri: varchar(2000)\n    new_curation_uri: varchar(2000)\n    \"\"\"\n\n    def make(self, key: dict):\n\"\"\"Create a Curation Figurl\n        Parameters\n        ----------\n        key : dict\n            primary key of an entry from CurationFigurlSelection table\n        \"\"\"\n\n        # get new_curation_uri from selection table\n        new_curation_uri = (CurationFigurlSelection &amp; key).fetch1(\n            \"new_curation_uri\"\n        )\n\n        # fetch\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n        recording_label = SpikeSortingRecording._get_recording_name(key)\n        sorting_label = SpikeSorting._get_sorting_name(key)\n        unit_metrics = _reformat_metrics(\n            (Curation &amp; key).fetch1(\"quality_metrics\")\n        )\n        initial_labels = (Curation &amp; key).fetch1(\"curation_labels\")\n        initial_merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n\n        # new_curation_uri = key[\"new_curation_uri\"]\n\n        # Create the initial curation and store it in kachery\n        for k, v in initial_labels.items():\n            new_list = []\n            for item in v:\n                if item not in new_list:\n                    new_list.append(item)\n            initial_labels[k] = new_list\n        initial_curation = {\n            \"labelsByUnit\": initial_labels,\n            \"mergeGroups\": initial_merge_groups,\n        }\n        initial_curation_uri = kcl.store_json(initial_curation)\n\n        # Get the recording/sorting extractors\n        R = si.load_extractor(recording_path)\n        if R.get_num_segments() &gt; 1:\n            R = si.concatenate_recordings([R])\n        S = si.load_extractor(sorting_path)\n\n        # Generate the figURL\n        url = _generate_the_figurl(\n            R=R,\n            S=S,\n            initial_curation_uri=initial_curation_uri,\n            new_curation_uri=new_curation_uri,\n            recording_label=recording_label,\n            sorting_label=sorting_label,\n            unit_metrics=unit_metrics,\n        )\n\n        # insert\n        key[\"url\"] = url\n        key[\"initial_curation_uri\"] = initial_curation_uri\n        key[\"new_curation_uri\"] = new_curation_uri\n        self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.curation_figurl.CurationFigurl.make", "title": "<code>make(key)</code>", "text": "<p>Create a Curation Figurl</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>primary key of an entry from CurationFigurlSelection table</p> required Source code in <code>src/spyglass/spikesorting/curation_figurl.py</code> <pre><code>def make(self, key: dict):\n\"\"\"Create a Curation Figurl\n    Parameters\n    ----------\n    key : dict\n        primary key of an entry from CurationFigurlSelection table\n    \"\"\"\n\n    # get new_curation_uri from selection table\n    new_curation_uri = (CurationFigurlSelection &amp; key).fetch1(\n        \"new_curation_uri\"\n    )\n\n    # fetch\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n    recording_label = SpikeSortingRecording._get_recording_name(key)\n    sorting_label = SpikeSorting._get_sorting_name(key)\n    unit_metrics = _reformat_metrics(\n        (Curation &amp; key).fetch1(\"quality_metrics\")\n    )\n    initial_labels = (Curation &amp; key).fetch1(\"curation_labels\")\n    initial_merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n\n    # new_curation_uri = key[\"new_curation_uri\"]\n\n    # Create the initial curation and store it in kachery\n    for k, v in initial_labels.items():\n        new_list = []\n        for item in v:\n            if item not in new_list:\n                new_list.append(item)\n        initial_labels[k] = new_list\n    initial_curation = {\n        \"labelsByUnit\": initial_labels,\n        \"mergeGroups\": initial_merge_groups,\n    }\n    initial_curation_uri = kcl.store_json(initial_curation)\n\n    # Get the recording/sorting extractors\n    R = si.load_extractor(recording_path)\n    if R.get_num_segments() &gt; 1:\n        R = si.concatenate_recordings([R])\n    S = si.load_extractor(sorting_path)\n\n    # Generate the figURL\n    url = _generate_the_figurl(\n        R=R,\n        S=S,\n        initial_curation_uri=initial_curation_uri,\n        new_curation_uri=new_curation_uri,\n        recording_label=recording_label,\n        sorting_label=sorting_label,\n        unit_metrics=unit_metrics,\n    )\n\n    # insert\n    key[\"url\"] = url\n    key[\"initial_curation_uri\"] = initial_curation_uri\n    key[\"new_curation_uri\"] = new_curation_uri\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.curation_figurl.Curation", "title": "<code>Curation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n    definition = \"\"\"\n    # Stores each spike sorting; similar to IntervalList\n    curation_id: int # a number corresponding to the index of this curation\n    -&gt; SpikeSorting\n    ---\n    parent_curation_id=-1: int\n    curation_labels: blob # a dictionary of labels for the units\n    merge_groups: blob # a list of merge groups for the units\n    quality_metrics: blob # a list of quality metrics for the units (if available)\n    description='': varchar(1000) #optional description for this curated sort\n    time_of_creation: int   # in Unix time, to the nearest second\n    \"\"\"\n\n    @staticmethod\n    def insert_curation(\n        sorting_key: dict,\n        parent_curation_id: int = -1,\n        labels=None,\n        merge_groups=None,\n        metrics=None,\n        description=\"\",\n    ):\n\"\"\"Given a SpikeSorting key and the parent_sorting_id (and optional\n        arguments) insert an entry into Curation.\n\n\n        Parameters\n        ----------\n        sorting_key : dict\n            The key for the original SpikeSorting\n        parent_curation_id : int, optional\n            The id of the parent sorting\n        labels : dict or None, optional\n        merge_groups : dict or None, optional\n        metrics : dict or None, optional\n            Computed metrics for sorting\n        description : str, optional\n            text description of this sort\n\n        Returns\n        -------\n        curation_key : dict\n\n        \"\"\"\n        if parent_curation_id == -1:\n            # check to see if this sorting with a parent of -1 has already been inserted and if so, warn the user\n            inserted_curation = (Curation &amp; sorting_key).fetch(\"KEY\")\n            if len(inserted_curation) &gt; 0:\n                Warning(\n                    \"Sorting has already been inserted, returning key to previously\"\n                    \"inserted curation\"\n                )\n                return inserted_curation[0]\n\n        if labels is None:\n            labels = {}\n        if merge_groups is None:\n            merge_groups = []\n        if metrics is None:\n            metrics = {}\n\n        # generate a unique number for this curation\n        id = (Curation &amp; sorting_key).fetch(\"curation_id\")\n        if len(id) &gt; 0:\n            curation_id = max(id) + 1\n        else:\n            curation_id = 0\n\n        # convert unit_ids in labels to integers for labels from sortingview.\n        new_labels = {int(unit_id): labels[unit_id] for unit_id in labels}\n\n        sorting_key[\"curation_id\"] = curation_id\n        sorting_key[\"parent_curation_id\"] = parent_curation_id\n        sorting_key[\"description\"] = description\n        sorting_key[\"curation_labels\"] = new_labels\n        sorting_key[\"merge_groups\"] = merge_groups\n        sorting_key[\"quality_metrics\"] = metrics\n        sorting_key[\"time_of_creation\"] = int(time.time())\n\n        # mike: added skip duplicates\n        Curation.insert1(sorting_key, skip_duplicates=True)\n\n        # get the primary key for this curation\n        c_key = Curation.fetch(\"KEY\")[0]\n        curation_key = {item: sorting_key[item] for item in c_key}\n\n        return curation_key\n\n    @staticmethod\n    def get_recording(key: dict):\n\"\"\"Returns the recording extractor for the recording related to this curation\n\n        Parameters\n        ----------\n        key : dict\n            SpikeSortingRecording key\n\n        Returns\n        -------\n        recording_extractor : spike interface recording extractor\n\n        \"\"\"\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        return si.load_extractor(recording_path)\n\n    @staticmethod\n    def get_curated_sorting(key: dict):\n\"\"\"Returns the sorting extractor related to this curation,\n        with merges applied.\n\n        Parameters\n        ----------\n        key : dict\n            Curation key\n\n        Returns\n        -------\n        sorting_extractor: spike interface sorting extractor\n\n        \"\"\"\n        sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n        sorting = si.load_extractor(sorting_path)\n        merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n        # TODO: write code to get merged sorting extractor\n        if len(merge_groups) != 0:\n            return MergedSortingExtractor(\n                parent_sorting=sorting, merge_groups=merge_groups\n            )\n        else:\n            return sorting\n\n    @staticmethod\n    def save_sorting_nwb(\n        key,\n        sorting,\n        timestamps,\n        sort_interval_list_name,\n        sort_interval,\n        labels=None,\n        metrics=None,\n        unit_ids=None,\n    ):\n\"\"\"Store a sorting in a new AnalysisNwbfile\n\n        Parameters\n        ----------\n        key : dict\n            key to SpikeSorting table\n        sorting : si.Sorting\n            sorting\n        timestamps : array_like\n            Time stamps of the sorted recoridng;\n            used to convert the spike timings from index to real time\n        sort_interval_list_name : str\n            name of sort interval\n        sort_interval : list\n            interval for start and end of sort\n        labels : dict, optional\n            curation labels, by default None\n        metrics : dict, optional\n            quality metrics, by default None\n        unit_ids : list, optional\n            IDs of units whose spiketrains to save, by default None\n\n        Returns\n        -------\n        analysis_file_name : str\n        units_object_id : str\n\n        \"\"\"\n\n        sort_interval_valid_times = (\n            IntervalList &amp; {\"interval_list_name\": sort_interval_list_name}\n        ).fetch1(\"valid_times\")\n\n        units = dict()\n        units_valid_times = dict()\n        units_sort_interval = dict()\n\n        if unit_ids is None:\n            unit_ids = sorting.get_unit_ids()\n\n        for unit_id in unit_ids:\n            spike_times_in_samples = sorting.get_unit_spike_train(\n                unit_id=unit_id\n            )\n            units[unit_id] = timestamps[spike_times_in_samples]\n            units_valid_times[unit_id] = sort_interval_valid_times\n            units_sort_interval[unit_id] = [sort_interval]\n\n        analysis_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n        object_ids = AnalysisNwbfile().add_units(\n            analysis_file_name,\n            units,\n            units_valid_times,\n            units_sort_interval,\n            metrics=metrics,\n            labels=labels,\n        )\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], analysis_file_name)\n\n        if object_ids == \"\":\n            print(\n                \"Sorting contains no units.\"\n                \"Created an empty analysis nwb file anyway.\"\n            )\n            units_object_id = \"\"\n        else:\n            units_object_id = object_ids[0]\n\n        return analysis_file_name, units_object_id\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.spikesorting_curation.Curation.insert_curation", "title": "<code>insert_curation(sorting_key, parent_curation_id=-1, labels=None, merge_groups=None, metrics=None, description='')</code>  <code>staticmethod</code>", "text": "<p>Given a SpikeSorting key and the parent_sorting_id (and optional arguments) insert an entry into Curation.</p> <p>Parameters:</p> Name Type Description Default <code>sorting_key</code> <code>dict</code> <p>The key for the original SpikeSorting</p> required <code>parent_curation_id</code> <code>int</code> <p>The id of the parent sorting</p> <code>-1</code> <code>labels</code> <code>dict or None</code> <code>None</code> <code>merge_groups</code> <code>dict or None</code> <code>None</code> <code>metrics</code> <code>dict or None</code> <p>Computed metrics for sorting</p> <code>None</code> <code>description</code> <code>str</code> <p>text description of this sort</p> <code>''</code> <p>Returns:</p> Name Type Description <code>curation_key</code> <code>dict</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef insert_curation(\n    sorting_key: dict,\n    parent_curation_id: int = -1,\n    labels=None,\n    merge_groups=None,\n    metrics=None,\n    description=\"\",\n):\n\"\"\"Given a SpikeSorting key and the parent_sorting_id (and optional\n    arguments) insert an entry into Curation.\n\n\n    Parameters\n    ----------\n    sorting_key : dict\n        The key for the original SpikeSorting\n    parent_curation_id : int, optional\n        The id of the parent sorting\n    labels : dict or None, optional\n    merge_groups : dict or None, optional\n    metrics : dict or None, optional\n        Computed metrics for sorting\n    description : str, optional\n        text description of this sort\n\n    Returns\n    -------\n    curation_key : dict\n\n    \"\"\"\n    if parent_curation_id == -1:\n        # check to see if this sorting with a parent of -1 has already been inserted and if so, warn the user\n        inserted_curation = (Curation &amp; sorting_key).fetch(\"KEY\")\n        if len(inserted_curation) &gt; 0:\n            Warning(\n                \"Sorting has already been inserted, returning key to previously\"\n                \"inserted curation\"\n            )\n            return inserted_curation[0]\n\n    if labels is None:\n        labels = {}\n    if merge_groups is None:\n        merge_groups = []\n    if metrics is None:\n        metrics = {}\n\n    # generate a unique number for this curation\n    id = (Curation &amp; sorting_key).fetch(\"curation_id\")\n    if len(id) &gt; 0:\n        curation_id = max(id) + 1\n    else:\n        curation_id = 0\n\n    # convert unit_ids in labels to integers for labels from sortingview.\n    new_labels = {int(unit_id): labels[unit_id] for unit_id in labels}\n\n    sorting_key[\"curation_id\"] = curation_id\n    sorting_key[\"parent_curation_id\"] = parent_curation_id\n    sorting_key[\"description\"] = description\n    sorting_key[\"curation_labels\"] = new_labels\n    sorting_key[\"merge_groups\"] = merge_groups\n    sorting_key[\"quality_metrics\"] = metrics\n    sorting_key[\"time_of_creation\"] = int(time.time())\n\n    # mike: added skip duplicates\n    Curation.insert1(sorting_key, skip_duplicates=True)\n\n    # get the primary key for this curation\n    c_key = Curation.fetch(\"KEY\")[0]\n    curation_key = {item: sorting_key[item] for item in c_key}\n\n    return curation_key\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.spikesorting_curation.Curation.get_recording", "title": "<code>get_recording(key)</code>  <code>staticmethod</code>", "text": "<p>Returns the recording extractor for the recording related to this curation</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>SpikeSortingRecording key</p> required <p>Returns:</p> Name Type Description <code>recording_extractor</code> <code>spike interface recording extractor</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef get_recording(key: dict):\n\"\"\"Returns the recording extractor for the recording related to this curation\n\n    Parameters\n    ----------\n    key : dict\n        SpikeSortingRecording key\n\n    Returns\n    -------\n    recording_extractor : spike interface recording extractor\n\n    \"\"\"\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    return si.load_extractor(recording_path)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.spikesorting_curation.Curation.get_curated_sorting", "title": "<code>get_curated_sorting(key)</code>  <code>staticmethod</code>", "text": "<p>Returns the sorting extractor related to this curation, with merges applied.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Curation key</p> required <p>Returns:</p> Name Type Description <code>sorting_extractor</code> <code>spike interface sorting extractor</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef get_curated_sorting(key: dict):\n\"\"\"Returns the sorting extractor related to this curation,\n    with merges applied.\n\n    Parameters\n    ----------\n    key : dict\n        Curation key\n\n    Returns\n    -------\n    sorting_extractor: spike interface sorting extractor\n\n    \"\"\"\n    sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n    sorting = si.load_extractor(sorting_path)\n    merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n    # TODO: write code to get merged sorting extractor\n    if len(merge_groups) != 0:\n        return MergedSortingExtractor(\n            parent_sorting=sorting, merge_groups=merge_groups\n        )\n    else:\n        return sorting\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.spikesorting_curation.Curation.save_sorting_nwb", "title": "<code>save_sorting_nwb(key, sorting, timestamps, sort_interval_list_name, sort_interval, labels=None, metrics=None, unit_ids=None)</code>  <code>staticmethod</code>", "text": "<p>Store a sorting in a new AnalysisNwbfile</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>key to SpikeSorting table</p> required <code>sorting</code> <code>si.Sorting</code> <p>sorting</p> required <code>timestamps</code> <code>array_like</code> <p>Time stamps of the sorted recoridng; used to convert the spike timings from index to real time</p> required <code>sort_interval_list_name</code> <code>str</code> <p>name of sort interval</p> required <code>sort_interval</code> <code>list</code> <p>interval for start and end of sort</p> required <code>labels</code> <code>dict</code> <p>curation labels, by default None</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>quality metrics, by default None</p> <code>None</code> <code>unit_ids</code> <code>list</code> <p>IDs of units whose spiketrains to save, by default None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <code>units_object_id</code> <code>str</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef save_sorting_nwb(\n    key,\n    sorting,\n    timestamps,\n    sort_interval_list_name,\n    sort_interval,\n    labels=None,\n    metrics=None,\n    unit_ids=None,\n):\n\"\"\"Store a sorting in a new AnalysisNwbfile\n\n    Parameters\n    ----------\n    key : dict\n        key to SpikeSorting table\n    sorting : si.Sorting\n        sorting\n    timestamps : array_like\n        Time stamps of the sorted recoridng;\n        used to convert the spike timings from index to real time\n    sort_interval_list_name : str\n        name of sort interval\n    sort_interval : list\n        interval for start and end of sort\n    labels : dict, optional\n        curation labels, by default None\n    metrics : dict, optional\n        quality metrics, by default None\n    unit_ids : list, optional\n        IDs of units whose spiketrains to save, by default None\n\n    Returns\n    -------\n    analysis_file_name : str\n    units_object_id : str\n\n    \"\"\"\n\n    sort_interval_valid_times = (\n        IntervalList &amp; {\"interval_list_name\": sort_interval_list_name}\n    ).fetch1(\"valid_times\")\n\n    units = dict()\n    units_valid_times = dict()\n    units_sort_interval = dict()\n\n    if unit_ids is None:\n        unit_ids = sorting.get_unit_ids()\n\n    for unit_id in unit_ids:\n        spike_times_in_samples = sorting.get_unit_spike_train(\n            unit_id=unit_id\n        )\n        units[unit_id] = timestamps[spike_times_in_samples]\n        units_valid_times[unit_id] = sort_interval_valid_times\n        units_sort_interval[unit_id] = [sort_interval]\n\n    analysis_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n    object_ids = AnalysisNwbfile().add_units(\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=metrics,\n        labels=labels,\n    )\n    AnalysisNwbfile().add(key[\"nwb_file_name\"], analysis_file_name)\n\n    if object_ids == \"\":\n        print(\n            \"Sorting contains no units.\"\n            \"Created an empty analysis nwb file anyway.\"\n        )\n        units_object_id = \"\"\n    else:\n        units_object_id = object_ids[0]\n\n    return analysis_file_name, units_object_id\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.curation_figurl.SpikeSorting", "title": "<code>SpikeSorting</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>@schema\nclass SpikeSorting(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingSelection\n    ---\n    sorting_path: varchar(1000)\n    time_of_sort: int   # in Unix time, to the nearest second\n    \"\"\"\n\n    def make(self, key: dict):\n\"\"\"Runs spike sorting on the data and parameters specified by the\n        SpikeSortingSelection table and inserts a new entry to SpikeSorting table.\n\n        Specifically,\n        1. Loads saved recording and runs the sort on it with spikeinterface\n        2. Saves the sorting with spikeinterface\n        3. Creates an analysis NWB file and saves the sorting there\n           (this is redundant with 2; will change in the future)\n\n        \"\"\"\n\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        recording = si.load_extractor(recording_path)\n\n        # first, get the timestamps\n        timestamps = SpikeSortingRecording._get_recording_timestamps(recording)\n        fs = recording.get_sampling_frequency()\n        # then concatenate the recordings\n        # Note: the timestamps are lost upon concatenation,\n        # i.e. concat_recording.get_times() doesn't return true timestamps anymore.\n        # but concat_recording.recoring_list[i].get_times() will return correct\n        # timestamps for ith recording.\n        if recording.get_num_segments() &gt; 1 and isinstance(\n            recording, si.AppendSegmentRecording\n        ):\n            recording = si.concatenate_recordings(recording.recording_list)\n        elif recording.get_num_segments() &gt; 1 and isinstance(\n            recording, si.BinaryRecordingExtractor\n        ):\n            recording = si.concatenate_recordings([recording])\n\n        # load artifact intervals\n        artifact_times = (\n            ArtifactRemovedIntervalList\n            &amp; {\n                \"artifact_removed_interval_list_name\": key[\n                    \"artifact_removed_interval_list_name\"\n                ]\n            }\n        ).fetch1(\"artifact_times\")\n        if len(artifact_times):\n            if artifact_times.ndim == 1:\n                artifact_times = np.expand_dims(artifact_times, 0)\n\n            # convert artifact intervals to indices\n            list_triggers = []\n            for interval in artifact_times:\n                list_triggers.append(\n                    np.arange(\n                        np.searchsorted(timestamps, interval[0]),\n                        np.searchsorted(timestamps, interval[1]),\n                    )\n                )\n            list_triggers = [list(np.concatenate(list_triggers))]\n            recording = sip.remove_artifacts(\n                recording=recording,\n                list_triggers=list_triggers,\n                ms_before=None,\n                ms_after=None,\n                mode=\"zeros\",\n            )\n\n        print(f\"Running spike sorting on {key}...\")\n        sorter, sorter_params = (SpikeSorterParameters &amp; key).fetch1(\n            \"sorter\", \"sorter_params\"\n        )\n\n        sorter_temp_dir = tempfile.TemporaryDirectory(\n            dir=os.getenv(\"SPYGLASS_TEMP_DIR\")\n        )\n        # add tempdir option for mountainsort\n        sorter_params[\"tempdir\"] = sorter_temp_dir.name\n\n        if sorter == \"clusterless_thresholder\":\n            # need to remove tempdir and whiten from sorter_params\n            sorter_params.pop(\"tempdir\", None)\n            sorter_params.pop(\"whiten\", None)\n\n            # Detect peaks for clusterless decoding\n            detected_spikes = detect_peaks(recording, **sorter_params)\n            sorting = si.NumpySorting.from_times_labels(\n                times_list=detected_spikes[\"sample_ind\"],\n                labels_list=np.zeros(len(detected_spikes), dtype=np.int),\n                sampling_frequency=recording.get_sampling_frequency(),\n            )\n        else:\n            if \"whiten\" in sorter_params.keys():\n                if sorter_params[\"whiten\"]:\n                    sorter_params[\"whiten\"] = False  # set whiten to False\n            # whiten recording separately; make sure dtype is float32\n            # to avoid downstream error with svd\n            recording = sip.whiten(recording, dtype=\"float32\")\n            sorting = sis.run_sorter(\n                sorter,\n                recording,\n                output_folder=sorter_temp_dir.name,\n                delete_output_folder=True,\n                **sorter_params,\n            )\n        key[\"time_of_sort\"] = int(time.time())\n\n        print(\"Saving sorting results...\")\n        sorting_folder = Path(os.getenv(\"SPYGLASS_SORTING_DIR\"))\n        sorting_name = self._get_sorting_name(key)\n        key[\"sorting_path\"] = str(sorting_folder / Path(sorting_name))\n        if os.path.exists(key[\"sorting_path\"]):\n            shutil.rmtree(key[\"sorting_path\"])\n        sorting = sorting.save(folder=key[\"sorting_path\"])\n        self.insert1(key)\n\n    def delete(self):\n\"\"\"Extends the delete method of base class to implement permission checking.\n        Note that this is NOT a security feature, as anyone that has access to source code\n        can disable it; it just makes it less likely to accidentally delete entries.\n        \"\"\"\n        current_user_name = dj.config[\"database.user\"]\n        entries = self.fetch()\n        permission_bool = np.zeros((len(entries),))\n        print(\n            f\"Attempting to delete {len(entries)} entries, checking permission...\"\n        )\n\n        for entry_idx in range(len(entries)):\n            # check the team name for the entry, then look up the members in that team,\n            # then get their datajoint user names\n            team_name = (\n                SpikeSortingRecordingSelection\n                &amp; (SpikeSortingRecordingSelection &amp; entries[entry_idx]).proj()\n            ).fetch1()[\"team_name\"]\n            lab_member_name_list = (\n                LabTeam.LabTeamMember &amp; {\"team_name\": team_name}\n            ).fetch(\"lab_member_name\")\n            datajoint_user_names = []\n            for lab_member_name in lab_member_name_list:\n                datajoint_user_names.append(\n                    (\n                        LabMember.LabMemberInfo\n                        &amp; {\"lab_member_name\": lab_member_name}\n                    ).fetch1(\"datajoint_user_name\")\n                )\n            permission_bool[entry_idx] = (\n                current_user_name in datajoint_user_names\n            )\n        if np.sum(permission_bool) == len(entries):\n            print(\"Permission to delete all specified entries granted.\")\n            super().delete()\n        else:\n            raise Exception(\n                \"You do not have permission to delete all specified\"\n                \"entries. Not deleting anything.\"\n            )\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        raise NotImplementedError\n        return None\n        # return fetch_nwb(self, (AnalysisNwbfile, 'analysis_file_abs_path'), *attrs, **kwargs)\n\n    def nightly_cleanup(self):\n\"\"\"Clean up spike sorting directories that are not in the SpikeSorting table.\n        This should be run after AnalysisNwbFile().nightly_cleanup()\n        \"\"\"\n        # get a list of the files in the spike sorting storage directory\n        dir_names = next(os.walk(os.environ[\"SPYGLASS_SORTING_DIR\"]))[1]\n        # now retrieve a list of the currently used analysis nwb files\n        analysis_file_names = self.fetch(\"analysis_file_name\")\n        for dir in dir_names:\n            if dir not in analysis_file_names:\n                full_path = str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n                print(f\"removing {full_path}\")\n                shutil.rmtree(\n                    str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n                )\n\n    @staticmethod\n    def _get_sorting_name(key):\n        recording_name = SpikeSortingRecording._get_recording_name(key)\n        sorting_name = (\n            recording_name + \"_\" + str(uuid.uuid4())[0:8] + \"_spikesorting\"\n        )\n        return sorting_name\n\n    # TODO: write a function to import sorting done outside of dj\n\n    def _import_sorting(self, key):\n        raise NotImplementedError\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.make", "title": "<code>make(key)</code>", "text": "<p>Runs spike sorting on the data and parameters specified by the SpikeSortingSelection table and inserts a new entry to SpikeSorting table.</p> <p>Specifically, 1. Loads saved recording and runs the sort on it with spikeinterface 2. Saves the sorting with spikeinterface 3. Creates an analysis NWB file and saves the sorting there    (this is redundant with 2; will change in the future)</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def make(self, key: dict):\n\"\"\"Runs spike sorting on the data and parameters specified by the\n    SpikeSortingSelection table and inserts a new entry to SpikeSorting table.\n\n    Specifically,\n    1. Loads saved recording and runs the sort on it with spikeinterface\n    2. Saves the sorting with spikeinterface\n    3. Creates an analysis NWB file and saves the sorting there\n       (this is redundant with 2; will change in the future)\n\n    \"\"\"\n\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    recording = si.load_extractor(recording_path)\n\n    # first, get the timestamps\n    timestamps = SpikeSortingRecording._get_recording_timestamps(recording)\n    fs = recording.get_sampling_frequency()\n    # then concatenate the recordings\n    # Note: the timestamps are lost upon concatenation,\n    # i.e. concat_recording.get_times() doesn't return true timestamps anymore.\n    # but concat_recording.recoring_list[i].get_times() will return correct\n    # timestamps for ith recording.\n    if recording.get_num_segments() &gt; 1 and isinstance(\n        recording, si.AppendSegmentRecording\n    ):\n        recording = si.concatenate_recordings(recording.recording_list)\n    elif recording.get_num_segments() &gt; 1 and isinstance(\n        recording, si.BinaryRecordingExtractor\n    ):\n        recording = si.concatenate_recordings([recording])\n\n    # load artifact intervals\n    artifact_times = (\n        ArtifactRemovedIntervalList\n        &amp; {\n            \"artifact_removed_interval_list_name\": key[\n                \"artifact_removed_interval_list_name\"\n            ]\n        }\n    ).fetch1(\"artifact_times\")\n    if len(artifact_times):\n        if artifact_times.ndim == 1:\n            artifact_times = np.expand_dims(artifact_times, 0)\n\n        # convert artifact intervals to indices\n        list_triggers = []\n        for interval in artifact_times:\n            list_triggers.append(\n                np.arange(\n                    np.searchsorted(timestamps, interval[0]),\n                    np.searchsorted(timestamps, interval[1]),\n                )\n            )\n        list_triggers = [list(np.concatenate(list_triggers))]\n        recording = sip.remove_artifacts(\n            recording=recording,\n            list_triggers=list_triggers,\n            ms_before=None,\n            ms_after=None,\n            mode=\"zeros\",\n        )\n\n    print(f\"Running spike sorting on {key}...\")\n    sorter, sorter_params = (SpikeSorterParameters &amp; key).fetch1(\n        \"sorter\", \"sorter_params\"\n    )\n\n    sorter_temp_dir = tempfile.TemporaryDirectory(\n        dir=os.getenv(\"SPYGLASS_TEMP_DIR\")\n    )\n    # add tempdir option for mountainsort\n    sorter_params[\"tempdir\"] = sorter_temp_dir.name\n\n    if sorter == \"clusterless_thresholder\":\n        # need to remove tempdir and whiten from sorter_params\n        sorter_params.pop(\"tempdir\", None)\n        sorter_params.pop(\"whiten\", None)\n\n        # Detect peaks for clusterless decoding\n        detected_spikes = detect_peaks(recording, **sorter_params)\n        sorting = si.NumpySorting.from_times_labels(\n            times_list=detected_spikes[\"sample_ind\"],\n            labels_list=np.zeros(len(detected_spikes), dtype=np.int),\n            sampling_frequency=recording.get_sampling_frequency(),\n        )\n    else:\n        if \"whiten\" in sorter_params.keys():\n            if sorter_params[\"whiten\"]:\n                sorter_params[\"whiten\"] = False  # set whiten to False\n        # whiten recording separately; make sure dtype is float32\n        # to avoid downstream error with svd\n        recording = sip.whiten(recording, dtype=\"float32\")\n        sorting = sis.run_sorter(\n            sorter,\n            recording,\n            output_folder=sorter_temp_dir.name,\n            delete_output_folder=True,\n            **sorter_params,\n        )\n    key[\"time_of_sort\"] = int(time.time())\n\n    print(\"Saving sorting results...\")\n    sorting_folder = Path(os.getenv(\"SPYGLASS_SORTING_DIR\"))\n    sorting_name = self._get_sorting_name(key)\n    key[\"sorting_path\"] = str(sorting_folder / Path(sorting_name))\n    if os.path.exists(key[\"sorting_path\"]):\n        shutil.rmtree(key[\"sorting_path\"])\n    sorting = sorting.save(folder=key[\"sorting_path\"])\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.delete", "title": "<code>delete()</code>", "text": "<p>Extends the delete method of base class to implement permission checking. Note that this is NOT a security feature, as anyone that has access to source code can disable it; it just makes it less likely to accidentally delete entries.</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def delete(self):\n\"\"\"Extends the delete method of base class to implement permission checking.\n    Note that this is NOT a security feature, as anyone that has access to source code\n    can disable it; it just makes it less likely to accidentally delete entries.\n    \"\"\"\n    current_user_name = dj.config[\"database.user\"]\n    entries = self.fetch()\n    permission_bool = np.zeros((len(entries),))\n    print(\n        f\"Attempting to delete {len(entries)} entries, checking permission...\"\n    )\n\n    for entry_idx in range(len(entries)):\n        # check the team name for the entry, then look up the members in that team,\n        # then get their datajoint user names\n        team_name = (\n            SpikeSortingRecordingSelection\n            &amp; (SpikeSortingRecordingSelection &amp; entries[entry_idx]).proj()\n        ).fetch1()[\"team_name\"]\n        lab_member_name_list = (\n            LabTeam.LabTeamMember &amp; {\"team_name\": team_name}\n        ).fetch(\"lab_member_name\")\n        datajoint_user_names = []\n        for lab_member_name in lab_member_name_list:\n            datajoint_user_names.append(\n                (\n                    LabMember.LabMemberInfo\n                    &amp; {\"lab_member_name\": lab_member_name}\n                ).fetch1(\"datajoint_user_name\")\n            )\n        permission_bool[entry_idx] = (\n            current_user_name in datajoint_user_names\n        )\n    if np.sum(permission_bool) == len(entries):\n        print(\"Permission to delete all specified entries granted.\")\n        super().delete()\n    else:\n        raise Exception(\n            \"You do not have permission to delete all specified\"\n            \"entries. Not deleting anything.\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.nightly_cleanup", "title": "<code>nightly_cleanup()</code>", "text": "<p>Clean up spike sorting directories that are not in the SpikeSorting table. This should be run after AnalysisNwbFile().nightly_cleanup()</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def nightly_cleanup(self):\n\"\"\"Clean up spike sorting directories that are not in the SpikeSorting table.\n    This should be run after AnalysisNwbFile().nightly_cleanup()\n    \"\"\"\n    # get a list of the files in the spike sorting storage directory\n    dir_names = next(os.walk(os.environ[\"SPYGLASS_SORTING_DIR\"]))[1]\n    # now retrieve a list of the currently used analysis nwb files\n    analysis_file_names = self.fetch(\"analysis_file_name\")\n    for dir in dir_names:\n        if dir not in analysis_file_names:\n            full_path = str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n            print(f\"removing {full_path}\")\n            shutil.rmtree(\n                str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n            )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/curation_figurl/#src.spyglass.spikesorting.curation_figurl.SpikeSortingRecording", "title": "<code>SpikeSortingRecording</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>@schema\nclass SpikeSortingRecording(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingRecordingSelection\n    ---\n    recording_path: varchar(1000)\n    -&gt; IntervalList.proj(sort_interval_list_name='interval_list_name')\n    \"\"\"\n\n    def make(self, key):\n        sort_interval_valid_times = self._get_sort_interval_valid_times(key)\n        recording = self._get_filtered_recording(key)\n        recording_name = self._get_recording_name(key)\n\n        tmp_key = {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": recording_name,\n            \"valid_times\": sort_interval_valid_times,\n        }\n        IntervalList.insert1(tmp_key, replace=True)\n\n        # store the list of valid times for the sort\n        key[\"sort_interval_list_name\"] = tmp_key[\"interval_list_name\"]\n\n        # Path to files that will hold the recording extractors\n        recording_folder = Path(os.getenv(\"SPYGLASS_RECORDING_DIR\"))\n        key[\"recording_path\"] = str(recording_folder / Path(recording_name))\n        if os.path.exists(key[\"recording_path\"]):\n            shutil.rmtree(key[\"recording_path\"])\n        recording = recording.save(\n            folder=key[\"recording_path\"], chunk_duration=\"10000ms\", n_jobs=8\n        )\n\n        self.insert1(key)\n\n    @staticmethod\n    def _get_recording_name(key):\n        recording_name = (\n            key[\"nwb_file_name\"]\n            + \"_\"\n            + key[\"sort_interval_name\"]\n            + \"_\"\n            + str(key[\"sort_group_id\"])\n            + \"_\"\n            + key[\"preproc_params_name\"]\n        )\n        return recording_name\n\n    @staticmethod\n    def _get_recording_timestamps(recording):\n        if recording.get_num_segments() &gt; 1:\n            frames_per_segment = [0]\n            for i in range(recording.get_num_segments()):\n                frames_per_segment.append(\n                    recording.get_num_frames(segment_index=i)\n                )\n\n            cumsum_frames = np.cumsum(frames_per_segment)\n            total_frames = np.sum(frames_per_segment)\n\n            timestamps = np.zeros((total_frames,))\n            for i in range(recording.get_num_segments()):\n                timestamps[\n                    cumsum_frames[i] : cumsum_frames[i + 1]\n                ] = recording.get_times(segment_index=i)\n        else:\n            timestamps = recording.get_times()\n        return timestamps\n\n    def _get_sort_interval_valid_times(self, key):\n\"\"\"Identifies the intersection between sort interval specified by the user\n        and the valid times (times for which neural data exist)\n\n        Parameters\n        ----------\n        key: dict\n            specifies a (partially filled) entry of SpikeSorting table\n\n        Returns\n        -------\n        sort_interval_valid_times: ndarray of tuples\n            (start, end) times for valid stretches of the sorting interval\n\n        \"\"\"\n        sort_interval = (\n            SortInterval\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_interval_name\": key[\"sort_interval_name\"],\n            }\n        ).fetch1(\"sort_interval\")\n        interval_list_name = (SpikeSortingRecordingSelection &amp; key).fetch1(\n            \"interval_list_name\"\n        )\n        valid_interval_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n        valid_sort_times = interval_list_intersect(\n            sort_interval, valid_interval_times\n        )\n        # Exclude intervals shorter than specified length\n        params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        if \"min_segment_length\" in params:\n            valid_sort_times = intervals_by_length(\n                valid_sort_times, min_length=params[\"min_segment_length\"]\n            )\n        return valid_sort_times\n\n    def _get_filtered_recording(self, key: dict):\n\"\"\"Filters and references a recording\n        * Loads the NWB file created during insertion as a spikeinterface Recording\n        * Slices recording in time (interval) and space (channels);\n          recording chunks from disjoint intervals are concatenated\n        * Applies referencing and bandpass filtering\n\n        Parameters\n        ----------\n        key: dict,\n            primary key of SpikeSortingRecording table\n\n        Returns\n        -------\n        recording: si.Recording\n        \"\"\"\n\n        nwb_file_abs_path = Nwbfile().get_abs_path(key[\"nwb_file_name\"])\n        recording = se.read_nwb_recording(\n            nwb_file_abs_path, load_time_vector=True\n        )\n\n        valid_sort_times = self._get_sort_interval_valid_times(key)\n        # shape is (N, 2)\n        valid_sort_times_indices = np.array(\n            [\n                np.searchsorted(recording.get_times(), interval)\n                for interval in valid_sort_times\n            ]\n        )\n        # join intervals of indices that are adjacent\n        valid_sort_times_indices = reduce(\n            union_adjacent_index, valid_sort_times_indices\n        )\n        if valid_sort_times_indices.ndim == 1:\n            valid_sort_times_indices = np.expand_dims(\n                valid_sort_times_indices, 0\n            )\n\n        # create an AppendRecording if there is more than one disjoint sort interval\n        if len(valid_sort_times_indices) &gt; 1:\n            recordings_list = []\n            for interval_indices in valid_sort_times_indices:\n                recording_single = recording.frame_slice(\n                    start_frame=interval_indices[0],\n                    end_frame=interval_indices[1],\n                )\n                recordings_list.append(recording_single)\n            recording = si.append_recordings(recordings_list)\n        else:\n            recording = recording.frame_slice(\n                start_frame=valid_sort_times_indices[0][0],\n                end_frame=valid_sort_times_indices[0][1],\n            )\n\n        channel_ids = (\n            SortGroup.SortGroupElectrode\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch(\"electrode_id\")\n        ref_channel_id = (\n            SortGroup\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch1(\"sort_reference_electrode_id\")\n        channel_ids = np.setdiff1d(channel_ids, ref_channel_id)\n\n        # include ref channel in first slice, then exclude it in second slice\n        if ref_channel_id &gt;= 0:\n            channel_ids_ref = np.append(channel_ids, ref_channel_id)\n            recording = recording.channel_slice(channel_ids=channel_ids_ref)\n\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"single\", ref_channel_ids=ref_channel_id\n            )\n            recording = recording.channel_slice(channel_ids=channel_ids)\n        elif ref_channel_id == -2:\n            recording = recording.channel_slice(channel_ids=channel_ids)\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"global\", operator=\"median\"\n            )\n        else:\n            raise ValueError(\"Invalid reference channel ID\")\n        filter_params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        recording = si.preprocessing.bandpass_filter(\n            recording,\n            freq_min=filter_params[\"frequency_min\"],\n            freq_max=filter_params[\"frequency_max\"],\n        )\n\n        # if the sort group is a tetrode, change the channel location\n        # note that this is a workaround that would be deprecated when spikeinterface uses 3D probe locations\n        probe_type = []\n        electrode_group = []\n        for channel_id in channel_ids:\n            probe_type.append(\n                (\n                    Electrode * Probe\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"probe_type\")\n            )\n            electrode_group.append(\n                (\n                    Electrode\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"electrode_group_name\")\n            )\n        if (\n            all(p == \"tetrode_12.5\" for p in probe_type)\n            and len(probe_type) == 4\n            and all(eg == electrode_group[0] for eg in electrode_group)\n        ):\n            tetrode = pi.Probe(ndim=2)\n            position = [[0, 0], [0, 12.5], [12.5, 0], [12.5, 12.5]]\n            tetrode.set_contacts(\n                position, shapes=\"circle\", shape_params={\"radius\": 6.25}\n            )\n            tetrode.set_contact_ids(channel_ids)\n            tetrode.set_device_channel_indices(np.arange(4))\n            recording = recording.set_probe(tetrode, in_place=True)\n\n        return recording\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/merged_sorting_extractor/", "title": "merged_sorting_extractor.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/sortingview/", "title": "sortingview.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.LabMember", "title": "<code>LabMember</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass LabMember(dj.Manual):\n    definition = \"\"\"\n    lab_member_name: varchar(80)\n    ---\n    first_name: varchar(200)\n    last_name: varchar(200)\n    \"\"\"\n\n    # NOTE that names must be unique here. If there are two neuroscientists named Jack Black that have data in this\n    # database, this will create an incorrect linkage. NWB does not yet provide unique IDs for names.\n\n    class LabMemberInfo(dj.Part):\n        definition = \"\"\"\n        # Information about lab member in the context of Frank lab network\n        -&gt; LabMember\n        ---\n        google_user_name: varchar(200)              # used for permission to curate\n        datajoint_user_name = \"\": varchar(200)      # used for permission to delete entries\n        \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab member information from an NWB file.\n\n        Parameters\n        ----------\n        nwbf: pynwb.NWBFile\n            The NWB file with experimenter information.\n        \"\"\"\n        if nwbf.experimenter is None:\n            print(\"No experimenter metadata found.\\n\")\n            return\n        for experimenter in nwbf.experimenter:\n            cls.insert_from_name(experimenter)\n            # each person is by default the member of their own LabTeam (same as their name)\n            LabTeam.create_new_team(\n                team_name=experimenter, team_members=[experimenter]\n            )\n\n    @classmethod\n    def insert_from_name(cls, full_name):\n\"\"\"Insert a lab member by name.\n\n        The first name is the part of the name that precedes the last space, and the last name is the part of the\n        name that follows the last space.\n\n        Parameters\n        ----------\n        full_name : str\n            The name to be added.\n        \"\"\"\n        labmember_dict = dict()\n        labmember_dict[\"lab_member_name\"] = full_name\n        full_name_split = str.split(full_name)\n        labmember_dict[\"first_name\"] = \" \".join(full_name_split[:-1])\n        labmember_dict[\"last_name\"] = full_name_split[-1]\n        cls.insert1(labmember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.common.common_lab.LabMember.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert lab member information from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <p>The NWB file with experimenter information.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab member information from an NWB file.\n\n    Parameters\n    ----------\n    nwbf: pynwb.NWBFile\n        The NWB file with experimenter information.\n    \"\"\"\n    if nwbf.experimenter is None:\n        print(\"No experimenter metadata found.\\n\")\n        return\n    for experimenter in nwbf.experimenter:\n        cls.insert_from_name(experimenter)\n        # each person is by default the member of their own LabTeam (same as their name)\n        LabTeam.create_new_team(\n            team_name=experimenter, team_members=[experimenter]\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.common.common_lab.LabMember.insert_from_name", "title": "<code>insert_from_name(full_name)</code>  <code>classmethod</code>", "text": "<p>Insert a lab member by name.</p> <p>The first name is the part of the name that precedes the last space, and the last name is the part of the name that follows the last space.</p> <p>Parameters:</p> Name Type Description Default <code>full_name</code> <code>str</code> <p>The name to be added.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_name(cls, full_name):\n\"\"\"Insert a lab member by name.\n\n    The first name is the part of the name that precedes the last space, and the last name is the part of the\n    name that follows the last space.\n\n    Parameters\n    ----------\n    full_name : str\n        The name to be added.\n    \"\"\"\n    labmember_dict = dict()\n    labmember_dict[\"lab_member_name\"] = full_name\n    full_name_split = str.split(full_name)\n    labmember_dict[\"first_name\"] = \" \".join(full_name_split[:-1])\n    labmember_dict[\"last_name\"] = full_name_split[-1]\n    cls.insert1(labmember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.SortingviewWorkspace", "title": "<code>SortingviewWorkspace</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/sortingview.py</code> <pre><code>@schema\nclass SortingviewWorkspace(dj.Computed):\n    definition = \"\"\"\n    -&gt; SortingviewWorkspaceSelection\n    ---\n    workspace_uri: varchar(1000)\n    sortingview_recording_id: varchar(30)\n    sortingview_sorting_id: varchar(30)\n    channel = NULL : varchar(80)        # the name of kachery channel for data sharing (for kachery daemon, deprecated)\n    \"\"\"\n\n    # make class for parts table to hold URLs\n    class URL(dj.Part):\n        # Table for holding URLs\n        definition = \"\"\"\n        -&gt; SortingviewWorkspace\n        ---\n        curation_url: varchar(1000)   # URL with sortingview data\n        curation_jot: varchar(200)   # URI for saving manual curation tags\n        \"\"\"\n\n    def make(self, key: dict):\n\"\"\"Create a Sortingview workspace\n\n        Parameters\n        ----------\n        key : dict\n            primary key of an entry from SortingviewWorkspaceSelection table\n        \"\"\"\n\n        # fetch\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n        merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n        workspace_label = SpikeSortingRecording._get_recording_name(key)\n        recording_label = SpikeSortingRecording._get_recording_name(key)\n        sorting_label = SpikeSorting._get_sorting_name(key)\n        metrics = (Curation &amp; key).fetch1(\"quality_metrics\")\n        curation_labels = (Curation &amp; key).fetch1(\"curation_labels\")\n        team_name = (SpikeSortingRecording &amp; key).fetch1()[\"team_name\"]\n        team_members = (LabTeam.LabTeamMember &amp; {\"team_name\": team_name}).fetch(\n            \"lab_member_name\"\n        )\n        google_user_ids = []\n        for team_member in team_members:\n            google_user_id = (\n                LabMember.LabMemberInfo &amp; {\"lab_member_name\": team_member}\n            ).fetch(\"google_user_name\")\n            if len(google_user_id) != 1:\n                print(\n                    f\"Google user ID for {team_member} does not exist or more than one ID detected;\\\n                        permission not given to {team_member}, skipping...\"\n                )\n                continue\n            google_user_ids.append(google_user_id[0])\n\n        # do\n        (\n            workspace_uri,\n            recording_id,\n            sorting_id,\n        ) = _create_spikesortingview_workspace(\n            recording_path=recording_path,\n            sorting_path=sorting_path,\n            merge_groups=merge_groups,\n            workspace_label=workspace_label,\n            recording_label=recording_label,\n            sorting_label=sorting_label,\n            metrics=metrics,\n            curation_labels=curation_labels,\n            google_user_ids=google_user_ids,\n        )\n\n        # insert\n        key[\"workspace_uri\"] = workspace_uri\n        key[\"sortingview_recording_id\"] = recording_id\n        key[\"sortingview_sorting_id\"] = sorting_id\n        self.insert1(key)\n\n        # insert URLs\n        # remove non-primary keys\n        del key[\"workspace_uri\"]\n        del key[\"sortingview_recording_id\"]\n        del key[\"sortingview_sorting_id\"]\n\n        # generate URLs and add to key\n        url = self.url_trythis(key)\n        # url = _generate_url(key)\n        # print(\"URL:\", url)\n        key[\"curation_url\"] = url\n        key[\"curation_jot\"] = \"not ready yet\"\n\n        SortingviewWorkspace.URL.insert1(key)\n\n    def remove_sorting_from_workspace(self, key):\n        return NotImplementedError\n\n    def url_trythis(self, key: dict, sortingview_sorting_id: str = None):\n\"\"\"Generate a URL for visualizing and curating a sorting on the web.\n        Will print instructions on how to do the curation.\n\n        Parameters\n        ----------\n        key : dict\n            An entry from SortingviewWorkspace table\n        sortingview_sorting_id : str, optional\n            sortingview sorting ID to visualize. If None then chooses the first one\n\n        Returns\n        -------\n        url : str\n        \"\"\"\n        workspace_uri = (self &amp; key).fetch1(\"workspace_uri\")\n        workspace = sv.load_workspace(workspace_uri)\n        recording_id = workspace.recording_ids[0]\n        if sortingview_sorting_id is None:\n            sortingview_sorting_id = workspace.sorting_ids[0]\n\n        R = workspace.get_recording_extractor(recording_id)\n        S = workspace.get_sorting_extractor(sortingview_sorting_id)\n\n        initial_labels = (Curation &amp; key).fetch(\"curation_labels\")[0]\n        for k, v in initial_labels.items():\n            new_list = []\n            for item in v:\n                if item not in new_list:\n                    new_list.append(item)\n            initial_labels[k] = new_list\n        initial_curation = {\"labelsByUnit\": initial_labels}\n\n        # custom metrics\n        unit_metrics = workspace.get_unit_metrics_for_sorting(\n            sortingview_sorting_id\n        )\n\n        # This will print some instructions on how to do the curation\n        # old: sv.trythis_start_sorting_curation\n        url = _generate_url(\n            recording=R,\n            sorting=S,\n            label=workspace.label,\n            initial_curation=initial_curation,\n            raster_plot_subsample_max_firing_rate=50,\n            spike_amplitudes_subsample_max_firing_rate=50,\n            unit_metrics=unit_metrics,\n        )\n        return url\n\n    def insert_manual_curation(\n        self, key: dict, url: str, description=\"manually curated\"\n    ):\n\"\"\"Based on information in key for an SortingviewWorkspace, loads the\n        curated sorting from sortingview, saves it (with labels and the\n        optional description) and inserts it to CuratedSorting\n\n        Assumes that the workspace corresponding to the recording and (original) sorting exists\n\n        Parameters\n        ----------\n        key : dict\n            primary key of AutomaticCuration\n        description: str, optional\n            description of curated sorting\n        \"\"\"\n\n        # get the labels and remove the non-primary merged units\n        # labels = workspace.get_sorting_curation(sorting_id=sortingview_sorting_id)\n        # labels = sv.trythis_load_sorting_curation('jot://xTzzyDieQPkW')\n        labels = sv.trythis_load_sorting_curation(url)\n\n        # turn labels to list of str, only including accepted units.\n        # if bool(labels[\"mergeGroups\"]):\n        if bool(labels.get(\"mergeGroups\", [])):\n            # clusters were merged, so we empty out metrics\n            metrics = {}\n        else:\n            # get the metrics from the parent curation\n            metrics = (Curation &amp; key).fetch1(\"quality_metrics\")\n\n        # insert this curation into the  Table\n        return Curation.insert_curation(\n            key,\n            parent_curation_id=key[\"curation_id\"],\n            labels=labels[\"labelsByUnit\"],\n            merge_groups=labels.get(\"mergeGroups\", []),\n            metrics=metrics,\n            description=description,\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.SortingviewWorkspace.make", "title": "<code>make(key)</code>", "text": "<p>Create a Sortingview workspace</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>primary key of an entry from SortingviewWorkspaceSelection table</p> required Source code in <code>src/spyglass/spikesorting/sortingview.py</code> <pre><code>def make(self, key: dict):\n\"\"\"Create a Sortingview workspace\n\n    Parameters\n    ----------\n    key : dict\n        primary key of an entry from SortingviewWorkspaceSelection table\n    \"\"\"\n\n    # fetch\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n    merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n    workspace_label = SpikeSortingRecording._get_recording_name(key)\n    recording_label = SpikeSortingRecording._get_recording_name(key)\n    sorting_label = SpikeSorting._get_sorting_name(key)\n    metrics = (Curation &amp; key).fetch1(\"quality_metrics\")\n    curation_labels = (Curation &amp; key).fetch1(\"curation_labels\")\n    team_name = (SpikeSortingRecording &amp; key).fetch1()[\"team_name\"]\n    team_members = (LabTeam.LabTeamMember &amp; {\"team_name\": team_name}).fetch(\n        \"lab_member_name\"\n    )\n    google_user_ids = []\n    for team_member in team_members:\n        google_user_id = (\n            LabMember.LabMemberInfo &amp; {\"lab_member_name\": team_member}\n        ).fetch(\"google_user_name\")\n        if len(google_user_id) != 1:\n            print(\n                f\"Google user ID for {team_member} does not exist or more than one ID detected;\\\n                    permission not given to {team_member}, skipping...\"\n            )\n            continue\n        google_user_ids.append(google_user_id[0])\n\n    # do\n    (\n        workspace_uri,\n        recording_id,\n        sorting_id,\n    ) = _create_spikesortingview_workspace(\n        recording_path=recording_path,\n        sorting_path=sorting_path,\n        merge_groups=merge_groups,\n        workspace_label=workspace_label,\n        recording_label=recording_label,\n        sorting_label=sorting_label,\n        metrics=metrics,\n        curation_labels=curation_labels,\n        google_user_ids=google_user_ids,\n    )\n\n    # insert\n    key[\"workspace_uri\"] = workspace_uri\n    key[\"sortingview_recording_id\"] = recording_id\n    key[\"sortingview_sorting_id\"] = sorting_id\n    self.insert1(key)\n\n    # insert URLs\n    # remove non-primary keys\n    del key[\"workspace_uri\"]\n    del key[\"sortingview_recording_id\"]\n    del key[\"sortingview_sorting_id\"]\n\n    # generate URLs and add to key\n    url = self.url_trythis(key)\n    # url = _generate_url(key)\n    # print(\"URL:\", url)\n    key[\"curation_url\"] = url\n    key[\"curation_jot\"] = \"not ready yet\"\n\n    SortingviewWorkspace.URL.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.SortingviewWorkspace.url_trythis", "title": "<code>url_trythis(key, sortingview_sorting_id=None)</code>", "text": "<p>Generate a URL for visualizing and curating a sorting on the web. Will print instructions on how to do the curation.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>An entry from SortingviewWorkspace table</p> required <code>sortingview_sorting_id</code> <code>str</code> <p>sortingview sorting ID to visualize. If None then chooses the first one</p> <code>None</code> <p>Returns:</p> Name Type Description <code>url</code> <code>str</code> Source code in <code>src/spyglass/spikesorting/sortingview.py</code> <pre><code>def url_trythis(self, key: dict, sortingview_sorting_id: str = None):\n\"\"\"Generate a URL for visualizing and curating a sorting on the web.\n    Will print instructions on how to do the curation.\n\n    Parameters\n    ----------\n    key : dict\n        An entry from SortingviewWorkspace table\n    sortingview_sorting_id : str, optional\n        sortingview sorting ID to visualize. If None then chooses the first one\n\n    Returns\n    -------\n    url : str\n    \"\"\"\n    workspace_uri = (self &amp; key).fetch1(\"workspace_uri\")\n    workspace = sv.load_workspace(workspace_uri)\n    recording_id = workspace.recording_ids[0]\n    if sortingview_sorting_id is None:\n        sortingview_sorting_id = workspace.sorting_ids[0]\n\n    R = workspace.get_recording_extractor(recording_id)\n    S = workspace.get_sorting_extractor(sortingview_sorting_id)\n\n    initial_labels = (Curation &amp; key).fetch(\"curation_labels\")[0]\n    for k, v in initial_labels.items():\n        new_list = []\n        for item in v:\n            if item not in new_list:\n                new_list.append(item)\n        initial_labels[k] = new_list\n    initial_curation = {\"labelsByUnit\": initial_labels}\n\n    # custom metrics\n    unit_metrics = workspace.get_unit_metrics_for_sorting(\n        sortingview_sorting_id\n    )\n\n    # This will print some instructions on how to do the curation\n    # old: sv.trythis_start_sorting_curation\n    url = _generate_url(\n        recording=R,\n        sorting=S,\n        label=workspace.label,\n        initial_curation=initial_curation,\n        raster_plot_subsample_max_firing_rate=50,\n        spike_amplitudes_subsample_max_firing_rate=50,\n        unit_metrics=unit_metrics,\n    )\n    return url\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.SortingviewWorkspace.insert_manual_curation", "title": "<code>insert_manual_curation(key, url, description='manually curated')</code>", "text": "<p>Based on information in key for an SortingviewWorkspace, loads the curated sorting from sortingview, saves it (with labels and the optional description) and inserts it to CuratedSorting</p> <p>Assumes that the workspace corresponding to the recording and (original) sorting exists</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>primary key of AutomaticCuration</p> required <code>description</code> <p>description of curated sorting</p> <code>'manually curated'</code> Source code in <code>src/spyglass/spikesorting/sortingview.py</code> <pre><code>def insert_manual_curation(\n    self, key: dict, url: str, description=\"manually curated\"\n):\n\"\"\"Based on information in key for an SortingviewWorkspace, loads the\n    curated sorting from sortingview, saves it (with labels and the\n    optional description) and inserts it to CuratedSorting\n\n    Assumes that the workspace corresponding to the recording and (original) sorting exists\n\n    Parameters\n    ----------\n    key : dict\n        primary key of AutomaticCuration\n    description: str, optional\n        description of curated sorting\n    \"\"\"\n\n    # get the labels and remove the non-primary merged units\n    # labels = workspace.get_sorting_curation(sorting_id=sortingview_sorting_id)\n    # labels = sv.trythis_load_sorting_curation('jot://xTzzyDieQPkW')\n    labels = sv.trythis_load_sorting_curation(url)\n\n    # turn labels to list of str, only including accepted units.\n    # if bool(labels[\"mergeGroups\"]):\n    if bool(labels.get(\"mergeGroups\", [])):\n        # clusters were merged, so we empty out metrics\n        metrics = {}\n    else:\n        # get the metrics from the parent curation\n        metrics = (Curation &amp; key).fetch1(\"quality_metrics\")\n\n    # insert this curation into the  Table\n    return Curation.insert_curation(\n        key,\n        parent_curation_id=key[\"curation_id\"],\n        labels=labels[\"labelsByUnit\"],\n        merge_groups=labels.get(\"mergeGroups\", []),\n        metrics=metrics,\n        description=description,\n    )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.Curation", "title": "<code>Curation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n    definition = \"\"\"\n    # Stores each spike sorting; similar to IntervalList\n    curation_id: int # a number corresponding to the index of this curation\n    -&gt; SpikeSorting\n    ---\n    parent_curation_id=-1: int\n    curation_labels: blob # a dictionary of labels for the units\n    merge_groups: blob # a list of merge groups for the units\n    quality_metrics: blob # a list of quality metrics for the units (if available)\n    description='': varchar(1000) #optional description for this curated sort\n    time_of_creation: int   # in Unix time, to the nearest second\n    \"\"\"\n\n    @staticmethod\n    def insert_curation(\n        sorting_key: dict,\n        parent_curation_id: int = -1,\n        labels=None,\n        merge_groups=None,\n        metrics=None,\n        description=\"\",\n    ):\n\"\"\"Given a SpikeSorting key and the parent_sorting_id (and optional\n        arguments) insert an entry into Curation.\n\n\n        Parameters\n        ----------\n        sorting_key : dict\n            The key for the original SpikeSorting\n        parent_curation_id : int, optional\n            The id of the parent sorting\n        labels : dict or None, optional\n        merge_groups : dict or None, optional\n        metrics : dict or None, optional\n            Computed metrics for sorting\n        description : str, optional\n            text description of this sort\n\n        Returns\n        -------\n        curation_key : dict\n\n        \"\"\"\n        if parent_curation_id == -1:\n            # check to see if this sorting with a parent of -1 has already been inserted and if so, warn the user\n            inserted_curation = (Curation &amp; sorting_key).fetch(\"KEY\")\n            if len(inserted_curation) &gt; 0:\n                Warning(\n                    \"Sorting has already been inserted, returning key to previously\"\n                    \"inserted curation\"\n                )\n                return inserted_curation[0]\n\n        if labels is None:\n            labels = {}\n        if merge_groups is None:\n            merge_groups = []\n        if metrics is None:\n            metrics = {}\n\n        # generate a unique number for this curation\n        id = (Curation &amp; sorting_key).fetch(\"curation_id\")\n        if len(id) &gt; 0:\n            curation_id = max(id) + 1\n        else:\n            curation_id = 0\n\n        # convert unit_ids in labels to integers for labels from sortingview.\n        new_labels = {int(unit_id): labels[unit_id] for unit_id in labels}\n\n        sorting_key[\"curation_id\"] = curation_id\n        sorting_key[\"parent_curation_id\"] = parent_curation_id\n        sorting_key[\"description\"] = description\n        sorting_key[\"curation_labels\"] = new_labels\n        sorting_key[\"merge_groups\"] = merge_groups\n        sorting_key[\"quality_metrics\"] = metrics\n        sorting_key[\"time_of_creation\"] = int(time.time())\n\n        # mike: added skip duplicates\n        Curation.insert1(sorting_key, skip_duplicates=True)\n\n        # get the primary key for this curation\n        c_key = Curation.fetch(\"KEY\")[0]\n        curation_key = {item: sorting_key[item] for item in c_key}\n\n        return curation_key\n\n    @staticmethod\n    def get_recording(key: dict):\n\"\"\"Returns the recording extractor for the recording related to this curation\n\n        Parameters\n        ----------\n        key : dict\n            SpikeSortingRecording key\n\n        Returns\n        -------\n        recording_extractor : spike interface recording extractor\n\n        \"\"\"\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        return si.load_extractor(recording_path)\n\n    @staticmethod\n    def get_curated_sorting(key: dict):\n\"\"\"Returns the sorting extractor related to this curation,\n        with merges applied.\n\n        Parameters\n        ----------\n        key : dict\n            Curation key\n\n        Returns\n        -------\n        sorting_extractor: spike interface sorting extractor\n\n        \"\"\"\n        sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n        sorting = si.load_extractor(sorting_path)\n        merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n        # TODO: write code to get merged sorting extractor\n        if len(merge_groups) != 0:\n            return MergedSortingExtractor(\n                parent_sorting=sorting, merge_groups=merge_groups\n            )\n        else:\n            return sorting\n\n    @staticmethod\n    def save_sorting_nwb(\n        key,\n        sorting,\n        timestamps,\n        sort_interval_list_name,\n        sort_interval,\n        labels=None,\n        metrics=None,\n        unit_ids=None,\n    ):\n\"\"\"Store a sorting in a new AnalysisNwbfile\n\n        Parameters\n        ----------\n        key : dict\n            key to SpikeSorting table\n        sorting : si.Sorting\n            sorting\n        timestamps : array_like\n            Time stamps of the sorted recoridng;\n            used to convert the spike timings from index to real time\n        sort_interval_list_name : str\n            name of sort interval\n        sort_interval : list\n            interval for start and end of sort\n        labels : dict, optional\n            curation labels, by default None\n        metrics : dict, optional\n            quality metrics, by default None\n        unit_ids : list, optional\n            IDs of units whose spiketrains to save, by default None\n\n        Returns\n        -------\n        analysis_file_name : str\n        units_object_id : str\n\n        \"\"\"\n\n        sort_interval_valid_times = (\n            IntervalList &amp; {\"interval_list_name\": sort_interval_list_name}\n        ).fetch1(\"valid_times\")\n\n        units = dict()\n        units_valid_times = dict()\n        units_sort_interval = dict()\n\n        if unit_ids is None:\n            unit_ids = sorting.get_unit_ids()\n\n        for unit_id in unit_ids:\n            spike_times_in_samples = sorting.get_unit_spike_train(\n                unit_id=unit_id\n            )\n            units[unit_id] = timestamps[spike_times_in_samples]\n            units_valid_times[unit_id] = sort_interval_valid_times\n            units_sort_interval[unit_id] = [sort_interval]\n\n        analysis_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n        object_ids = AnalysisNwbfile().add_units(\n            analysis_file_name,\n            units,\n            units_valid_times,\n            units_sort_interval,\n            metrics=metrics,\n            labels=labels,\n        )\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], analysis_file_name)\n\n        if object_ids == \"\":\n            print(\n                \"Sorting contains no units.\"\n                \"Created an empty analysis nwb file anyway.\"\n            )\n            units_object_id = \"\"\n        else:\n            units_object_id = object_ids[0]\n\n        return analysis_file_name, units_object_id\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.spikesorting_curation.Curation.insert_curation", "title": "<code>insert_curation(sorting_key, parent_curation_id=-1, labels=None, merge_groups=None, metrics=None, description='')</code>  <code>staticmethod</code>", "text": "<p>Given a SpikeSorting key and the parent_sorting_id (and optional arguments) insert an entry into Curation.</p> <p>Parameters:</p> Name Type Description Default <code>sorting_key</code> <code>dict</code> <p>The key for the original SpikeSorting</p> required <code>parent_curation_id</code> <code>int</code> <p>The id of the parent sorting</p> <code>-1</code> <code>labels</code> <code>dict or None</code> <code>None</code> <code>merge_groups</code> <code>dict or None</code> <code>None</code> <code>metrics</code> <code>dict or None</code> <p>Computed metrics for sorting</p> <code>None</code> <code>description</code> <code>str</code> <p>text description of this sort</p> <code>''</code> <p>Returns:</p> Name Type Description <code>curation_key</code> <code>dict</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef insert_curation(\n    sorting_key: dict,\n    parent_curation_id: int = -1,\n    labels=None,\n    merge_groups=None,\n    metrics=None,\n    description=\"\",\n):\n\"\"\"Given a SpikeSorting key and the parent_sorting_id (and optional\n    arguments) insert an entry into Curation.\n\n\n    Parameters\n    ----------\n    sorting_key : dict\n        The key for the original SpikeSorting\n    parent_curation_id : int, optional\n        The id of the parent sorting\n    labels : dict or None, optional\n    merge_groups : dict or None, optional\n    metrics : dict or None, optional\n        Computed metrics for sorting\n    description : str, optional\n        text description of this sort\n\n    Returns\n    -------\n    curation_key : dict\n\n    \"\"\"\n    if parent_curation_id == -1:\n        # check to see if this sorting with a parent of -1 has already been inserted and if so, warn the user\n        inserted_curation = (Curation &amp; sorting_key).fetch(\"KEY\")\n        if len(inserted_curation) &gt; 0:\n            Warning(\n                \"Sorting has already been inserted, returning key to previously\"\n                \"inserted curation\"\n            )\n            return inserted_curation[0]\n\n    if labels is None:\n        labels = {}\n    if merge_groups is None:\n        merge_groups = []\n    if metrics is None:\n        metrics = {}\n\n    # generate a unique number for this curation\n    id = (Curation &amp; sorting_key).fetch(\"curation_id\")\n    if len(id) &gt; 0:\n        curation_id = max(id) + 1\n    else:\n        curation_id = 0\n\n    # convert unit_ids in labels to integers for labels from sortingview.\n    new_labels = {int(unit_id): labels[unit_id] for unit_id in labels}\n\n    sorting_key[\"curation_id\"] = curation_id\n    sorting_key[\"parent_curation_id\"] = parent_curation_id\n    sorting_key[\"description\"] = description\n    sorting_key[\"curation_labels\"] = new_labels\n    sorting_key[\"merge_groups\"] = merge_groups\n    sorting_key[\"quality_metrics\"] = metrics\n    sorting_key[\"time_of_creation\"] = int(time.time())\n\n    # mike: added skip duplicates\n    Curation.insert1(sorting_key, skip_duplicates=True)\n\n    # get the primary key for this curation\n    c_key = Curation.fetch(\"KEY\")[0]\n    curation_key = {item: sorting_key[item] for item in c_key}\n\n    return curation_key\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.spikesorting_curation.Curation.get_recording", "title": "<code>get_recording(key)</code>  <code>staticmethod</code>", "text": "<p>Returns the recording extractor for the recording related to this curation</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>SpikeSortingRecording key</p> required <p>Returns:</p> Name Type Description <code>recording_extractor</code> <code>spike interface recording extractor</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef get_recording(key: dict):\n\"\"\"Returns the recording extractor for the recording related to this curation\n\n    Parameters\n    ----------\n    key : dict\n        SpikeSortingRecording key\n\n    Returns\n    -------\n    recording_extractor : spike interface recording extractor\n\n    \"\"\"\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    return si.load_extractor(recording_path)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.spikesorting_curation.Curation.get_curated_sorting", "title": "<code>get_curated_sorting(key)</code>  <code>staticmethod</code>", "text": "<p>Returns the sorting extractor related to this curation, with merges applied.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Curation key</p> required <p>Returns:</p> Name Type Description <code>sorting_extractor</code> <code>spike interface sorting extractor</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef get_curated_sorting(key: dict):\n\"\"\"Returns the sorting extractor related to this curation,\n    with merges applied.\n\n    Parameters\n    ----------\n    key : dict\n        Curation key\n\n    Returns\n    -------\n    sorting_extractor: spike interface sorting extractor\n\n    \"\"\"\n    sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n    sorting = si.load_extractor(sorting_path)\n    merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n    # TODO: write code to get merged sorting extractor\n    if len(merge_groups) != 0:\n        return MergedSortingExtractor(\n            parent_sorting=sorting, merge_groups=merge_groups\n        )\n    else:\n        return sorting\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.spikesorting_curation.Curation.save_sorting_nwb", "title": "<code>save_sorting_nwb(key, sorting, timestamps, sort_interval_list_name, sort_interval, labels=None, metrics=None, unit_ids=None)</code>  <code>staticmethod</code>", "text": "<p>Store a sorting in a new AnalysisNwbfile</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>key to SpikeSorting table</p> required <code>sorting</code> <code>si.Sorting</code> <p>sorting</p> required <code>timestamps</code> <code>array_like</code> <p>Time stamps of the sorted recoridng; used to convert the spike timings from index to real time</p> required <code>sort_interval_list_name</code> <code>str</code> <p>name of sort interval</p> required <code>sort_interval</code> <code>list</code> <p>interval for start and end of sort</p> required <code>labels</code> <code>dict</code> <p>curation labels, by default None</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>quality metrics, by default None</p> <code>None</code> <code>unit_ids</code> <code>list</code> <p>IDs of units whose spiketrains to save, by default None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <code>units_object_id</code> <code>str</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef save_sorting_nwb(\n    key,\n    sorting,\n    timestamps,\n    sort_interval_list_name,\n    sort_interval,\n    labels=None,\n    metrics=None,\n    unit_ids=None,\n):\n\"\"\"Store a sorting in a new AnalysisNwbfile\n\n    Parameters\n    ----------\n    key : dict\n        key to SpikeSorting table\n    sorting : si.Sorting\n        sorting\n    timestamps : array_like\n        Time stamps of the sorted recoridng;\n        used to convert the spike timings from index to real time\n    sort_interval_list_name : str\n        name of sort interval\n    sort_interval : list\n        interval for start and end of sort\n    labels : dict, optional\n        curation labels, by default None\n    metrics : dict, optional\n        quality metrics, by default None\n    unit_ids : list, optional\n        IDs of units whose spiketrains to save, by default None\n\n    Returns\n    -------\n    analysis_file_name : str\n    units_object_id : str\n\n    \"\"\"\n\n    sort_interval_valid_times = (\n        IntervalList &amp; {\"interval_list_name\": sort_interval_list_name}\n    ).fetch1(\"valid_times\")\n\n    units = dict()\n    units_valid_times = dict()\n    units_sort_interval = dict()\n\n    if unit_ids is None:\n        unit_ids = sorting.get_unit_ids()\n\n    for unit_id in unit_ids:\n        spike_times_in_samples = sorting.get_unit_spike_train(\n            unit_id=unit_id\n        )\n        units[unit_id] = timestamps[spike_times_in_samples]\n        units_valid_times[unit_id] = sort_interval_valid_times\n        units_sort_interval[unit_id] = [sort_interval]\n\n    analysis_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n    object_ids = AnalysisNwbfile().add_units(\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=metrics,\n        labels=labels,\n    )\n    AnalysisNwbfile().add(key[\"nwb_file_name\"], analysis_file_name)\n\n    if object_ids == \"\":\n        print(\n            \"Sorting contains no units.\"\n            \"Created an empty analysis nwb file anyway.\"\n        )\n        units_object_id = \"\"\n    else:\n        units_object_id = object_ids[0]\n\n    return analysis_file_name, units_object_id\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.LabTeam", "title": "<code>LabTeam</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass LabTeam(dj.Manual):\n    definition = \"\"\"\n    team_name: varchar(80)\n    ---\n    team_description = \"\": varchar(2000)\n    \"\"\"\n\n    class LabTeamMember(dj.Part):\n        definition = \"\"\"\n        -&gt; LabTeam\n        -&gt; LabMember\n        \"\"\"\n\n    @classmethod\n    def create_new_team(\n        cls, team_name: str, team_members: list, team_description: str = \"\"\n    ):\n\"\"\"Create a new team with a list of team members.\n\n        If the lab member does not exist in the database, they will be added.\n\n        Parameters\n        ----------\n        team_name : str\n            The name of the team.\n        team_members : str\n            The full names of the lab members that are part of the team.\n        team_description: str\n            The description of the team.\n        \"\"\"\n        labteam_dict = dict()\n        labteam_dict[\"team_name\"] = team_name\n        labteam_dict[\"team_description\"] = team_description\n        cls.insert1(labteam_dict, skip_duplicates=True)\n\n        for team_member in team_members:\n            LabMember.insert_from_name(team_member)\n            query = (\n                LabMember.LabMemberInfo() &amp; {\"lab_member_name\": team_member}\n            ).fetch(\"google_user_name\")\n            if not len(query):\n                print(\n                    f\"Please add the Google user ID for {team_member} in the LabMember.LabMemberInfo table \"\n                    \"if you want to give them permission to manually curate sorting by this team.\"\n                )\n            labteammember_dict = dict()\n            labteammember_dict[\"team_name\"] = team_name\n            labteammember_dict[\"lab_member_name\"] = team_member\n            cls.LabTeamMember.insert1(labteammember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.common.common_lab.LabTeam.create_new_team", "title": "<code>create_new_team(team_name, team_members, team_description='')</code>  <code>classmethod</code>", "text": "<p>Create a new team with a list of team members.</p> <p>If the lab member does not exist in the database, they will be added.</p> <p>Parameters:</p> Name Type Description Default <code>team_name</code> <code>str</code> <p>The name of the team.</p> required <code>team_members</code> <code>str</code> <p>The full names of the lab members that are part of the team.</p> required <code>team_description</code> <code>str</code> <p>The description of the team.</p> <code>''</code> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef create_new_team(\n    cls, team_name: str, team_members: list, team_description: str = \"\"\n):\n\"\"\"Create a new team with a list of team members.\n\n    If the lab member does not exist in the database, they will be added.\n\n    Parameters\n    ----------\n    team_name : str\n        The name of the team.\n    team_members : str\n        The full names of the lab members that are part of the team.\n    team_description: str\n        The description of the team.\n    \"\"\"\n    labteam_dict = dict()\n    labteam_dict[\"team_name\"] = team_name\n    labteam_dict[\"team_description\"] = team_description\n    cls.insert1(labteam_dict, skip_duplicates=True)\n\n    for team_member in team_members:\n        LabMember.insert_from_name(team_member)\n        query = (\n            LabMember.LabMemberInfo() &amp; {\"lab_member_name\": team_member}\n        ).fetch(\"google_user_name\")\n        if not len(query):\n            print(\n                f\"Please add the Google user ID for {team_member} in the LabMember.LabMemberInfo table \"\n                \"if you want to give them permission to manually curate sorting by this team.\"\n            )\n        labteammember_dict = dict()\n        labteammember_dict[\"team_name\"] = team_name\n        labteammember_dict[\"lab_member_name\"] = team_member\n        cls.LabTeamMember.insert1(labteammember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.SpikeSorting", "title": "<code>SpikeSorting</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>@schema\nclass SpikeSorting(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingSelection\n    ---\n    sorting_path: varchar(1000)\n    time_of_sort: int   # in Unix time, to the nearest second\n    \"\"\"\n\n    def make(self, key: dict):\n\"\"\"Runs spike sorting on the data and parameters specified by the\n        SpikeSortingSelection table and inserts a new entry to SpikeSorting table.\n\n        Specifically,\n        1. Loads saved recording and runs the sort on it with spikeinterface\n        2. Saves the sorting with spikeinterface\n        3. Creates an analysis NWB file and saves the sorting there\n           (this is redundant with 2; will change in the future)\n\n        \"\"\"\n\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        recording = si.load_extractor(recording_path)\n\n        # first, get the timestamps\n        timestamps = SpikeSortingRecording._get_recording_timestamps(recording)\n        fs = recording.get_sampling_frequency()\n        # then concatenate the recordings\n        # Note: the timestamps are lost upon concatenation,\n        # i.e. concat_recording.get_times() doesn't return true timestamps anymore.\n        # but concat_recording.recoring_list[i].get_times() will return correct\n        # timestamps for ith recording.\n        if recording.get_num_segments() &gt; 1 and isinstance(\n            recording, si.AppendSegmentRecording\n        ):\n            recording = si.concatenate_recordings(recording.recording_list)\n        elif recording.get_num_segments() &gt; 1 and isinstance(\n            recording, si.BinaryRecordingExtractor\n        ):\n            recording = si.concatenate_recordings([recording])\n\n        # load artifact intervals\n        artifact_times = (\n            ArtifactRemovedIntervalList\n            &amp; {\n                \"artifact_removed_interval_list_name\": key[\n                    \"artifact_removed_interval_list_name\"\n                ]\n            }\n        ).fetch1(\"artifact_times\")\n        if len(artifact_times):\n            if artifact_times.ndim == 1:\n                artifact_times = np.expand_dims(artifact_times, 0)\n\n            # convert artifact intervals to indices\n            list_triggers = []\n            for interval in artifact_times:\n                list_triggers.append(\n                    np.arange(\n                        np.searchsorted(timestamps, interval[0]),\n                        np.searchsorted(timestamps, interval[1]),\n                    )\n                )\n            list_triggers = [list(np.concatenate(list_triggers))]\n            recording = sip.remove_artifacts(\n                recording=recording,\n                list_triggers=list_triggers,\n                ms_before=None,\n                ms_after=None,\n                mode=\"zeros\",\n            )\n\n        print(f\"Running spike sorting on {key}...\")\n        sorter, sorter_params = (SpikeSorterParameters &amp; key).fetch1(\n            \"sorter\", \"sorter_params\"\n        )\n\n        sorter_temp_dir = tempfile.TemporaryDirectory(\n            dir=os.getenv(\"SPYGLASS_TEMP_DIR\")\n        )\n        # add tempdir option for mountainsort\n        sorter_params[\"tempdir\"] = sorter_temp_dir.name\n\n        if sorter == \"clusterless_thresholder\":\n            # need to remove tempdir and whiten from sorter_params\n            sorter_params.pop(\"tempdir\", None)\n            sorter_params.pop(\"whiten\", None)\n\n            # Detect peaks for clusterless decoding\n            detected_spikes = detect_peaks(recording, **sorter_params)\n            sorting = si.NumpySorting.from_times_labels(\n                times_list=detected_spikes[\"sample_ind\"],\n                labels_list=np.zeros(len(detected_spikes), dtype=np.int),\n                sampling_frequency=recording.get_sampling_frequency(),\n            )\n        else:\n            if \"whiten\" in sorter_params.keys():\n                if sorter_params[\"whiten\"]:\n                    sorter_params[\"whiten\"] = False  # set whiten to False\n            # whiten recording separately; make sure dtype is float32\n            # to avoid downstream error with svd\n            recording = sip.whiten(recording, dtype=\"float32\")\n            sorting = sis.run_sorter(\n                sorter,\n                recording,\n                output_folder=sorter_temp_dir.name,\n                delete_output_folder=True,\n                **sorter_params,\n            )\n        key[\"time_of_sort\"] = int(time.time())\n\n        print(\"Saving sorting results...\")\n        sorting_folder = Path(os.getenv(\"SPYGLASS_SORTING_DIR\"))\n        sorting_name = self._get_sorting_name(key)\n        key[\"sorting_path\"] = str(sorting_folder / Path(sorting_name))\n        if os.path.exists(key[\"sorting_path\"]):\n            shutil.rmtree(key[\"sorting_path\"])\n        sorting = sorting.save(folder=key[\"sorting_path\"])\n        self.insert1(key)\n\n    def delete(self):\n\"\"\"Extends the delete method of base class to implement permission checking.\n        Note that this is NOT a security feature, as anyone that has access to source code\n        can disable it; it just makes it less likely to accidentally delete entries.\n        \"\"\"\n        current_user_name = dj.config[\"database.user\"]\n        entries = self.fetch()\n        permission_bool = np.zeros((len(entries),))\n        print(\n            f\"Attempting to delete {len(entries)} entries, checking permission...\"\n        )\n\n        for entry_idx in range(len(entries)):\n            # check the team name for the entry, then look up the members in that team,\n            # then get their datajoint user names\n            team_name = (\n                SpikeSortingRecordingSelection\n                &amp; (SpikeSortingRecordingSelection &amp; entries[entry_idx]).proj()\n            ).fetch1()[\"team_name\"]\n            lab_member_name_list = (\n                LabTeam.LabTeamMember &amp; {\"team_name\": team_name}\n            ).fetch(\"lab_member_name\")\n            datajoint_user_names = []\n            for lab_member_name in lab_member_name_list:\n                datajoint_user_names.append(\n                    (\n                        LabMember.LabMemberInfo\n                        &amp; {\"lab_member_name\": lab_member_name}\n                    ).fetch1(\"datajoint_user_name\")\n                )\n            permission_bool[entry_idx] = (\n                current_user_name in datajoint_user_names\n            )\n        if np.sum(permission_bool) == len(entries):\n            print(\"Permission to delete all specified entries granted.\")\n            super().delete()\n        else:\n            raise Exception(\n                \"You do not have permission to delete all specified\"\n                \"entries. Not deleting anything.\"\n            )\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        raise NotImplementedError\n        return None\n        # return fetch_nwb(self, (AnalysisNwbfile, 'analysis_file_abs_path'), *attrs, **kwargs)\n\n    def nightly_cleanup(self):\n\"\"\"Clean up spike sorting directories that are not in the SpikeSorting table.\n        This should be run after AnalysisNwbFile().nightly_cleanup()\n        \"\"\"\n        # get a list of the files in the spike sorting storage directory\n        dir_names = next(os.walk(os.environ[\"SPYGLASS_SORTING_DIR\"]))[1]\n        # now retrieve a list of the currently used analysis nwb files\n        analysis_file_names = self.fetch(\"analysis_file_name\")\n        for dir in dir_names:\n            if dir not in analysis_file_names:\n                full_path = str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n                print(f\"removing {full_path}\")\n                shutil.rmtree(\n                    str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n                )\n\n    @staticmethod\n    def _get_sorting_name(key):\n        recording_name = SpikeSortingRecording._get_recording_name(key)\n        sorting_name = (\n            recording_name + \"_\" + str(uuid.uuid4())[0:8] + \"_spikesorting\"\n        )\n        return sorting_name\n\n    # TODO: write a function to import sorting done outside of dj\n\n    def _import_sorting(self, key):\n        raise NotImplementedError\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.make", "title": "<code>make(key)</code>", "text": "<p>Runs spike sorting on the data and parameters specified by the SpikeSortingSelection table and inserts a new entry to SpikeSorting table.</p> <p>Specifically, 1. Loads saved recording and runs the sort on it with spikeinterface 2. Saves the sorting with spikeinterface 3. Creates an analysis NWB file and saves the sorting there    (this is redundant with 2; will change in the future)</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def make(self, key: dict):\n\"\"\"Runs spike sorting on the data and parameters specified by the\n    SpikeSortingSelection table and inserts a new entry to SpikeSorting table.\n\n    Specifically,\n    1. Loads saved recording and runs the sort on it with spikeinterface\n    2. Saves the sorting with spikeinterface\n    3. Creates an analysis NWB file and saves the sorting there\n       (this is redundant with 2; will change in the future)\n\n    \"\"\"\n\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    recording = si.load_extractor(recording_path)\n\n    # first, get the timestamps\n    timestamps = SpikeSortingRecording._get_recording_timestamps(recording)\n    fs = recording.get_sampling_frequency()\n    # then concatenate the recordings\n    # Note: the timestamps are lost upon concatenation,\n    # i.e. concat_recording.get_times() doesn't return true timestamps anymore.\n    # but concat_recording.recoring_list[i].get_times() will return correct\n    # timestamps for ith recording.\n    if recording.get_num_segments() &gt; 1 and isinstance(\n        recording, si.AppendSegmentRecording\n    ):\n        recording = si.concatenate_recordings(recording.recording_list)\n    elif recording.get_num_segments() &gt; 1 and isinstance(\n        recording, si.BinaryRecordingExtractor\n    ):\n        recording = si.concatenate_recordings([recording])\n\n    # load artifact intervals\n    artifact_times = (\n        ArtifactRemovedIntervalList\n        &amp; {\n            \"artifact_removed_interval_list_name\": key[\n                \"artifact_removed_interval_list_name\"\n            ]\n        }\n    ).fetch1(\"artifact_times\")\n    if len(artifact_times):\n        if artifact_times.ndim == 1:\n            artifact_times = np.expand_dims(artifact_times, 0)\n\n        # convert artifact intervals to indices\n        list_triggers = []\n        for interval in artifact_times:\n            list_triggers.append(\n                np.arange(\n                    np.searchsorted(timestamps, interval[0]),\n                    np.searchsorted(timestamps, interval[1]),\n                )\n            )\n        list_triggers = [list(np.concatenate(list_triggers))]\n        recording = sip.remove_artifacts(\n            recording=recording,\n            list_triggers=list_triggers,\n            ms_before=None,\n            ms_after=None,\n            mode=\"zeros\",\n        )\n\n    print(f\"Running spike sorting on {key}...\")\n    sorter, sorter_params = (SpikeSorterParameters &amp; key).fetch1(\n        \"sorter\", \"sorter_params\"\n    )\n\n    sorter_temp_dir = tempfile.TemporaryDirectory(\n        dir=os.getenv(\"SPYGLASS_TEMP_DIR\")\n    )\n    # add tempdir option for mountainsort\n    sorter_params[\"tempdir\"] = sorter_temp_dir.name\n\n    if sorter == \"clusterless_thresholder\":\n        # need to remove tempdir and whiten from sorter_params\n        sorter_params.pop(\"tempdir\", None)\n        sorter_params.pop(\"whiten\", None)\n\n        # Detect peaks for clusterless decoding\n        detected_spikes = detect_peaks(recording, **sorter_params)\n        sorting = si.NumpySorting.from_times_labels(\n            times_list=detected_spikes[\"sample_ind\"],\n            labels_list=np.zeros(len(detected_spikes), dtype=np.int),\n            sampling_frequency=recording.get_sampling_frequency(),\n        )\n    else:\n        if \"whiten\" in sorter_params.keys():\n            if sorter_params[\"whiten\"]:\n                sorter_params[\"whiten\"] = False  # set whiten to False\n        # whiten recording separately; make sure dtype is float32\n        # to avoid downstream error with svd\n        recording = sip.whiten(recording, dtype=\"float32\")\n        sorting = sis.run_sorter(\n            sorter,\n            recording,\n            output_folder=sorter_temp_dir.name,\n            delete_output_folder=True,\n            **sorter_params,\n        )\n    key[\"time_of_sort\"] = int(time.time())\n\n    print(\"Saving sorting results...\")\n    sorting_folder = Path(os.getenv(\"SPYGLASS_SORTING_DIR\"))\n    sorting_name = self._get_sorting_name(key)\n    key[\"sorting_path\"] = str(sorting_folder / Path(sorting_name))\n    if os.path.exists(key[\"sorting_path\"]):\n        shutil.rmtree(key[\"sorting_path\"])\n    sorting = sorting.save(folder=key[\"sorting_path\"])\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.delete", "title": "<code>delete()</code>", "text": "<p>Extends the delete method of base class to implement permission checking. Note that this is NOT a security feature, as anyone that has access to source code can disable it; it just makes it less likely to accidentally delete entries.</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def delete(self):\n\"\"\"Extends the delete method of base class to implement permission checking.\n    Note that this is NOT a security feature, as anyone that has access to source code\n    can disable it; it just makes it less likely to accidentally delete entries.\n    \"\"\"\n    current_user_name = dj.config[\"database.user\"]\n    entries = self.fetch()\n    permission_bool = np.zeros((len(entries),))\n    print(\n        f\"Attempting to delete {len(entries)} entries, checking permission...\"\n    )\n\n    for entry_idx in range(len(entries)):\n        # check the team name for the entry, then look up the members in that team,\n        # then get their datajoint user names\n        team_name = (\n            SpikeSortingRecordingSelection\n            &amp; (SpikeSortingRecordingSelection &amp; entries[entry_idx]).proj()\n        ).fetch1()[\"team_name\"]\n        lab_member_name_list = (\n            LabTeam.LabTeamMember &amp; {\"team_name\": team_name}\n        ).fetch(\"lab_member_name\")\n        datajoint_user_names = []\n        for lab_member_name in lab_member_name_list:\n            datajoint_user_names.append(\n                (\n                    LabMember.LabMemberInfo\n                    &amp; {\"lab_member_name\": lab_member_name}\n                ).fetch1(\"datajoint_user_name\")\n            )\n        permission_bool[entry_idx] = (\n            current_user_name in datajoint_user_names\n        )\n    if np.sum(permission_bool) == len(entries):\n        print(\"Permission to delete all specified entries granted.\")\n        super().delete()\n    else:\n        raise Exception(\n            \"You do not have permission to delete all specified\"\n            \"entries. Not deleting anything.\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.nightly_cleanup", "title": "<code>nightly_cleanup()</code>", "text": "<p>Clean up spike sorting directories that are not in the SpikeSorting table. This should be run after AnalysisNwbFile().nightly_cleanup()</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def nightly_cleanup(self):\n\"\"\"Clean up spike sorting directories that are not in the SpikeSorting table.\n    This should be run after AnalysisNwbFile().nightly_cleanup()\n    \"\"\"\n    # get a list of the files in the spike sorting storage directory\n    dir_names = next(os.walk(os.environ[\"SPYGLASS_SORTING_DIR\"]))[1]\n    # now retrieve a list of the currently used analysis nwb files\n    analysis_file_names = self.fetch(\"analysis_file_name\")\n    for dir in dir_names:\n        if dir not in analysis_file_names:\n            full_path = str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n            print(f\"removing {full_path}\")\n            shutil.rmtree(\n                str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n            )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview/#src.spyglass.spikesorting.sortingview.SpikeSortingRecording", "title": "<code>SpikeSortingRecording</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>@schema\nclass SpikeSortingRecording(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingRecordingSelection\n    ---\n    recording_path: varchar(1000)\n    -&gt; IntervalList.proj(sort_interval_list_name='interval_list_name')\n    \"\"\"\n\n    def make(self, key):\n        sort_interval_valid_times = self._get_sort_interval_valid_times(key)\n        recording = self._get_filtered_recording(key)\n        recording_name = self._get_recording_name(key)\n\n        tmp_key = {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": recording_name,\n            \"valid_times\": sort_interval_valid_times,\n        }\n        IntervalList.insert1(tmp_key, replace=True)\n\n        # store the list of valid times for the sort\n        key[\"sort_interval_list_name\"] = tmp_key[\"interval_list_name\"]\n\n        # Path to files that will hold the recording extractors\n        recording_folder = Path(os.getenv(\"SPYGLASS_RECORDING_DIR\"))\n        key[\"recording_path\"] = str(recording_folder / Path(recording_name))\n        if os.path.exists(key[\"recording_path\"]):\n            shutil.rmtree(key[\"recording_path\"])\n        recording = recording.save(\n            folder=key[\"recording_path\"], chunk_duration=\"10000ms\", n_jobs=8\n        )\n\n        self.insert1(key)\n\n    @staticmethod\n    def _get_recording_name(key):\n        recording_name = (\n            key[\"nwb_file_name\"]\n            + \"_\"\n            + key[\"sort_interval_name\"]\n            + \"_\"\n            + str(key[\"sort_group_id\"])\n            + \"_\"\n            + key[\"preproc_params_name\"]\n        )\n        return recording_name\n\n    @staticmethod\n    def _get_recording_timestamps(recording):\n        if recording.get_num_segments() &gt; 1:\n            frames_per_segment = [0]\n            for i in range(recording.get_num_segments()):\n                frames_per_segment.append(\n                    recording.get_num_frames(segment_index=i)\n                )\n\n            cumsum_frames = np.cumsum(frames_per_segment)\n            total_frames = np.sum(frames_per_segment)\n\n            timestamps = np.zeros((total_frames,))\n            for i in range(recording.get_num_segments()):\n                timestamps[\n                    cumsum_frames[i] : cumsum_frames[i + 1]\n                ] = recording.get_times(segment_index=i)\n        else:\n            timestamps = recording.get_times()\n        return timestamps\n\n    def _get_sort_interval_valid_times(self, key):\n\"\"\"Identifies the intersection between sort interval specified by the user\n        and the valid times (times for which neural data exist)\n\n        Parameters\n        ----------\n        key: dict\n            specifies a (partially filled) entry of SpikeSorting table\n\n        Returns\n        -------\n        sort_interval_valid_times: ndarray of tuples\n            (start, end) times for valid stretches of the sorting interval\n\n        \"\"\"\n        sort_interval = (\n            SortInterval\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_interval_name\": key[\"sort_interval_name\"],\n            }\n        ).fetch1(\"sort_interval\")\n        interval_list_name = (SpikeSortingRecordingSelection &amp; key).fetch1(\n            \"interval_list_name\"\n        )\n        valid_interval_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n        valid_sort_times = interval_list_intersect(\n            sort_interval, valid_interval_times\n        )\n        # Exclude intervals shorter than specified length\n        params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        if \"min_segment_length\" in params:\n            valid_sort_times = intervals_by_length(\n                valid_sort_times, min_length=params[\"min_segment_length\"]\n            )\n        return valid_sort_times\n\n    def _get_filtered_recording(self, key: dict):\n\"\"\"Filters and references a recording\n        * Loads the NWB file created during insertion as a spikeinterface Recording\n        * Slices recording in time (interval) and space (channels);\n          recording chunks from disjoint intervals are concatenated\n        * Applies referencing and bandpass filtering\n\n        Parameters\n        ----------\n        key: dict,\n            primary key of SpikeSortingRecording table\n\n        Returns\n        -------\n        recording: si.Recording\n        \"\"\"\n\n        nwb_file_abs_path = Nwbfile().get_abs_path(key[\"nwb_file_name\"])\n        recording = se.read_nwb_recording(\n            nwb_file_abs_path, load_time_vector=True\n        )\n\n        valid_sort_times = self._get_sort_interval_valid_times(key)\n        # shape is (N, 2)\n        valid_sort_times_indices = np.array(\n            [\n                np.searchsorted(recording.get_times(), interval)\n                for interval in valid_sort_times\n            ]\n        )\n        # join intervals of indices that are adjacent\n        valid_sort_times_indices = reduce(\n            union_adjacent_index, valid_sort_times_indices\n        )\n        if valid_sort_times_indices.ndim == 1:\n            valid_sort_times_indices = np.expand_dims(\n                valid_sort_times_indices, 0\n            )\n\n        # create an AppendRecording if there is more than one disjoint sort interval\n        if len(valid_sort_times_indices) &gt; 1:\n            recordings_list = []\n            for interval_indices in valid_sort_times_indices:\n                recording_single = recording.frame_slice(\n                    start_frame=interval_indices[0],\n                    end_frame=interval_indices[1],\n                )\n                recordings_list.append(recording_single)\n            recording = si.append_recordings(recordings_list)\n        else:\n            recording = recording.frame_slice(\n                start_frame=valid_sort_times_indices[0][0],\n                end_frame=valid_sort_times_indices[0][1],\n            )\n\n        channel_ids = (\n            SortGroup.SortGroupElectrode\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch(\"electrode_id\")\n        ref_channel_id = (\n            SortGroup\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch1(\"sort_reference_electrode_id\")\n        channel_ids = np.setdiff1d(channel_ids, ref_channel_id)\n\n        # include ref channel in first slice, then exclude it in second slice\n        if ref_channel_id &gt;= 0:\n            channel_ids_ref = np.append(channel_ids, ref_channel_id)\n            recording = recording.channel_slice(channel_ids=channel_ids_ref)\n\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"single\", ref_channel_ids=ref_channel_id\n            )\n            recording = recording.channel_slice(channel_ids=channel_ids)\n        elif ref_channel_id == -2:\n            recording = recording.channel_slice(channel_ids=channel_ids)\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"global\", operator=\"median\"\n            )\n        else:\n            raise ValueError(\"Invalid reference channel ID\")\n        filter_params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        recording = si.preprocessing.bandpass_filter(\n            recording,\n            freq_min=filter_params[\"frequency_min\"],\n            freq_max=filter_params[\"frequency_max\"],\n        )\n\n        # if the sort group is a tetrode, change the channel location\n        # note that this is a workaround that would be deprecated when spikeinterface uses 3D probe locations\n        probe_type = []\n        electrode_group = []\n        for channel_id in channel_ids:\n            probe_type.append(\n                (\n                    Electrode * Probe\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"probe_type\")\n            )\n            electrode_group.append(\n                (\n                    Electrode\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"electrode_group_name\")\n            )\n        if (\n            all(p == \"tetrode_12.5\" for p in probe_type)\n            and len(probe_type) == 4\n            and all(eg == electrode_group[0] for eg in electrode_group)\n        ):\n            tetrode = pi.Probe(ndim=2)\n            position = [[0, 0], [0, 12.5], [12.5, 0], [12.5, 12.5]]\n            tetrode.set_contacts(\n                position, shapes=\"circle\", shape_params={\"radius\": 6.25}\n            )\n            tetrode.set_contact_ids(channel_ids)\n            tetrode.set_device_channel_indices(np.arange(4))\n            recording = recording.set_probe(tetrode, in_place=True)\n\n        return recording\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/sortingview_helper_fn/", "title": "sortingview_helper_fn.py", "text": "<p>Sortingview helper functions</p>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_artifact/", "title": "spikesorting_artifact.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/spikesorting_artifact/#src.spyglass.spikesorting.spikesorting_artifact.IntervalList", "title": "<code>IntervalList</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@schema\nclass IntervalList(dj.Manual):\n    definition = \"\"\"\n    # Time intervals used for analysis\n    -&gt; Session\n    interval_list_name: varchar(200)  # descriptive name of this interval list\n    ---\n    valid_times: longblob  # numpy array with start and end times for each interval\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n        The interval list name for each epoch is set to the first tag for the epoch.\n        If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n        (0-indexed) of the epoch in the epochs table.\n        The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n        [start time, stop time] for each epoch.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        nwb_file_name : str\n            The file name of the NWB file, used as a primary key to the Session table.\n        \"\"\"\n        if nwbf.epochs is None:\n            print(\"No epochs found in NWB file.\")\n            return\n        epochs = nwbf.epochs.to_dataframe()\n        for epoch_index, epoch_data in epochs.iterrows():\n            epoch_dict = dict()\n            epoch_dict[\"nwb_file_name\"] = nwb_file_name\n            if epoch_data.tags[0]:\n                epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n            else:\n                epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                    epoch_index\n                )\n            epoch_dict[\"valid_times\"] = np.asarray(\n                [[epoch_data.start_time, epoch_data.stop_time]]\n            )\n            cls.insert1(epoch_dict, skip_duplicates=True)\n\n    def plot_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=figsize)\n        interval_count = 0\n        for row in interval_list.itertuples(index=False):\n            for interval in row.valid_times:\n                ax.plot(interval, [interval_count, interval_count])\n                ax.scatter(\n                    interval,\n                    [interval_count, interval_count],\n                    alpha=0.8,\n                    zorder=2,\n                )\n            interval_count += 1\n        ax.set_yticks(np.arange(interval_list.shape[0]))\n        ax.set_yticklabels(interval_list.interval_list_name)\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n\n    def plot_epoch_pos_raw_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=(30, 3))\n\n        raw_data_valid_times = interval_list.loc[\n            interval_list.interval_list_name == \"raw data valid times\"\n        ].valid_times\n        interval_y = 1\n\n        for interval in np.asarray(raw_data_valid_times)[0]:\n            ax.plot(interval, [interval_y, interval_y])\n            ax.scatter(interval, [interval_y, interval_y], alpha=0.8, zorder=2)\n\n        epoch_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n        interval_y = 2\n        for epoch, valid_times in zip(\n            epoch_valid_times.index, epoch_valid_times\n        ):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        pos_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^pos \\d+ valid times$\", axis=0)\n            .valid_times\n        ).sort_index(key=lambda index: [int(name.split()[1]) for name in index])\n        interval_y = 0\n        for epoch, valid_times in zip(pos_valid_times.index, pos_valid_times):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch.strip(\" valid times\"),\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        ax.set_ylim((-0.25, 2.25))\n        ax.set_yticks(np.arange(3))\n        ax.set_yticklabels([\"pos valid times\", \"raw data valid times\", \"epoch\"])\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_artifact/#src.spyglass.common.common_interval.IntervalList.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, *, nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add each entry in the NWB file epochs table to the IntervalList table.</p> <p>The interval list name for each epoch is set to the first tag for the epoch. If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index (0-indexed) of the epoch in the epochs table. The start time and stop time of the epoch are stored in the valid_times field as a numpy array of [start time, stop time] for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>nwb_file_name</code> <code>str</code> <p>The file name of the NWB file, used as a primary key to the Session table.</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n    The interval list name for each epoch is set to the first tag for the epoch.\n    If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n    (0-indexed) of the epoch in the epochs table.\n    The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n    [start time, stop time] for each epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    nwb_file_name : str\n        The file name of the NWB file, used as a primary key to the Session table.\n    \"\"\"\n    if nwbf.epochs is None:\n        print(\"No epochs found in NWB file.\")\n        return\n    epochs = nwbf.epochs.to_dataframe()\n    for epoch_index, epoch_data in epochs.iterrows():\n        epoch_dict = dict()\n        epoch_dict[\"nwb_file_name\"] = nwb_file_name\n        if epoch_data.tags[0]:\n            epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n        else:\n            epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                epoch_index\n            )\n        epoch_dict[\"valid_times\"] = np.asarray(\n            [[epoch_data.start_time, epoch_data.stop_time]]\n        )\n        cls.insert1(epoch_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_artifact/#src.spyglass.spikesorting.spikesorting_artifact.ArtifactDetectionParameters", "title": "<code>ArtifactDetectionParameters</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_artifact.py</code> <pre><code>@schema\nclass ArtifactDetectionParameters(dj.Manual):\n    definition = \"\"\"\n    # Parameters for detecting artifact times within a sort group.\n    artifact_params_name: varchar(200)\n    ---\n    artifact_params: blob  # dictionary of parameters\n    \"\"\"\n\n    def insert_default(self):\n\"\"\"Insert the default artifact parameters with an appropriate parameter dict.\"\"\"\n        artifact_params = {}\n        artifact_params[\"zscore_thresh\"] = None  # must be None or &gt;= 0\n        artifact_params[\"amplitude_thresh\"] = 3000  # must be None or &gt;= 0\n        # all electrodes of sort group\n        artifact_params[\"proportion_above_thresh\"] = 1.0\n        artifact_params[\"removal_window_ms\"] = 1.0  # in milliseconds\n        self.insert1([\"default\", artifact_params], skip_duplicates=True)\n\n        artifact_params_none = {}\n        artifact_params_none[\"zscore_thresh\"] = None\n        artifact_params_none[\"amplitude_thresh\"] = None\n        self.insert1([\"none\", artifact_params_none], skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_artifact/#src.spyglass.spikesorting.spikesorting_artifact.ArtifactDetectionParameters.insert_default", "title": "<code>insert_default()</code>", "text": "<p>Insert the default artifact parameters with an appropriate parameter dict.</p> Source code in <code>src/spyglass/spikesorting/spikesorting_artifact.py</code> <pre><code>def insert_default(self):\n\"\"\"Insert the default artifact parameters with an appropriate parameter dict.\"\"\"\n    artifact_params = {}\n    artifact_params[\"zscore_thresh\"] = None  # must be None or &gt;= 0\n    artifact_params[\"amplitude_thresh\"] = 3000  # must be None or &gt;= 0\n    # all electrodes of sort group\n    artifact_params[\"proportion_above_thresh\"] = 1.0\n    artifact_params[\"removal_window_ms\"] = 1.0  # in milliseconds\n    self.insert1([\"default\", artifact_params], skip_duplicates=True)\n\n    artifact_params_none = {}\n    artifact_params_none[\"zscore_thresh\"] = None\n    artifact_params_none[\"amplitude_thresh\"] = None\n    self.insert1([\"none\", artifact_params_none], skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_artifact/#src.spyglass.spikesorting.spikesorting_artifact.get_valid_intervals", "title": "<code>get_valid_intervals(timestamps, sampling_rate, gap_proportion, min_valid_len)</code>", "text": "<p>Finds the set of all valid intervals in a list of timestamps. Valid interval: (start time, stop time) during which there are no gaps (i.e. missing samples).</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>numpy.ndarray</code> <p>1D numpy array of timestamp values.</p> required <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data.</p> required <code>gap_proportion</code> <code>float, greater than 1; unit: samples</code> <p>Threshold for detecting a gap; i.e. if the difference (in samples) between consecutive timestamps exceeds gap_proportion, it is considered a gap</p> required <code>min_valid_len</code> <code>float</code> <p>Length of smallest valid interval.</p> required <p>Returns:</p> Name Type Description <code>valid_times</code> <code>np.ndarray</code> <p>Array of start and stop times of shape (N, 2) for valid data.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_valid_intervals(\n    timestamps, sampling_rate, gap_proportion, min_valid_len\n):\n\"\"\"Finds the set of all valid intervals in a list of timestamps.\n    Valid interval: (start time, stop time) during which there are\n    no gaps (i.e. missing samples).\n\n    Parameters\n    ----------\n    timestamps : numpy.ndarray\n        1D numpy array of timestamp values.\n    sampling_rate : float\n        Sampling rate of the data.\n    gap_proportion : float, greater than 1; unit: samples\n        Threshold for detecting a gap;\n        i.e. if the difference (in samples) between\n        consecutive timestamps exceeds gap_proportion,\n        it is considered a gap\n    min_valid_len : float\n        Length of smallest valid interval.\n\n    Returns\n    -------\n    valid_times : np.ndarray\n        Array of start and stop times of shape (N, 2) for valid data.\n    \"\"\"\n\n    eps = 0.0000001\n\n    # get rid of NaN elements\n    timestamps = timestamps[~np.isnan(timestamps)]\n    # find gaps\n    gap = np.diff(timestamps) &gt; 1.0 / sampling_rate * gap_proportion\n\n    # all true entries of gap represent gaps. Get the times bounding these intervals.\n    gapind = np.asarray(np.where(gap))\n    # The end of each valid interval are the indices of the gaps and the final value\n    valid_end = np.append(gapind, np.asarray(len(timestamps) - 1))\n\n    # the beginning of the gaps are the first element and gapind+1\n    valid_start = np.insert(gapind + 1, 0, 0)\n\n    valid_indices = np.vstack([valid_start, valid_end]).transpose()\n\n    valid_times = timestamps[valid_indices]\n    # adjust the times to deal with single valid samples\n    valid_times[:, 0] = valid_times[:, 0] - eps\n    valid_times[:, 1] = valid_times[:, 1] + eps\n\n    valid_intervals = (valid_times[:, 1] - valid_times[:, 0]) &gt; min_valid_len\n\n    return valid_times[valid_intervals, :]\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_artifact/#src.spyglass.spikesorting.spikesorting_artifact.SpikeSortingRecording", "title": "<code>SpikeSortingRecording</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>@schema\nclass SpikeSortingRecording(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingRecordingSelection\n    ---\n    recording_path: varchar(1000)\n    -&gt; IntervalList.proj(sort_interval_list_name='interval_list_name')\n    \"\"\"\n\n    def make(self, key):\n        sort_interval_valid_times = self._get_sort_interval_valid_times(key)\n        recording = self._get_filtered_recording(key)\n        recording_name = self._get_recording_name(key)\n\n        tmp_key = {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": recording_name,\n            \"valid_times\": sort_interval_valid_times,\n        }\n        IntervalList.insert1(tmp_key, replace=True)\n\n        # store the list of valid times for the sort\n        key[\"sort_interval_list_name\"] = tmp_key[\"interval_list_name\"]\n\n        # Path to files that will hold the recording extractors\n        recording_folder = Path(os.getenv(\"SPYGLASS_RECORDING_DIR\"))\n        key[\"recording_path\"] = str(recording_folder / Path(recording_name))\n        if os.path.exists(key[\"recording_path\"]):\n            shutil.rmtree(key[\"recording_path\"])\n        recording = recording.save(\n            folder=key[\"recording_path\"], chunk_duration=\"10000ms\", n_jobs=8\n        )\n\n        self.insert1(key)\n\n    @staticmethod\n    def _get_recording_name(key):\n        recording_name = (\n            key[\"nwb_file_name\"]\n            + \"_\"\n            + key[\"sort_interval_name\"]\n            + \"_\"\n            + str(key[\"sort_group_id\"])\n            + \"_\"\n            + key[\"preproc_params_name\"]\n        )\n        return recording_name\n\n    @staticmethod\n    def _get_recording_timestamps(recording):\n        if recording.get_num_segments() &gt; 1:\n            frames_per_segment = [0]\n            for i in range(recording.get_num_segments()):\n                frames_per_segment.append(\n                    recording.get_num_frames(segment_index=i)\n                )\n\n            cumsum_frames = np.cumsum(frames_per_segment)\n            total_frames = np.sum(frames_per_segment)\n\n            timestamps = np.zeros((total_frames,))\n            for i in range(recording.get_num_segments()):\n                timestamps[\n                    cumsum_frames[i] : cumsum_frames[i + 1]\n                ] = recording.get_times(segment_index=i)\n        else:\n            timestamps = recording.get_times()\n        return timestamps\n\n    def _get_sort_interval_valid_times(self, key):\n\"\"\"Identifies the intersection between sort interval specified by the user\n        and the valid times (times for which neural data exist)\n\n        Parameters\n        ----------\n        key: dict\n            specifies a (partially filled) entry of SpikeSorting table\n\n        Returns\n        -------\n        sort_interval_valid_times: ndarray of tuples\n            (start, end) times for valid stretches of the sorting interval\n\n        \"\"\"\n        sort_interval = (\n            SortInterval\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_interval_name\": key[\"sort_interval_name\"],\n            }\n        ).fetch1(\"sort_interval\")\n        interval_list_name = (SpikeSortingRecordingSelection &amp; key).fetch1(\n            \"interval_list_name\"\n        )\n        valid_interval_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n        valid_sort_times = interval_list_intersect(\n            sort_interval, valid_interval_times\n        )\n        # Exclude intervals shorter than specified length\n        params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        if \"min_segment_length\" in params:\n            valid_sort_times = intervals_by_length(\n                valid_sort_times, min_length=params[\"min_segment_length\"]\n            )\n        return valid_sort_times\n\n    def _get_filtered_recording(self, key: dict):\n\"\"\"Filters and references a recording\n        * Loads the NWB file created during insertion as a spikeinterface Recording\n        * Slices recording in time (interval) and space (channels);\n          recording chunks from disjoint intervals are concatenated\n        * Applies referencing and bandpass filtering\n\n        Parameters\n        ----------\n        key: dict,\n            primary key of SpikeSortingRecording table\n\n        Returns\n        -------\n        recording: si.Recording\n        \"\"\"\n\n        nwb_file_abs_path = Nwbfile().get_abs_path(key[\"nwb_file_name\"])\n        recording = se.read_nwb_recording(\n            nwb_file_abs_path, load_time_vector=True\n        )\n\n        valid_sort_times = self._get_sort_interval_valid_times(key)\n        # shape is (N, 2)\n        valid_sort_times_indices = np.array(\n            [\n                np.searchsorted(recording.get_times(), interval)\n                for interval in valid_sort_times\n            ]\n        )\n        # join intervals of indices that are adjacent\n        valid_sort_times_indices = reduce(\n            union_adjacent_index, valid_sort_times_indices\n        )\n        if valid_sort_times_indices.ndim == 1:\n            valid_sort_times_indices = np.expand_dims(\n                valid_sort_times_indices, 0\n            )\n\n        # create an AppendRecording if there is more than one disjoint sort interval\n        if len(valid_sort_times_indices) &gt; 1:\n            recordings_list = []\n            for interval_indices in valid_sort_times_indices:\n                recording_single = recording.frame_slice(\n                    start_frame=interval_indices[0],\n                    end_frame=interval_indices[1],\n                )\n                recordings_list.append(recording_single)\n            recording = si.append_recordings(recordings_list)\n        else:\n            recording = recording.frame_slice(\n                start_frame=valid_sort_times_indices[0][0],\n                end_frame=valid_sort_times_indices[0][1],\n            )\n\n        channel_ids = (\n            SortGroup.SortGroupElectrode\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch(\"electrode_id\")\n        ref_channel_id = (\n            SortGroup\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch1(\"sort_reference_electrode_id\")\n        channel_ids = np.setdiff1d(channel_ids, ref_channel_id)\n\n        # include ref channel in first slice, then exclude it in second slice\n        if ref_channel_id &gt;= 0:\n            channel_ids_ref = np.append(channel_ids, ref_channel_id)\n            recording = recording.channel_slice(channel_ids=channel_ids_ref)\n\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"single\", ref_channel_ids=ref_channel_id\n            )\n            recording = recording.channel_slice(channel_ids=channel_ids)\n        elif ref_channel_id == -2:\n            recording = recording.channel_slice(channel_ids=channel_ids)\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"global\", operator=\"median\"\n            )\n        else:\n            raise ValueError(\"Invalid reference channel ID\")\n        filter_params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        recording = si.preprocessing.bandpass_filter(\n            recording,\n            freq_min=filter_params[\"frequency_min\"],\n            freq_max=filter_params[\"frequency_max\"],\n        )\n\n        # if the sort group is a tetrode, change the channel location\n        # note that this is a workaround that would be deprecated when spikeinterface uses 3D probe locations\n        probe_type = []\n        electrode_group = []\n        for channel_id in channel_ids:\n            probe_type.append(\n                (\n                    Electrode * Probe\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"probe_type\")\n            )\n            electrode_group.append(\n                (\n                    Electrode\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"electrode_group_name\")\n            )\n        if (\n            all(p == \"tetrode_12.5\" for p in probe_type)\n            and len(probe_type) == 4\n            and all(eg == electrode_group[0] for eg in electrode_group)\n        ):\n            tetrode = pi.Probe(ndim=2)\n            position = [[0, 0], [0, 12.5], [12.5, 0], [12.5, 12.5]]\n            tetrode.set_contacts(\n                position, shapes=\"circle\", shape_params={\"radius\": 6.25}\n            )\n            tetrode.set_contact_ids(channel_ids)\n            tetrode.set_device_channel_indices(np.arange(4))\n            recording = recording.set_probe(tetrode, in_place=True)\n\n        return recording\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_artifact/#src.spyglass.spikesorting.spikesorting_artifact.interval_from_inds", "title": "<code>interval_from_inds(list_frames)</code>", "text": "<p>Converts a list of indices to a list of intervals. e.g. [2,3,4,6,7,8,9,10] -&gt; [[2,4],[6,10]]</p> <p>Parameters:</p> Name Type Description Default <code>list_frames</code> <code>array_like of int</code> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_from_inds(list_frames):\n\"\"\"Converts a list of indices to a list of intervals.\n    e.g. [2,3,4,6,7,8,9,10] -&gt; [[2,4],[6,10]]\n\n    Parameters\n    ----------\n    list_frames : array_like of int\n    \"\"\"\n    list_frames = np.unique(list_frames)\n    interval_list = []\n    for key, group in itertools.groupby(\n        enumerate(list_frames), lambda t: t[1] - t[0]\n    ):\n        group = list(group)\n        interval_list.append([group[0][1], group[-1][1]])\n    return np.asarray(interval_list)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_artifact/#src.spyglass.spikesorting.spikesorting_artifact.interval_set_difference_inds", "title": "<code>interval_set_difference_inds(intervals1, intervals2)</code>", "text": "<p>e.g. intervals1 = [(0, 5), (8, 10)] intervals2 = [(1, 2), (3, 4), (6, 9)]</p> <p>result = [(0, 1), (4, 5), (9, 10)]</p> <p>Parameters:</p> Name Type Description Default <code>intervals1</code> <code>_type_</code> <p>description</p> required <code>intervals2</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Type Description <code>_type_</code> <p>description</p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_set_difference_inds(intervals1, intervals2):\n\"\"\"\n    e.g.\n    intervals1 = [(0, 5), (8, 10)]\n    intervals2 = [(1, 2), (3, 4), (6, 9)]\n\n    result = [(0, 1), (4, 5), (9, 10)]\n\n    Parameters\n    ----------\n    intervals1 : _type_\n        _description_\n    intervals2 : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    result = []\n    i = j = 0\n    while i &lt; len(intervals1) and j &lt; len(intervals2):\n        if intervals1[i][1] &lt;= intervals2[j][0]:\n            result.append(intervals1[i])\n            i += 1\n        elif intervals2[j][1] &lt;= intervals1[i][0]:\n            j += 1\n        else:\n            if intervals1[i][0] &lt; intervals2[j][0]:\n                result.append((intervals1[i][0], intervals2[j][0]))\n            if intervals1[i][1] &gt; intervals2[j][1]:\n                intervals1[i] = (intervals2[j][1], intervals1[i][1])\n                j += 1\n            else:\n                i += 1\n    result += intervals1[i:]\n    return result\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/", "title": "spikesorting_curation.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.IntervalList", "title": "<code>IntervalList</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@schema\nclass IntervalList(dj.Manual):\n    definition = \"\"\"\n    # Time intervals used for analysis\n    -&gt; Session\n    interval_list_name: varchar(200)  # descriptive name of this interval list\n    ---\n    valid_times: longblob  # numpy array with start and end times for each interval\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n        The interval list name for each epoch is set to the first tag for the epoch.\n        If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n        (0-indexed) of the epoch in the epochs table.\n        The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n        [start time, stop time] for each epoch.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        nwb_file_name : str\n            The file name of the NWB file, used as a primary key to the Session table.\n        \"\"\"\n        if nwbf.epochs is None:\n            print(\"No epochs found in NWB file.\")\n            return\n        epochs = nwbf.epochs.to_dataframe()\n        for epoch_index, epoch_data in epochs.iterrows():\n            epoch_dict = dict()\n            epoch_dict[\"nwb_file_name\"] = nwb_file_name\n            if epoch_data.tags[0]:\n                epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n            else:\n                epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                    epoch_index\n                )\n            epoch_dict[\"valid_times\"] = np.asarray(\n                [[epoch_data.start_time, epoch_data.stop_time]]\n            )\n            cls.insert1(epoch_dict, skip_duplicates=True)\n\n    def plot_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=figsize)\n        interval_count = 0\n        for row in interval_list.itertuples(index=False):\n            for interval in row.valid_times:\n                ax.plot(interval, [interval_count, interval_count])\n                ax.scatter(\n                    interval,\n                    [interval_count, interval_count],\n                    alpha=0.8,\n                    zorder=2,\n                )\n            interval_count += 1\n        ax.set_yticks(np.arange(interval_list.shape[0]))\n        ax.set_yticklabels(interval_list.interval_list_name)\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n\n    def plot_epoch_pos_raw_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=(30, 3))\n\n        raw_data_valid_times = interval_list.loc[\n            interval_list.interval_list_name == \"raw data valid times\"\n        ].valid_times\n        interval_y = 1\n\n        for interval in np.asarray(raw_data_valid_times)[0]:\n            ax.plot(interval, [interval_y, interval_y])\n            ax.scatter(interval, [interval_y, interval_y], alpha=0.8, zorder=2)\n\n        epoch_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n        interval_y = 2\n        for epoch, valid_times in zip(\n            epoch_valid_times.index, epoch_valid_times\n        ):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        pos_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^pos \\d+ valid times$\", axis=0)\n            .valid_times\n        ).sort_index(key=lambda index: [int(name.split()[1]) for name in index])\n        interval_y = 0\n        for epoch, valid_times in zip(pos_valid_times.index, pos_valid_times):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch.strip(\" valid times\"),\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        ax.set_ylim((-0.25, 2.25))\n        ax.set_yticks(np.arange(3))\n        ax.set_yticklabels([\"pos valid times\", \"raw data valid times\", \"epoch\"])\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.common.common_interval.IntervalList.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, *, nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add each entry in the NWB file epochs table to the IntervalList table.</p> <p>The interval list name for each epoch is set to the first tag for the epoch. If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index (0-indexed) of the epoch in the epochs table. The start time and stop time of the epoch are stored in the valid_times field as a numpy array of [start time, stop time] for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>nwb_file_name</code> <code>str</code> <p>The file name of the NWB file, used as a primary key to the Session table.</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n    The interval list name for each epoch is set to the first tag for the epoch.\n    If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n    (0-indexed) of the epoch in the epochs table.\n    The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n    [start time, stop time] for each epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    nwb_file_name : str\n        The file name of the NWB file, used as a primary key to the Session table.\n    \"\"\"\n    if nwbf.epochs is None:\n        print(\"No epochs found in NWB file.\")\n        return\n    epochs = nwbf.epochs.to_dataframe()\n    for epoch_index, epoch_data in epochs.iterrows():\n        epoch_dict = dict()\n        epoch_dict[\"nwb_file_name\"] = nwb_file_name\n        if epoch_data.tags[0]:\n            epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n        else:\n            epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                epoch_index\n            )\n        epoch_dict[\"valid_times\"] = np.asarray(\n            [[epoch_data.start_time, epoch_data.stop_time]]\n        )\n        cls.insert1(epoch_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Curation", "title": "<code>Curation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n    definition = \"\"\"\n    # Stores each spike sorting; similar to IntervalList\n    curation_id: int # a number corresponding to the index of this curation\n    -&gt; SpikeSorting\n    ---\n    parent_curation_id=-1: int\n    curation_labels: blob # a dictionary of labels for the units\n    merge_groups: blob # a list of merge groups for the units\n    quality_metrics: blob # a list of quality metrics for the units (if available)\n    description='': varchar(1000) #optional description for this curated sort\n    time_of_creation: int   # in Unix time, to the nearest second\n    \"\"\"\n\n    @staticmethod\n    def insert_curation(\n        sorting_key: dict,\n        parent_curation_id: int = -1,\n        labels=None,\n        merge_groups=None,\n        metrics=None,\n        description=\"\",\n    ):\n\"\"\"Given a SpikeSorting key and the parent_sorting_id (and optional\n        arguments) insert an entry into Curation.\n\n\n        Parameters\n        ----------\n        sorting_key : dict\n            The key for the original SpikeSorting\n        parent_curation_id : int, optional\n            The id of the parent sorting\n        labels : dict or None, optional\n        merge_groups : dict or None, optional\n        metrics : dict or None, optional\n            Computed metrics for sorting\n        description : str, optional\n            text description of this sort\n\n        Returns\n        -------\n        curation_key : dict\n\n        \"\"\"\n        if parent_curation_id == -1:\n            # check to see if this sorting with a parent of -1 has already been inserted and if so, warn the user\n            inserted_curation = (Curation &amp; sorting_key).fetch(\"KEY\")\n            if len(inserted_curation) &gt; 0:\n                Warning(\n                    \"Sorting has already been inserted, returning key to previously\"\n                    \"inserted curation\"\n                )\n                return inserted_curation[0]\n\n        if labels is None:\n            labels = {}\n        if merge_groups is None:\n            merge_groups = []\n        if metrics is None:\n            metrics = {}\n\n        # generate a unique number for this curation\n        id = (Curation &amp; sorting_key).fetch(\"curation_id\")\n        if len(id) &gt; 0:\n            curation_id = max(id) + 1\n        else:\n            curation_id = 0\n\n        # convert unit_ids in labels to integers for labels from sortingview.\n        new_labels = {int(unit_id): labels[unit_id] for unit_id in labels}\n\n        sorting_key[\"curation_id\"] = curation_id\n        sorting_key[\"parent_curation_id\"] = parent_curation_id\n        sorting_key[\"description\"] = description\n        sorting_key[\"curation_labels\"] = new_labels\n        sorting_key[\"merge_groups\"] = merge_groups\n        sorting_key[\"quality_metrics\"] = metrics\n        sorting_key[\"time_of_creation\"] = int(time.time())\n\n        # mike: added skip duplicates\n        Curation.insert1(sorting_key, skip_duplicates=True)\n\n        # get the primary key for this curation\n        c_key = Curation.fetch(\"KEY\")[0]\n        curation_key = {item: sorting_key[item] for item in c_key}\n\n        return curation_key\n\n    @staticmethod\n    def get_recording(key: dict):\n\"\"\"Returns the recording extractor for the recording related to this curation\n\n        Parameters\n        ----------\n        key : dict\n            SpikeSortingRecording key\n\n        Returns\n        -------\n        recording_extractor : spike interface recording extractor\n\n        \"\"\"\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        return si.load_extractor(recording_path)\n\n    @staticmethod\n    def get_curated_sorting(key: dict):\n\"\"\"Returns the sorting extractor related to this curation,\n        with merges applied.\n\n        Parameters\n        ----------\n        key : dict\n            Curation key\n\n        Returns\n        -------\n        sorting_extractor: spike interface sorting extractor\n\n        \"\"\"\n        sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n        sorting = si.load_extractor(sorting_path)\n        merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n        # TODO: write code to get merged sorting extractor\n        if len(merge_groups) != 0:\n            return MergedSortingExtractor(\n                parent_sorting=sorting, merge_groups=merge_groups\n            )\n        else:\n            return sorting\n\n    @staticmethod\n    def save_sorting_nwb(\n        key,\n        sorting,\n        timestamps,\n        sort_interval_list_name,\n        sort_interval,\n        labels=None,\n        metrics=None,\n        unit_ids=None,\n    ):\n\"\"\"Store a sorting in a new AnalysisNwbfile\n\n        Parameters\n        ----------\n        key : dict\n            key to SpikeSorting table\n        sorting : si.Sorting\n            sorting\n        timestamps : array_like\n            Time stamps of the sorted recoridng;\n            used to convert the spike timings from index to real time\n        sort_interval_list_name : str\n            name of sort interval\n        sort_interval : list\n            interval for start and end of sort\n        labels : dict, optional\n            curation labels, by default None\n        metrics : dict, optional\n            quality metrics, by default None\n        unit_ids : list, optional\n            IDs of units whose spiketrains to save, by default None\n\n        Returns\n        -------\n        analysis_file_name : str\n        units_object_id : str\n\n        \"\"\"\n\n        sort_interval_valid_times = (\n            IntervalList &amp; {\"interval_list_name\": sort_interval_list_name}\n        ).fetch1(\"valid_times\")\n\n        units = dict()\n        units_valid_times = dict()\n        units_sort_interval = dict()\n\n        if unit_ids is None:\n            unit_ids = sorting.get_unit_ids()\n\n        for unit_id in unit_ids:\n            spike_times_in_samples = sorting.get_unit_spike_train(\n                unit_id=unit_id\n            )\n            units[unit_id] = timestamps[spike_times_in_samples]\n            units_valid_times[unit_id] = sort_interval_valid_times\n            units_sort_interval[unit_id] = [sort_interval]\n\n        analysis_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n        object_ids = AnalysisNwbfile().add_units(\n            analysis_file_name,\n            units,\n            units_valid_times,\n            units_sort_interval,\n            metrics=metrics,\n            labels=labels,\n        )\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], analysis_file_name)\n\n        if object_ids == \"\":\n            print(\n                \"Sorting contains no units.\"\n                \"Created an empty analysis nwb file anyway.\"\n            )\n            units_object_id = \"\"\n        else:\n            units_object_id = object_ids[0]\n\n        return analysis_file_name, units_object_id\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Curation.insert_curation", "title": "<code>insert_curation(sorting_key, parent_curation_id=-1, labels=None, merge_groups=None, metrics=None, description='')</code>  <code>staticmethod</code>", "text": "<p>Given a SpikeSorting key and the parent_sorting_id (and optional arguments) insert an entry into Curation.</p> <p>Parameters:</p> Name Type Description Default <code>sorting_key</code> <code>dict</code> <p>The key for the original SpikeSorting</p> required <code>parent_curation_id</code> <code>int</code> <p>The id of the parent sorting</p> <code>-1</code> <code>labels</code> <code>dict or None</code> <code>None</code> <code>merge_groups</code> <code>dict or None</code> <code>None</code> <code>metrics</code> <code>dict or None</code> <p>Computed metrics for sorting</p> <code>None</code> <code>description</code> <code>str</code> <p>text description of this sort</p> <code>''</code> <p>Returns:</p> Name Type Description <code>curation_key</code> <code>dict</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef insert_curation(\n    sorting_key: dict,\n    parent_curation_id: int = -1,\n    labels=None,\n    merge_groups=None,\n    metrics=None,\n    description=\"\",\n):\n\"\"\"Given a SpikeSorting key and the parent_sorting_id (and optional\n    arguments) insert an entry into Curation.\n\n\n    Parameters\n    ----------\n    sorting_key : dict\n        The key for the original SpikeSorting\n    parent_curation_id : int, optional\n        The id of the parent sorting\n    labels : dict or None, optional\n    merge_groups : dict or None, optional\n    metrics : dict or None, optional\n        Computed metrics for sorting\n    description : str, optional\n        text description of this sort\n\n    Returns\n    -------\n    curation_key : dict\n\n    \"\"\"\n    if parent_curation_id == -1:\n        # check to see if this sorting with a parent of -1 has already been inserted and if so, warn the user\n        inserted_curation = (Curation &amp; sorting_key).fetch(\"KEY\")\n        if len(inserted_curation) &gt; 0:\n            Warning(\n                \"Sorting has already been inserted, returning key to previously\"\n                \"inserted curation\"\n            )\n            return inserted_curation[0]\n\n    if labels is None:\n        labels = {}\n    if merge_groups is None:\n        merge_groups = []\n    if metrics is None:\n        metrics = {}\n\n    # generate a unique number for this curation\n    id = (Curation &amp; sorting_key).fetch(\"curation_id\")\n    if len(id) &gt; 0:\n        curation_id = max(id) + 1\n    else:\n        curation_id = 0\n\n    # convert unit_ids in labels to integers for labels from sortingview.\n    new_labels = {int(unit_id): labels[unit_id] for unit_id in labels}\n\n    sorting_key[\"curation_id\"] = curation_id\n    sorting_key[\"parent_curation_id\"] = parent_curation_id\n    sorting_key[\"description\"] = description\n    sorting_key[\"curation_labels\"] = new_labels\n    sorting_key[\"merge_groups\"] = merge_groups\n    sorting_key[\"quality_metrics\"] = metrics\n    sorting_key[\"time_of_creation\"] = int(time.time())\n\n    # mike: added skip duplicates\n    Curation.insert1(sorting_key, skip_duplicates=True)\n\n    # get the primary key for this curation\n    c_key = Curation.fetch(\"KEY\")[0]\n    curation_key = {item: sorting_key[item] for item in c_key}\n\n    return curation_key\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Curation.get_recording", "title": "<code>get_recording(key)</code>  <code>staticmethod</code>", "text": "<p>Returns the recording extractor for the recording related to this curation</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>SpikeSortingRecording key</p> required <p>Returns:</p> Name Type Description <code>recording_extractor</code> <code>spike interface recording extractor</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef get_recording(key: dict):\n\"\"\"Returns the recording extractor for the recording related to this curation\n\n    Parameters\n    ----------\n    key : dict\n        SpikeSortingRecording key\n\n    Returns\n    -------\n    recording_extractor : spike interface recording extractor\n\n    \"\"\"\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    return si.load_extractor(recording_path)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Curation.get_curated_sorting", "title": "<code>get_curated_sorting(key)</code>  <code>staticmethod</code>", "text": "<p>Returns the sorting extractor related to this curation, with merges applied.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Curation key</p> required <p>Returns:</p> Name Type Description <code>sorting_extractor</code> <code>spike interface sorting extractor</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef get_curated_sorting(key: dict):\n\"\"\"Returns the sorting extractor related to this curation,\n    with merges applied.\n\n    Parameters\n    ----------\n    key : dict\n        Curation key\n\n    Returns\n    -------\n    sorting_extractor: spike interface sorting extractor\n\n    \"\"\"\n    sorting_path = (SpikeSorting &amp; key).fetch1(\"sorting_path\")\n    sorting = si.load_extractor(sorting_path)\n    merge_groups = (Curation &amp; key).fetch1(\"merge_groups\")\n    # TODO: write code to get merged sorting extractor\n    if len(merge_groups) != 0:\n        return MergedSortingExtractor(\n            parent_sorting=sorting, merge_groups=merge_groups\n        )\n    else:\n        return sorting\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Curation.save_sorting_nwb", "title": "<code>save_sorting_nwb(key, sorting, timestamps, sort_interval_list_name, sort_interval, labels=None, metrics=None, unit_ids=None)</code>  <code>staticmethod</code>", "text": "<p>Store a sorting in a new AnalysisNwbfile</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>key to SpikeSorting table</p> required <code>sorting</code> <code>si.Sorting</code> <p>sorting</p> required <code>timestamps</code> <code>array_like</code> <p>Time stamps of the sorted recoridng; used to convert the spike timings from index to real time</p> required <code>sort_interval_list_name</code> <code>str</code> <p>name of sort interval</p> required <code>sort_interval</code> <code>list</code> <p>interval for start and end of sort</p> required <code>labels</code> <code>dict</code> <p>curation labels, by default None</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>quality metrics, by default None</p> <code>None</code> <code>unit_ids</code> <code>list</code> <p>IDs of units whose spiketrains to save, by default None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <code>units_object_id</code> <code>str</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef save_sorting_nwb(\n    key,\n    sorting,\n    timestamps,\n    sort_interval_list_name,\n    sort_interval,\n    labels=None,\n    metrics=None,\n    unit_ids=None,\n):\n\"\"\"Store a sorting in a new AnalysisNwbfile\n\n    Parameters\n    ----------\n    key : dict\n        key to SpikeSorting table\n    sorting : si.Sorting\n        sorting\n    timestamps : array_like\n        Time stamps of the sorted recoridng;\n        used to convert the spike timings from index to real time\n    sort_interval_list_name : str\n        name of sort interval\n    sort_interval : list\n        interval for start and end of sort\n    labels : dict, optional\n        curation labels, by default None\n    metrics : dict, optional\n        quality metrics, by default None\n    unit_ids : list, optional\n        IDs of units whose spiketrains to save, by default None\n\n    Returns\n    -------\n    analysis_file_name : str\n    units_object_id : str\n\n    \"\"\"\n\n    sort_interval_valid_times = (\n        IntervalList &amp; {\"interval_list_name\": sort_interval_list_name}\n    ).fetch1(\"valid_times\")\n\n    units = dict()\n    units_valid_times = dict()\n    units_sort_interval = dict()\n\n    if unit_ids is None:\n        unit_ids = sorting.get_unit_ids()\n\n    for unit_id in unit_ids:\n        spike_times_in_samples = sorting.get_unit_spike_train(\n            unit_id=unit_id\n        )\n        units[unit_id] = timestamps[spike_times_in_samples]\n        units_valid_times[unit_id] = sort_interval_valid_times\n        units_sort_interval[unit_id] = [sort_interval]\n\n    analysis_file_name = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n    object_ids = AnalysisNwbfile().add_units(\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=metrics,\n        labels=labels,\n    )\n    AnalysisNwbfile().add(key[\"nwb_file_name\"], analysis_file_name)\n\n    if object_ids == \"\":\n        print(\n            \"Sorting contains no units.\"\n            \"Created an empty analysis nwb file anyway.\"\n        )\n        units_object_id = \"\"\n    else:\n        units_object_id = object_ids[0]\n\n    return analysis_file_name, units_object_id\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.SpikeSorting", "title": "<code>SpikeSorting</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>@schema\nclass SpikeSorting(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingSelection\n    ---\n    sorting_path: varchar(1000)\n    time_of_sort: int   # in Unix time, to the nearest second\n    \"\"\"\n\n    def make(self, key: dict):\n\"\"\"Runs spike sorting on the data and parameters specified by the\n        SpikeSortingSelection table and inserts a new entry to SpikeSorting table.\n\n        Specifically,\n        1. Loads saved recording and runs the sort on it with spikeinterface\n        2. Saves the sorting with spikeinterface\n        3. Creates an analysis NWB file and saves the sorting there\n           (this is redundant with 2; will change in the future)\n\n        \"\"\"\n\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        recording = si.load_extractor(recording_path)\n\n        # first, get the timestamps\n        timestamps = SpikeSortingRecording._get_recording_timestamps(recording)\n        fs = recording.get_sampling_frequency()\n        # then concatenate the recordings\n        # Note: the timestamps are lost upon concatenation,\n        # i.e. concat_recording.get_times() doesn't return true timestamps anymore.\n        # but concat_recording.recoring_list[i].get_times() will return correct\n        # timestamps for ith recording.\n        if recording.get_num_segments() &gt; 1 and isinstance(\n            recording, si.AppendSegmentRecording\n        ):\n            recording = si.concatenate_recordings(recording.recording_list)\n        elif recording.get_num_segments() &gt; 1 and isinstance(\n            recording, si.BinaryRecordingExtractor\n        ):\n            recording = si.concatenate_recordings([recording])\n\n        # load artifact intervals\n        artifact_times = (\n            ArtifactRemovedIntervalList\n            &amp; {\n                \"artifact_removed_interval_list_name\": key[\n                    \"artifact_removed_interval_list_name\"\n                ]\n            }\n        ).fetch1(\"artifact_times\")\n        if len(artifact_times):\n            if artifact_times.ndim == 1:\n                artifact_times = np.expand_dims(artifact_times, 0)\n\n            # convert artifact intervals to indices\n            list_triggers = []\n            for interval in artifact_times:\n                list_triggers.append(\n                    np.arange(\n                        np.searchsorted(timestamps, interval[0]),\n                        np.searchsorted(timestamps, interval[1]),\n                    )\n                )\n            list_triggers = [list(np.concatenate(list_triggers))]\n            recording = sip.remove_artifacts(\n                recording=recording,\n                list_triggers=list_triggers,\n                ms_before=None,\n                ms_after=None,\n                mode=\"zeros\",\n            )\n\n        print(f\"Running spike sorting on {key}...\")\n        sorter, sorter_params = (SpikeSorterParameters &amp; key).fetch1(\n            \"sorter\", \"sorter_params\"\n        )\n\n        sorter_temp_dir = tempfile.TemporaryDirectory(\n            dir=os.getenv(\"SPYGLASS_TEMP_DIR\")\n        )\n        # add tempdir option for mountainsort\n        sorter_params[\"tempdir\"] = sorter_temp_dir.name\n\n        if sorter == \"clusterless_thresholder\":\n            # need to remove tempdir and whiten from sorter_params\n            sorter_params.pop(\"tempdir\", None)\n            sorter_params.pop(\"whiten\", None)\n\n            # Detect peaks for clusterless decoding\n            detected_spikes = detect_peaks(recording, **sorter_params)\n            sorting = si.NumpySorting.from_times_labels(\n                times_list=detected_spikes[\"sample_ind\"],\n                labels_list=np.zeros(len(detected_spikes), dtype=np.int),\n                sampling_frequency=recording.get_sampling_frequency(),\n            )\n        else:\n            if \"whiten\" in sorter_params.keys():\n                if sorter_params[\"whiten\"]:\n                    sorter_params[\"whiten\"] = False  # set whiten to False\n            # whiten recording separately; make sure dtype is float32\n            # to avoid downstream error with svd\n            recording = sip.whiten(recording, dtype=\"float32\")\n            sorting = sis.run_sorter(\n                sorter,\n                recording,\n                output_folder=sorter_temp_dir.name,\n                delete_output_folder=True,\n                **sorter_params,\n            )\n        key[\"time_of_sort\"] = int(time.time())\n\n        print(\"Saving sorting results...\")\n        sorting_folder = Path(os.getenv(\"SPYGLASS_SORTING_DIR\"))\n        sorting_name = self._get_sorting_name(key)\n        key[\"sorting_path\"] = str(sorting_folder / Path(sorting_name))\n        if os.path.exists(key[\"sorting_path\"]):\n            shutil.rmtree(key[\"sorting_path\"])\n        sorting = sorting.save(folder=key[\"sorting_path\"])\n        self.insert1(key)\n\n    def delete(self):\n\"\"\"Extends the delete method of base class to implement permission checking.\n        Note that this is NOT a security feature, as anyone that has access to source code\n        can disable it; it just makes it less likely to accidentally delete entries.\n        \"\"\"\n        current_user_name = dj.config[\"database.user\"]\n        entries = self.fetch()\n        permission_bool = np.zeros((len(entries),))\n        print(\n            f\"Attempting to delete {len(entries)} entries, checking permission...\"\n        )\n\n        for entry_idx in range(len(entries)):\n            # check the team name for the entry, then look up the members in that team,\n            # then get their datajoint user names\n            team_name = (\n                SpikeSortingRecordingSelection\n                &amp; (SpikeSortingRecordingSelection &amp; entries[entry_idx]).proj()\n            ).fetch1()[\"team_name\"]\n            lab_member_name_list = (\n                LabTeam.LabTeamMember &amp; {\"team_name\": team_name}\n            ).fetch(\"lab_member_name\")\n            datajoint_user_names = []\n            for lab_member_name in lab_member_name_list:\n                datajoint_user_names.append(\n                    (\n                        LabMember.LabMemberInfo\n                        &amp; {\"lab_member_name\": lab_member_name}\n                    ).fetch1(\"datajoint_user_name\")\n                )\n            permission_bool[entry_idx] = (\n                current_user_name in datajoint_user_names\n            )\n        if np.sum(permission_bool) == len(entries):\n            print(\"Permission to delete all specified entries granted.\")\n            super().delete()\n        else:\n            raise Exception(\n                \"You do not have permission to delete all specified\"\n                \"entries. Not deleting anything.\"\n            )\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        raise NotImplementedError\n        return None\n        # return fetch_nwb(self, (AnalysisNwbfile, 'analysis_file_abs_path'), *attrs, **kwargs)\n\n    def nightly_cleanup(self):\n\"\"\"Clean up spike sorting directories that are not in the SpikeSorting table.\n        This should be run after AnalysisNwbFile().nightly_cleanup()\n        \"\"\"\n        # get a list of the files in the spike sorting storage directory\n        dir_names = next(os.walk(os.environ[\"SPYGLASS_SORTING_DIR\"]))[1]\n        # now retrieve a list of the currently used analysis nwb files\n        analysis_file_names = self.fetch(\"analysis_file_name\")\n        for dir in dir_names:\n            if dir not in analysis_file_names:\n                full_path = str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n                print(f\"removing {full_path}\")\n                shutil.rmtree(\n                    str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n                )\n\n    @staticmethod\n    def _get_sorting_name(key):\n        recording_name = SpikeSortingRecording._get_recording_name(key)\n        sorting_name = (\n            recording_name + \"_\" + str(uuid.uuid4())[0:8] + \"_spikesorting\"\n        )\n        return sorting_name\n\n    # TODO: write a function to import sorting done outside of dj\n\n    def _import_sorting(self, key):\n        raise NotImplementedError\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.make", "title": "<code>make(key)</code>", "text": "<p>Runs spike sorting on the data and parameters specified by the SpikeSortingSelection table and inserts a new entry to SpikeSorting table.</p> <p>Specifically, 1. Loads saved recording and runs the sort on it with spikeinterface 2. Saves the sorting with spikeinterface 3. Creates an analysis NWB file and saves the sorting there    (this is redundant with 2; will change in the future)</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def make(self, key: dict):\n\"\"\"Runs spike sorting on the data and parameters specified by the\n    SpikeSortingSelection table and inserts a new entry to SpikeSorting table.\n\n    Specifically,\n    1. Loads saved recording and runs the sort on it with spikeinterface\n    2. Saves the sorting with spikeinterface\n    3. Creates an analysis NWB file and saves the sorting there\n       (this is redundant with 2; will change in the future)\n\n    \"\"\"\n\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    recording = si.load_extractor(recording_path)\n\n    # first, get the timestamps\n    timestamps = SpikeSortingRecording._get_recording_timestamps(recording)\n    fs = recording.get_sampling_frequency()\n    # then concatenate the recordings\n    # Note: the timestamps are lost upon concatenation,\n    # i.e. concat_recording.get_times() doesn't return true timestamps anymore.\n    # but concat_recording.recoring_list[i].get_times() will return correct\n    # timestamps for ith recording.\n    if recording.get_num_segments() &gt; 1 and isinstance(\n        recording, si.AppendSegmentRecording\n    ):\n        recording = si.concatenate_recordings(recording.recording_list)\n    elif recording.get_num_segments() &gt; 1 and isinstance(\n        recording, si.BinaryRecordingExtractor\n    ):\n        recording = si.concatenate_recordings([recording])\n\n    # load artifact intervals\n    artifact_times = (\n        ArtifactRemovedIntervalList\n        &amp; {\n            \"artifact_removed_interval_list_name\": key[\n                \"artifact_removed_interval_list_name\"\n            ]\n        }\n    ).fetch1(\"artifact_times\")\n    if len(artifact_times):\n        if artifact_times.ndim == 1:\n            artifact_times = np.expand_dims(artifact_times, 0)\n\n        # convert artifact intervals to indices\n        list_triggers = []\n        for interval in artifact_times:\n            list_triggers.append(\n                np.arange(\n                    np.searchsorted(timestamps, interval[0]),\n                    np.searchsorted(timestamps, interval[1]),\n                )\n            )\n        list_triggers = [list(np.concatenate(list_triggers))]\n        recording = sip.remove_artifacts(\n            recording=recording,\n            list_triggers=list_triggers,\n            ms_before=None,\n            ms_after=None,\n            mode=\"zeros\",\n        )\n\n    print(f\"Running spike sorting on {key}...\")\n    sorter, sorter_params = (SpikeSorterParameters &amp; key).fetch1(\n        \"sorter\", \"sorter_params\"\n    )\n\n    sorter_temp_dir = tempfile.TemporaryDirectory(\n        dir=os.getenv(\"SPYGLASS_TEMP_DIR\")\n    )\n    # add tempdir option for mountainsort\n    sorter_params[\"tempdir\"] = sorter_temp_dir.name\n\n    if sorter == \"clusterless_thresholder\":\n        # need to remove tempdir and whiten from sorter_params\n        sorter_params.pop(\"tempdir\", None)\n        sorter_params.pop(\"whiten\", None)\n\n        # Detect peaks for clusterless decoding\n        detected_spikes = detect_peaks(recording, **sorter_params)\n        sorting = si.NumpySorting.from_times_labels(\n            times_list=detected_spikes[\"sample_ind\"],\n            labels_list=np.zeros(len(detected_spikes), dtype=np.int),\n            sampling_frequency=recording.get_sampling_frequency(),\n        )\n    else:\n        if \"whiten\" in sorter_params.keys():\n            if sorter_params[\"whiten\"]:\n                sorter_params[\"whiten\"] = False  # set whiten to False\n        # whiten recording separately; make sure dtype is float32\n        # to avoid downstream error with svd\n        recording = sip.whiten(recording, dtype=\"float32\")\n        sorting = sis.run_sorter(\n            sorter,\n            recording,\n            output_folder=sorter_temp_dir.name,\n            delete_output_folder=True,\n            **sorter_params,\n        )\n    key[\"time_of_sort\"] = int(time.time())\n\n    print(\"Saving sorting results...\")\n    sorting_folder = Path(os.getenv(\"SPYGLASS_SORTING_DIR\"))\n    sorting_name = self._get_sorting_name(key)\n    key[\"sorting_path\"] = str(sorting_folder / Path(sorting_name))\n    if os.path.exists(key[\"sorting_path\"]):\n        shutil.rmtree(key[\"sorting_path\"])\n    sorting = sorting.save(folder=key[\"sorting_path\"])\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.delete", "title": "<code>delete()</code>", "text": "<p>Extends the delete method of base class to implement permission checking. Note that this is NOT a security feature, as anyone that has access to source code can disable it; it just makes it less likely to accidentally delete entries.</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def delete(self):\n\"\"\"Extends the delete method of base class to implement permission checking.\n    Note that this is NOT a security feature, as anyone that has access to source code\n    can disable it; it just makes it less likely to accidentally delete entries.\n    \"\"\"\n    current_user_name = dj.config[\"database.user\"]\n    entries = self.fetch()\n    permission_bool = np.zeros((len(entries),))\n    print(\n        f\"Attempting to delete {len(entries)} entries, checking permission...\"\n    )\n\n    for entry_idx in range(len(entries)):\n        # check the team name for the entry, then look up the members in that team,\n        # then get their datajoint user names\n        team_name = (\n            SpikeSortingRecordingSelection\n            &amp; (SpikeSortingRecordingSelection &amp; entries[entry_idx]).proj()\n        ).fetch1()[\"team_name\"]\n        lab_member_name_list = (\n            LabTeam.LabTeamMember &amp; {\"team_name\": team_name}\n        ).fetch(\"lab_member_name\")\n        datajoint_user_names = []\n        for lab_member_name in lab_member_name_list:\n            datajoint_user_names.append(\n                (\n                    LabMember.LabMemberInfo\n                    &amp; {\"lab_member_name\": lab_member_name}\n                ).fetch1(\"datajoint_user_name\")\n            )\n        permission_bool[entry_idx] = (\n            current_user_name in datajoint_user_names\n        )\n    if np.sum(permission_bool) == len(entries):\n        print(\"Permission to delete all specified entries granted.\")\n        super().delete()\n    else:\n        raise Exception(\n            \"You do not have permission to delete all specified\"\n            \"entries. Not deleting anything.\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.nightly_cleanup", "title": "<code>nightly_cleanup()</code>", "text": "<p>Clean up spike sorting directories that are not in the SpikeSorting table. This should be run after AnalysisNwbFile().nightly_cleanup()</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def nightly_cleanup(self):\n\"\"\"Clean up spike sorting directories that are not in the SpikeSorting table.\n    This should be run after AnalysisNwbFile().nightly_cleanup()\n    \"\"\"\n    # get a list of the files in the spike sorting storage directory\n    dir_names = next(os.walk(os.environ[\"SPYGLASS_SORTING_DIR\"]))[1]\n    # now retrieve a list of the currently used analysis nwb files\n    analysis_file_names = self.fetch(\"analysis_file_name\")\n    for dir in dir_names:\n        if dir not in analysis_file_names:\n            full_path = str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n            print(f\"removing {full_path}\")\n            shutil.rmtree(\n                str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n            )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Waveforms", "title": "<code>Waveforms</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass Waveforms(dj.Computed):\n    definition = \"\"\"\n    -&gt; WaveformSelection\n    ---\n    waveform_extractor_path: varchar(400)\n    -&gt; AnalysisNwbfile\n    waveforms_object_id: varchar(40)   # Object ID for the waveforms in NWB file\n    \"\"\"\n\n    def make(self, key):\n        recording = Curation.get_recording(key)\n        if recording.get_num_segments() &gt; 1:\n            recording = si.concatenate_recordings([recording])\n\n        sorting = Curation.get_curated_sorting(key)\n\n        print(\"Extracting waveforms...\")\n        waveform_params = (WaveformParameters &amp; key).fetch1(\"waveform_params\")\n        if \"whiten\" in waveform_params:\n            if waveform_params.pop(\"whiten\"):\n                recording = sip.whiten(recording, dtype=\"float32\")\n\n        waveform_extractor_name = self._get_waveform_extractor_name(key)\n        key[\"waveform_extractor_path\"] = str(\n            Path(os.environ[\"SPYGLASS_WAVEFORMS_DIR\"])\n            / Path(waveform_extractor_name)\n        )\n        if os.path.exists(key[\"waveform_extractor_path\"]):\n            shutil.rmtree(key[\"waveform_extractor_path\"])\n        waveforms = si.extract_waveforms(\n            recording=recording,\n            sorting=sorting,\n            folder=key[\"waveform_extractor_path\"],\n            **waveform_params,\n        )\n\n        key[\"analysis_file_name\"] = AnalysisNwbfile().create(\n            key[\"nwb_file_name\"]\n        )\n        object_id = AnalysisNwbfile().add_units_waveforms(\n            key[\"analysis_file_name\"], waveform_extractor=waveforms\n        )\n        key[\"waveforms_object_id\"] = object_id\n        AnalysisNwbfile().add(key[\"nwb_file_name\"], key[\"analysis_file_name\"])\n\n        self.insert1(key)\n\n    def load_waveforms(self, key: dict):\n\"\"\"Returns a spikeinterface waveform extractor specified by key\n\n        Parameters\n        ----------\n        key : dict\n            Could be an entry in Waveforms, or some other key that uniquely defines\n            an entry in Waveforms\n\n        Returns\n        -------\n        we : spikeinterface.WaveformExtractor\n        \"\"\"\n        we_path = (self &amp; key).fetch1(\"waveform_extractor_path\")\n        we = si.WaveformExtractor.load_from_folder(we_path)\n        return we\n\n    def fetch_nwb(self, key):\n        # TODO: implement fetching waveforms from NWB\n        return NotImplementedError\n\n    def _get_waveform_extractor_name(self, key):\n        waveform_params_name = (WaveformParameters &amp; key).fetch1(\n            \"waveform_params_name\"\n        )\n\n        return (\n            f'{key[\"nwb_file_name\"]}_{str(uuid.uuid4())[0:8]}_'\n            f'{key[\"curation_id\"]}_{waveform_params_name}_waveforms'\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.Waveforms.load_waveforms", "title": "<code>load_waveforms(key)</code>", "text": "<p>Returns a spikeinterface waveform extractor specified by key</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Could be an entry in Waveforms, or some other key that uniquely defines an entry in Waveforms</p> required <p>Returns:</p> Name Type Description <code>we</code> <code>spikeinterface.WaveformExtractor</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>def load_waveforms(self, key: dict):\n\"\"\"Returns a spikeinterface waveform extractor specified by key\n\n    Parameters\n    ----------\n    key : dict\n        Could be an entry in Waveforms, or some other key that uniquely defines\n        an entry in Waveforms\n\n    Returns\n    -------\n    we : spikeinterface.WaveformExtractor\n    \"\"\"\n    we_path = (self &amp; key).fetch1(\"waveform_extractor_path\")\n    we = si.WaveformExtractor.load_from_folder(we_path)\n    return we\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.SpikeSortingRecording", "title": "<code>SpikeSortingRecording</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>@schema\nclass SpikeSortingRecording(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingRecordingSelection\n    ---\n    recording_path: varchar(1000)\n    -&gt; IntervalList.proj(sort_interval_list_name='interval_list_name')\n    \"\"\"\n\n    def make(self, key):\n        sort_interval_valid_times = self._get_sort_interval_valid_times(key)\n        recording = self._get_filtered_recording(key)\n        recording_name = self._get_recording_name(key)\n\n        tmp_key = {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": recording_name,\n            \"valid_times\": sort_interval_valid_times,\n        }\n        IntervalList.insert1(tmp_key, replace=True)\n\n        # store the list of valid times for the sort\n        key[\"sort_interval_list_name\"] = tmp_key[\"interval_list_name\"]\n\n        # Path to files that will hold the recording extractors\n        recording_folder = Path(os.getenv(\"SPYGLASS_RECORDING_DIR\"))\n        key[\"recording_path\"] = str(recording_folder / Path(recording_name))\n        if os.path.exists(key[\"recording_path\"]):\n            shutil.rmtree(key[\"recording_path\"])\n        recording = recording.save(\n            folder=key[\"recording_path\"], chunk_duration=\"10000ms\", n_jobs=8\n        )\n\n        self.insert1(key)\n\n    @staticmethod\n    def _get_recording_name(key):\n        recording_name = (\n            key[\"nwb_file_name\"]\n            + \"_\"\n            + key[\"sort_interval_name\"]\n            + \"_\"\n            + str(key[\"sort_group_id\"])\n            + \"_\"\n            + key[\"preproc_params_name\"]\n        )\n        return recording_name\n\n    @staticmethod\n    def _get_recording_timestamps(recording):\n        if recording.get_num_segments() &gt; 1:\n            frames_per_segment = [0]\n            for i in range(recording.get_num_segments()):\n                frames_per_segment.append(\n                    recording.get_num_frames(segment_index=i)\n                )\n\n            cumsum_frames = np.cumsum(frames_per_segment)\n            total_frames = np.sum(frames_per_segment)\n\n            timestamps = np.zeros((total_frames,))\n            for i in range(recording.get_num_segments()):\n                timestamps[\n                    cumsum_frames[i] : cumsum_frames[i + 1]\n                ] = recording.get_times(segment_index=i)\n        else:\n            timestamps = recording.get_times()\n        return timestamps\n\n    def _get_sort_interval_valid_times(self, key):\n\"\"\"Identifies the intersection between sort interval specified by the user\n        and the valid times (times for which neural data exist)\n\n        Parameters\n        ----------\n        key: dict\n            specifies a (partially filled) entry of SpikeSorting table\n\n        Returns\n        -------\n        sort_interval_valid_times: ndarray of tuples\n            (start, end) times for valid stretches of the sorting interval\n\n        \"\"\"\n        sort_interval = (\n            SortInterval\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_interval_name\": key[\"sort_interval_name\"],\n            }\n        ).fetch1(\"sort_interval\")\n        interval_list_name = (SpikeSortingRecordingSelection &amp; key).fetch1(\n            \"interval_list_name\"\n        )\n        valid_interval_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n        valid_sort_times = interval_list_intersect(\n            sort_interval, valid_interval_times\n        )\n        # Exclude intervals shorter than specified length\n        params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        if \"min_segment_length\" in params:\n            valid_sort_times = intervals_by_length(\n                valid_sort_times, min_length=params[\"min_segment_length\"]\n            )\n        return valid_sort_times\n\n    def _get_filtered_recording(self, key: dict):\n\"\"\"Filters and references a recording\n        * Loads the NWB file created during insertion as a spikeinterface Recording\n        * Slices recording in time (interval) and space (channels);\n          recording chunks from disjoint intervals are concatenated\n        * Applies referencing and bandpass filtering\n\n        Parameters\n        ----------\n        key: dict,\n            primary key of SpikeSortingRecording table\n\n        Returns\n        -------\n        recording: si.Recording\n        \"\"\"\n\n        nwb_file_abs_path = Nwbfile().get_abs_path(key[\"nwb_file_name\"])\n        recording = se.read_nwb_recording(\n            nwb_file_abs_path, load_time_vector=True\n        )\n\n        valid_sort_times = self._get_sort_interval_valid_times(key)\n        # shape is (N, 2)\n        valid_sort_times_indices = np.array(\n            [\n                np.searchsorted(recording.get_times(), interval)\n                for interval in valid_sort_times\n            ]\n        )\n        # join intervals of indices that are adjacent\n        valid_sort_times_indices = reduce(\n            union_adjacent_index, valid_sort_times_indices\n        )\n        if valid_sort_times_indices.ndim == 1:\n            valid_sort_times_indices = np.expand_dims(\n                valid_sort_times_indices, 0\n            )\n\n        # create an AppendRecording if there is more than one disjoint sort interval\n        if len(valid_sort_times_indices) &gt; 1:\n            recordings_list = []\n            for interval_indices in valid_sort_times_indices:\n                recording_single = recording.frame_slice(\n                    start_frame=interval_indices[0],\n                    end_frame=interval_indices[1],\n                )\n                recordings_list.append(recording_single)\n            recording = si.append_recordings(recordings_list)\n        else:\n            recording = recording.frame_slice(\n                start_frame=valid_sort_times_indices[0][0],\n                end_frame=valid_sort_times_indices[0][1],\n            )\n\n        channel_ids = (\n            SortGroup.SortGroupElectrode\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch(\"electrode_id\")\n        ref_channel_id = (\n            SortGroup\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch1(\"sort_reference_electrode_id\")\n        channel_ids = np.setdiff1d(channel_ids, ref_channel_id)\n\n        # include ref channel in first slice, then exclude it in second slice\n        if ref_channel_id &gt;= 0:\n            channel_ids_ref = np.append(channel_ids, ref_channel_id)\n            recording = recording.channel_slice(channel_ids=channel_ids_ref)\n\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"single\", ref_channel_ids=ref_channel_id\n            )\n            recording = recording.channel_slice(channel_ids=channel_ids)\n        elif ref_channel_id == -2:\n            recording = recording.channel_slice(channel_ids=channel_ids)\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"global\", operator=\"median\"\n            )\n        else:\n            raise ValueError(\"Invalid reference channel ID\")\n        filter_params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        recording = si.preprocessing.bandpass_filter(\n            recording,\n            freq_min=filter_params[\"frequency_min\"],\n            freq_max=filter_params[\"frequency_max\"],\n        )\n\n        # if the sort group is a tetrode, change the channel location\n        # note that this is a workaround that would be deprecated when spikeinterface uses 3D probe locations\n        probe_type = []\n        electrode_group = []\n        for channel_id in channel_ids:\n            probe_type.append(\n                (\n                    Electrode * Probe\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"probe_type\")\n            )\n            electrode_group.append(\n                (\n                    Electrode\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"electrode_group_name\")\n            )\n        if (\n            all(p == \"tetrode_12.5\" for p in probe_type)\n            and len(probe_type) == 4\n            and all(eg == electrode_group[0] for eg in electrode_group)\n        ):\n            tetrode = pi.Probe(ndim=2)\n            position = [[0, 0], [0, 12.5], [12.5, 0], [12.5, 12.5]]\n            tetrode.set_contacts(\n                position, shapes=\"circle\", shape_params={\"radius\": 6.25}\n            )\n            tetrode.set_contact_ids(channel_ids)\n            tetrode.set_device_channel_indices(np.arange(4))\n            recording = recording.set_probe(tetrode, in_place=True)\n\n        return recording\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.MetricParameters", "title": "<code>MetricParameters</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass MetricParameters(dj.Manual):\n    definition = \"\"\"\n    # Parameters for computing quality metrics of sorted units\n    metric_params_name: varchar(200)\n    ---\n    metric_params: blob\n    \"\"\"\n    metric_default_params = {\n        \"snr\": {\n            \"peak_sign\": \"neg\",\n            \"random_chunk_kwargs_dict\": {\n                \"num_chunks_per_segment\": 20,\n                \"chunk_size\": 10000,\n                \"seed\": 0,\n            },\n        },\n        \"isi_violation\": {\"isi_threshold_ms\": 1.5, \"min_isi_ms\": 0.0},\n        \"nn_isolation\": {\n            \"max_spikes\": 1000,\n            \"min_spikes\": 10,\n            \"n_neighbors\": 5,\n            \"n_components\": 7,\n            \"radius_um\": 100,\n            \"seed\": 0,\n        },\n        \"nn_noise_overlap\": {\n            \"max_spikes\": 1000,\n            \"min_spikes\": 10,\n            \"n_neighbors\": 5,\n            \"n_components\": 7,\n            \"radius_um\": 100,\n            \"seed\": 0,\n        },\n        \"peak_channel\": {\"peak_sign\": \"neg\"},\n        \"num_spikes\": {},\n    }\n    # Example of peak_offset parameters 'peak_offset': {'peak_sign': 'neg'}\n    available_metrics = [\n        \"snr\",\n        \"isi_violation\",\n        \"nn_isolation\",\n        \"nn_noise_overlap\",\n        \"peak_offset\",\n        \"peak_channel\",\n        \"num_spikes\",\n    ]\n\n    def get_metric_default_params(self, metric: str):\n        \"Returns default params for the given metric\"\n        return self.metric_default_params(metric)\n\n    def insert_default(self):\n        self.insert1(\n            [\"franklab_default3\", self.metric_default_params],\n            skip_duplicates=True,\n        )\n\n    def get_available_metrics(self):\n        for metric in _metric_name_to_func:\n            if metric in self.available_metrics:\n                metric_doc = _metric_name_to_func[metric].__doc__.split(\"\\n\")[0]\n                metric_string = (\"{metric_name} : {metric_doc}\").format(\n                    metric_name=metric, metric_doc=metric_doc\n                )\n                print(metric_string + \"\\n\")\n\n    # TODO\n    def _validate_metrics_list(self, key):\n\"\"\"Checks whether a row to be inserted contains only the available metrics\"\"\"\n        # get available metrics list\n        # get metric list from key\n        # compare\n        return NotImplementedError\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.MetricParameters.get_metric_default_params", "title": "<code>get_metric_default_params(metric)</code>", "text": "<p>Returns default params for the given metric</p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>def get_metric_default_params(self, metric: str):\n    \"Returns default params for the given metric\"\n    return self.metric_default_params(metric)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.AutomaticCuration", "title": "<code>AutomaticCuration</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass AutomaticCuration(dj.Computed):\n    definition = \"\"\"\n    -&gt; AutomaticCurationSelection\n    ---\n    auto_curation_key: blob # the key to the curation inserted by make\n    \"\"\"\n\n    def make(self, key):\n        metrics_path = (QualityMetrics &amp; key).fetch1(\"quality_metrics_path\")\n        with open(metrics_path) as f:\n            quality_metrics = json.load(f)\n\n        # get the curation information and the curated sorting\n        parent_curation = (Curation &amp; key).fetch(as_dict=True)[0]\n        parent_merge_groups = parent_curation[\"merge_groups\"]\n        parent_labels = parent_curation[\"curation_labels\"]\n        parent_curation_id = parent_curation[\"curation_id\"]\n        parent_sorting = Curation.get_curated_sorting(key)\n\n        merge_params = (AutomaticCurationParameters &amp; key).fetch1(\n            \"merge_params\"\n        )\n        merge_groups, units_merged = self.get_merge_groups(\n            parent_sorting, parent_merge_groups, quality_metrics, merge_params\n        )\n\n        label_params = (AutomaticCurationParameters &amp; key).fetch1(\n            \"label_params\"\n        )\n        labels = self.get_labels(\n            parent_sorting, parent_labels, quality_metrics, label_params\n        )\n\n        # keep the quality metrics only if no merging occurred.\n        metrics = quality_metrics if not units_merged else None\n\n        # insert this sorting into the CuratedSpikeSorting Table\n        # first remove keys that aren't part of the Sorting (the primary key of curation)\n        c_key = (SpikeSorting &amp; key).fetch(\"KEY\")[0]\n        curation_key = {item: key[item] for item in key if item in c_key}\n        key[\"auto_curation_key\"] = Curation.insert_curation(\n            curation_key,\n            parent_curation_id=parent_curation_id,\n            labels=labels,\n            merge_groups=merge_groups,\n            metrics=metrics,\n            description=\"auto curated\",\n        )\n\n        self.insert1(key)\n\n    @staticmethod\n    def get_merge_groups(\n        sorting, parent_merge_groups, quality_metrics, merge_params\n    ):\n\"\"\"Identifies units to be merged based on the quality_metrics and\n        merge parameters and returns an updated list of merges for the curation.\n\n        Parameters\n        ---------\n        sorting : spikeinterface.sorting\n        parent_merge_groups : list\n            Information about previous merges\n        quality_metrics : list\n        merge_params : dict\n\n        Returns\n        -------\n        merge_groups : list of lists\n        merge_occurred : bool\n\n        \"\"\"\n\n        # overview:\n        # 1. Use quality metrics to determine merge groups for units\n        # 2. Combine merge groups with current merge groups to produce union of merges\n\n        if not merge_params:\n            return parent_merge_groups, False\n        else:\n            # TODO: use the metrics to identify clusters that should be merged\n            # new_merges should then reflect those merges and the line below should be deleted.\n            new_merges = []\n            # append these merges to the parent merge_groups\n            for new_merge in new_merges:\n                # check to see if the first cluster listed is in a current merge group\n                for previous_merge in parent_merge_groups:\n                    if new_merge[0] == previous_merge[0]:\n                        # add the additional units in new_merge to the identified merge group.\n                        previous_merge.extend(new_merge[1:])\n                        previous_merge.sort()\n                        break\n                else:\n                    # append this merge group to the list if no previous merge\n                    parent_merge_groups.append(new_merge)\n            return parent_merge_groups.sort(), True\n\n    @staticmethod\n    def get_labels(sorting, parent_labels, quality_metrics, label_params):\n\"\"\"Returns a dictionary of labels using quality_metrics and label\n        parameters.\n\n        Parameters\n        ---------\n        sorting : spikeinterface.sorting\n        parent_labels : list\n            Information about previous merges\n        quality_metrics : list\n        label_params : dict\n\n        Returns\n        -------\n        parent_labels : list\n\n        \"\"\"\n        # overview:\n        # 1. Use quality metrics to determine labels for units\n        # 2. Append labels to current labels, checking for inconsistencies\n        if not label_params:\n            return parent_labels\n        else:\n            for metric in label_params:\n                if metric not in quality_metrics:\n                    Warning(f\"{metric} not found in quality metrics; skipping\")\n                else:\n                    compare = _comparison_to_function[label_params[metric][0]]\n\n                    for unit_id in quality_metrics[metric].keys():\n                        # compare the quality metric to the threshold with the specified operator\n                        # note that label_params[metric] is a three element list with a comparison operator as a string,\n                        # the threshold value, and a list of labels to be applied if the comparison is true\n                        if compare(\n                            quality_metrics[metric][unit_id],\n                            label_params[metric][1],\n                        ):\n                            if unit_id not in parent_labels:\n                                parent_labels[unit_id] = label_params[metric][2]\n                            # check if the label is already there, and if not, add it\n                            elif (\n                                label_params[metric][2]\n                                not in parent_labels[unit_id]\n                            ):\n                                parent_labels[unit_id].extend(\n                                    label_params[metric][2]\n                                )\n            return parent_labels\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.AutomaticCuration.get_merge_groups", "title": "<code>get_merge_groups(sorting, parent_merge_groups, quality_metrics, merge_params)</code>  <code>staticmethod</code>", "text": "<p>Identifies units to be merged based on the quality_metrics and merge parameters and returns an updated list of merges for the curation.</p> <p>Parameters:</p> Name Type Description Default <code>sorting</code> <code>spikeinterface.sorting</code> required <code>parent_merge_groups</code> <code>list</code> <p>Information about previous merges</p> required <code>quality_metrics</code> <code>list</code> required <code>merge_params</code> <code>dict</code> required <p>Returns:</p> Name Type Description <code>merge_groups</code> <code>list of lists</code> <code>merge_occurred</code> <code>bool</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef get_merge_groups(\n    sorting, parent_merge_groups, quality_metrics, merge_params\n):\n\"\"\"Identifies units to be merged based on the quality_metrics and\n    merge parameters and returns an updated list of merges for the curation.\n\n    Parameters\n    ---------\n    sorting : spikeinterface.sorting\n    parent_merge_groups : list\n        Information about previous merges\n    quality_metrics : list\n    merge_params : dict\n\n    Returns\n    -------\n    merge_groups : list of lists\n    merge_occurred : bool\n\n    \"\"\"\n\n    # overview:\n    # 1. Use quality metrics to determine merge groups for units\n    # 2. Combine merge groups with current merge groups to produce union of merges\n\n    if not merge_params:\n        return parent_merge_groups, False\n    else:\n        # TODO: use the metrics to identify clusters that should be merged\n        # new_merges should then reflect those merges and the line below should be deleted.\n        new_merges = []\n        # append these merges to the parent merge_groups\n        for new_merge in new_merges:\n            # check to see if the first cluster listed is in a current merge group\n            for previous_merge in parent_merge_groups:\n                if new_merge[0] == previous_merge[0]:\n                    # add the additional units in new_merge to the identified merge group.\n                    previous_merge.extend(new_merge[1:])\n                    previous_merge.sort()\n                    break\n            else:\n                # append this merge group to the list if no previous merge\n                parent_merge_groups.append(new_merge)\n        return parent_merge_groups.sort(), True\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.AutomaticCuration.get_labels", "title": "<code>get_labels(sorting, parent_labels, quality_metrics, label_params)</code>  <code>staticmethod</code>", "text": "<p>Returns a dictionary of labels using quality_metrics and label parameters.</p> <p>Parameters:</p> Name Type Description Default <code>sorting</code> <code>spikeinterface.sorting</code> required <code>parent_labels</code> <code>list</code> <p>Information about previous merges</p> required <code>quality_metrics</code> <code>list</code> required <code>label_params</code> <code>dict</code> required <p>Returns:</p> Name Type Description <code>parent_labels</code> <code>list</code> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@staticmethod\ndef get_labels(sorting, parent_labels, quality_metrics, label_params):\n\"\"\"Returns a dictionary of labels using quality_metrics and label\n    parameters.\n\n    Parameters\n    ---------\n    sorting : spikeinterface.sorting\n    parent_labels : list\n        Information about previous merges\n    quality_metrics : list\n    label_params : dict\n\n    Returns\n    -------\n    parent_labels : list\n\n    \"\"\"\n    # overview:\n    # 1. Use quality metrics to determine labels for units\n    # 2. Append labels to current labels, checking for inconsistencies\n    if not label_params:\n        return parent_labels\n    else:\n        for metric in label_params:\n            if metric not in quality_metrics:\n                Warning(f\"{metric} not found in quality metrics; skipping\")\n            else:\n                compare = _comparison_to_function[label_params[metric][0]]\n\n                for unit_id in quality_metrics[metric].keys():\n                    # compare the quality metric to the threshold with the specified operator\n                    # note that label_params[metric] is a three element list with a comparison operator as a string,\n                    # the threshold value, and a list of labels to be applied if the comparison is true\n                    if compare(\n                        quality_metrics[metric][unit_id],\n                        label_params[metric][1],\n                    ):\n                        if unit_id not in parent_labels:\n                            parent_labels[unit_id] = label_params[metric][2]\n                        # check if the label is already there, and if not, add it\n                        elif (\n                            label_params[metric][2]\n                            not in parent_labels[unit_id]\n                        ):\n                            parent_labels[unit_id].extend(\n                                label_params[metric][2]\n                            )\n        return parent_labels\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.CuratedSpikeSorting", "title": "<code>CuratedSpikeSorting</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass CuratedSpikeSorting(dj.Computed):\n    definition = \"\"\"\n    -&gt; CuratedSpikeSortingSelection\n    ---\n    -&gt; AnalysisNwbfile\n    units_object_id: varchar(40)\n    \"\"\"\n\n    class Unit(dj.Part):\n        definition = \"\"\"\n        # Table for holding sorted units\n        -&gt; CuratedSpikeSorting\n        unit_id: int   # ID for each unit\n        ---\n        label='': varchar(200)   # optional set of labels for each unit\n        nn_noise_overlap=-1: float   # noise overlap metric for each unit\n        nn_isolation=-1: float   # isolation score metric for each unit\n        isi_violation=-1: float   # ISI violation score for each unit\n        snr=0: float            # SNR for each unit\n        firing_rate=-1: float   # firing rate\n        num_spikes=-1: int   # total number of spikes\n        peak_channel=null: int # channel of maximum amplitude for each unit\n        \"\"\"\n\n    def make(self, key):\n        unit_labels_to_remove = [\"reject\"]\n        # check that the Curation has metrics\n        metrics = (Curation &amp; key).fetch1(\"quality_metrics\")\n        if metrics == {}:\n            Warning(\n                f\"Metrics for Curation {key} should normally be calculated before insertion here\"\n            )\n\n        sorting = Curation.get_curated_sorting(key)\n        unit_ids = sorting.get_unit_ids()\n        # Get the labels for the units, add only those units that do not have 'reject' or 'noise' labels\n        unit_labels = (Curation &amp; key).fetch1(\"curation_labels\")\n        accepted_units = []\n        for unit_id in unit_ids:\n            if unit_id in unit_labels:\n                if (\n                    len(set(unit_labels_to_remove) &amp; set(unit_labels[unit_id]))\n                    == 0\n                ):\n                    accepted_units.append(unit_id)\n            else:\n                accepted_units.append(unit_id)\n\n        # get the labels for the accepted units\n        labels = {}\n        for unit_id in accepted_units:\n            if unit_id in unit_labels:\n                labels[unit_id] = \",\".join(unit_labels[unit_id])\n\n        # convert unit_ids in metrics to integers, including only accepted units.\n        #  TODO: convert to int this somewhere else\n        final_metrics = {}\n        for metric in metrics:\n            final_metrics[metric] = {\n                int(unit_id): metrics[metric][unit_id]\n                for unit_id in metrics[metric]\n                if int(unit_id) in accepted_units\n            }\n\n        print(f\"Found {len(accepted_units)} accepted units\")\n\n        # get the sorting and save it in the NWB file\n        sorting = Curation.get_curated_sorting(key)\n        recording = Curation.get_recording(key)\n\n        # get the sort_interval and sorting interval list\n        sort_interval_name = (SpikeSortingRecording &amp; key).fetch1(\n            \"sort_interval_name\"\n        )\n        sort_interval = (SortInterval &amp; key).fetch1(\"sort_interval\")\n        sort_interval_list_name = (SpikeSorting &amp; key).fetch1(\n            \"artifact_removed_interval_list_name\"\n        )\n\n        timestamps = SpikeSortingRecording._get_recording_timestamps(recording)\n\n        (\n            key[\"analysis_file_name\"],\n            key[\"units_object_id\"],\n        ) = Curation().save_sorting_nwb(\n            key,\n            sorting,\n            timestamps,\n            sort_interval_list_name,\n            sort_interval,\n            metrics=final_metrics,\n            unit_ids=accepted_units,\n            labels=labels,\n        )\n        self.insert1(key)\n\n        # now add the units\n        # Remove the non primary key entries.\n        del key[\"units_object_id\"]\n        del key[\"analysis_file_name\"]\n\n        metric_fields = self.metrics_fields()\n        for unit_id in accepted_units:\n            key[\"unit_id\"] = unit_id\n            if unit_id in labels:\n                key[\"label\"] = labels[unit_id]\n            for field in metric_fields:\n                if field in final_metrics:\n                    key[field] = final_metrics[field][unit_id]\n                else:\n                    Warning(\n                        f\"No metric named {field} in computed unit quality metrics; skipping\"\n                    )\n            CuratedSpikeSorting.Unit.insert1(key)\n\n    def metrics_fields(self):\n\"\"\"Returns a list of the metrics that are currently in the Units table.\"\"\"\n        unit_info = self.Unit().fetch(limit=1, format=\"frame\")\n        unit_fields = [column for column in unit_info.columns]\n        unit_fields.remove(\"label\")\n        return unit_fields\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        return fetch_nwb(\n            self, (AnalysisNwbfile, \"analysis_file_abs_path\"), *attrs, **kwargs\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.CuratedSpikeSorting.metrics_fields", "title": "<code>metrics_fields()</code>", "text": "<p>Returns a list of the metrics that are currently in the Units table.</p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>def metrics_fields(self):\n\"\"\"Returns a list of the metrics that are currently in the Units table.\"\"\"\n    unit_info = self.Unit().fetch(limit=1, format=\"frame\")\n    unit_fields = [column for column in unit_info.columns]\n    unit_fields.remove(\"label\")\n    return unit_fields\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.UnitInclusionParameters", "title": "<code>UnitInclusionParameters</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>@schema\nclass UnitInclusionParameters(dj.Manual):\n    definition = \"\"\"\n    unit_inclusion_param_name: varchar(80) # the name of the list of thresholds for unit inclusion\n    ---\n    inclusion_param_dict: blob # the dictionary of inclusion / exclusion parameters\n    \"\"\"\n\n    def insert1(self, key, **kwargs):\n        # check to see that the dictionary fits the specifications\n        # The inclusion parameter dict has the following form:\n        # param_dict['metric_name'] = (operator, value)\n        #    where operator is '&lt;', '&gt;', &lt;=', '&gt;=', or '==' and value is the comparison (float) value to be used ()\n        # param_dict['exclude_labels'] = [list of labels to exclude]\n        pdict = key[\"inclusion_param_dict\"]\n        metrics_list = CuratedSpikeSorting().metrics_fields()\n\n        for k in pdict:\n            if k not in metrics_list and k != \"exclude_labels\":\n                raise Exception(\n                    f\"key {k} is not a valid element of the inclusion_param_dict\"\n                )\n            if k in metrics_list:\n                if pdict[k][0] not in _comparison_to_function:\n                    raise Exception(\n                        f\"operator {pdict[k][0]} for metric {k} is not in the valid operators list: {_comparison_to_function.keys()}\"\n                    )\n            if k == \"exclude_labels\":\n                for label in pdict[k]:\n                    if label not in valid_labels:\n                        raise Exception(\n                            f\"exclude label {label} is not in the valid_labels list: {valid_labels}\"\n                        )\n        super().insert1(key, **kwargs)\n\n    def get_included_units(\n        self, curated_sorting_key, unit_inclusion_param_name\n    ):\n\"\"\"given a reference to a set of curated sorting units and the name of a unit inclusion parameter list, returns\n\n        Parameters\n        ----------\n        curated_sorting_key : dict\n            key to select a set of curated sorting\n        unit_inclusion_param_name : str\n            name of a unit inclusion parameter entry\n\n        Returns\n        ------unit key\n        dict\n            key to select all of the included units\n        \"\"\"\n        curated_sortings = (CuratedSpikeSorting() &amp; curated_sorting_key).fetch()\n        inc_param_dict = (\n            UnitInclusionParameters\n            &amp; {\"unit_inclusion_param_name\": unit_inclusion_param_name}\n        ).fetch1(\"inclusion_param_dict\")\n        units = (CuratedSpikeSorting().Unit() &amp; curated_sortings).fetch()\n        units_key = (CuratedSpikeSorting().Unit() &amp; curated_sortings).fetch(\n            \"KEY\"\n        )\n        # get a list of the metrics in the units table\n        metrics_list = CuratedSpikeSorting().metrics_fields()\n        # get the list of labels to exclude if there is one\n        if \"exclude_labels\" in inc_param_dict:\n            exclude_labels = inc_param_dict[\"exclude_labels\"]\n            del inc_param_dict[\"exclude_labels\"]\n        else:\n            exclude_labels = []\n\n        # create a list of the units to kepp.\n        keep = np.asarray([True] * len(units))\n        for metric in inc_param_dict:\n            # for all units, go through each metric, compare it to the value specified, and update the list to be kept\n            keep = np.logical_and(\n                keep,\n                _comparison_to_function[inc_param_dict[metric][0]](\n                    units[metric], inc_param_dict[metric][1]\n                ),\n            )\n\n        # now exclude by label if it is specified\n        if len(exclude_labels):\n            included_units = []\n            for unit_ind in np.ravel(np.argwhere(keep)):\n                labels = units[unit_ind][\"label\"].split(\",\")\n                exclude = False\n                for label in labels:\n                    if label in exclude_labels:\n                        keep[unit_ind] = False\n                        break\n        # return units that passed all of the tests\n        # TODO: Make this more efficient\n        return {i: units_key[i] for i in np.ravel(np.argwhere(keep))}\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_curation/#src.spyglass.spikesorting.spikesorting_curation.UnitInclusionParameters.get_included_units", "title": "<code>get_included_units(curated_sorting_key, unit_inclusion_param_name)</code>", "text": "<p>given a reference to a set of curated sorting units and the name of a unit inclusion parameter list, returns</p> <p>Parameters:</p> Name Type Description Default <code>curated_sorting_key</code> <code>dict</code> <p>key to select a set of curated sorting</p> required <code>unit_inclusion_param_name</code> <code>str</code> <p>name of a unit inclusion parameter entry</p> required <p>Returns ------unit key dict     key to select all of the included units</p> Source code in <code>src/spyglass/spikesorting/spikesorting_curation.py</code> <pre><code>def get_included_units(\n    self, curated_sorting_key, unit_inclusion_param_name\n):\n\"\"\"given a reference to a set of curated sorting units and the name of a unit inclusion parameter list, returns\n\n    Parameters\n    ----------\n    curated_sorting_key : dict\n        key to select a set of curated sorting\n    unit_inclusion_param_name : str\n        name of a unit inclusion parameter entry\n\n    Returns\n    ------unit key\n    dict\n        key to select all of the included units\n    \"\"\"\n    curated_sortings = (CuratedSpikeSorting() &amp; curated_sorting_key).fetch()\n    inc_param_dict = (\n        UnitInclusionParameters\n        &amp; {\"unit_inclusion_param_name\": unit_inclusion_param_name}\n    ).fetch1(\"inclusion_param_dict\")\n    units = (CuratedSpikeSorting().Unit() &amp; curated_sortings).fetch()\n    units_key = (CuratedSpikeSorting().Unit() &amp; curated_sortings).fetch(\n        \"KEY\"\n    )\n    # get a list of the metrics in the units table\n    metrics_list = CuratedSpikeSorting().metrics_fields()\n    # get the list of labels to exclude if there is one\n    if \"exclude_labels\" in inc_param_dict:\n        exclude_labels = inc_param_dict[\"exclude_labels\"]\n        del inc_param_dict[\"exclude_labels\"]\n    else:\n        exclude_labels = []\n\n    # create a list of the units to kepp.\n    keep = np.asarray([True] * len(units))\n    for metric in inc_param_dict:\n        # for all units, go through each metric, compare it to the value specified, and update the list to be kept\n        keep = np.logical_and(\n            keep,\n            _comparison_to_function[inc_param_dict[metric][0]](\n                units[metric], inc_param_dict[metric][1]\n            ),\n        )\n\n    # now exclude by label if it is specified\n    if len(exclude_labels):\n        included_units = []\n        for unit_ind in np.ravel(np.argwhere(keep)):\n            labels = units[unit_ind][\"label\"].split(\",\")\n            exclude = False\n            for label in labels:\n                if label in exclude_labels:\n                    keep[unit_ind] = False\n                    break\n    # return units that passed all of the tests\n    # TODO: Make this more efficient\n    return {i: units_key[i] for i in np.ravel(np.argwhere(keep))}\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/", "title": "spikesorting_recording.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.dj_replace", "title": "<code>dj_replace(original_table, new_values, key_column, replace_column)</code>", "text": "<p>Given the output of a fetch() call from a schema and a 2D array made up of (key_value, replace_value) tuples, find each instance of key_value in the key_column of the original table and replace the specified replace_column with the associated replace_value. Key values must be unique.</p> <p>Parameters:</p> Name Type Description Default <code>original_table</code> <p>Result of a datajoint .fetch() call on a schema query.</p> required <code>new_values</code> <code>list</code> <p>List of tuples, each containing (key_value, replace_value).</p> required <code>replace_column</code> <code>str</code> <p>The name of the column where to-be-replaced values are located.</p> required <p>Returns:</p> Type Description <code>original_table</code> <p>Structured array of new table entries that can be inserted back into the schema</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def dj_replace(original_table, new_values, key_column, replace_column):\n\"\"\"Given the output of a fetch() call from a schema and a 2D array made up\n    of (key_value, replace_value) tuples, find each instance of key_value in\n    the key_column of the original table and replace the specified\n    replace_column with the associated replace_value. Key values must be\n    unique.\n\n    Parameters\n    ----------\n    original_table\n        Result of a datajoint .fetch() call on a schema query.\n    new_values : list\n        List of tuples, each containing (key_value, replace_value).\n    replace_column : str\n        The name of the column where to-be-replaced values are located.\n\n    Returns\n    -------\n    original_table\n        Structured array of new table entries that can be inserted back into the schema\n    \"\"\"\n\n    # check to make sure the new_values are a list or array of tuples and fix if not\n    if isinstance(new_values, tuple):\n        tmp = list()\n        tmp.append(new_values)\n        new_values = tmp\n\n    new_val_array = np.asarray(new_values)\n    replace_ind = np.where(\n        np.isin(original_table[key_column], new_val_array[:, 0])\n    )\n    original_table[replace_column][replace_ind] = new_val_array[:, 1]\n    return original_table\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.IntervalList", "title": "<code>IntervalList</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@schema\nclass IntervalList(dj.Manual):\n    definition = \"\"\"\n    # Time intervals used for analysis\n    -&gt; Session\n    interval_list_name: varchar(200)  # descriptive name of this interval list\n    ---\n    valid_times: longblob  # numpy array with start and end times for each interval\n    \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n        The interval list name for each epoch is set to the first tag for the epoch.\n        If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n        (0-indexed) of the epoch in the epochs table.\n        The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n        [start time, stop time] for each epoch.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        nwb_file_name : str\n            The file name of the NWB file, used as a primary key to the Session table.\n        \"\"\"\n        if nwbf.epochs is None:\n            print(\"No epochs found in NWB file.\")\n            return\n        epochs = nwbf.epochs.to_dataframe()\n        for epoch_index, epoch_data in epochs.iterrows():\n            epoch_dict = dict()\n            epoch_dict[\"nwb_file_name\"] = nwb_file_name\n            if epoch_data.tags[0]:\n                epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n            else:\n                epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                    epoch_index\n                )\n            epoch_dict[\"valid_times\"] = np.asarray(\n                [[epoch_data.start_time, epoch_data.stop_time]]\n            )\n            cls.insert1(epoch_dict, skip_duplicates=True)\n\n    def plot_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=figsize)\n        interval_count = 0\n        for row in interval_list.itertuples(index=False):\n            for interval in row.valid_times:\n                ax.plot(interval, [interval_count, interval_count])\n                ax.scatter(\n                    interval,\n                    [interval_count, interval_count],\n                    alpha=0.8,\n                    zorder=2,\n                )\n            interval_count += 1\n        ax.set_yticks(np.arange(interval_list.shape[0]))\n        ax.set_yticklabels(interval_list.interval_list_name)\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n\n    def plot_epoch_pos_raw_intervals(self, figsize=(20, 5)):\n        interval_list = pd.DataFrame(self)\n        fig, ax = plt.subplots(figsize=(30, 3))\n\n        raw_data_valid_times = interval_list.loc[\n            interval_list.interval_list_name == \"raw data valid times\"\n        ].valid_times\n        interval_y = 1\n\n        for interval in np.asarray(raw_data_valid_times)[0]:\n            ax.plot(interval, [interval_y, interval_y])\n            ax.scatter(interval, [interval_y, interval_y], alpha=0.8, zorder=2)\n\n        epoch_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^[0-9]\", axis=0)\n            .valid_times\n        )\n        interval_y = 2\n        for epoch, valid_times in zip(\n            epoch_valid_times.index, epoch_valid_times\n        ):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch,\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        pos_valid_times = (\n            interval_list.set_index(\"interval_list_name\")\n            .filter(regex=r\"^pos \\d+ valid times$\", axis=0)\n            .valid_times\n        ).sort_index(key=lambda index: [int(name.split()[1]) for name in index])\n        interval_y = 0\n        for epoch, valid_times in zip(pos_valid_times.index, pos_valid_times):\n            for interval in valid_times:\n                ax.plot(interval, [interval_y, interval_y])\n                ax.scatter(\n                    interval, [interval_y, interval_y], alpha=0.8, zorder=2\n                )\n                ax.text(\n                    interval[0] + np.diff(interval)[0] / 2,\n                    interval_y,\n                    epoch.strip(\" valid times\"),\n                    ha=\"center\",\n                    va=\"bottom\",\n                )\n\n        ax.set_ylim((-0.25, 2.25))\n        ax.set_yticks(np.arange(3))\n        ax.set_yticklabels([\"pos valid times\", \"raw data valid times\", \"epoch\"])\n        ax.set_xlabel(\"Time [s]\")\n        ax.grid(True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.common.common_interval.IntervalList.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, *, nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Add each entry in the NWB file epochs table to the IntervalList table.</p> <p>The interval list name for each epoch is set to the first tag for the epoch. If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index (0-indexed) of the epoch in the epochs table. The start time and stop time of the epoch are stored in the valid_times field as a numpy array of [start time, stop time] for each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>nwb_file_name</code> <code>str</code> <p>The file name of the NWB file, used as a primary key to the Session table.</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, *, nwb_file_name):\n\"\"\"Add each entry in the NWB file epochs table to the IntervalList table.\n\n    The interval list name for each epoch is set to the first tag for the epoch.\n    If the epoch has no tags, then 'interval_x' will be used as the interval list name, where x is the index\n    (0-indexed) of the epoch in the epochs table.\n    The start time and stop time of the epoch are stored in the valid_times field as a numpy array of\n    [start time, stop time] for each epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    nwb_file_name : str\n        The file name of the NWB file, used as a primary key to the Session table.\n    \"\"\"\n    if nwbf.epochs is None:\n        print(\"No epochs found in NWB file.\")\n        return\n    epochs = nwbf.epochs.to_dataframe()\n    for epoch_index, epoch_data in epochs.iterrows():\n        epoch_dict = dict()\n        epoch_dict[\"nwb_file_name\"] = nwb_file_name\n        if epoch_data.tags[0]:\n            epoch_dict[\"interval_list_name\"] = epoch_data.tags[0]\n        else:\n            epoch_dict[\"interval_list_name\"] = \"interval_\" + str(\n                epoch_index\n            )\n        epoch_dict[\"valid_times\"] = np.asarray(\n            [[epoch_data.start_time, epoch_data.stop_time]]\n        )\n        cls.insert1(epoch_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.SortGroup", "title": "<code>SortGroup</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>@schema\nclass SortGroup(dj.Manual):\n    definition = \"\"\"\n    # Set of electrodes that will be sorted together\n    -&gt; Session\n    sort_group_id: int  # identifier for a group of electrodes\n    ---\n    sort_reference_electrode_id = -1: int  # the electrode to use for reference. -1: no reference, -2: common median\n    \"\"\"\n\n    class SortGroupElectrode(dj.Part):\n        definition = \"\"\"\n        -&gt; SortGroup\n        -&gt; Electrode\n        \"\"\"\n\n    def set_group_by_shank(\n        self,\n        nwb_file_name: str,\n        references: dict = None,\n        omit_ref_electrode_group=False,\n        omit_unitrode=True,\n    ):\n\"\"\"Divides electrodes into groups based on their shank position.\n\n        * Electrodes from probes with 1 shank (e.g. tetrodes) are placed in a\n          single group\n        * Electrodes from probes with multiple shanks (e.g. polymer probes) are\n          placed in one group per shank\n        * Bad channels are omitted\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            the name of the NWB file whose electrodes should be put into sorting groups\n        references : dict, optional\n            If passed, used to set references. Otherwise, references set using\n            original reference electrodes from config. Keys: electrode groups. Values: reference electrode.\n        omit_ref_electrode_group : bool\n            Optional. If True, no sort group is defined for electrode group of reference.\n        omit_unitrode : bool\n            Optional. If True, no sort groups are defined for unitrodes.\n        \"\"\"\n        # delete any current groups\n        (SortGroup &amp; {\"nwb_file_name\": nwb_file_name}).delete()\n        # get the electrodes from this NWB file\n        electrodes = (\n            Electrode()\n            &amp; {\"nwb_file_name\": nwb_file_name}\n            &amp; {\"bad_channel\": \"False\"}\n        ).fetch()\n        e_groups = list(np.unique(electrodes[\"electrode_group_name\"]))\n        e_groups.sort(key=int)  # sort electrode groups numerically\n        sort_group = 0\n        sg_key = dict()\n        sge_key = dict()\n        sg_key[\"nwb_file_name\"] = sge_key[\"nwb_file_name\"] = nwb_file_name\n        for e_group in e_groups:\n            # for each electrode group, get a list of the unique shank numbers\n            shank_list = np.unique(\n                electrodes[\"probe_shank\"][\n                    electrodes[\"electrode_group_name\"] == e_group\n                ]\n            )\n            sge_key[\"electrode_group_name\"] = e_group\n            # get the indices of all electrodes in this group / shank and set their sorting group\n            for shank in shank_list:\n                sg_key[\"sort_group_id\"] = sge_key[\"sort_group_id\"] = sort_group\n                # specify reference electrode. Use 'references' if passed, otherwise use reference from config\n                if not references:\n                    shank_elect_ref = electrodes[\n                        \"original_reference_electrode\"\n                    ][\n                        np.logical_and(\n                            electrodes[\"electrode_group_name\"] == e_group,\n                            electrodes[\"probe_shank\"] == shank,\n                        )\n                    ]\n                    if np.max(shank_elect_ref) == np.min(shank_elect_ref):\n                        sg_key[\"sort_reference_electrode_id\"] = shank_elect_ref[\n                            0\n                        ]\n                    else:\n                        ValueError(\n                            f\"Error in electrode group {e_group}: reference electrodes are not all the same\"\n                        )\n                else:\n                    if e_group not in references.keys():\n                        raise Exception(\n                            f\"electrode group {e_group} not a key in references, so cannot set reference\"\n                        )\n                    else:\n                        sg_key[\"sort_reference_electrode_id\"] = references[\n                            e_group\n                        ]\n                # Insert sort group and sort group electrodes\n                reference_electrode_group = electrodes[\n                    electrodes[\"electrode_id\"]\n                    == sg_key[\"sort_reference_electrode_id\"]\n                ][\n                    \"electrode_group_name\"\n                ]  # reference for this electrode group\n                if (\n                    len(reference_electrode_group) == 1\n                ):  # unpack single reference\n                    reference_electrode_group = reference_electrode_group[0]\n                elif (int(sg_key[\"sort_reference_electrode_id\"]) &gt; 0) and (\n                    len(reference_electrode_group) != 1\n                ):\n                    raise Exception(\n                        f\"Should have found exactly one electrode group for reference electrode,\"\n                        f\"but found {len(reference_electrode_group)}.\"\n                    )\n                if omit_ref_electrode_group and (\n                    str(e_group) == str(reference_electrode_group)\n                ):\n                    print(\n                        f\"Omitting electrode group {e_group} from sort groups because contains reference.\"\n                    )\n                    continue\n                shank_elect = electrodes[\"electrode_id\"][\n                    np.logical_and(\n                        electrodes[\"electrode_group_name\"] == e_group,\n                        electrodes[\"probe_shank\"] == shank,\n                    )\n                ]\n                if (\n                    omit_unitrode and len(shank_elect) == 1\n                ):  # omit unitrodes if indicated\n                    print(\n                        f\"Omitting electrode group {e_group}, shank {shank} from sort groups because unitrode.\"\n                    )\n                    continue\n                self.insert1(sg_key)\n                for elect in shank_elect:\n                    sge_key[\"electrode_id\"] = elect\n                    self.SortGroupElectrode().insert1(sge_key)\n                sort_group += 1\n\n    def set_group_by_electrode_group(self, nwb_file_name: str):\n\"\"\"Assign groups to all non-bad channel electrodes based on their electrode group\n        and sets the reference for each group to the reference for the first channel of the group.\n\n        Parameters\n        ----------\n        nwb_file_name: str\n            the name of the nwb whose electrodes should be put into sorting groups\n        \"\"\"\n        # delete any current groups\n        (SortGroup &amp; {\"nwb_file_name\": nwb_file_name}).delete()\n        # get the electrodes from this NWB file\n        electrodes = (\n            Electrode()\n            &amp; {\"nwb_file_name\": nwb_file_name}\n            &amp; {\"bad_channel\": \"False\"}\n        ).fetch()\n        e_groups = np.unique(electrodes[\"electrode_group_name\"])\n        sg_key = dict()\n        sge_key = dict()\n        sg_key[\"nwb_file_name\"] = sge_key[\"nwb_file_name\"] = nwb_file_name\n        sort_group = 0\n        for e_group in e_groups:\n            sge_key[\"electrode_group_name\"] = e_group\n            # sg_key['sort_group_id'] = sge_key['sort_group_id'] = sort_group\n            # TEST\n            sg_key[\"sort_group_id\"] = sge_key[\"sort_group_id\"] = int(e_group)\n            # get the list of references and make sure they are all the same\n            shank_elect_ref = electrodes[\"original_reference_electrode\"][\n                electrodes[\"electrode_group_name\"] == e_group\n            ]\n            if np.max(shank_elect_ref) == np.min(shank_elect_ref):\n                sg_key[\"sort_reference_electrode_id\"] = shank_elect_ref[0]\n            else:\n                ValueError(\n                    f\"Error in electrode group {e_group}: reference electrodes are not all the same\"\n                )\n            self.insert1(sg_key)\n\n            shank_elect = electrodes[\"electrode_id\"][\n                electrodes[\"electrode_group_name\"] == e_group\n            ]\n            for elect in shank_elect:\n                sge_key[\"electrode_id\"] = elect\n                self.SortGroupElectrode().insert1(sge_key)\n            sort_group += 1\n\n    def set_reference_from_list(self, nwb_file_name, sort_group_ref_list):\n\"\"\"\n        Set the reference electrode from a list containing sort groups and reference electrodes\n        :param: sort_group_ref_list - 2D array or list where each row is [sort_group_id reference_electrode]\n        :param: nwb_file_name - The name of the NWB file whose electrodes' references should be updated\n        :return: Null\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        sort_group_list = (SortGroup() &amp; key).fetch1()\n        for sort_group in sort_group_list:\n            key[\"sort_group_id\"] = sort_group\n            self.insert(\n                dj_replace(\n                    sort_group_list,\n                    sort_group_ref_list,\n                    \"sort_group_id\",\n                    \"sort_reference_electrode_id\",\n                ),\n                replace=\"True\",\n            )\n\n    def get_geometry(self, sort_group_id, nwb_file_name):\n\"\"\"\n        Returns a list with the x,y coordinates of the electrodes in the sort group\n        for use with the SpikeInterface package.\n\n        Converts z locations to y where appropriate.\n\n        Parameters\n        ----------\n        sort_group_id : int\n        nwb_file_name : str\n\n        Returns\n        -------\n        geometry : list\n            List of coordinate pairs, one per electrode\n        \"\"\"\n\n        # create the channel_groups dictiorary\n        channel_group = dict()\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        electrodes = (Electrode() &amp; key).fetch()\n\n        key[\"sort_group_id\"] = sort_group_id\n        sort_group_electrodes = (SortGroup.SortGroupElectrode() &amp; key).fetch()\n        electrode_group_name = sort_group_electrodes[\"electrode_group_name\"][0]\n        probe_id = (\n            ElectrodeGroup\n            &amp; {\n                \"nwb_file_name\": nwb_file_name,\n                \"electrode_group_name\": electrode_group_name,\n            }\n        ).fetch1(\"probe_id\")\n        channel_group[sort_group_id] = dict()\n        channel_group[sort_group_id][\"channels\"] = sort_group_electrodes[\n            \"electrode_id\"\n        ].tolist()\n\n        n_chan = len(channel_group[sort_group_id][\"channels\"])\n\n        geometry = np.zeros((n_chan, 2), dtype=\"float\")\n        tmp_geom = np.zeros((n_chan, 3), dtype=\"float\")\n        for i, electrode_id in enumerate(\n            channel_group[sort_group_id][\"channels\"]\n        ):\n            # get the relative x and y locations of this channel from the probe table\n            probe_electrode = int(\n                electrodes[\"probe_electrode\"][\n                    electrodes[\"electrode_id\"] == electrode_id\n                ]\n            )\n            rel_x, rel_y, rel_z = (\n                Probe().Electrode()\n                &amp; {\"probe_id\": probe_id, \"probe_electrode\": probe_electrode}\n            ).fetch(\"rel_x\", \"rel_y\", \"rel_z\")\n            # TODO: Fix this HACK when we can use probeinterface:\n            rel_x = float(rel_x)\n            rel_y = float(rel_y)\n            rel_z = float(rel_z)\n            tmp_geom[i, :] = [rel_x, rel_y, rel_z]\n\n        # figure out which columns have coordinates\n        n_found = 0\n        for i in range(3):\n            if np.any(np.nonzero(tmp_geom[:, i])):\n                if n_found &lt; 2:\n                    geometry[:, n_found] = tmp_geom[:, i]\n                    n_found += 1\n                else:\n                    Warning(\n                        \"Relative electrode locations have three coordinates; only two are currently supported\"\n                    )\n        return np.ndarray.tolist(geometry)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.SortGroup.set_group_by_shank", "title": "<code>set_group_by_shank(nwb_file_name, references=None, omit_ref_electrode_group=False, omit_unitrode=True)</code>", "text": "<p>Divides electrodes into groups based on their shank position.</p> <ul> <li>Electrodes from probes with 1 shank (e.g. tetrodes) are placed in a   single group</li> <li>Electrodes from probes with multiple shanks (e.g. polymer probes) are   placed in one group per shank</li> <li>Bad channels are omitted</li> </ul> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>the name of the NWB file whose electrodes should be put into sorting groups</p> required <code>references</code> <code>dict</code> <p>If passed, used to set references. Otherwise, references set using original reference electrodes from config. Keys: electrode groups. Values: reference electrode.</p> <code>None</code> <code>omit_ref_electrode_group</code> <code>bool</code> <p>Optional. If True, no sort group is defined for electrode group of reference.</p> <code>False</code> <code>omit_unitrode</code> <code>bool</code> <p>Optional. If True, no sort groups are defined for unitrodes.</p> <code>True</code> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>def set_group_by_shank(\n    self,\n    nwb_file_name: str,\n    references: dict = None,\n    omit_ref_electrode_group=False,\n    omit_unitrode=True,\n):\n\"\"\"Divides electrodes into groups based on their shank position.\n\n    * Electrodes from probes with 1 shank (e.g. tetrodes) are placed in a\n      single group\n    * Electrodes from probes with multiple shanks (e.g. polymer probes) are\n      placed in one group per shank\n    * Bad channels are omitted\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        the name of the NWB file whose electrodes should be put into sorting groups\n    references : dict, optional\n        If passed, used to set references. Otherwise, references set using\n        original reference electrodes from config. Keys: electrode groups. Values: reference electrode.\n    omit_ref_electrode_group : bool\n        Optional. If True, no sort group is defined for electrode group of reference.\n    omit_unitrode : bool\n        Optional. If True, no sort groups are defined for unitrodes.\n    \"\"\"\n    # delete any current groups\n    (SortGroup &amp; {\"nwb_file_name\": nwb_file_name}).delete()\n    # get the electrodes from this NWB file\n    electrodes = (\n        Electrode()\n        &amp; {\"nwb_file_name\": nwb_file_name}\n        &amp; {\"bad_channel\": \"False\"}\n    ).fetch()\n    e_groups = list(np.unique(electrodes[\"electrode_group_name\"]))\n    e_groups.sort(key=int)  # sort electrode groups numerically\n    sort_group = 0\n    sg_key = dict()\n    sge_key = dict()\n    sg_key[\"nwb_file_name\"] = sge_key[\"nwb_file_name\"] = nwb_file_name\n    for e_group in e_groups:\n        # for each electrode group, get a list of the unique shank numbers\n        shank_list = np.unique(\n            electrodes[\"probe_shank\"][\n                electrodes[\"electrode_group_name\"] == e_group\n            ]\n        )\n        sge_key[\"electrode_group_name\"] = e_group\n        # get the indices of all electrodes in this group / shank and set their sorting group\n        for shank in shank_list:\n            sg_key[\"sort_group_id\"] = sge_key[\"sort_group_id\"] = sort_group\n            # specify reference electrode. Use 'references' if passed, otherwise use reference from config\n            if not references:\n                shank_elect_ref = electrodes[\n                    \"original_reference_electrode\"\n                ][\n                    np.logical_and(\n                        electrodes[\"electrode_group_name\"] == e_group,\n                        electrodes[\"probe_shank\"] == shank,\n                    )\n                ]\n                if np.max(shank_elect_ref) == np.min(shank_elect_ref):\n                    sg_key[\"sort_reference_electrode_id\"] = shank_elect_ref[\n                        0\n                    ]\n                else:\n                    ValueError(\n                        f\"Error in electrode group {e_group}: reference electrodes are not all the same\"\n                    )\n            else:\n                if e_group not in references.keys():\n                    raise Exception(\n                        f\"electrode group {e_group} not a key in references, so cannot set reference\"\n                    )\n                else:\n                    sg_key[\"sort_reference_electrode_id\"] = references[\n                        e_group\n                    ]\n            # Insert sort group and sort group electrodes\n            reference_electrode_group = electrodes[\n                electrodes[\"electrode_id\"]\n                == sg_key[\"sort_reference_electrode_id\"]\n            ][\n                \"electrode_group_name\"\n            ]  # reference for this electrode group\n            if (\n                len(reference_electrode_group) == 1\n            ):  # unpack single reference\n                reference_electrode_group = reference_electrode_group[0]\n            elif (int(sg_key[\"sort_reference_electrode_id\"]) &gt; 0) and (\n                len(reference_electrode_group) != 1\n            ):\n                raise Exception(\n                    f\"Should have found exactly one electrode group for reference electrode,\"\n                    f\"but found {len(reference_electrode_group)}.\"\n                )\n            if omit_ref_electrode_group and (\n                str(e_group) == str(reference_electrode_group)\n            ):\n                print(\n                    f\"Omitting electrode group {e_group} from sort groups because contains reference.\"\n                )\n                continue\n            shank_elect = electrodes[\"electrode_id\"][\n                np.logical_and(\n                    electrodes[\"electrode_group_name\"] == e_group,\n                    electrodes[\"probe_shank\"] == shank,\n                )\n            ]\n            if (\n                omit_unitrode and len(shank_elect) == 1\n            ):  # omit unitrodes if indicated\n                print(\n                    f\"Omitting electrode group {e_group}, shank {shank} from sort groups because unitrode.\"\n                )\n                continue\n            self.insert1(sg_key)\n            for elect in shank_elect:\n                sge_key[\"electrode_id\"] = elect\n                self.SortGroupElectrode().insert1(sge_key)\n            sort_group += 1\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.SortGroup.set_group_by_electrode_group", "title": "<code>set_group_by_electrode_group(nwb_file_name)</code>", "text": "<p>Assign groups to all non-bad channel electrodes based on their electrode group and sets the reference for each group to the reference for the first channel of the group.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>the name of the nwb whose electrodes should be put into sorting groups</p> required Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>def set_group_by_electrode_group(self, nwb_file_name: str):\n\"\"\"Assign groups to all non-bad channel electrodes based on their electrode group\n    and sets the reference for each group to the reference for the first channel of the group.\n\n    Parameters\n    ----------\n    nwb_file_name: str\n        the name of the nwb whose electrodes should be put into sorting groups\n    \"\"\"\n    # delete any current groups\n    (SortGroup &amp; {\"nwb_file_name\": nwb_file_name}).delete()\n    # get the electrodes from this NWB file\n    electrodes = (\n        Electrode()\n        &amp; {\"nwb_file_name\": nwb_file_name}\n        &amp; {\"bad_channel\": \"False\"}\n    ).fetch()\n    e_groups = np.unique(electrodes[\"electrode_group_name\"])\n    sg_key = dict()\n    sge_key = dict()\n    sg_key[\"nwb_file_name\"] = sge_key[\"nwb_file_name\"] = nwb_file_name\n    sort_group = 0\n    for e_group in e_groups:\n        sge_key[\"electrode_group_name\"] = e_group\n        # sg_key['sort_group_id'] = sge_key['sort_group_id'] = sort_group\n        # TEST\n        sg_key[\"sort_group_id\"] = sge_key[\"sort_group_id\"] = int(e_group)\n        # get the list of references and make sure they are all the same\n        shank_elect_ref = electrodes[\"original_reference_electrode\"][\n            electrodes[\"electrode_group_name\"] == e_group\n        ]\n        if np.max(shank_elect_ref) == np.min(shank_elect_ref):\n            sg_key[\"sort_reference_electrode_id\"] = shank_elect_ref[0]\n        else:\n            ValueError(\n                f\"Error in electrode group {e_group}: reference electrodes are not all the same\"\n            )\n        self.insert1(sg_key)\n\n        shank_elect = electrodes[\"electrode_id\"][\n            electrodes[\"electrode_group_name\"] == e_group\n        ]\n        for elect in shank_elect:\n            sge_key[\"electrode_id\"] = elect\n            self.SortGroupElectrode().insert1(sge_key)\n        sort_group += 1\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.SortGroup.set_reference_from_list", "title": "<code>set_reference_from_list(nwb_file_name, sort_group_ref_list)</code>", "text": "<p>Set the reference electrode from a list containing sort groups and reference electrodes :param: sort_group_ref_list - 2D array or list where each row is [sort_group_id reference_electrode] :param: nwb_file_name - The name of the NWB file whose electrodes' references should be updated :return: Null</p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>def set_reference_from_list(self, nwb_file_name, sort_group_ref_list):\n\"\"\"\n    Set the reference electrode from a list containing sort groups and reference electrodes\n    :param: sort_group_ref_list - 2D array or list where each row is [sort_group_id reference_electrode]\n    :param: nwb_file_name - The name of the NWB file whose electrodes' references should be updated\n    :return: Null\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    sort_group_list = (SortGroup() &amp; key).fetch1()\n    for sort_group in sort_group_list:\n        key[\"sort_group_id\"] = sort_group\n        self.insert(\n            dj_replace(\n                sort_group_list,\n                sort_group_ref_list,\n                \"sort_group_id\",\n                \"sort_reference_electrode_id\",\n            ),\n            replace=\"True\",\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.SortGroup.get_geometry", "title": "<code>get_geometry(sort_group_id, nwb_file_name)</code>", "text": "<p>Returns a list with the x,y coordinates of the electrodes in the sort group for use with the SpikeInterface package.</p> <p>Converts z locations to y where appropriate.</p> <p>Parameters:</p> Name Type Description Default <code>sort_group_id</code> <code>int</code> required <code>nwb_file_name</code> <code>str</code> required <p>Returns:</p> Name Type Description <code>geometry</code> <code>list</code> <p>List of coordinate pairs, one per electrode</p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>def get_geometry(self, sort_group_id, nwb_file_name):\n\"\"\"\n    Returns a list with the x,y coordinates of the electrodes in the sort group\n    for use with the SpikeInterface package.\n\n    Converts z locations to y where appropriate.\n\n    Parameters\n    ----------\n    sort_group_id : int\n    nwb_file_name : str\n\n    Returns\n    -------\n    geometry : list\n        List of coordinate pairs, one per electrode\n    \"\"\"\n\n    # create the channel_groups dictiorary\n    channel_group = dict()\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    electrodes = (Electrode() &amp; key).fetch()\n\n    key[\"sort_group_id\"] = sort_group_id\n    sort_group_electrodes = (SortGroup.SortGroupElectrode() &amp; key).fetch()\n    electrode_group_name = sort_group_electrodes[\"electrode_group_name\"][0]\n    probe_id = (\n        ElectrodeGroup\n        &amp; {\n            \"nwb_file_name\": nwb_file_name,\n            \"electrode_group_name\": electrode_group_name,\n        }\n    ).fetch1(\"probe_id\")\n    channel_group[sort_group_id] = dict()\n    channel_group[sort_group_id][\"channels\"] = sort_group_electrodes[\n        \"electrode_id\"\n    ].tolist()\n\n    n_chan = len(channel_group[sort_group_id][\"channels\"])\n\n    geometry = np.zeros((n_chan, 2), dtype=\"float\")\n    tmp_geom = np.zeros((n_chan, 3), dtype=\"float\")\n    for i, electrode_id in enumerate(\n        channel_group[sort_group_id][\"channels\"]\n    ):\n        # get the relative x and y locations of this channel from the probe table\n        probe_electrode = int(\n            electrodes[\"probe_electrode\"][\n                electrodes[\"electrode_id\"] == electrode_id\n            ]\n        )\n        rel_x, rel_y, rel_z = (\n            Probe().Electrode()\n            &amp; {\"probe_id\": probe_id, \"probe_electrode\": probe_electrode}\n        ).fetch(\"rel_x\", \"rel_y\", \"rel_z\")\n        # TODO: Fix this HACK when we can use probeinterface:\n        rel_x = float(rel_x)\n        rel_y = float(rel_y)\n        rel_z = float(rel_z)\n        tmp_geom[i, :] = [rel_x, rel_y, rel_z]\n\n    # figure out which columns have coordinates\n    n_found = 0\n    for i in range(3):\n        if np.any(np.nonzero(tmp_geom[:, i])):\n            if n_found &lt; 2:\n                geometry[:, n_found] = tmp_geom[:, i]\n                n_found += 1\n            else:\n                Warning(\n                    \"Relative electrode locations have three coordinates; only two are currently supported\"\n                )\n    return np.ndarray.tolist(geometry)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.Nwbfile", "title": "<code>Nwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass Nwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files.\n    nwb_file_name: varchar(255)   # name of the NWB file\n    ---\n    nwb_file_abs_path: filepath@raw\n    INDEX (nwb_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    @classmethod\n    def insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The relative path to the NWB file.\n        \"\"\"\n        nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n        assert os.path.exists(\n            nwb_file_abs_path\n        ), f\"File does not exist: {nwb_file_abs_path}\"\n\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n        cls.insert1(key, skip_duplicates=True)\n\n    @staticmethod\n    def get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n        Returns\n        -------\n        nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n        nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n        return str(nwb_file_abspath)\n\n    @staticmethod\n    def add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n        The NWB_LOCK_FILE environment variable must be set.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file that has been inserted into the Nwbfile() schema.\n        \"\"\"\n        key = {\"nwb_file_name\": nwb_file_name}\n        # check to make sure the file exists\n        assert (\n            len((Nwbfile() &amp; key).fetch()) &gt; 0\n        ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n        lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n        lock_file.write(f\"{nwb_file_name}\\n\")\n        lock_file.close()\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        This does not delete the files themselves unless delete_files=True is specified\n        Run this after deleting the Nwbfile() entries themselves.\n        \"\"\"\n        schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.common.common_nwbfile.Nwbfile.insert_from_relative_file_name", "title": "<code>insert_from_relative_file_name(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Insert a new session from an existing NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The relative path to the NWB file.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef insert_from_relative_file_name(cls, nwb_file_name):\n\"\"\"Insert a new session from an existing NWB file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The relative path to the NWB file.\n    \"\"\"\n    nwb_file_abs_path = Nwbfile.get_abs_path(nwb_file_name)\n    assert os.path.exists(\n        nwb_file_abs_path\n    ), f\"File does not exist: {nwb_file_abs_path}\"\n\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"nwb_file_abs_path\"] = nwb_file_abs_path\n    cls.insert1(key, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.common.common_nwbfile.Nwbfile.get_abs_path", "title": "<code>get_abs_path(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored raw NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required <p>Returns:</p> Name Type Description <code>nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(nwb_file_name):\n\"\"\"Return the absolute path for a stored raw NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n\n    Returns\n    -------\n    nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR or provide the base_dir argument\"\n\n    nwb_file_abspath = base_dir / \"raw\" / nwb_file_name\n    return str(nwb_file_abspath)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.common.common_nwbfile.Nwbfile.add_to_lock", "title": "<code>add_to_lock(nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Add the specified NWB file to the file with the list of NWB files to be locked.</p> <p>The NWB_LOCK_FILE environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file that has been inserted into the Nwbfile() schema.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef add_to_lock(nwb_file_name):\n\"\"\"Add the specified NWB file to the file with the list of NWB files to be locked.\n\n    The NWB_LOCK_FILE environment variable must be set.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file that has been inserted into the Nwbfile() schema.\n    \"\"\"\n    key = {\"nwb_file_name\": nwb_file_name}\n    # check to make sure the file exists\n    assert (\n        len((Nwbfile() &amp; key).fetch()) &gt; 0\n    ), f\"Error adding {nwb_file_name} to lock file, not in Nwbfile() schema\"\n\n    lock_file = open(os.getenv(\"NWB_LOCK_FILE\"), \"a+\")\n    lock_file.write(f\"{nwb_file_name}\\n\")\n    lock_file.close()\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.common.common_nwbfile.Nwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>This does not delete the files themselves unless delete_files=True is specified Run this after deleting the Nwbfile() entries themselves.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    This does not delete the files themselves unless delete_files=True is specified\n    Run this after deleting the Nwbfile() entries themselves.\n    \"\"\"\n    schema.external[\"raw\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.LabTeam", "title": "<code>LabTeam</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass LabTeam(dj.Manual):\n    definition = \"\"\"\n    team_name: varchar(80)\n    ---\n    team_description = \"\": varchar(2000)\n    \"\"\"\n\n    class LabTeamMember(dj.Part):\n        definition = \"\"\"\n        -&gt; LabTeam\n        -&gt; LabMember\n        \"\"\"\n\n    @classmethod\n    def create_new_team(\n        cls, team_name: str, team_members: list, team_description: str = \"\"\n    ):\n\"\"\"Create a new team with a list of team members.\n\n        If the lab member does not exist in the database, they will be added.\n\n        Parameters\n        ----------\n        team_name : str\n            The name of the team.\n        team_members : str\n            The full names of the lab members that are part of the team.\n        team_description: str\n            The description of the team.\n        \"\"\"\n        labteam_dict = dict()\n        labteam_dict[\"team_name\"] = team_name\n        labteam_dict[\"team_description\"] = team_description\n        cls.insert1(labteam_dict, skip_duplicates=True)\n\n        for team_member in team_members:\n            LabMember.insert_from_name(team_member)\n            query = (\n                LabMember.LabMemberInfo() &amp; {\"lab_member_name\": team_member}\n            ).fetch(\"google_user_name\")\n            if not len(query):\n                print(\n                    f\"Please add the Google user ID for {team_member} in the LabMember.LabMemberInfo table \"\n                    \"if you want to give them permission to manually curate sorting by this team.\"\n                )\n            labteammember_dict = dict()\n            labteammember_dict[\"team_name\"] = team_name\n            labteammember_dict[\"lab_member_name\"] = team_member\n            cls.LabTeamMember.insert1(labteammember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.common.common_lab.LabTeam.create_new_team", "title": "<code>create_new_team(team_name, team_members, team_description='')</code>  <code>classmethod</code>", "text": "<p>Create a new team with a list of team members.</p> <p>If the lab member does not exist in the database, they will be added.</p> <p>Parameters:</p> Name Type Description Default <code>team_name</code> <code>str</code> <p>The name of the team.</p> required <code>team_members</code> <code>str</code> <p>The full names of the lab members that are part of the team.</p> required <code>team_description</code> <code>str</code> <p>The description of the team.</p> <code>''</code> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef create_new_team(\n    cls, team_name: str, team_members: list, team_description: str = \"\"\n):\n\"\"\"Create a new team with a list of team members.\n\n    If the lab member does not exist in the database, they will be added.\n\n    Parameters\n    ----------\n    team_name : str\n        The name of the team.\n    team_members : str\n        The full names of the lab members that are part of the team.\n    team_description: str\n        The description of the team.\n    \"\"\"\n    labteam_dict = dict()\n    labteam_dict[\"team_name\"] = team_name\n    labteam_dict[\"team_description\"] = team_description\n    cls.insert1(labteam_dict, skip_duplicates=True)\n\n    for team_member in team_members:\n        LabMember.insert_from_name(team_member)\n        query = (\n            LabMember.LabMemberInfo() &amp; {\"lab_member_name\": team_member}\n        ).fetch(\"google_user_name\")\n        if not len(query):\n            print(\n                f\"Please add the Google user ID for {team_member} in the LabMember.LabMemberInfo table \"\n                \"if you want to give them permission to manually curate sorting by this team.\"\n            )\n        labteammember_dict = dict()\n        labteammember_dict[\"team_name\"] = team_name\n        labteammember_dict[\"lab_member_name\"] = team_member\n        cls.LabTeamMember.insert1(labteammember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.Electrode", "title": "<code>Electrode</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>@schema\nclass Electrode(dj.Imported):\n    definition = \"\"\"\n    -&gt; ElectrodeGroup\n    electrode_id: int                      # the unique number for this electrode\n    ---\n    -&gt; [nullable] Probe.Electrode\n    -&gt; BrainRegion\n    name = \"\": varchar(200)                 # unique label for each contact\n    original_reference_electrode = -1: int  # the configured reference electrode for this electrode\n    x = NULL: float                         # the x coordinate of the electrode position in the brain\n    y = NULL: float                         # the y coordinate of the electrode position in the brain\n    z = NULL: float                         # the z coordinate of the electrode position in the brain\n    filtering: varchar(2000)                # description of the signal filtering\n    impedance = NULL: float                 # electrode impedance\n    bad_channel = \"False\": enum(\"True\", \"False\")  # if electrode is \"good\" or \"bad\" as observed during recording\n    x_warped = NULL: float                  # x coordinate of electrode position warped to common template brain\n    y_warped = NULL: float                  # y coordinate of electrode position warped to common template brain\n    z_warped = NULL: float                  # z coordinate of electrode position warped to common template brain\n    contacts: varchar(200)                  # label of electrode contacts used for a bipolar signal - current workaround\n    \"\"\"\n\n    def make(self, key):\n        nwb_file_name = key[\"nwb_file_name\"]\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        config = get_config(nwb_file_abspath)\n\n        if \"Electrode\" in config:\n            electrode_config_dicts = {\n                electrode_dict[\"electrode_id\"]: electrode_dict\n                for electrode_dict in config[\"Electrode\"]\n            }\n        else:\n            electrode_config_dicts = dict()\n\n        electrodes = nwbf.electrodes.to_dataframe()\n        for elect_id, elect_data in electrodes.iterrows():\n            key[\"electrode_id\"] = elect_id\n            key[\"name\"] = str(elect_id)\n            key[\"electrode_group_name\"] = elect_data.group_name\n            key[\"region_id\"] = BrainRegion.fetch_add(\n                region_name=elect_data.group.location\n            )\n            key[\"x\"] = elect_data.x\n            key[\"y\"] = elect_data.y\n            key[\"z\"] = elect_data.z\n            key[\"x_warped\"] = 0\n            key[\"y_warped\"] = 0\n            key[\"z_warped\"] = 0\n            key[\"contacts\"] = \"\"\n            key[\"filtering\"] = elect_data.filtering\n            key[\"impedance\"] = elect_data.get(\"imp\")\n\n            # rough check of whether the electrodes table was created by rec_to_nwb and has\n            # the appropriate custom columns used by rec_to_nwb\n            # TODO this could be better resolved by making an extension for the electrodes table\n            if (\n                isinstance(elect_data.group.device, ndx_franklab_novela.Probe)\n                and \"probe_shank\" in elect_data\n                and \"probe_electrode\" in elect_data\n                and \"bad_channel\" in elect_data\n                and \"ref_elect_id\" in elect_data\n            ):\n                key[\"probe_id\"] = elect_data.group.device.probe_type\n                key[\"probe_shank\"] = elect_data.probe_shank\n                key[\"probe_electrode\"] = elect_data.probe_electrode\n                key[\"bad_channel\"] = (\n                    \"True\" if elect_data.bad_channel else \"False\"\n                )\n                key[\"original_reference_electrode\"] = elect_data.ref_elect_id\n\n            # override with information from the config YAML based on primary key (electrode id)\n            if elect_id in electrode_config_dicts:\n                # check whether the Probe.Electrode being referenced exists\n                query = Probe.Electrode &amp; electrode_config_dicts[elect_id]\n                if len(query) == 0:\n                    warnings.warn(\n                        f\"No Probe.Electrode exists that matches the data: {electrode_config_dicts[elect_id]}. \"\n                        f\"The config YAML for Electrode with electrode_id {elect_id} will be ignored.\"\n                    )\n                else:\n                    key.update(electrode_config_dicts[elect_id])\n\n            self.insert1(key, skip_duplicates=True)\n\n    @classmethod\n    def create_from_config(cls, nwb_file_name: str):\n\"\"\"Create or update Electrode entries from what is specified in the config YAML file.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        nwbf = get_nwb_file(nwb_file_abspath)\n        config = get_config(nwb_file_abspath)\n        if \"Electrode\" not in config:\n            return\n\n        # map electrode id to dictionary of electrode information from config YAML\n        electrode_dicts = {\n            electrode_dict[\"electrode_id\"]: electrode_dict\n            for electrode_dict in config[\"Electrode\"]\n        }\n\n        electrodes = nwbf.electrodes.to_dataframe()\n        for nwbfile_elect_id, elect_data in electrodes.iterrows():\n            if nwbfile_elect_id in electrode_dicts:\n                # use the information in the electrodes table to start and then add (or overwrite) values from the\n                # config YAML\n                key = dict()\n                key[\"nwb_file_name\"] = nwb_file_name\n                key[\"name\"] = str(nwbfile_elect_id)\n                key[\"electrode_group_name\"] = elect_data.group_name\n                key[\"region_id\"] = BrainRegion.fetch_add(\n                    region_name=elect_data.group.location\n                )\n                key[\"x\"] = elect_data.x\n                key[\"y\"] = elect_data.y\n                key[\"z\"] = elect_data.z\n                key[\"x_warped\"] = 0\n                key[\"y_warped\"] = 0\n                key[\"z_warped\"] = 0\n                key[\"contacts\"] = \"\"\n                key[\"filtering\"] = elect_data.filtering\n                key[\"impedance\"] = elect_data.get(\"imp\")\n                key.update(electrode_dicts[nwbfile_elect_id])\n                query = Electrode &amp; {\"electrode_id\": nwbfile_elect_id}\n                if len(query):\n                    cls.update1(key)\n                    print(f\"Updated Electrode with ID {nwbfile_elect_id}.\")\n                else:\n                    cls.insert1(\n                        key, skip_duplicates=True, allow_direct_insert=True\n                    )\n                    print(f\"Inserted Electrode with ID {nwbfile_elect_id}.\")\n            else:\n                warnings.warn(\n                    f\"Electrode ID {nwbfile_elect_id} exists in the NWB file but has no corresponding \"\n                    \"config YAML entry.\"\n                )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.common.common_ephys.Electrode.create_from_config", "title": "<code>create_from_config(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Create or update Electrode entries from what is specified in the config YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file.</p> required Source code in <code>src/spyglass/common/common_ephys.py</code> <pre><code>@classmethod\ndef create_from_config(cls, nwb_file_name: str):\n\"\"\"Create or update Electrode entries from what is specified in the config YAML file.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    nwbf = get_nwb_file(nwb_file_abspath)\n    config = get_config(nwb_file_abspath)\n    if \"Electrode\" not in config:\n        return\n\n    # map electrode id to dictionary of electrode information from config YAML\n    electrode_dicts = {\n        electrode_dict[\"electrode_id\"]: electrode_dict\n        for electrode_dict in config[\"Electrode\"]\n    }\n\n    electrodes = nwbf.electrodes.to_dataframe()\n    for nwbfile_elect_id, elect_data in electrodes.iterrows():\n        if nwbfile_elect_id in electrode_dicts:\n            # use the information in the electrodes table to start and then add (or overwrite) values from the\n            # config YAML\n            key = dict()\n            key[\"nwb_file_name\"] = nwb_file_name\n            key[\"name\"] = str(nwbfile_elect_id)\n            key[\"electrode_group_name\"] = elect_data.group_name\n            key[\"region_id\"] = BrainRegion.fetch_add(\n                region_name=elect_data.group.location\n            )\n            key[\"x\"] = elect_data.x\n            key[\"y\"] = elect_data.y\n            key[\"z\"] = elect_data.z\n            key[\"x_warped\"] = 0\n            key[\"y_warped\"] = 0\n            key[\"z_warped\"] = 0\n            key[\"contacts\"] = \"\"\n            key[\"filtering\"] = elect_data.filtering\n            key[\"impedance\"] = elect_data.get(\"imp\")\n            key.update(electrode_dicts[nwbfile_elect_id])\n            query = Electrode &amp; {\"electrode_id\": nwbfile_elect_id}\n            if len(query):\n                cls.update1(key)\n                print(f\"Updated Electrode with ID {nwbfile_elect_id}.\")\n            else:\n                cls.insert1(\n                    key, skip_duplicates=True, allow_direct_insert=True\n                )\n                print(f\"Inserted Electrode with ID {nwbfile_elect_id}.\")\n        else:\n            warnings.warn(\n                f\"Electrode ID {nwbfile_elect_id} exists in the NWB file but has no corresponding \"\n                \"config YAML entry.\"\n            )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.intervals_by_length", "title": "<code>intervals_by_length(interval_list, min_length=0.0, max_length=10000000000.0)</code>", "text": "<p>Select intervals of certain lengths from an interval list.</p> <p>Parameters:</p> Name Type Description Default <code>interval_list</code> <code>array_like</code> <p>Each element is (start time, stop time), i.e. an interval. Unit is seconds.</p> required <code>min_length</code> <code>float</code> <p>Minimum interval length in seconds. Defaults to 0.0.</p> <code>0.0</code> <code>max_length</code> <code>float</code> <p>Maximum interval length in seconds. Defaults to 1e10.</p> <code>10000000000.0</code> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def intervals_by_length(interval_list, min_length=0.0, max_length=1e10):\n\"\"\"Select intervals of certain lengths from an interval list.\n\n    Parameters\n    ----------\n    interval_list : array_like\n        Each element is (start time, stop time), i.e. an interval. Unit is seconds.\n    min_length : float, optional\n        Minimum interval length in seconds. Defaults to 0.0.\n    max_length : float, optional\n        Maximum interval length in seconds. Defaults to 1e10.\n    \"\"\"\n    lengths = np.ravel(np.diff(interval_list))\n    return interval_list[\n        np.logical_and(lengths &gt; min_length, lengths &lt; max_length)\n    ]\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.interval_list_intersect", "title": "<code>interval_list_intersect(interval_list1, interval_list2, min_length=0)</code>", "text": "<p>Finds the intersections between two interval lists</p> <p>Parameters:</p> Name Type Description Default <code>interval_list1</code> <code>np.array, (N,2) where N = number of intervals</code> required <code>interval_list2</code> <code>np.array, (N,2) where N = number of intervals</code> required <code>min_length</code> <code>float, optional.</code> <p>Minimum length of intervals to include, default 0</p> <code>0</code> <p>Each interval is (start time, stop time)</p> <p>Returns:</p> Name Type Description <code>interval_list</code> <code>np.array, N, 2</code> Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def interval_list_intersect(interval_list1, interval_list2, min_length=0):\n\"\"\"Finds the intersections between two interval lists\n\n    Parameters\n    ----------\n    interval_list1 : np.array, (N,2) where N = number of intervals\n    interval_list2 : np.array, (N,2) where N = number of intervals\n    min_length : float, optional.\n        Minimum length of intervals to include, default 0\n\n    Each interval is (start time, stop time)\n\n    Returns\n    -------\n    interval_list: np.array, (N,2)\n    \"\"\"\n\n    # first, consolidate interval lists to disjoint intervals by sorting and applying union\n    if interval_list1.ndim == 1:\n        interval_list1 = np.expand_dims(interval_list1, 0)\n    else:\n        interval_list1 = interval_list1[np.argsort(interval_list1[:, 0])]\n        interval_list1 = reduce(_union_concat, interval_list1)\n        # the following check is needed in the case where the interval list is a single element (behavior of reduce)\n        if interval_list1.ndim == 1:\n            interval_list1 = np.expand_dims(interval_list1, 0)\n\n    if interval_list2.ndim == 1:\n        interval_list2 = np.expand_dims(interval_list2, 0)\n    else:\n        interval_list2 = interval_list2[np.argsort(interval_list2[:, 0])]\n        interval_list2 = reduce(_union_concat, interval_list2)\n        # the following check is needed in the case where the interval list is a single element (behavior of reduce)\n        if interval_list2.ndim == 1:\n            interval_list2 = np.expand_dims(interval_list2, 0)\n\n    # then do pairwise comparison and collect intersections\n    intersecting_intervals = []\n    for interval2 in interval_list2:\n        for interval1 in interval_list1:\n            if _intersection(interval2, interval1) is not None:\n                intersecting_intervals.append(\n                    _intersection(interval1, interval2)\n                )\n\n    # if no intersection, then return an empty list\n    if not intersecting_intervals:\n        return []\n    else:\n        intersecting_intervals = np.asarray(intersecting_intervals)\n        intersecting_intervals = intersecting_intervals[\n            np.argsort(intersecting_intervals[:, 0])\n        ]\n\n        return intervals_by_length(\n            intersecting_intervals, min_length=min_length\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.Probe", "title": "<code>Probe</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@schema\nclass Probe(dj.Manual):\n    definition = \"\"\"\n    # A configuration of a ProbeType. For most probe types, there is only one configuration, and that configuration\n    # should always be used. For Neuropixels probes, the specific channel map (which electrodes are used,\n    # where are they, and in what order) can differ between users and sessions, and each configuration should have a\n    # different ProbeType.\n    probe_id: varchar(80)     # a unique ID for this probe and dynamic configuration\n    ---\n    -&gt; ProbeType              # the type of probe, selected from a controlled list of probe types\n    -&gt; [nullable] DataAcquisitionDevice  # the data acquisition device used with this Probe\n    contact_side_numbering: enum(\"True\", \"False\")  # if True, then electrode contacts are facing you when numbering them\n    \"\"\"\n\n    class Shank(dj.Part):\n        definition = \"\"\"\n        -&gt; Probe\n        probe_shank: int              # shank number within probe. should be unique within a Probe\n        \"\"\"\n\n    class Electrode(dj.Part):\n        definition = \"\"\"\n        -&gt; Probe.Shank\n        probe_electrode: int          # electrode ID that is output from the data acquisition system\n                                      # probe_electrode should be unique within a Probe\n        ---\n        contact_size = NULL: float    # (um) contact size\n        rel_x = NULL: float           # (um) x coordinate of the electrode within the probe\n        rel_y = NULL: float           # (um) y coordinate of the electrode within the probe\n        rel_z = NULL: float           # (um) z coordinate of the electrode within the probe\n        \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf, config):\n\"\"\"Insert probe devices from an NWB file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : list\n            List of probe device types found in the NWB file.\n        \"\"\"\n        all_probes_types, ndx_probes, _ = cls.get_all_probe_names(nwbf, config)\n\n        for probe_type in all_probes_types:\n            new_probe_type_dict = dict()\n            new_probe_dict = dict()\n            shank_dict = dict()\n            elect_dict = dict()\n            num_shanks = 0\n\n            if probe_type in ndx_probes:\n                # read probe properties into new_probe_dict from PyNWB extension probe object\n                nwb_probe_obj = ndx_probes[probe_type]\n                cls.__read_ndx_probe_data(\n                    nwb_probe_obj,\n                    new_probe_type_dict,\n                    new_probe_dict,\n                    shank_dict,\n                    elect_dict,\n                )\n\n            # check that number of shanks is consistent\n            num_shanks = new_probe_type_dict[\"num_shanks\"]\n            assert num_shanks == 0 or num_shanks == len(\n                shank_dict\n            ), \"`num_shanks` is not equal to the number of shanks.\"\n\n            # if probe id already exists, do not overwrite anything or create new Shanks and Electrodes\n            # TODO test whether the Shanks and Electrodes in the NWB file match the ones in the database\n            query = Probe &amp; {\"probe_id\": new_probe_dict[\"probe_id\"]}\n            if len(query) &gt; 0:\n                print(\n                    f\"Probe ID '{new_probe_dict['probe_id']}' already exists in the database. Spyglass will use \"\n                    \"that and not create a new Probe, Shanks, or Electrodes.\"\n                )\n                continue\n\n            cls.insert1(new_probe_dict, skip_duplicates=True)\n\n            for shank in shank_dict.values():\n                cls.Shank.insert1(shank, skip_duplicates=True)\n            for electrode in elect_dict.values():\n                cls.Electrode.insert1(electrode, skip_duplicates=True)\n\n        if all_probes_types:\n            print(f\"Inserted probes {all_probes_types}\")\n        else:\n            print(\"No conforming probe metadata found.\")\n\n        return all_probes_types\n\n    @classmethod\n    def get_all_probe_names(cls, nwbf, config):\n\"\"\"Get a list of all device names in the NWB file, after appending and overwriting by the config file.\n\n        Parameters\n        ----------\n        nwbf : pynwb.NWBFile\n            The source NWB file object.\n        config : dict\n            Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n        Returns\n        -------\n        device_name_list : list\n            List of data acquisition object names found in the NWB file.\n        \"\"\"\n\n        # make a dict mapping probe type to PyNWB object for all devices in the NWB file that are\n        # of type ndx_franklab_novela.Probe and thus have the required metadata\n        ndx_probes = {\n            device_obj.probe_type: device_obj\n            for device_obj in nwbf.devices.values()\n            if isinstance(device_obj, ndx_franklab_novela.Probe)\n        }\n\n        # make a dict mapping probe type to dict of device metadata from the config YAML if exists\n        if \"Probe\" in config:\n            config_probes = [\n                probe_dict[\"probe_type\"] for probe_dict in config[\"Probe\"]\n            ]\n        else:\n            config_probes = list()\n\n        # get all the probe types from the NWB file plus the config YAML\n        all_probes_types = set(ndx_probes.keys()).union(set(config_probes))\n\n        return all_probes_types, ndx_probes, config_probes\n\n    @classmethod\n    def __read_ndx_probe_data(\n        cls,\n        nwb_probe_obj: ndx_franklab_novela.Probe,\n        new_probe_type_dict: dict,\n        new_probe_dict: dict,\n        shank_dict: dict,\n        elect_dict: dict,\n    ):\n        # construct dictionary of values to add to ProbeType\n        new_probe_type_dict[\"manufacturer\"] = (\n            getattr(nwb_probe_obj, \"manufacturer\") or \"\"\n        )\n        new_probe_type_dict[\"probe_type\"] = nwb_probe_obj.probe_type\n        new_probe_type_dict[\n            \"probe_description\"\n        ] = nwb_probe_obj.probe_description\n        new_probe_type_dict[\"num_shanks\"] = len(nwb_probe_obj.shanks)\n\n        cls._add_probe_type(new_probe_type_dict)\n\n        new_probe_dict[\"probe_id\"] = nwb_probe_obj.probe_type\n        new_probe_dict[\"probe_type\"] = nwb_probe_obj.probe_type\n        new_probe_dict[\"contact_side_numbering\"] = (\n            \"True\" if nwb_probe_obj.contact_side_numbering else \"False\"\n        )\n\n        # go through the shanks and add each one to the Shank table\n        for shank in nwb_probe_obj.shanks.values():\n            shank_dict[shank.name] = dict()\n            shank_dict[shank.name][\"probe_id\"] = new_probe_dict[\"probe_type\"]\n            shank_dict[shank.name][\"probe_shank\"] = int(shank.name)\n\n            # go through the electrodes and add each one to the Electrode table\n            for electrode in shank.shanks_electrodes.values():\n                # the next line will need to be fixed if we have different sized contacts on a shank\n                elect_dict[electrode.name] = dict()\n                elect_dict[electrode.name][\"probe_id\"] = new_probe_dict[\n                    \"probe_type\"\n                ]\n                elect_dict[electrode.name][\"probe_shank\"] = shank_dict[\n                    shank.name\n                ][\"probe_shank\"]\n                elect_dict[electrode.name][\n                    \"contact_size\"\n                ] = nwb_probe_obj.contact_size\n                elect_dict[electrode.name][\"probe_electrode\"] = int(\n                    electrode.name\n                )\n                elect_dict[electrode.name][\"rel_x\"] = electrode.rel_x\n                elect_dict[electrode.name][\"rel_y\"] = electrode.rel_y\n                elect_dict[electrode.name][\"rel_z\"] = electrode.rel_z\n\n    @classmethod\n    def _add_probe_type(cls, new_probe_type_dict):\n\"\"\"Check the probe type value against the values in the database.\n\n        Parameters\n        ----------\n        new_probe_type_dict : dict\n            Dictionary of probe type properties. See ProbeType for keys.\n\n        Raises\n        ------\n        PopulateException\n            If user chooses not to add a probe type to the database when prompted.\n\n        Returns\n        -------\n        probe_type : str\n            The probe type value that was added to the database.\n        \"\"\"\n        probe_type = new_probe_type_dict[\"probe_type\"]\n        all_values = ProbeType.fetch(\"probe_type\").tolist()\n        if probe_type not in all_values:\n            print(\n                f\"\\nProbe type '{probe_type}' was not found in the database. \"\n                f\"The current values are: {all_values}. \"\n                \"Please ensure that the probe type you want to add does not already \"\n                \"exist in the database under a different name or spelling. \"\n                \"If you want to use an existing name in the database, \"\n                \"please change the corresponding Probe object in the NWB file. \"\n                \"Entering 'N' will raise an exception.\"\n            )\n            val = input(\n                f\"Do you want to add probe type '{probe_type}' to the database? (y/N)\"\n            )\n            if val.lower() in [\"y\", \"yes\"]:\n                ProbeType.insert1(new_probe_type_dict, skip_duplicates=True)\n                return\n            raise PopulateException(\n                f\"User chose not to add probe type '{probe_type}' to the database.\"\n            )\n\n        # effectively else (entry exists)\n        # check whether the values provided match the values stored in the database\n        db_dict = (ProbeType &amp; {\"probe_type\": probe_type}).fetch1()\n        if db_dict != new_probe_type_dict:\n            raise PopulateException(\n                f\"\\nProbe type properties of PyNWB Probe object with name '{probe_type}': \"\n                f\"{new_probe_type_dict} do not match properties of the corresponding database entry: {db_dict}.\"\n            )\n        return probe_type\n\n    @classmethod\n    def create_from_nwbfile(\n        cls,\n        nwb_file_name: str,\n        nwb_device_name: str,\n        probe_id: str,\n        probe_type: str,\n        contact_side_numbering: bool,\n    ):\n\"\"\"Create a Probe entry and corresponding part table entries using the data in the NWB file.\n\n        This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices\n        (as probes) in the NWB file, but only ones that are associated with the device that matches the given\n        `nwb_device_name`.\n\n        Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe,\n        the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.\n\n        Example usage:\n        ```\n        sgc.Probe.create_from_nwbfile(\n            nwbfile=nwb_file_name,\n            nwb_device_name=\"Device\",\n            probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n            probe_type=\"Neuropixels 1.0\",\n            contact_side_numbering=True\n        )\n        ```\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the NWB file.\n        nwb_device_name : str\n            The name of the PyNWB Device object that represents the probe to read in the NWB file.\n        probe_id : str\n            A unique ID for the probe and its configuration, to be used as the primary key for the new Probe entry.\n        probe_type : str\n            The existing ProbeType entry that represents the type of probe being created. It must exist.\n        contact_side_numbering : bool\n            Whether the electrode contacts are facing you when numbering them. Stored in the new Probe entry.\n        \"\"\"\n\n        from .common_nwbfile import Nwbfile\n\n        nwb_file_path = Nwbfile.get_abs_path(nwb_file_name)\n        nwbfile = get_nwb_file(nwb_file_path)\n\n        query = ProbeType &amp; {\"probe_type\": probe_type}\n        if len(query) == 0:\n            print(\n                f\"No ProbeType found with probe_type '{probe_type}'. Aborting.\"\n            )\n            return\n\n        new_probe_dict = dict()\n        shank_dict = dict()\n        elect_dict = dict()\n\n        new_probe_dict[\"probe_id\"] = probe_id\n        new_probe_dict[\"probe_type\"] = probe_type\n        new_probe_dict[\"contact_side_numbering\"] = (\n            \"True\" if contact_side_numbering else \"False\"\n        )\n\n        # iterate through the electrodes table in the NWB file\n        # and use the group column (ElectrodeGroup) to create shanks\n        # and use the device attribute of each ElectrodeGroup to create a probe\n        created_shanks = dict()  # map device name to shank_index (int)\n        device_found = False\n        for elec_index in range(len(nwbfile.electrodes)):\n            electrode_group = nwbfile.electrodes[elec_index, \"group\"]\n            eg_device_name = electrode_group.device.name\n\n            # only look at electrodes where the associated device is the one specified\n            if eg_device_name == nwb_device_name:\n                device_found = True\n\n                # if a Shank has not yet been created from the electrode group, then create it\n                if electrode_group.name not in created_shanks:\n                    shank_index = len(created_shanks)\n                    created_shanks[electrode_group.name] = shank_index\n\n                    # build the dictionary of Probe.Shank data\n                    shank_dict[shank_index] = dict()\n                    shank_dict[shank_index][\"probe_id\"] = new_probe_dict[\n                        \"probe_id\"\n                    ]\n                    shank_dict[shank_index][\"probe_shank\"] = shank_index\n\n                # get the probe shank index associated with this Electrode\n                probe_shank = created_shanks[electrode_group.name]\n\n                # build the dictionary of Probe.Electrode data\n                elect_dict[elec_index] = dict()\n                elect_dict[elec_index][\"probe_id\"] = new_probe_dict[\"probe_id\"]\n                elect_dict[elec_index][\"probe_shank\"] = probe_shank\n                elect_dict[elec_index][\"probe_electrode\"] = elec_index\n                if \"rel_x\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_x\"] = nwbfile.electrodes[\n                        elec_index, \"rel_x\"\n                    ]\n                if \"rel_y\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_y\"] = nwbfile.electrodes[\n                        elec_index, \"rel_y\"\n                    ]\n                if \"rel_z\" in nwbfile.electrodes[elec_index]:\n                    elect_dict[elec_index][\"rel_z\"] = nwbfile.electrodes[\n                        elec_index, \"rel_z\"\n                    ]\n\n        if not device_found:\n            print(\n                f\"No electrodes in the NWB file were associated with a device named '{nwb_device_name}'.\"\n            )\n            return\n\n        # insert the Probe, then the Shank parts, and then the Electrode parts\n        cls.insert1(new_probe_dict, skip_duplicates=True)\n\n        for shank in shank_dict.values():\n            cls.Shank.insert1(shank, skip_duplicates=True)\n        for electrode in elect_dict.values():\n            cls.Electrode.insert1(electrode, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.common.common_device.Probe.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Insert probe devices from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of probe device types found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf, config):\n\"\"\"Insert probe devices from an NWB file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : list\n        List of probe device types found in the NWB file.\n    \"\"\"\n    all_probes_types, ndx_probes, _ = cls.get_all_probe_names(nwbf, config)\n\n    for probe_type in all_probes_types:\n        new_probe_type_dict = dict()\n        new_probe_dict = dict()\n        shank_dict = dict()\n        elect_dict = dict()\n        num_shanks = 0\n\n        if probe_type in ndx_probes:\n            # read probe properties into new_probe_dict from PyNWB extension probe object\n            nwb_probe_obj = ndx_probes[probe_type]\n            cls.__read_ndx_probe_data(\n                nwb_probe_obj,\n                new_probe_type_dict,\n                new_probe_dict,\n                shank_dict,\n                elect_dict,\n            )\n\n        # check that number of shanks is consistent\n        num_shanks = new_probe_type_dict[\"num_shanks\"]\n        assert num_shanks == 0 or num_shanks == len(\n            shank_dict\n        ), \"`num_shanks` is not equal to the number of shanks.\"\n\n        # if probe id already exists, do not overwrite anything or create new Shanks and Electrodes\n        # TODO test whether the Shanks and Electrodes in the NWB file match the ones in the database\n        query = Probe &amp; {\"probe_id\": new_probe_dict[\"probe_id\"]}\n        if len(query) &gt; 0:\n            print(\n                f\"Probe ID '{new_probe_dict['probe_id']}' already exists in the database. Spyglass will use \"\n                \"that and not create a new Probe, Shanks, or Electrodes.\"\n            )\n            continue\n\n        cls.insert1(new_probe_dict, skip_duplicates=True)\n\n        for shank in shank_dict.values():\n            cls.Shank.insert1(shank, skip_duplicates=True)\n        for electrode in elect_dict.values():\n            cls.Electrode.insert1(electrode, skip_duplicates=True)\n\n    if all_probes_types:\n        print(f\"Inserted probes {all_probes_types}\")\n    else:\n        print(\"No conforming probe metadata found.\")\n\n    return all_probes_types\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.common.common_device.Probe.get_all_probe_names", "title": "<code>get_all_probe_names(nwbf, config)</code>  <code>classmethod</code>", "text": "<p>Get a list of all device names in the NWB file, after appending and overwriting by the config file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>config</code> <code>dict</code> <p>Dictionary read from a user-defined YAML file containing values to replace in the NWB file.</p> required <p>Returns:</p> Name Type Description <code>device_name_list</code> <code>list</code> <p>List of data acquisition object names found in the NWB file.</p> Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef get_all_probe_names(cls, nwbf, config):\n\"\"\"Get a list of all device names in the NWB file, after appending and overwriting by the config file.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    config : dict\n        Dictionary read from a user-defined YAML file containing values to replace in the NWB file.\n\n    Returns\n    -------\n    device_name_list : list\n        List of data acquisition object names found in the NWB file.\n    \"\"\"\n\n    # make a dict mapping probe type to PyNWB object for all devices in the NWB file that are\n    # of type ndx_franklab_novela.Probe and thus have the required metadata\n    ndx_probes = {\n        device_obj.probe_type: device_obj\n        for device_obj in nwbf.devices.values()\n        if isinstance(device_obj, ndx_franklab_novela.Probe)\n    }\n\n    # make a dict mapping probe type to dict of device metadata from the config YAML if exists\n    if \"Probe\" in config:\n        config_probes = [\n            probe_dict[\"probe_type\"] for probe_dict in config[\"Probe\"]\n        ]\n    else:\n        config_probes = list()\n\n    # get all the probe types from the NWB file plus the config YAML\n    all_probes_types = set(ndx_probes.keys()).union(set(config_probes))\n\n    return all_probes_types, ndx_probes, config_probes\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.common.common_device.Probe.create_from_nwbfile", "title": "<code>create_from_nwbfile(nwb_file_name, nwb_device_name, probe_id, probe_type, contact_side_numbering)</code>  <code>classmethod</code>", "text": "<p>Create a Probe entry and corresponding part table entries using the data in the NWB file.</p> <p>This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices (as probes) in the NWB file, but only ones that are associated with the device that matches the given <code>nwb_device_name</code>.</p> <p>Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe, the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.</p> <p>Example usage:</p> <pre><code>sgc.Probe.create_from_nwbfile(\n    nwbfile=nwb_file_name,\n    nwb_device_name=\"Device\",\n    probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n    probe_type=\"Neuropixels 1.0\",\n    contact_side_numbering=True\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the NWB file.</p> required <code>nwb_device_name</code> <code>str</code> <p>The name of the PyNWB Device object that represents the probe to read in the NWB file.</p> required <code>probe_id</code> <code>str</code> <p>A unique ID for the probe and its configuration, to be used as the primary key for the new Probe entry.</p> required <code>probe_type</code> <code>str</code> <p>The existing ProbeType entry that represents the type of probe being created. It must exist.</p> required <code>contact_side_numbering</code> <code>bool</code> <p>Whether the electrode contacts are facing you when numbering them. Stored in the new Probe entry.</p> required Source code in <code>src/spyglass/common/common_device.py</code> <pre><code>@classmethod\ndef create_from_nwbfile(\n    cls,\n    nwb_file_name: str,\n    nwb_device_name: str,\n    probe_id: str,\n    probe_type: str,\n    contact_side_numbering: bool,\n):\n\"\"\"Create a Probe entry and corresponding part table entries using the data in the NWB file.\n\n    This method will parse the electrodes in the electrodes table, electrode groups (as shanks), and devices\n    (as probes) in the NWB file, but only ones that are associated with the device that matches the given\n    `nwb_device_name`.\n\n    Note that this code assumes the relatively standard convention where the NWB device corresponds to a Probe,\n    the NWB electrode group corresponds to a Shank, and the NWB electrode corresponds to an Electrode.\n\n    Example usage:\n    ```\n    sgc.Probe.create_from_nwbfile(\n        nwbfile=nwb_file_name,\n        nwb_device_name=\"Device\",\n        probe_id=\"Neuropixels 1.0 Giocomo Lab Configuration\",\n        probe_type=\"Neuropixels 1.0\",\n        contact_side_numbering=True\n    )\n    ```\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the NWB file.\n    nwb_device_name : str\n        The name of the PyNWB Device object that represents the probe to read in the NWB file.\n    probe_id : str\n        A unique ID for the probe and its configuration, to be used as the primary key for the new Probe entry.\n    probe_type : str\n        The existing ProbeType entry that represents the type of probe being created. It must exist.\n    contact_side_numbering : bool\n        Whether the electrode contacts are facing you when numbering them. Stored in the new Probe entry.\n    \"\"\"\n\n    from .common_nwbfile import Nwbfile\n\n    nwb_file_path = Nwbfile.get_abs_path(nwb_file_name)\n    nwbfile = get_nwb_file(nwb_file_path)\n\n    query = ProbeType &amp; {\"probe_type\": probe_type}\n    if len(query) == 0:\n        print(\n            f\"No ProbeType found with probe_type '{probe_type}'. Aborting.\"\n        )\n        return\n\n    new_probe_dict = dict()\n    shank_dict = dict()\n    elect_dict = dict()\n\n    new_probe_dict[\"probe_id\"] = probe_id\n    new_probe_dict[\"probe_type\"] = probe_type\n    new_probe_dict[\"contact_side_numbering\"] = (\n        \"True\" if contact_side_numbering else \"False\"\n    )\n\n    # iterate through the electrodes table in the NWB file\n    # and use the group column (ElectrodeGroup) to create shanks\n    # and use the device attribute of each ElectrodeGroup to create a probe\n    created_shanks = dict()  # map device name to shank_index (int)\n    device_found = False\n    for elec_index in range(len(nwbfile.electrodes)):\n        electrode_group = nwbfile.electrodes[elec_index, \"group\"]\n        eg_device_name = electrode_group.device.name\n\n        # only look at electrodes where the associated device is the one specified\n        if eg_device_name == nwb_device_name:\n            device_found = True\n\n            # if a Shank has not yet been created from the electrode group, then create it\n            if electrode_group.name not in created_shanks:\n                shank_index = len(created_shanks)\n                created_shanks[electrode_group.name] = shank_index\n\n                # build the dictionary of Probe.Shank data\n                shank_dict[shank_index] = dict()\n                shank_dict[shank_index][\"probe_id\"] = new_probe_dict[\n                    \"probe_id\"\n                ]\n                shank_dict[shank_index][\"probe_shank\"] = shank_index\n\n            # get the probe shank index associated with this Electrode\n            probe_shank = created_shanks[electrode_group.name]\n\n            # build the dictionary of Probe.Electrode data\n            elect_dict[elec_index] = dict()\n            elect_dict[elec_index][\"probe_id\"] = new_probe_dict[\"probe_id\"]\n            elect_dict[elec_index][\"probe_shank\"] = probe_shank\n            elect_dict[elec_index][\"probe_electrode\"] = elec_index\n            if \"rel_x\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_x\"] = nwbfile.electrodes[\n                    elec_index, \"rel_x\"\n                ]\n            if \"rel_y\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_y\"] = nwbfile.electrodes[\n                    elec_index, \"rel_y\"\n                ]\n            if \"rel_z\" in nwbfile.electrodes[elec_index]:\n                elect_dict[elec_index][\"rel_z\"] = nwbfile.electrodes[\n                    elec_index, \"rel_z\"\n                ]\n\n    if not device_found:\n        print(\n            f\"No electrodes in the NWB file were associated with a device named '{nwb_device_name}'.\"\n        )\n        return\n\n    # insert the Probe, then the Shank parts, and then the Electrode parts\n    cls.insert1(new_probe_dict, skip_duplicates=True)\n\n    for shank in shank_dict.values():\n        cls.Shank.insert1(shank, skip_duplicates=True)\n    for electrode in elect_dict.values():\n        cls.Electrode.insert1(electrode, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.union_adjacent_index", "title": "<code>union_adjacent_index(interval1, interval2)</code>", "text": "<p>unions two intervals that are adjacent in index e.g. [a,b] and [b+1, c] is converted to [a,c] if not adjacent, just concatenates interval2 at the end of interval1</p> <p>Parameters:</p> Name Type Description Default <code>interval1</code> <code>np.array</code> <p>[description]</p> required <code>interval2</code> <code>np.array</code> <p>[description]</p> required Source code in <code>src/spyglass/common/common_interval.py</code> <pre><code>def union_adjacent_index(interval1, interval2):\n\"\"\"unions two intervals that are adjacent in index\n    e.g. [a,b] and [b+1, c] is converted to [a,c]\n    if not adjacent, just concatenates interval2 at the end of interval1\n\n    Parameters\n    ----------\n    interval1 : np.array\n        [description]\n    interval2 : np.array\n        [description]\n    \"\"\"\n    if interval1.ndim == 1:\n        interval1 = np.expand_dims(interval1, 0)\n    if interval2.ndim == 1:\n        interval2 = np.expand_dims(interval2, 0)\n\n    if (\n        interval1[-1][1] + 1 == interval2[0][0]\n        or interval2[0][1] + 1 == interval1[-1][0]\n    ):\n        x = np.array(\n            [\n                [\n                    np.min([interval1[-1][0], interval2[0][0]]),\n                    np.max([interval1[-1][1], interval2[0][1]]),\n                ]\n            ]\n        )\n        return np.concatenate((interval1[:-1], x), axis=0)\n    else:\n        return np.concatenate((interval1, interval2), axis=0)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_recording/#src.spyglass.spikesorting.spikesorting_recording.SpikeSortingRecording", "title": "<code>SpikeSortingRecording</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>@schema\nclass SpikeSortingRecording(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingRecordingSelection\n    ---\n    recording_path: varchar(1000)\n    -&gt; IntervalList.proj(sort_interval_list_name='interval_list_name')\n    \"\"\"\n\n    def make(self, key):\n        sort_interval_valid_times = self._get_sort_interval_valid_times(key)\n        recording = self._get_filtered_recording(key)\n        recording_name = self._get_recording_name(key)\n\n        tmp_key = {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": recording_name,\n            \"valid_times\": sort_interval_valid_times,\n        }\n        IntervalList.insert1(tmp_key, replace=True)\n\n        # store the list of valid times for the sort\n        key[\"sort_interval_list_name\"] = tmp_key[\"interval_list_name\"]\n\n        # Path to files that will hold the recording extractors\n        recording_folder = Path(os.getenv(\"SPYGLASS_RECORDING_DIR\"))\n        key[\"recording_path\"] = str(recording_folder / Path(recording_name))\n        if os.path.exists(key[\"recording_path\"]):\n            shutil.rmtree(key[\"recording_path\"])\n        recording = recording.save(\n            folder=key[\"recording_path\"], chunk_duration=\"10000ms\", n_jobs=8\n        )\n\n        self.insert1(key)\n\n    @staticmethod\n    def _get_recording_name(key):\n        recording_name = (\n            key[\"nwb_file_name\"]\n            + \"_\"\n            + key[\"sort_interval_name\"]\n            + \"_\"\n            + str(key[\"sort_group_id\"])\n            + \"_\"\n            + key[\"preproc_params_name\"]\n        )\n        return recording_name\n\n    @staticmethod\n    def _get_recording_timestamps(recording):\n        if recording.get_num_segments() &gt; 1:\n            frames_per_segment = [0]\n            for i in range(recording.get_num_segments()):\n                frames_per_segment.append(\n                    recording.get_num_frames(segment_index=i)\n                )\n\n            cumsum_frames = np.cumsum(frames_per_segment)\n            total_frames = np.sum(frames_per_segment)\n\n            timestamps = np.zeros((total_frames,))\n            for i in range(recording.get_num_segments()):\n                timestamps[\n                    cumsum_frames[i] : cumsum_frames[i + 1]\n                ] = recording.get_times(segment_index=i)\n        else:\n            timestamps = recording.get_times()\n        return timestamps\n\n    def _get_sort_interval_valid_times(self, key):\n\"\"\"Identifies the intersection between sort interval specified by the user\n        and the valid times (times for which neural data exist)\n\n        Parameters\n        ----------\n        key: dict\n            specifies a (partially filled) entry of SpikeSorting table\n\n        Returns\n        -------\n        sort_interval_valid_times: ndarray of tuples\n            (start, end) times for valid stretches of the sorting interval\n\n        \"\"\"\n        sort_interval = (\n            SortInterval\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_interval_name\": key[\"sort_interval_name\"],\n            }\n        ).fetch1(\"sort_interval\")\n        interval_list_name = (SpikeSortingRecordingSelection &amp; key).fetch1(\n            \"interval_list_name\"\n        )\n        valid_interval_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n        valid_sort_times = interval_list_intersect(\n            sort_interval, valid_interval_times\n        )\n        # Exclude intervals shorter than specified length\n        params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        if \"min_segment_length\" in params:\n            valid_sort_times = intervals_by_length(\n                valid_sort_times, min_length=params[\"min_segment_length\"]\n            )\n        return valid_sort_times\n\n    def _get_filtered_recording(self, key: dict):\n\"\"\"Filters and references a recording\n        * Loads the NWB file created during insertion as a spikeinterface Recording\n        * Slices recording in time (interval) and space (channels);\n          recording chunks from disjoint intervals are concatenated\n        * Applies referencing and bandpass filtering\n\n        Parameters\n        ----------\n        key: dict,\n            primary key of SpikeSortingRecording table\n\n        Returns\n        -------\n        recording: si.Recording\n        \"\"\"\n\n        nwb_file_abs_path = Nwbfile().get_abs_path(key[\"nwb_file_name\"])\n        recording = se.read_nwb_recording(\n            nwb_file_abs_path, load_time_vector=True\n        )\n\n        valid_sort_times = self._get_sort_interval_valid_times(key)\n        # shape is (N, 2)\n        valid_sort_times_indices = np.array(\n            [\n                np.searchsorted(recording.get_times(), interval)\n                for interval in valid_sort_times\n            ]\n        )\n        # join intervals of indices that are adjacent\n        valid_sort_times_indices = reduce(\n            union_adjacent_index, valid_sort_times_indices\n        )\n        if valid_sort_times_indices.ndim == 1:\n            valid_sort_times_indices = np.expand_dims(\n                valid_sort_times_indices, 0\n            )\n\n        # create an AppendRecording if there is more than one disjoint sort interval\n        if len(valid_sort_times_indices) &gt; 1:\n            recordings_list = []\n            for interval_indices in valid_sort_times_indices:\n                recording_single = recording.frame_slice(\n                    start_frame=interval_indices[0],\n                    end_frame=interval_indices[1],\n                )\n                recordings_list.append(recording_single)\n            recording = si.append_recordings(recordings_list)\n        else:\n            recording = recording.frame_slice(\n                start_frame=valid_sort_times_indices[0][0],\n                end_frame=valid_sort_times_indices[0][1],\n            )\n\n        channel_ids = (\n            SortGroup.SortGroupElectrode\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch(\"electrode_id\")\n        ref_channel_id = (\n            SortGroup\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch1(\"sort_reference_electrode_id\")\n        channel_ids = np.setdiff1d(channel_ids, ref_channel_id)\n\n        # include ref channel in first slice, then exclude it in second slice\n        if ref_channel_id &gt;= 0:\n            channel_ids_ref = np.append(channel_ids, ref_channel_id)\n            recording = recording.channel_slice(channel_ids=channel_ids_ref)\n\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"single\", ref_channel_ids=ref_channel_id\n            )\n            recording = recording.channel_slice(channel_ids=channel_ids)\n        elif ref_channel_id == -2:\n            recording = recording.channel_slice(channel_ids=channel_ids)\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"global\", operator=\"median\"\n            )\n        else:\n            raise ValueError(\"Invalid reference channel ID\")\n        filter_params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        recording = si.preprocessing.bandpass_filter(\n            recording,\n            freq_min=filter_params[\"frequency_min\"],\n            freq_max=filter_params[\"frequency_max\"],\n        )\n\n        # if the sort group is a tetrode, change the channel location\n        # note that this is a workaround that would be deprecated when spikeinterface uses 3D probe locations\n        probe_type = []\n        electrode_group = []\n        for channel_id in channel_ids:\n            probe_type.append(\n                (\n                    Electrode * Probe\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"probe_type\")\n            )\n            electrode_group.append(\n                (\n                    Electrode\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"electrode_group_name\")\n            )\n        if (\n            all(p == \"tetrode_12.5\" for p in probe_type)\n            and len(probe_type) == 4\n            and all(eg == electrode_group[0] for eg in electrode_group)\n        ):\n            tetrode = pi.Probe(ndim=2)\n            position = [[0, 0], [0, 12.5], [12.5, 0], [12.5, 12.5]]\n            tetrode.set_contacts(\n                position, shapes=\"circle\", shape_params={\"radius\": 6.25}\n            )\n            tetrode.set_contact_ids(channel_ids)\n            tetrode.set_device_channel_indices(np.arange(4))\n            recording = recording.set_probe(tetrode, in_place=True)\n\n        return recording\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/", "title": "spikesorting_sorting.py", "text": ""}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.LabMember", "title": "<code>LabMember</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass LabMember(dj.Manual):\n    definition = \"\"\"\n    lab_member_name: varchar(80)\n    ---\n    first_name: varchar(200)\n    last_name: varchar(200)\n    \"\"\"\n\n    # NOTE that names must be unique here. If there are two neuroscientists named Jack Black that have data in this\n    # database, this will create an incorrect linkage. NWB does not yet provide unique IDs for names.\n\n    class LabMemberInfo(dj.Part):\n        definition = \"\"\"\n        # Information about lab member in the context of Frank lab network\n        -&gt; LabMember\n        ---\n        google_user_name: varchar(200)              # used for permission to curate\n        datajoint_user_name = \"\": varchar(200)      # used for permission to delete entries\n        \"\"\"\n\n    @classmethod\n    def insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab member information from an NWB file.\n\n        Parameters\n        ----------\n        nwbf: pynwb.NWBFile\n            The NWB file with experimenter information.\n        \"\"\"\n        if nwbf.experimenter is None:\n            print(\"No experimenter metadata found.\\n\")\n            return\n        for experimenter in nwbf.experimenter:\n            cls.insert_from_name(experimenter)\n            # each person is by default the member of their own LabTeam (same as their name)\n            LabTeam.create_new_team(\n                team_name=experimenter, team_members=[experimenter]\n            )\n\n    @classmethod\n    def insert_from_name(cls, full_name):\n\"\"\"Insert a lab member by name.\n\n        The first name is the part of the name that precedes the last space, and the last name is the part of the\n        name that follows the last space.\n\n        Parameters\n        ----------\n        full_name : str\n            The name to be added.\n        \"\"\"\n        labmember_dict = dict()\n        labmember_dict[\"lab_member_name\"] = full_name\n        full_name_split = str.split(full_name)\n        labmember_dict[\"first_name\"] = \" \".join(full_name_split[:-1])\n        labmember_dict[\"last_name\"] = full_name_split[-1]\n        cls.insert1(labmember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_lab.LabMember.insert_from_nwbfile", "title": "<code>insert_from_nwbfile(nwbf)</code>  <code>classmethod</code>", "text": "<p>Insert lab member information from an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <p>The NWB file with experimenter information.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_nwbfile(cls, nwbf):\n\"\"\"Insert lab member information from an NWB file.\n\n    Parameters\n    ----------\n    nwbf: pynwb.NWBFile\n        The NWB file with experimenter information.\n    \"\"\"\n    if nwbf.experimenter is None:\n        print(\"No experimenter metadata found.\\n\")\n        return\n    for experimenter in nwbf.experimenter:\n        cls.insert_from_name(experimenter)\n        # each person is by default the member of their own LabTeam (same as their name)\n        LabTeam.create_new_team(\n            team_name=experimenter, team_members=[experimenter]\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_lab.LabMember.insert_from_name", "title": "<code>insert_from_name(full_name)</code>  <code>classmethod</code>", "text": "<p>Insert a lab member by name.</p> <p>The first name is the part of the name that precedes the last space, and the last name is the part of the name that follows the last space.</p> <p>Parameters:</p> Name Type Description Default <code>full_name</code> <code>str</code> <p>The name to be added.</p> required Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef insert_from_name(cls, full_name):\n\"\"\"Insert a lab member by name.\n\n    The first name is the part of the name that precedes the last space, and the last name is the part of the\n    name that follows the last space.\n\n    Parameters\n    ----------\n    full_name : str\n        The name to be added.\n    \"\"\"\n    labmember_dict = dict()\n    labmember_dict[\"lab_member_name\"] = full_name\n    full_name_split = str.split(full_name)\n    labmember_dict[\"first_name\"] = \" \".join(full_name_split[:-1])\n    labmember_dict[\"last_name\"] = full_name_split[-1]\n    cls.insert1(labmember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorterParameters", "title": "<code>SpikeSorterParameters</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>@schema\nclass SpikeSorterParameters(dj.Manual):\n    definition = \"\"\"\n    sorter: varchar(200)\n    sorter_params_name: varchar(200)\n    ---\n    sorter_params: blob\n    \"\"\"\n\n    def insert_default(self):\n\"\"\"Default params from spike sorters available via spikeinterface\"\"\"\n        sorters = sis.available_sorters()\n        for sorter in sorters:\n            sorter_params = sis.get_default_sorter_params(sorter)\n            self.insert1(\n                [sorter, \"default\", sorter_params], skip_duplicates=True\n            )\n\n        # Insert Frank lab defaults\n        # Hippocampus tetrode default\n        sorter = \"mountainsort4\"\n        sorter_params_name = \"franklab_tetrode_hippocampus_30KHz\"\n        sorter_params = {\n            \"detect_sign\": -1,\n            \"adjacency_radius\": 100,\n            \"freq_min\": 600,\n            \"freq_max\": 6000,\n            \"filter\": False,\n            \"whiten\": True,\n            \"num_workers\": 1,\n            \"clip_size\": 40,\n            \"detect_threshold\": 3,\n            \"detect_interval\": 10,\n        }\n        self.insert1(\n            [sorter, sorter_params_name, sorter_params], skip_duplicates=True\n        )\n\n        # Cortical probe default\n        sorter = \"mountainsort4\"\n        sorter_params_name = \"franklab_probe_ctx_30KHz\"\n        sorter_params = {\n            \"detect_sign\": -1,\n            \"adjacency_radius\": 100,\n            \"freq_min\": 300,\n            \"freq_max\": 6000,\n            \"filter\": False,\n            \"whiten\": True,\n            \"num_workers\": 1,\n            \"clip_size\": 40,\n            \"detect_threshold\": 3,\n            \"detect_interval\": 10,\n        }\n        self.insert1(\n            [sorter, sorter_params_name, sorter_params], skip_duplicates=True\n        )\n\n        # clusterless defaults\n        sorter = \"clusterless_thresholder\"\n        sorter_params_name = \"default_clusterless\"\n        sorter_params = dict(\n            detect_threshold=100.0,  # uV\n            # Locally exclusive means one unit per spike detected\n            method=\"locally_exclusive\",\n            peak_sign=\"neg\",\n            exclude_sweep_ms=0.1,\n            local_radius_um=100,\n            # noise levels needs to be 1.0 so the units are in uV and not MAD\n            noise_levels=np.asarray([1.0]),\n            random_chunk_kwargs={},\n            # output needs to be set to sorting for the rest of the pipeline\n            outputs=\"sorting\",\n        )\n        self.insert1(\n            [sorter, sorter_params_name, sorter_params], skip_duplicates=True\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorterParameters.insert_default", "title": "<code>insert_default()</code>", "text": "<p>Default params from spike sorters available via spikeinterface</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def insert_default(self):\n\"\"\"Default params from spike sorters available via spikeinterface\"\"\"\n    sorters = sis.available_sorters()\n    for sorter in sorters:\n        sorter_params = sis.get_default_sorter_params(sorter)\n        self.insert1(\n            [sorter, \"default\", sorter_params], skip_duplicates=True\n        )\n\n    # Insert Frank lab defaults\n    # Hippocampus tetrode default\n    sorter = \"mountainsort4\"\n    sorter_params_name = \"franklab_tetrode_hippocampus_30KHz\"\n    sorter_params = {\n        \"detect_sign\": -1,\n        \"adjacency_radius\": 100,\n        \"freq_min\": 600,\n        \"freq_max\": 6000,\n        \"filter\": False,\n        \"whiten\": True,\n        \"num_workers\": 1,\n        \"clip_size\": 40,\n        \"detect_threshold\": 3,\n        \"detect_interval\": 10,\n    }\n    self.insert1(\n        [sorter, sorter_params_name, sorter_params], skip_duplicates=True\n    )\n\n    # Cortical probe default\n    sorter = \"mountainsort4\"\n    sorter_params_name = \"franklab_probe_ctx_30KHz\"\n    sorter_params = {\n        \"detect_sign\": -1,\n        \"adjacency_radius\": 100,\n        \"freq_min\": 300,\n        \"freq_max\": 6000,\n        \"filter\": False,\n        \"whiten\": True,\n        \"num_workers\": 1,\n        \"clip_size\": 40,\n        \"detect_threshold\": 3,\n        \"detect_interval\": 10,\n    }\n    self.insert1(\n        [sorter, sorter_params_name, sorter_params], skip_duplicates=True\n    )\n\n    # clusterless defaults\n    sorter = \"clusterless_thresholder\"\n    sorter_params_name = \"default_clusterless\"\n    sorter_params = dict(\n        detect_threshold=100.0,  # uV\n        # Locally exclusive means one unit per spike detected\n        method=\"locally_exclusive\",\n        peak_sign=\"neg\",\n        exclude_sweep_ms=0.1,\n        local_radius_um=100,\n        # noise levels needs to be 1.0 so the units are in uV and not MAD\n        noise_levels=np.asarray([1.0]),\n        random_chunk_kwargs={},\n        # output needs to be set to sorting for the rest of the pipeline\n        outputs=\"sorting\",\n    )\n    self.insert1(\n        [sorter, sorter_params_name, sorter_params], skip_duplicates=True\n    )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.LabTeam", "title": "<code>LabTeam</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@schema\nclass LabTeam(dj.Manual):\n    definition = \"\"\"\n    team_name: varchar(80)\n    ---\n    team_description = \"\": varchar(2000)\n    \"\"\"\n\n    class LabTeamMember(dj.Part):\n        definition = \"\"\"\n        -&gt; LabTeam\n        -&gt; LabMember\n        \"\"\"\n\n    @classmethod\n    def create_new_team(\n        cls, team_name: str, team_members: list, team_description: str = \"\"\n    ):\n\"\"\"Create a new team with a list of team members.\n\n        If the lab member does not exist in the database, they will be added.\n\n        Parameters\n        ----------\n        team_name : str\n            The name of the team.\n        team_members : str\n            The full names of the lab members that are part of the team.\n        team_description: str\n            The description of the team.\n        \"\"\"\n        labteam_dict = dict()\n        labteam_dict[\"team_name\"] = team_name\n        labteam_dict[\"team_description\"] = team_description\n        cls.insert1(labteam_dict, skip_duplicates=True)\n\n        for team_member in team_members:\n            LabMember.insert_from_name(team_member)\n            query = (\n                LabMember.LabMemberInfo() &amp; {\"lab_member_name\": team_member}\n            ).fetch(\"google_user_name\")\n            if not len(query):\n                print(\n                    f\"Please add the Google user ID for {team_member} in the LabMember.LabMemberInfo table \"\n                    \"if you want to give them permission to manually curate sorting by this team.\"\n                )\n            labteammember_dict = dict()\n            labteammember_dict[\"team_name\"] = team_name\n            labteammember_dict[\"lab_member_name\"] = team_member\n            cls.LabTeamMember.insert1(labteammember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_lab.LabTeam.create_new_team", "title": "<code>create_new_team(team_name, team_members, team_description='')</code>  <code>classmethod</code>", "text": "<p>Create a new team with a list of team members.</p> <p>If the lab member does not exist in the database, they will be added.</p> <p>Parameters:</p> Name Type Description Default <code>team_name</code> <code>str</code> <p>The name of the team.</p> required <code>team_members</code> <code>str</code> <p>The full names of the lab members that are part of the team.</p> required <code>team_description</code> <code>str</code> <p>The description of the team.</p> <code>''</code> Source code in <code>src/spyglass/common/common_lab.py</code> <pre><code>@classmethod\ndef create_new_team(\n    cls, team_name: str, team_members: list, team_description: str = \"\"\n):\n\"\"\"Create a new team with a list of team members.\n\n    If the lab member does not exist in the database, they will be added.\n\n    Parameters\n    ----------\n    team_name : str\n        The name of the team.\n    team_members : str\n        The full names of the lab members that are part of the team.\n    team_description: str\n        The description of the team.\n    \"\"\"\n    labteam_dict = dict()\n    labteam_dict[\"team_name\"] = team_name\n    labteam_dict[\"team_description\"] = team_description\n    cls.insert1(labteam_dict, skip_duplicates=True)\n\n    for team_member in team_members:\n        LabMember.insert_from_name(team_member)\n        query = (\n            LabMember.LabMemberInfo() &amp; {\"lab_member_name\": team_member}\n        ).fetch(\"google_user_name\")\n        if not len(query):\n            print(\n                f\"Please add the Google user ID for {team_member} in the LabMember.LabMemberInfo table \"\n                \"if you want to give them permission to manually curate sorting by this team.\"\n            )\n        labteammember_dict = dict()\n        labteammember_dict[\"team_name\"] = team_name\n        labteammember_dict[\"lab_member_name\"] = team_member\n        cls.LabTeamMember.insert1(labteammember_dict, skip_duplicates=True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting", "title": "<code>SpikeSorting</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>@schema\nclass SpikeSorting(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingSelection\n    ---\n    sorting_path: varchar(1000)\n    time_of_sort: int   # in Unix time, to the nearest second\n    \"\"\"\n\n    def make(self, key: dict):\n\"\"\"Runs spike sorting on the data and parameters specified by the\n        SpikeSortingSelection table and inserts a new entry to SpikeSorting table.\n\n        Specifically,\n        1. Loads saved recording and runs the sort on it with spikeinterface\n        2. Saves the sorting with spikeinterface\n        3. Creates an analysis NWB file and saves the sorting there\n           (this is redundant with 2; will change in the future)\n\n        \"\"\"\n\n        recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n        recording = si.load_extractor(recording_path)\n\n        # first, get the timestamps\n        timestamps = SpikeSortingRecording._get_recording_timestamps(recording)\n        fs = recording.get_sampling_frequency()\n        # then concatenate the recordings\n        # Note: the timestamps are lost upon concatenation,\n        # i.e. concat_recording.get_times() doesn't return true timestamps anymore.\n        # but concat_recording.recoring_list[i].get_times() will return correct\n        # timestamps for ith recording.\n        if recording.get_num_segments() &gt; 1 and isinstance(\n            recording, si.AppendSegmentRecording\n        ):\n            recording = si.concatenate_recordings(recording.recording_list)\n        elif recording.get_num_segments() &gt; 1 and isinstance(\n            recording, si.BinaryRecordingExtractor\n        ):\n            recording = si.concatenate_recordings([recording])\n\n        # load artifact intervals\n        artifact_times = (\n            ArtifactRemovedIntervalList\n            &amp; {\n                \"artifact_removed_interval_list_name\": key[\n                    \"artifact_removed_interval_list_name\"\n                ]\n            }\n        ).fetch1(\"artifact_times\")\n        if len(artifact_times):\n            if artifact_times.ndim == 1:\n                artifact_times = np.expand_dims(artifact_times, 0)\n\n            # convert artifact intervals to indices\n            list_triggers = []\n            for interval in artifact_times:\n                list_triggers.append(\n                    np.arange(\n                        np.searchsorted(timestamps, interval[0]),\n                        np.searchsorted(timestamps, interval[1]),\n                    )\n                )\n            list_triggers = [list(np.concatenate(list_triggers))]\n            recording = sip.remove_artifacts(\n                recording=recording,\n                list_triggers=list_triggers,\n                ms_before=None,\n                ms_after=None,\n                mode=\"zeros\",\n            )\n\n        print(f\"Running spike sorting on {key}...\")\n        sorter, sorter_params = (SpikeSorterParameters &amp; key).fetch1(\n            \"sorter\", \"sorter_params\"\n        )\n\n        sorter_temp_dir = tempfile.TemporaryDirectory(\n            dir=os.getenv(\"SPYGLASS_TEMP_DIR\")\n        )\n        # add tempdir option for mountainsort\n        sorter_params[\"tempdir\"] = sorter_temp_dir.name\n\n        if sorter == \"clusterless_thresholder\":\n            # need to remove tempdir and whiten from sorter_params\n            sorter_params.pop(\"tempdir\", None)\n            sorter_params.pop(\"whiten\", None)\n\n            # Detect peaks for clusterless decoding\n            detected_spikes = detect_peaks(recording, **sorter_params)\n            sorting = si.NumpySorting.from_times_labels(\n                times_list=detected_spikes[\"sample_ind\"],\n                labels_list=np.zeros(len(detected_spikes), dtype=np.int),\n                sampling_frequency=recording.get_sampling_frequency(),\n            )\n        else:\n            if \"whiten\" in sorter_params.keys():\n                if sorter_params[\"whiten\"]:\n                    sorter_params[\"whiten\"] = False  # set whiten to False\n            # whiten recording separately; make sure dtype is float32\n            # to avoid downstream error with svd\n            recording = sip.whiten(recording, dtype=\"float32\")\n            sorting = sis.run_sorter(\n                sorter,\n                recording,\n                output_folder=sorter_temp_dir.name,\n                delete_output_folder=True,\n                **sorter_params,\n            )\n        key[\"time_of_sort\"] = int(time.time())\n\n        print(\"Saving sorting results...\")\n        sorting_folder = Path(os.getenv(\"SPYGLASS_SORTING_DIR\"))\n        sorting_name = self._get_sorting_name(key)\n        key[\"sorting_path\"] = str(sorting_folder / Path(sorting_name))\n        if os.path.exists(key[\"sorting_path\"]):\n            shutil.rmtree(key[\"sorting_path\"])\n        sorting = sorting.save(folder=key[\"sorting_path\"])\n        self.insert1(key)\n\n    def delete(self):\n\"\"\"Extends the delete method of base class to implement permission checking.\n        Note that this is NOT a security feature, as anyone that has access to source code\n        can disable it; it just makes it less likely to accidentally delete entries.\n        \"\"\"\n        current_user_name = dj.config[\"database.user\"]\n        entries = self.fetch()\n        permission_bool = np.zeros((len(entries),))\n        print(\n            f\"Attempting to delete {len(entries)} entries, checking permission...\"\n        )\n\n        for entry_idx in range(len(entries)):\n            # check the team name for the entry, then look up the members in that team,\n            # then get their datajoint user names\n            team_name = (\n                SpikeSortingRecordingSelection\n                &amp; (SpikeSortingRecordingSelection &amp; entries[entry_idx]).proj()\n            ).fetch1()[\"team_name\"]\n            lab_member_name_list = (\n                LabTeam.LabTeamMember &amp; {\"team_name\": team_name}\n            ).fetch(\"lab_member_name\")\n            datajoint_user_names = []\n            for lab_member_name in lab_member_name_list:\n                datajoint_user_names.append(\n                    (\n                        LabMember.LabMemberInfo\n                        &amp; {\"lab_member_name\": lab_member_name}\n                    ).fetch1(\"datajoint_user_name\")\n                )\n            permission_bool[entry_idx] = (\n                current_user_name in datajoint_user_names\n            )\n        if np.sum(permission_bool) == len(entries):\n            print(\"Permission to delete all specified entries granted.\")\n            super().delete()\n        else:\n            raise Exception(\n                \"You do not have permission to delete all specified\"\n                \"entries. Not deleting anything.\"\n            )\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        raise NotImplementedError\n        return None\n        # return fetch_nwb(self, (AnalysisNwbfile, 'analysis_file_abs_path'), *attrs, **kwargs)\n\n    def nightly_cleanup(self):\n\"\"\"Clean up spike sorting directories that are not in the SpikeSorting table.\n        This should be run after AnalysisNwbFile().nightly_cleanup()\n        \"\"\"\n        # get a list of the files in the spike sorting storage directory\n        dir_names = next(os.walk(os.environ[\"SPYGLASS_SORTING_DIR\"]))[1]\n        # now retrieve a list of the currently used analysis nwb files\n        analysis_file_names = self.fetch(\"analysis_file_name\")\n        for dir in dir_names:\n            if dir not in analysis_file_names:\n                full_path = str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n                print(f\"removing {full_path}\")\n                shutil.rmtree(\n                    str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n                )\n\n    @staticmethod\n    def _get_sorting_name(key):\n        recording_name = SpikeSortingRecording._get_recording_name(key)\n        sorting_name = (\n            recording_name + \"_\" + str(uuid.uuid4())[0:8] + \"_spikesorting\"\n        )\n        return sorting_name\n\n    # TODO: write a function to import sorting done outside of dj\n\n    def _import_sorting(self, key):\n        raise NotImplementedError\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.make", "title": "<code>make(key)</code>", "text": "<p>Runs spike sorting on the data and parameters specified by the SpikeSortingSelection table and inserts a new entry to SpikeSorting table.</p> <p>Specifically, 1. Loads saved recording and runs the sort on it with spikeinterface 2. Saves the sorting with spikeinterface 3. Creates an analysis NWB file and saves the sorting there    (this is redundant with 2; will change in the future)</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def make(self, key: dict):\n\"\"\"Runs spike sorting on the data and parameters specified by the\n    SpikeSortingSelection table and inserts a new entry to SpikeSorting table.\n\n    Specifically,\n    1. Loads saved recording and runs the sort on it with spikeinterface\n    2. Saves the sorting with spikeinterface\n    3. Creates an analysis NWB file and saves the sorting there\n       (this is redundant with 2; will change in the future)\n\n    \"\"\"\n\n    recording_path = (SpikeSortingRecording &amp; key).fetch1(\"recording_path\")\n    recording = si.load_extractor(recording_path)\n\n    # first, get the timestamps\n    timestamps = SpikeSortingRecording._get_recording_timestamps(recording)\n    fs = recording.get_sampling_frequency()\n    # then concatenate the recordings\n    # Note: the timestamps are lost upon concatenation,\n    # i.e. concat_recording.get_times() doesn't return true timestamps anymore.\n    # but concat_recording.recoring_list[i].get_times() will return correct\n    # timestamps for ith recording.\n    if recording.get_num_segments() &gt; 1 and isinstance(\n        recording, si.AppendSegmentRecording\n    ):\n        recording = si.concatenate_recordings(recording.recording_list)\n    elif recording.get_num_segments() &gt; 1 and isinstance(\n        recording, si.BinaryRecordingExtractor\n    ):\n        recording = si.concatenate_recordings([recording])\n\n    # load artifact intervals\n    artifact_times = (\n        ArtifactRemovedIntervalList\n        &amp; {\n            \"artifact_removed_interval_list_name\": key[\n                \"artifact_removed_interval_list_name\"\n            ]\n        }\n    ).fetch1(\"artifact_times\")\n    if len(artifact_times):\n        if artifact_times.ndim == 1:\n            artifact_times = np.expand_dims(artifact_times, 0)\n\n        # convert artifact intervals to indices\n        list_triggers = []\n        for interval in artifact_times:\n            list_triggers.append(\n                np.arange(\n                    np.searchsorted(timestamps, interval[0]),\n                    np.searchsorted(timestamps, interval[1]),\n                )\n            )\n        list_triggers = [list(np.concatenate(list_triggers))]\n        recording = sip.remove_artifacts(\n            recording=recording,\n            list_triggers=list_triggers,\n            ms_before=None,\n            ms_after=None,\n            mode=\"zeros\",\n        )\n\n    print(f\"Running spike sorting on {key}...\")\n    sorter, sorter_params = (SpikeSorterParameters &amp; key).fetch1(\n        \"sorter\", \"sorter_params\"\n    )\n\n    sorter_temp_dir = tempfile.TemporaryDirectory(\n        dir=os.getenv(\"SPYGLASS_TEMP_DIR\")\n    )\n    # add tempdir option for mountainsort\n    sorter_params[\"tempdir\"] = sorter_temp_dir.name\n\n    if sorter == \"clusterless_thresholder\":\n        # need to remove tempdir and whiten from sorter_params\n        sorter_params.pop(\"tempdir\", None)\n        sorter_params.pop(\"whiten\", None)\n\n        # Detect peaks for clusterless decoding\n        detected_spikes = detect_peaks(recording, **sorter_params)\n        sorting = si.NumpySorting.from_times_labels(\n            times_list=detected_spikes[\"sample_ind\"],\n            labels_list=np.zeros(len(detected_spikes), dtype=np.int),\n            sampling_frequency=recording.get_sampling_frequency(),\n        )\n    else:\n        if \"whiten\" in sorter_params.keys():\n            if sorter_params[\"whiten\"]:\n                sorter_params[\"whiten\"] = False  # set whiten to False\n        # whiten recording separately; make sure dtype is float32\n        # to avoid downstream error with svd\n        recording = sip.whiten(recording, dtype=\"float32\")\n        sorting = sis.run_sorter(\n            sorter,\n            recording,\n            output_folder=sorter_temp_dir.name,\n            delete_output_folder=True,\n            **sorter_params,\n        )\n    key[\"time_of_sort\"] = int(time.time())\n\n    print(\"Saving sorting results...\")\n    sorting_folder = Path(os.getenv(\"SPYGLASS_SORTING_DIR\"))\n    sorting_name = self._get_sorting_name(key)\n    key[\"sorting_path\"] = str(sorting_folder / Path(sorting_name))\n    if os.path.exists(key[\"sorting_path\"]):\n        shutil.rmtree(key[\"sorting_path\"])\n    sorting = sorting.save(folder=key[\"sorting_path\"])\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.delete", "title": "<code>delete()</code>", "text": "<p>Extends the delete method of base class to implement permission checking. Note that this is NOT a security feature, as anyone that has access to source code can disable it; it just makes it less likely to accidentally delete entries.</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def delete(self):\n\"\"\"Extends the delete method of base class to implement permission checking.\n    Note that this is NOT a security feature, as anyone that has access to source code\n    can disable it; it just makes it less likely to accidentally delete entries.\n    \"\"\"\n    current_user_name = dj.config[\"database.user\"]\n    entries = self.fetch()\n    permission_bool = np.zeros((len(entries),))\n    print(\n        f\"Attempting to delete {len(entries)} entries, checking permission...\"\n    )\n\n    for entry_idx in range(len(entries)):\n        # check the team name for the entry, then look up the members in that team,\n        # then get their datajoint user names\n        team_name = (\n            SpikeSortingRecordingSelection\n            &amp; (SpikeSortingRecordingSelection &amp; entries[entry_idx]).proj()\n        ).fetch1()[\"team_name\"]\n        lab_member_name_list = (\n            LabTeam.LabTeamMember &amp; {\"team_name\": team_name}\n        ).fetch(\"lab_member_name\")\n        datajoint_user_names = []\n        for lab_member_name in lab_member_name_list:\n            datajoint_user_names.append(\n                (\n                    LabMember.LabMemberInfo\n                    &amp; {\"lab_member_name\": lab_member_name}\n                ).fetch1(\"datajoint_user_name\")\n            )\n        permission_bool[entry_idx] = (\n            current_user_name in datajoint_user_names\n        )\n    if np.sum(permission_bool) == len(entries):\n        print(\"Permission to delete all specified entries granted.\")\n        super().delete()\n    else:\n        raise Exception(\n            \"You do not have permission to delete all specified\"\n            \"entries. Not deleting anything.\"\n        )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSorting.nightly_cleanup", "title": "<code>nightly_cleanup()</code>", "text": "<p>Clean up spike sorting directories that are not in the SpikeSorting table. This should be run after AnalysisNwbFile().nightly_cleanup()</p> Source code in <code>src/spyglass/spikesorting/spikesorting_sorting.py</code> <pre><code>def nightly_cleanup(self):\n\"\"\"Clean up spike sorting directories that are not in the SpikeSorting table.\n    This should be run after AnalysisNwbFile().nightly_cleanup()\n    \"\"\"\n    # get a list of the files in the spike sorting storage directory\n    dir_names = next(os.walk(os.environ[\"SPYGLASS_SORTING_DIR\"]))[1]\n    # now retrieve a list of the currently used analysis nwb files\n    analysis_file_names = self.fetch(\"analysis_file_name\")\n    for dir in dir_names:\n        if dir not in analysis_file_names:\n            full_path = str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n            print(f\"removing {full_path}\")\n            shutil.rmtree(\n                str(Path(os.environ[\"SPYGLASS_SORTING_DIR\"]) / dir)\n            )\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.AnalysisNwbfile", "title": "<code>AnalysisNwbfile</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@schema\nclass AnalysisNwbfile(dj.Manual):\n    definition = \"\"\"\n    # Table for holding the NWB files that contain results of analysis, such as spike sorting.\n    analysis_file_name: varchar(255)               # name of the file\n    ---\n    -&gt; Nwbfile                                     # name of the parent NWB file. Used for naming and metadata copy\n    analysis_file_abs_path: filepath@analysis      # the full path to the file\n    analysis_file_description = \"\": varchar(2000)  # an optional description of this analysis\n    analysis_parameters = NULL: blob               # additional relevant parameters. Currently used only for analyses\n                                                   # that span multiple NWB files\n    INDEX (analysis_file_abs_path)\n    \"\"\"\n    # NOTE the INDEX above is implicit from filepath@... above but needs to be explicit\n    # so that alter() can work\n\n    def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of an NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # pop off the unnecessary elements to save space\n            nwb_fields = nwbf.fields\n            for field in nwb_fields:\n                if field not in NWB_KEEP_FIELDS:\n                    nwb_object = getattr(nwbf, field)\n                    if isinstance(nwb_object, pynwb.core.LabelledDict):\n                        for module in list(nwb_object.keys()):\n                            nwb_object.pop(module)\n\n            analysis_file_name = self.__get_new_file_name(nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        # change the permissions to only allow owner to write\n        permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n        os.chmod(analysis_file_abs_path, permissions)\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_new_file_name(cls, nwb_file_name):\n        # each file ends with a random string of 10 digits, so we generate that string and redo if by some miracle\n        # it's already there\n        file_in_table = True\n        while file_in_table:\n            analysis_file_name = (\n                os.path.splitext(nwb_file_name)[0]\n                + \"\".join(\n                    random.choices(string.ascii_uppercase + string.digits, k=10)\n                )\n                + \".nwb\"\n            )\n            file_in_table = AnalysisNwbfile &amp; {\n                \"analysis_file_name\": analysis_file_name\n            }\n\n        return analysis_file_name\n\n    @classmethod\n    def __get_analysis_file_dir(cls, analysis_file_name: str):\n        # strip off everything after and including the final underscore and return the result\n        return analysis_file_name[0 : analysis_file_name.rfind(\"_\")]\n\n    @classmethod\n    def copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n        Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the analysis NWB file to be copied.\n\n        Returns\n        -------\n        analysis_file_name : str\n            The name of the new NWB file.\n        \"\"\"\n        nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n        with pynwb.NWBHDF5IO(\n            path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n        ) as io:\n            nwbf = io.read()\n            # get the current number of analysis files related to this nwb file\n            query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n            original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n            analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n            # write the new file\n            print(f\"Writing new NWB file {analysis_file_name}...\")\n            analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n                analysis_file_name\n            )\n            # export the new NWB file\n            with pynwb.NWBHDF5IO(\n                path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n            ) as export_io:\n                export_io.export(io, nwbf)\n\n        return analysis_file_name\n\n    def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n        Parameters\n        ----------\n        nwb_file_name : str\n            The name of the parent NWB file.\n        analysis_file_name : str\n            The name of the analysis NWB file that was created.\n        \"\"\"\n        key = dict()\n        key[\"nwb_file_name\"] = nwb_file_name\n        key[\"analysis_file_name\"] = analysis_file_name\n        key[\"analysis_file_description\"] = \"\"\n        key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        self.insert1(key)\n\n    @staticmethod\n    def get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n        The SPYGLASS_BASE_DIR environment variable must be set.\n\n        Parameters\n        ----------\n        analysis_nwb_file_name : str\n            The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n        Returns\n        -------\n        analysis_nwb_file_abspath : str\n            The absolute path for the given file name.\n        \"\"\"\n        base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n        assert (\n            base_dir is not None\n        ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n        # see if the file exists and is stored in the base analysis dir\n        test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n        if os.path.exists(test_path):\n            return test_path\n        else:\n            # use the new path\n            analysis_file_base_path = (\n                base_dir\n                / \"analysis\"\n                / AnalysisNwbfile.__get_analysis_file_dir(\n                    analysis_nwb_file_name\n                )\n            )\n            if not analysis_file_base_path.exists():\n                os.mkdir(str(analysis_file_base_path))\n            return str(analysis_file_base_path / analysis_nwb_file_name)\n\n    def add_nwb_object(\n        self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n    ):\n        # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n        # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        nwb_object : pynwb.core.NWBDataInterface\n            The NWB object created by PyNWB.\n        table_name : str (optional, defaults to 'pandas_table')\n            The name of the DynamicTable made from a dataframe.\n\n        Returns\n        -------\n        nwb_object_id : str\n            The NWB object ID of the added object.\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            if isinstance(nwb_object, pd.DataFrame):\n                dt_object = DynamicTable.from_dataframe(\n                    name=table_name, df=nwb_object\n                )\n                nwbf.add_scratch(dt_object)\n                io.write(nwbf)\n                return dt_object.object_id\n            else:\n                nwbf.add_scratch(nwb_object)\n                io.write(nwbf)\n                return nwb_object.object_id\n\n    def add_units(\n        self,\n        analysis_file_name,\n        units,\n        units_valid_times,\n        units_sort_interval,\n        metrics=None,\n        units_waveforms=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        units : dict\n            keys are unit ids, values are spike times\n        units_valid_times : dict\n            Dictionary of units and valid times with unit ids as keys.\n        units_sort_interval : dict\n            Dictionary of units and sort_interval with unit ids as keys.\n        units_waveforms : dict, optional\n            Dictionary of unit waveforms with unit ids as keys.\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id, waveforms_object_id : str, str\n            The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n        \"\"\"\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            sort_intervals = list()\n            if len(units.keys()):\n                # Add spike times and valid time range for the sort\n                for id in units.keys():\n                    nwbf.add_unit(\n                        spike_times=units[id],\n                        id=id,\n                        # waveform_mean = units_templates[id],\n                        obs_intervals=units_valid_times[id],\n                    )\n                    sort_intervals.append(units_sort_interval[id])\n                # Add a column for the sort interval (subset of valid time)\n                nwbf.add_unit_column(\n                    name=\"sort_interval\",\n                    description=\"the interval used for spike sorting\",\n                    data=sort_intervals,\n                )\n                # If metrics were specified, add one column per metric\n                if metrics is not None:\n                    for metric in metrics:\n                        if metrics[metric]:\n                            unit_ids = np.array(list(metrics[metric].keys()))\n                            metric_values = np.array(\n                                list(metrics[metric].values())\n                            )\n                            # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                            metric_values = metric_values[np.argsort(unit_ids)]\n                            print(f\"Adding metric {metric} : {metric_values}\")\n                            nwbf.add_unit_column(\n                                name=metric,\n                                description=f\"{metric} metric\",\n                                data=metric_values,\n                            )\n                if labels is not None:\n                    unit_ids = np.array(list(units.keys()))\n                    for unit in unit_ids:\n                        if unit not in labels:\n                            labels[unit] = \"\"\n                    label_values = np.array(list(labels.values()))\n                    label_values = label_values[np.argsort(unit_ids)].tolist()\n                    nwbf.add_unit_column(\n                        name=\"label\",\n                        description=\"label given during curation\",\n                        data=label_values,\n                    )\n                # If the waveforms were specified, add them as a dataframe to scratch\n                waveforms_object_id = \"\"\n                if units_waveforms is not None:\n                    waveforms_df = pd.DataFrame.from_dict(\n                        units_waveforms, orient=\"index\"\n                    )\n                    waveforms_df.columns = [\"waveforms\"]\n                    nwbf.add_scratch(\n                        waveforms_df,\n                        name=\"units_waveforms\",\n                        notes=\"spike waveforms for each unit\",\n                    )\n                    waveforms_object_id = nwbf.scratch[\n                        \"units_waveforms\"\n                    ].object_id\n\n                io.write(nwbf)\n                return nwbf.units.object_id, waveforms_object_id\n            else:\n                return \"\"\n\n    def add_units_waveforms(\n        self,\n        analysis_file_name,\n        waveform_extractor: si.WaveformExtractor,\n        metrics=None,\n        labels=None,\n    ):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        waveform_extractor : si.WaveformExtractor object\n        metrics : dict, optional\n            Cluster metrics.\n        labels : dict, optional\n            Curation labels for clusters\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in waveform_extractor.sorting.get_unit_ids():\n                # (spikes, samples, channels)\n                waveforms = waveform_extractor.get_waveforms(unit_id=id)\n                # (channels, spikes, samples)\n                waveforms = np.moveaxis(waveforms, source=2, destination=0)\n                nwbf.add_unit(\n                    spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                        unit_id=id\n                    ),\n                    id=id,\n                    electrodes=waveform_extractor.recording.get_channel_ids(),\n                    waveforms=waveforms,\n                )\n\n            # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n            # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n            # or\n            # nwbfile = pynwb.NWBFile(...)\n            # (channels, spikes, samples)\n            # wfs = [\n            #         [     # elec 1\n            #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 2\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ], [  # elec 3\n            #             [1, 2, 3],  # spike 1\n            #             [1, 2, 3],  # spike 2\n            #             [1, 2, 3],  # spike 3\n            #             [1, 2, 3]   # spike 4\n            #         ]\n            # ]\n            # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n            # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric_name, metric_dict in metrics.items():\n                    print(f\"Adding metric {metric_name} : {metric_dict}\")\n                    metric_data = metric_dict.values().to_list()\n                    nwbf.add_unit_column(\n                        name=metric_name,\n                        description=metric_name,\n                        data=metric_data,\n                    )\n            if labels is not None:\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=labels,\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        metrics : dict, optional\n            Cluster metrics.\n\n        Returns\n        -------\n        units_object_id : str\n            The NWB object id of the Units object\n        \"\"\"\n        metric_names = list(metrics.keys())\n        unit_ids = list(metrics[metric_names[0]].keys())\n        with pynwb.NWBHDF5IO(\n            path=self.get_abs_path(analysis_file_name),\n            mode=\"a\",\n            load_namespaces=True,\n        ) as io:\n            nwbf = io.read()\n            for id in unit_ids:\n                nwbf.add_unit(id=id)\n\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = list(metric_dict.values())\n                nwbf.add_unit_column(\n                    name=metric_name, description=metric_name, data=metric_data\n                )\n\n            io.write(nwbf)\n            return nwbf.units.object_id\n\n    @classmethod\n    def get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n        Parameters\n        ----------\n        analysis_file_name : str\n            The name of the analysis NWB file.\n        electrode_ids : numpy array or list\n            Array or list of electrode IDs.\n\n        Returns\n        -------\n        electrode_indices : numpy array\n            Array of indices in the electrodes table for the given electrode IDs.\n        \"\"\"\n        nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n        return get_electrode_indices(nwbf.electrodes, electrode_ids)\n\n    @staticmethod\n    def cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n        Does not delete the files themselves unless delete_files=True is specified.\n        Run this after deleting the Nwbfile() entries themselves.\n\n        Parameters\n        ----------\n        delete_files : bool, optional\n            Whether the original files should be deleted (default False).\n        \"\"\"\n        schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n\n    @staticmethod\n    def nightly_cleanup():\n        child_tables = get_child_tables(AnalysisNwbfile)\n        (AnalysisNwbfile - child_tables).delete_quick()\n\n        # a separate external files clean up required - this is to be done\n        # during times when no other transactions are in progress.\n        AnalysisNwbfile.cleanup(True)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.create", "title": "<code>create(nwb_file_name)</code>", "text": "<p>Open the NWB file, create a copy, write the copy to disk and return the name of the new file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of an NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def create(self, nwb_file_name):\n\"\"\"Open the NWB file, create a copy, write the copy to disk and return the name of the new file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of an NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = Nwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # pop off the unnecessary elements to save space\n        nwb_fields = nwbf.fields\n        for field in nwb_fields:\n            if field not in NWB_KEEP_FIELDS:\n                nwb_object = getattr(nwbf, field)\n                if isinstance(nwb_object, pynwb.core.LabelledDict):\n                    for module in list(nwb_object.keys()):\n                        nwb_object.pop(module)\n\n        analysis_file_name = self.__get_new_file_name(nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    # change the permissions to only allow owner to write\n    permissions = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH\n    os.chmod(analysis_file_abs_path, permissions)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.copy", "title": "<code>copy(nwb_file_name)</code>  <code>classmethod</code>", "text": "<p>Make a copy of an analysis NWB file.</p> <p>Note that this does NOT add the file to the schema; that needs to be done after data are written to it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the analysis NWB file to be copied.</p> required <p>Returns:</p> Name Type Description <code>analysis_file_name</code> <code>str</code> <p>The name of the new NWB file.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef copy(cls, nwb_file_name):\n\"\"\"Make a copy of an analysis NWB file.\n\n    Note that this does NOT add the file to the schema; that needs to be done after data are written to it.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the analysis NWB file to be copied.\n\n    Returns\n    -------\n    analysis_file_name : str\n        The name of the new NWB file.\n    \"\"\"\n    nwb_file_abspath = AnalysisNwbfile.get_abs_path(nwb_file_name)\n    with pynwb.NWBHDF5IO(\n        path=nwb_file_abspath, mode=\"r\", load_namespaces=True\n    ) as io:\n        nwbf = io.read()\n        # get the current number of analysis files related to this nwb file\n        query = AnalysisNwbfile &amp; {\"analysis_file_name\": nwb_file_name}\n        original_nwb_file_name = query.fetch(\"nwb_file_name\")[0]\n        analysis_file_name = cls.__get_new_file_name(original_nwb_file_name)\n        # write the new file\n        print(f\"Writing new NWB file {analysis_file_name}...\")\n        analysis_file_abs_path = AnalysisNwbfile.get_abs_path(\n            analysis_file_name\n        )\n        # export the new NWB file\n        with pynwb.NWBHDF5IO(\n            path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n        ) as export_io:\n            export_io.export(io, nwbf)\n\n    return analysis_file_name\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add", "title": "<code>add(nwb_file_name, analysis_file_name)</code>", "text": "<p>Add the specified file to AnalysisNWBfile table.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_name</code> <code>str</code> <p>The name of the parent NWB file.</p> required <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file that was created.</p> required Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add(self, nwb_file_name, analysis_file_name):\n\"\"\"Add the specified file to AnalysisNWBfile table.\n\n    Parameters\n    ----------\n    nwb_file_name : str\n        The name of the parent NWB file.\n    analysis_file_name : str\n        The name of the analysis NWB file that was created.\n    \"\"\"\n    key = dict()\n    key[\"nwb_file_name\"] = nwb_file_name\n    key[\"analysis_file_name\"] = analysis_file_name\n    key[\"analysis_file_description\"] = \"\"\n    key[\"analysis_file_abs_path\"] = AnalysisNwbfile.get_abs_path(\n        analysis_file_name\n    )\n    self.insert1(key)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_abs_path", "title": "<code>get_abs_path(analysis_nwb_file_name)</code>  <code>staticmethod</code>", "text": "<p>Return the absolute path for a stored analysis NWB file given just the file name.</p> <p>The SPYGLASS_BASE_DIR environment variable must be set.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_nwb_file_name</code> <code>str</code> <p>The name of the NWB file that has been inserted into the AnalysisNwbfile() schema</p> required <p>Returns:</p> Name Type Description <code>analysis_nwb_file_abspath</code> <code>str</code> <p>The absolute path for the given file name.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef get_abs_path(analysis_nwb_file_name):\n\"\"\"Return the absolute path for a stored analysis NWB file given just the file name.\n\n    The SPYGLASS_BASE_DIR environment variable must be set.\n\n    Parameters\n    ----------\n    analysis_nwb_file_name : str\n        The name of the NWB file that has been inserted into the AnalysisNwbfile() schema\n\n    Returns\n    -------\n    analysis_nwb_file_abspath : str\n        The absolute path for the given file name.\n    \"\"\"\n    base_dir = pathlib.Path(os.getenv(\"SPYGLASS_BASE_DIR\", None))\n    assert (\n        base_dir is not None\n    ), \"You must set SPYGLASS_BASE_DIR environment variable.\"\n\n    # see if the file exists and is stored in the base analysis dir\n    test_path = str(base_dir / \"analysis\" / analysis_nwb_file_name)\n\n    if os.path.exists(test_path):\n        return test_path\n    else:\n        # use the new path\n        analysis_file_base_path = (\n            base_dir\n            / \"analysis\"\n            / AnalysisNwbfile.__get_analysis_file_dir(\n                analysis_nwb_file_name\n            )\n        )\n        if not analysis_file_base_path.exists():\n            os.mkdir(str(analysis_file_base_path))\n        return str(analysis_file_base_path / analysis_nwb_file_name)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_nwb_object", "title": "<code>add_nwb_object(analysis_file_name, nwb_object, table_name='pandas_table')</code>", "text": "<p>Add an NWB object to the analysis file in the scratch area and returns the NWB object ID</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>nwb_object</code> <code>pynwb.core.NWBDataInterface</code> <p>The NWB object created by PyNWB.</p> required <code>table_name</code> <code>str (optional, defaults to 'pandas_table')</code> <p>The name of the DynamicTable made from a dataframe.</p> <code>'pandas_table'</code> <p>Returns:</p> Name Type Description <code>nwb_object_id</code> <code>str</code> <p>The NWB object ID of the added object.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_nwb_object(\n    self, analysis_file_name, nwb_object, table_name=\"pandas_table\"\n):\n    # TODO: change to add_object with checks for object type and a name parameter, which should be specified if\n    # it is not an NWB container\n\"\"\"Add an NWB object to the analysis file in the scratch area and returns the NWB object ID\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    nwb_object : pynwb.core.NWBDataInterface\n        The NWB object created by PyNWB.\n    table_name : str (optional, defaults to 'pandas_table')\n        The name of the DynamicTable made from a dataframe.\n\n    Returns\n    -------\n    nwb_object_id : str\n        The NWB object ID of the added object.\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        if isinstance(nwb_object, pd.DataFrame):\n            dt_object = DynamicTable.from_dataframe(\n                name=table_name, df=nwb_object\n            )\n            nwbf.add_scratch(dt_object)\n            io.write(nwbf)\n            return dt_object.object_id\n        else:\n            nwbf.add_scratch(nwb_object)\n            io.write(nwbf)\n            return nwb_object.object_id\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units", "title": "<code>add_units(analysis_file_name, units, units_valid_times, units_sort_interval, metrics=None, units_waveforms=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>units</code> <code>dict</code> <p>keys are unit ids, values are spike times</p> required <code>units_valid_times</code> <code>dict</code> <p>Dictionary of units and valid times with unit ids as keys.</p> required <code>units_sort_interval</code> <code>dict</code> <p>Dictionary of units and sort_interval with unit ids as keys.</p> required <code>units_waveforms</code> <code>dict</code> <p>Dictionary of unit waveforms with unit ids as keys.</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Type Description <code>units_object_id, waveforms_object_id : str, str</code> <p>The NWB object id of the Units object and the object id of the waveforms object ('' if None)</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units(\n    self,\n    analysis_file_name,\n    units,\n    units_valid_times,\n    units_sort_interval,\n    metrics=None,\n    units_waveforms=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    units : dict\n        keys are unit ids, values are spike times\n    units_valid_times : dict\n        Dictionary of units and valid times with unit ids as keys.\n    units_sort_interval : dict\n        Dictionary of units and sort_interval with unit ids as keys.\n    units_waveforms : dict, optional\n        Dictionary of unit waveforms with unit ids as keys.\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id, waveforms_object_id : str, str\n        The NWB object id of the Units object and the object id of the waveforms object ('' if None)\n    \"\"\"\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        sort_intervals = list()\n        if len(units.keys()):\n            # Add spike times and valid time range for the sort\n            for id in units.keys():\n                nwbf.add_unit(\n                    spike_times=units[id],\n                    id=id,\n                    # waveform_mean = units_templates[id],\n                    obs_intervals=units_valid_times[id],\n                )\n                sort_intervals.append(units_sort_interval[id])\n            # Add a column for the sort interval (subset of valid time)\n            nwbf.add_unit_column(\n                name=\"sort_interval\",\n                description=\"the interval used for spike sorting\",\n                data=sort_intervals,\n            )\n            # If metrics were specified, add one column per metric\n            if metrics is not None:\n                for metric in metrics:\n                    if metrics[metric]:\n                        unit_ids = np.array(list(metrics[metric].keys()))\n                        metric_values = np.array(\n                            list(metrics[metric].values())\n                        )\n                        # sort by unit_ids and apply that sorting to values to ensure that things go in the right order\n                        metric_values = metric_values[np.argsort(unit_ids)]\n                        print(f\"Adding metric {metric} : {metric_values}\")\n                        nwbf.add_unit_column(\n                            name=metric,\n                            description=f\"{metric} metric\",\n                            data=metric_values,\n                        )\n            if labels is not None:\n                unit_ids = np.array(list(units.keys()))\n                for unit in unit_ids:\n                    if unit not in labels:\n                        labels[unit] = \"\"\n                label_values = np.array(list(labels.values()))\n                label_values = label_values[np.argsort(unit_ids)].tolist()\n                nwbf.add_unit_column(\n                    name=\"label\",\n                    description=\"label given during curation\",\n                    data=label_values,\n                )\n            # If the waveforms were specified, add them as a dataframe to scratch\n            waveforms_object_id = \"\"\n            if units_waveforms is not None:\n                waveforms_df = pd.DataFrame.from_dict(\n                    units_waveforms, orient=\"index\"\n                )\n                waveforms_df.columns = [\"waveforms\"]\n                nwbf.add_scratch(\n                    waveforms_df,\n                    name=\"units_waveforms\",\n                    notes=\"spike waveforms for each unit\",\n                )\n                waveforms_object_id = nwbf.scratch[\n                    \"units_waveforms\"\n                ].object_id\n\n            io.write(nwbf)\n            return nwbf.units.object_id, waveforms_object_id\n        else:\n            return \"\"\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_waveforms", "title": "<code>add_units_waveforms(analysis_file_name, waveform_extractor, metrics=None, labels=None)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>waveform_extractor</code> <code>si.WaveformExtractor object</code> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> <code>None</code> <code>labels</code> <code>dict</code> <p>Curation labels for clusters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_waveforms(\n    self,\n    analysis_file_name,\n    waveform_extractor: si.WaveformExtractor,\n    metrics=None,\n    labels=None,\n):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    waveform_extractor : si.WaveformExtractor object\n    metrics : dict, optional\n        Cluster metrics.\n    labels : dict, optional\n        Curation labels for clusters\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in waveform_extractor.sorting.get_unit_ids():\n            # (spikes, samples, channels)\n            waveforms = waveform_extractor.get_waveforms(unit_id=id)\n            # (channels, spikes, samples)\n            waveforms = np.moveaxis(waveforms, source=2, destination=0)\n            nwbf.add_unit(\n                spike_times=waveform_extractor.sorting.get_unit_spike_train(\n                    unit_id=id\n                ),\n                id=id,\n                electrodes=waveform_extractor.recording.get_channel_ids(),\n                waveforms=waveforms,\n            )\n\n        # The following is a rough sketch of AnalysisNwbfile().add_waveforms\n        # analysis_file_name = AnalysisNwbfile().create(key['nwb_file_name'])\n        # or\n        # nwbfile = pynwb.NWBFile(...)\n        # (channels, spikes, samples)\n        # wfs = [\n        #         [     # elec 1\n        #             [1, 2, 3],  # spike 1, [sample 1, sample 2, sample 3]\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 2\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ], [  # elec 3\n        #             [1, 2, 3],  # spike 1\n        #             [1, 2, 3],  # spike 2\n        #             [1, 2, 3],  # spike 3\n        #             [1, 2, 3]   # spike 4\n        #         ]\n        # ]\n        # elecs = ... # DynamicTableRegion referring to three electrodes (rows) of the electrodes table\n        # nwbfile.add_unit(spike_times=[1, 2, 3], electrodes=elecs, waveforms=wfs)\n\n        # If metrics were specified, add one column per metric\n        if metrics is not None:\n            for metric_name, metric_dict in metrics.items():\n                print(f\"Adding metric {metric_name} : {metric_dict}\")\n                metric_data = metric_dict.values().to_list()\n                nwbf.add_unit_column(\n                    name=metric_name,\n                    description=metric_name,\n                    data=metric_data,\n                )\n        if labels is not None:\n            nwbf.add_unit_column(\n                name=\"label\",\n                description=\"label given during curation\",\n                data=labels,\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.add_units_metrics", "title": "<code>add_units_metrics(analysis_file_name, metrics)</code>", "text": "<p>Add units to analysis NWB file along with the waveforms</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>metrics</code> <code>dict</code> <p>Cluster metrics.</p> required <p>Returns:</p> Name Type Description <code>units_object_id</code> <code>str</code> <p>The NWB object id of the Units object</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>def add_units_metrics(self, analysis_file_name, metrics):\n\"\"\"Add units to analysis NWB file along with the waveforms\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    metrics : dict, optional\n        Cluster metrics.\n\n    Returns\n    -------\n    units_object_id : str\n        The NWB object id of the Units object\n    \"\"\"\n    metric_names = list(metrics.keys())\n    unit_ids = list(metrics[metric_names[0]].keys())\n    with pynwb.NWBHDF5IO(\n        path=self.get_abs_path(analysis_file_name),\n        mode=\"a\",\n        load_namespaces=True,\n    ) as io:\n        nwbf = io.read()\n        for id in unit_ids:\n            nwbf.add_unit(id=id)\n\n        for metric_name, metric_dict in metrics.items():\n            print(f\"Adding metric {metric_name} : {metric_dict}\")\n            metric_data = list(metric_dict.values())\n            nwbf.add_unit_column(\n                name=metric_name, description=metric_name, data=metric_data\n            )\n\n        io.write(nwbf)\n        return nwbf.units.object_id\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.get_electrode_indices", "title": "<code>get_electrode_indices(analysis_file_name, electrode_ids)</code>  <code>classmethod</code>", "text": "<p>Given an analysis NWB file name, returns the indices of the specified electrode_ids.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_file_name</code> <code>str</code> <p>The name of the analysis NWB file.</p> required <code>electrode_ids</code> <code>numpy array or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>numpy array</code> <p>Array of indices in the electrodes table for the given electrode IDs.</p> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@classmethod\ndef get_electrode_indices(cls, analysis_file_name, electrode_ids):\n\"\"\"Given an analysis NWB file name, returns the indices of the specified electrode_ids.\n\n    Parameters\n    ----------\n    analysis_file_name : str\n        The name of the analysis NWB file.\n    electrode_ids : numpy array or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : numpy array\n        Array of indices in the electrodes table for the given electrode IDs.\n    \"\"\"\n    nwbf = get_nwb_file(cls.get_abs_path(analysis_file_name))\n    return get_electrode_indices(nwbf.electrodes, electrode_ids)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.common.common_nwbfile.AnalysisNwbfile.cleanup", "title": "<code>cleanup(delete_files=False)</code>  <code>staticmethod</code>", "text": "<p>Remove the filepath entries for NWB files that are not in use.</p> <p>Does not delete the files themselves unless delete_files=True is specified. Run this after deleting the Nwbfile() entries themselves.</p> <p>Parameters:</p> Name Type Description Default <code>delete_files</code> <code>bool</code> <p>Whether the original files should be deleted (default False).</p> <code>False</code> Source code in <code>src/spyglass/common/common_nwbfile.py</code> <pre><code>@staticmethod\ndef cleanup(delete_files=False):\n\"\"\"Remove the filepath entries for NWB files that are not in use.\n\n    Does not delete the files themselves unless delete_files=True is specified.\n    Run this after deleting the Nwbfile() entries themselves.\n\n    Parameters\n    ----------\n    delete_files : bool, optional\n        Whether the original files should be deleted (default False).\n    \"\"\"\n    schema.external[\"analysis\"].delete(delete_external_files=delete_files)\n</code></pre>"}, {"location": "api/src/spyglass/spikesorting/spikesorting_sorting/#src.spyglass.spikesorting.spikesorting_sorting.SpikeSortingRecording", "title": "<code>SpikeSortingRecording</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> Source code in <code>src/spyglass/spikesorting/spikesorting_recording.py</code> <pre><code>@schema\nclass SpikeSortingRecording(dj.Computed):\n    definition = \"\"\"\n    -&gt; SpikeSortingRecordingSelection\n    ---\n    recording_path: varchar(1000)\n    -&gt; IntervalList.proj(sort_interval_list_name='interval_list_name')\n    \"\"\"\n\n    def make(self, key):\n        sort_interval_valid_times = self._get_sort_interval_valid_times(key)\n        recording = self._get_filtered_recording(key)\n        recording_name = self._get_recording_name(key)\n\n        tmp_key = {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": recording_name,\n            \"valid_times\": sort_interval_valid_times,\n        }\n        IntervalList.insert1(tmp_key, replace=True)\n\n        # store the list of valid times for the sort\n        key[\"sort_interval_list_name\"] = tmp_key[\"interval_list_name\"]\n\n        # Path to files that will hold the recording extractors\n        recording_folder = Path(os.getenv(\"SPYGLASS_RECORDING_DIR\"))\n        key[\"recording_path\"] = str(recording_folder / Path(recording_name))\n        if os.path.exists(key[\"recording_path\"]):\n            shutil.rmtree(key[\"recording_path\"])\n        recording = recording.save(\n            folder=key[\"recording_path\"], chunk_duration=\"10000ms\", n_jobs=8\n        )\n\n        self.insert1(key)\n\n    @staticmethod\n    def _get_recording_name(key):\n        recording_name = (\n            key[\"nwb_file_name\"]\n            + \"_\"\n            + key[\"sort_interval_name\"]\n            + \"_\"\n            + str(key[\"sort_group_id\"])\n            + \"_\"\n            + key[\"preproc_params_name\"]\n        )\n        return recording_name\n\n    @staticmethod\n    def _get_recording_timestamps(recording):\n        if recording.get_num_segments() &gt; 1:\n            frames_per_segment = [0]\n            for i in range(recording.get_num_segments()):\n                frames_per_segment.append(\n                    recording.get_num_frames(segment_index=i)\n                )\n\n            cumsum_frames = np.cumsum(frames_per_segment)\n            total_frames = np.sum(frames_per_segment)\n\n            timestamps = np.zeros((total_frames,))\n            for i in range(recording.get_num_segments()):\n                timestamps[\n                    cumsum_frames[i] : cumsum_frames[i + 1]\n                ] = recording.get_times(segment_index=i)\n        else:\n            timestamps = recording.get_times()\n        return timestamps\n\n    def _get_sort_interval_valid_times(self, key):\n\"\"\"Identifies the intersection between sort interval specified by the user\n        and the valid times (times for which neural data exist)\n\n        Parameters\n        ----------\n        key: dict\n            specifies a (partially filled) entry of SpikeSorting table\n\n        Returns\n        -------\n        sort_interval_valid_times: ndarray of tuples\n            (start, end) times for valid stretches of the sorting interval\n\n        \"\"\"\n        sort_interval = (\n            SortInterval\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_interval_name\": key[\"sort_interval_name\"],\n            }\n        ).fetch1(\"sort_interval\")\n        interval_list_name = (SpikeSortingRecordingSelection &amp; key).fetch1(\n            \"interval_list_name\"\n        )\n        valid_interval_times = (\n            IntervalList\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"interval_list_name\": interval_list_name,\n            }\n        ).fetch1(\"valid_times\")\n        valid_sort_times = interval_list_intersect(\n            sort_interval, valid_interval_times\n        )\n        # Exclude intervals shorter than specified length\n        params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        if \"min_segment_length\" in params:\n            valid_sort_times = intervals_by_length(\n                valid_sort_times, min_length=params[\"min_segment_length\"]\n            )\n        return valid_sort_times\n\n    def _get_filtered_recording(self, key: dict):\n\"\"\"Filters and references a recording\n        * Loads the NWB file created during insertion as a spikeinterface Recording\n        * Slices recording in time (interval) and space (channels);\n          recording chunks from disjoint intervals are concatenated\n        * Applies referencing and bandpass filtering\n\n        Parameters\n        ----------\n        key: dict,\n            primary key of SpikeSortingRecording table\n\n        Returns\n        -------\n        recording: si.Recording\n        \"\"\"\n\n        nwb_file_abs_path = Nwbfile().get_abs_path(key[\"nwb_file_name\"])\n        recording = se.read_nwb_recording(\n            nwb_file_abs_path, load_time_vector=True\n        )\n\n        valid_sort_times = self._get_sort_interval_valid_times(key)\n        # shape is (N, 2)\n        valid_sort_times_indices = np.array(\n            [\n                np.searchsorted(recording.get_times(), interval)\n                for interval in valid_sort_times\n            ]\n        )\n        # join intervals of indices that are adjacent\n        valid_sort_times_indices = reduce(\n            union_adjacent_index, valid_sort_times_indices\n        )\n        if valid_sort_times_indices.ndim == 1:\n            valid_sort_times_indices = np.expand_dims(\n                valid_sort_times_indices, 0\n            )\n\n        # create an AppendRecording if there is more than one disjoint sort interval\n        if len(valid_sort_times_indices) &gt; 1:\n            recordings_list = []\n            for interval_indices in valid_sort_times_indices:\n                recording_single = recording.frame_slice(\n                    start_frame=interval_indices[0],\n                    end_frame=interval_indices[1],\n                )\n                recordings_list.append(recording_single)\n            recording = si.append_recordings(recordings_list)\n        else:\n            recording = recording.frame_slice(\n                start_frame=valid_sort_times_indices[0][0],\n                end_frame=valid_sort_times_indices[0][1],\n            )\n\n        channel_ids = (\n            SortGroup.SortGroupElectrode\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch(\"electrode_id\")\n        ref_channel_id = (\n            SortGroup\n            &amp; {\n                \"nwb_file_name\": key[\"nwb_file_name\"],\n                \"sort_group_id\": key[\"sort_group_id\"],\n            }\n        ).fetch1(\"sort_reference_electrode_id\")\n        channel_ids = np.setdiff1d(channel_ids, ref_channel_id)\n\n        # include ref channel in first slice, then exclude it in second slice\n        if ref_channel_id &gt;= 0:\n            channel_ids_ref = np.append(channel_ids, ref_channel_id)\n            recording = recording.channel_slice(channel_ids=channel_ids_ref)\n\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"single\", ref_channel_ids=ref_channel_id\n            )\n            recording = recording.channel_slice(channel_ids=channel_ids)\n        elif ref_channel_id == -2:\n            recording = recording.channel_slice(channel_ids=channel_ids)\n            recording = si.preprocessing.common_reference(\n                recording, reference=\"global\", operator=\"median\"\n            )\n        else:\n            raise ValueError(\"Invalid reference channel ID\")\n        filter_params = (SpikeSortingPreprocessingParameters &amp; key).fetch1(\n            \"preproc_params\"\n        )\n        recording = si.preprocessing.bandpass_filter(\n            recording,\n            freq_min=filter_params[\"frequency_min\"],\n            freq_max=filter_params[\"frequency_max\"],\n        )\n\n        # if the sort group is a tetrode, change the channel location\n        # note that this is a workaround that would be deprecated when spikeinterface uses 3D probe locations\n        probe_type = []\n        electrode_group = []\n        for channel_id in channel_ids:\n            probe_type.append(\n                (\n                    Electrode * Probe\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"probe_type\")\n            )\n            electrode_group.append(\n                (\n                    Electrode\n                    &amp; {\n                        \"nwb_file_name\": key[\"nwb_file_name\"],\n                        \"electrode_id\": channel_id,\n                    }\n                ).fetch1(\"electrode_group_name\")\n            )\n        if (\n            all(p == \"tetrode_12.5\" for p in probe_type)\n            and len(probe_type) == 4\n            and all(eg == electrode_group[0] for eg in electrode_group)\n        ):\n            tetrode = pi.Probe(ndim=2)\n            position = [[0, 0], [0, 12.5], [12.5, 0], [12.5, 12.5]]\n            tetrode.set_contacts(\n                position, shapes=\"circle\", shape_params={\"radius\": 6.25}\n            )\n            tetrode.set_contact_ids(channel_ids)\n            tetrode.set_device_channel_indices(np.arange(4))\n            recording = recording.set_probe(tetrode, in_place=True)\n\n        return recording\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_helper_fn/", "title": "dj_helper_fn.py", "text": "<p>Helper functions for manipulating information from DataJoint fetch calls.</p>"}, {"location": "api/src/spyglass/utils/dj_helper_fn/#src.spyglass.utils.dj_helper_fn.dj_replace", "title": "<code>dj_replace(original_table, new_values, key_column, replace_column)</code>", "text": "<p>Given the output of a fetch() call from a schema and a 2D array made up of (key_value, replace_value) tuples, find each instance of key_value in the key_column of the original table and replace the specified replace_column with the associated replace_value. Key values must be unique.</p> <p>Parameters:</p> Name Type Description Default <code>original_table</code> <p>Result of a datajoint .fetch() call on a schema query.</p> required <code>new_values</code> <code>list</code> <p>List of tuples, each containing (key_value, replace_value).</p> required <code>replace_column</code> <code>str</code> <p>The name of the column where to-be-replaced values are located.</p> required <p>Returns:</p> Type Description <code>original_table</code> <p>Structured array of new table entries that can be inserted back into the schema</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def dj_replace(original_table, new_values, key_column, replace_column):\n\"\"\"Given the output of a fetch() call from a schema and a 2D array made up\n    of (key_value, replace_value) tuples, find each instance of key_value in\n    the key_column of the original table and replace the specified\n    replace_column with the associated replace_value. Key values must be\n    unique.\n\n    Parameters\n    ----------\n    original_table\n        Result of a datajoint .fetch() call on a schema query.\n    new_values : list\n        List of tuples, each containing (key_value, replace_value).\n    replace_column : str\n        The name of the column where to-be-replaced values are located.\n\n    Returns\n    -------\n    original_table\n        Structured array of new table entries that can be inserted back into the schema\n    \"\"\"\n\n    # check to make sure the new_values are a list or array of tuples and fix if not\n    if isinstance(new_values, tuple):\n        tmp = list()\n        tmp.append(new_values)\n        new_values = tmp\n\n    new_val_array = np.asarray(new_values)\n    replace_ind = np.where(\n        np.isin(original_table[key_column], new_val_array[:, 0])\n    )\n    original_table[replace_column][replace_ind] = new_val_array[:, 1]\n    return original_table\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_helper_fn/#src.spyglass.utils.dj_helper_fn.get_nwb_file", "title": "<code>get_nwb_file(nwb_file_path)</code>", "text": "<p>Return an NWBFile object with the given file path in read mode.    If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Path to the NWB file.</p> required <p>Returns:</p> Name Type Description <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>NWB file object for the given path opened in read mode.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_file(nwb_file_path):\n\"\"\"Return an NWBFile object with the given file path in read mode.\n       If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.\n\n    Parameters\n    ----------\n    nwb_file_path : str\n        Path to the NWB file.\n\n    Returns\n    -------\n    nwbfile : pynwb.NWBFile\n        NWB file object for the given path opened in read mode.\n    \"\"\"\n    _, nwbfile = __open_nwb_files.get(nwb_file_path, (None, None))\n    nwb_uri = None\n    nwb_raw_uri = None\n    if nwbfile is None:\n        # check to see if the file exists\n        if not os.path.exists(nwb_file_path):\n            print(\n                f\"NWB file {nwb_file_path} does not exist locally; checking kachery\"\n            )\n            # first try the analysis files\n            from ..sharing.sharing_kachery import AnalysisNwbfileKachery\n\n            # the download functions assume just the filename, so we need to get that from the path\n            if not AnalysisNwbfileKachery.download_file(\n                os.path.basename(nwb_file_path)\n            ):\n                return None\n        # now open the file\n        io = pynwb.NWBHDF5IO(\n            path=nwb_file_path, mode=\"r\", load_namespaces=True\n        )  # keep file open\n        nwbfile = io.read()\n        __open_nwb_files[nwb_file_path] = (io, nwbfile)\n\n    return nwbfile\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_helper_fn/#src.spyglass.utils.dj_helper_fn.fetch_nwb", "title": "<code>fetch_nwb(query_expression, nwb_master, *attrs, **kwargs)</code>", "text": "<p>Get an NWB object from the given DataJoint query.</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <code>query</code> <p>A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.</p> required <code>nwb_master</code> <code>tuple</code> <p>Tuple (table, attr) to get the NWB filepath from. i.e. absolute path to NWB file can be obtained by looking up attr column of table table is usually Nwbfile or AnalysisNwbfile; attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'</p> required <code>*attrs</code> <code>list</code> <p>Attributes from normal DataJoint fetch call.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments from normal DataJoint fetch call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>nwb_objects</code> <code>list</code> <p>List of dicts containing fetch results and NWB objects.</p> Source code in <code>src/spyglass/utils/dj_helper_fn.py</code> <pre><code>def fetch_nwb(query_expression, nwb_master, *attrs, **kwargs):\n\"\"\"Get an NWB object from the given DataJoint query.\n\n    Parameters\n    ----------\n    query_expression : query\n        A DataJoint query expression (e.g., join, restrict) or a table to call fetch on.\n    nwb_master : tuple\n        Tuple (table, attr) to get the NWB filepath from.\n        i.e. absolute path to NWB file can be obtained by looking up attr column of table\n        table is usually Nwbfile or AnalysisNwbfile;\n        attr is usually 'nwb_file_abs_path' or 'analysis_file_abs_path'\n    *attrs : list\n        Attributes from normal DataJoint fetch call.\n    **kwargs : dict\n        Keyword arguments from normal DataJoint fetch call.\n\n    Returns\n    -------\n    nwb_objects : list\n        List of dicts containing fetch results and NWB objects.\n    \"\"\"\n    kwargs[\"as_dict\"] = True  # force return as dictionary\n    tbl, attr_name = nwb_master\n\n    if not attrs:\n        attrs = query_expression.heading.names\n\n    # get the list of analysis or nwb files\n    file_name_str = (\n        \"analysis_file_name\" if \"analysis\" in nwb_master[1] else \"nwb_file_name\"\n    )\n    # TODO: avoid this import?\n    from ..common.common_nwbfile import AnalysisNwbfile, Nwbfile\n\n    file_path_fn = (\n        AnalysisNwbfile.get_abs_path\n        if \"analysis\" in nwb_master[1]\n        else Nwbfile.get_abs_path\n    )\n\n    # TODO: check that the query_expression restricts tbl - CBroz\n    nwb_files = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(file_name_str)\n    for file_name in nwb_files:\n        file_path = file_path_fn(file_name)\n        if not os.path.exists(file_path):\n            # retrieve the file from kachery. This also opens the file and stores the file object\n            get_nwb_file(file_path)\n\n    rec_dicts = (\n        query_expression * tbl.proj(nwb2load_filepath=attr_name)\n    ).fetch(*attrs, \"nwb2load_filepath\", **kwargs)\n\n    if not rec_dicts or not np.any(\n        [\"object_id\" in key for key in rec_dicts[0]]\n    ):\n        return rec_dicts\n\n    ret = []\n    for rec_dict in rec_dicts:\n        nwbf = get_nwb_file(rec_dict.pop(\"nwb2load_filepath\"))\n        # for each attr that contains substring 'object_id', store key-value: attr name to NWB object\n        # remove '_object_id' from attr name\n        nwb_objs = {\n            id_attr.replace(\"_object_id\", \"\"): _get_nwb_object(\n                nwbf.objects, rec_dict[id_attr]\n            )\n            for id_attr in attrs\n            if \"object_id\" in id_attr and rec_dict[id_attr] != \"\"\n        }\n        ret.append({**rec_dict, **nwb_objs})\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/", "title": "dj_merge_tables.py", "text": ""}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge", "title": "<code>Merge</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Adds funcs to support standard Merge table operations.</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>class Merge(dj.Manual):\n\"\"\"Adds funcs to support standard Merge table operations.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._reserved_pk = RESERVED_PRIMARY_KEY\n        self._reserved_sk = RESERVED_SECONDARY_KEY\n        merge_def = (\n            f\"\\n    {self._reserved_pk}: uuid\\n    ---\\n\"\n            + f\"    {self._reserved_sk}: varchar({RESERVED_SK_LENGTH})\\n    \"\n        )\n        # TODO: Change warnings to logger. Throw error? - CBroz1\n        if not self.is_declared:\n            if self.definition != merge_def:\n                print(\n                    \"WARNING: merge table declared with non-default definition\\n\\t\"\n                    + f\"Expected: {merge_def.strip()}\\n\\t\"\n                    + f\"Actual  : {self.definition.strip()}\"\n                )\n            for part in self.parts(as_objects=True):\n                if part.primary_key != self.primary_key:\n                    print(\n                        f\"WARNING: unexpected primary key for {part.table_name}\\n\\t\"\n                        + f\"Expected: {self.primary_key}\\n\\t\"\n                        + f\"Actual  : {part.primary_key}\"\n                    )\n\n    @classmethod\n    def _merge_restrict_parts(\n        cls,\n        restriction: str = True,\n        as_objects: bool = True,\n        return_empties: bool = True,\n    ) -&gt; list:\n\"\"\"Returns a list of parts with restrictions applied.\n\n        Parameters\n        ---------\n        restriction: str, optional\n            Restriction to apply to the merged view. Default True, no restrictions.\n        as_objects: bool, optional\n            Default True. Return part tables as objects\n        return_empties: bool, optional\n            Default True. Return empty part tables\n\n        Returns\n        ------\n        list\n            list of datajoint tables, parts of Merge Table\n        \"\"\"\n        if not dj.conn.connection.dependencies._loaded:\n            dj.conn.connection.dependencies.load()  # Otherwise parts returns none\n\n        if not restriction:\n            restriction = True\n\n        # Normalize restriction to sql string\n        restriction = make_condition(cls(), restriction, set())\n\n        parts_all = cls.parts(as_objects=True)\n        # If the restriction makes ref to a source, we only want that part\n        if isinstance(restriction, str) and cls()._reserved_sk in restriction:\n            parts_all = [\n                part\n                for part in parts_all\n                if from_camel_case(\n                    restriction.split(f'`{cls()._reserved_sk}`=\"')[-1].split(\n                        '\"'\n                    )[0]\n                )  # Only look at source part table\n                in part.full_table_name\n            ]\n\n        parts = []\n        for part in parts_all:\n            try:\n                parts.append(part.restrict(restriction))\n            except DataJointError:  # If restriction not valid on given part\n                parts.append(part)\n\n        if not return_empties:\n            parts = [p for p in parts if len(p)]\n        if not as_objects:\n            parts = [p.full_table_name for p in parts]\n\n        return parts\n\n    @classmethod\n    def _merge_restrict_parents(\n        cls,\n        restriction: str = True,\n        as_objects: bool = True,\n        return_empties: bool = True,\n    ) -&gt; list:\n\"\"\"Returns a list of part parents with restrictions applied.\n\n        Rather than part tables, we look at parents of those parts, the source\n        of the data.\n\n        Parameters\n        ---------\n        restriction: str, optional\n            Restriction to apply to the merged view. Default True, no restrictions.\n        as_objects: bool, optional\n            Default True. Return part tables as objects\n        return_empties: bool, optional\n            Default True. Return empty part tables\n\n        Returns\n        ------\n        list\n            list of datajoint tables, parents of parts of Merge Table\n        \"\"\"\n        part_parents = [\n            parent  # Below, restricting parent to info from restricted part\n            &amp; part.fetch(*part.heading.secondary_attributes, as_dict=True)\n            for part in cls()._merge_restrict_parts(\n                restriction=restriction, return_empties=return_empties\n            )\n            for parent in part.parents(as_objects=True)  # ID respective parents\n            if cls().table_name not in parent.full_table_name  # Not merge table\n        ]\n        if not as_objects:\n            part_parents = [p.full_table_name for p in part_parents]\n\n        return part_parents\n\n    @classmethod\n    def _merge_repr(\n        cls, restriction: str = True, **kwargs\n    ) -&gt; dj.expression.Union:\n\"\"\"Merged view, including null entries for columns unique to one part table.\n\n        Parameters\n        ---------\n        restriction: str, optional\n            Restriction to apply to the merged view\n\n        Returns\n        ------\n        datajoint.expression.Union\n        \"\"\"\n\n        parts = [\n            cls() * p  # join with master to include sec key (i.e., 'source')\n            for p in cls._merge_restrict_parts(restriction=restriction)\n        ]\n\n        primary_attrs = list(\n            dict.fromkeys(  # get all columns from parts\n                iter_chain.from_iterable([p.heading.names for p in parts])\n            )\n        )\n        # primary_attrs.append(cls()._reserved_sk)\n        query = dj.U(*primary_attrs) * parts[0].proj(  # declare query\n            ...,  # include all attributes from part 0\n            **{\n                a: \"NULL\"  # add null value where part has no column\n                for a in primary_attrs\n                if a not in parts[0].heading.names\n            },\n        )\n        for part in parts[1:]:  # add to declared query for each part\n            query += dj.U(*primary_attrs) * part.proj(\n                ...,\n                **{\n                    a: \"NULL\"\n                    for a in primary_attrs\n                    if a not in part.heading.names\n                },\n            )\n        return query\n\n    @classmethod\n    def _merge_insert(cls, rows: list, **kwargs) -&gt; None:\n\"\"\"Insert rows into merge table, ensuring db integrity and mutual exclusivity\n\n        Parameters\n        ---------\n        rows: List[dict]\n            An iterable where an element is a dictionary.\n\n        Raises\n        ------\n        TypeError\n            If rows is not a list of dicts\n        ValueError\n            If entry already exists, mutual exclusivity errors\n            If data doesn't exist in part parents, integrity error\n        \"\"\"\n\n        try:\n            for r in iter(rows):\n                assert isinstance(\n                    r, dict\n                ), 'Input \"rows\" must be a list of dictionaries'\n        except TypeError:\n            raise TypeError('Input \"rows\" must be a list of dictionaries')\n\n        parts = cls._merge_restrict_parts(as_objects=True)\n        master_entries = []\n        parts_entries = {p: [] for p in parts}\n        for row in rows:\n            key = {}\n            for part in parts:\n                master = part.parents(as_objects=True)[-1]\n                part_name = to_camel_case(part.table_name.split(\"__\")[-1])\n                if master &amp; row:\n                    if not key:\n                        key = (master &amp; row).fetch1(\"KEY\")\n                        master_pk = {\n                            cls()._reserved_pk: dj.hash.key_hash(key),\n                        }\n                        parts_entries[part].append({**master_pk, **key})\n                        master_entries.append(\n                            {**master_pk, cls()._reserved_sk: part_name}\n                        )\n                    else:\n                        raise ValueError(\n                            \"Mutual Exclusivity Error! Entry exists in more \"\n                            + f\"than one table - Entry: {row}\"\n                        )\n\n            if not key:\n                raise ValueError(\n                    \"Non-existing entry in any of the parent tables - Entry: \"\n                    + f\"{row}\"\n                )\n\n        # 1. nullcontext() allows use within `make` but decreases reliability\n        # 2. cls.connection.transaction is more reliable but throws errors if\n        # used within another transaction, i.e. in `make`\n\n        with nullcontext():  # TODO: ensure this block within transaction\n            super().insert(cls(), master_entries, **kwargs)\n            for part, part_entries in parts_entries.items():\n                part.insert(part_entries, **kwargs)\n\n    @classmethod\n    def insert(cls, rows: list, **kwargs):\n\"\"\"Merges table specific insert\n\n        Ensuring db integrity and mutual exclusivity\n\n        Parameters\n        ---------\n        rows: List[dict]\n            An iterable where an element is a dictionary.\n\n        Raises\n        ------\n        TypeError\n            If rows is not a list of dicts\n        ValueError\n            If entry already exists, mutual exclusivity errors\n            If data doesn't exist in part parents, integrity error\n        \"\"\"\n        cls._merge_insert(rows, **kwargs)\n\n    @classmethod\n    def merge_view(cls, restriction: str = True):\n\"\"\"Prints merged view, including null entries for unique columns.\n\n        Parameters\n        ---------\n        restriction: str, optional\n            Restriction to apply to the merged view\n        \"\"\"\n\n        # If we overwrite `preview`, we then encounter issues with operators\n        # getting passed a `Union`, which doesn't have a method we can\n        # intercept to manage master/parts\n\n        return pprint(cls._merge_repr(restriction=restriction))\n\n    @classmethod\n    def merge_html(cls, restriction: str = True):\n\"\"\"Displays HTML in notebooks.\"\"\"\n\n        return HTML(repr_html(cls._merge_repr(restriction=restriction)))\n\n    @classmethod\n    def merge_restrict(cls, restriction: str = True) -&gt; dj.U:\n\"\"\"Given a restriction string, return a merged view with restriction applied.\n\n        Example\n        -------\n            &gt;&gt;&gt; MergeTable.merge_restrict(\"field = 1\")\n\n        Parameters\n        ----------\n        restriction: str\n            Restriction one would apply if `merge_view` was a real table.\n\n        Returns\n        -------\n        datajoint.Union\n            Merged view with restriction applied.\n        \"\"\"\n        return cls._merge_repr(restriction=restriction)\n\n    @classmethod\n    def merge_delete(cls, restriction: str = True, **kwargs):\n\"\"\"Given a restriction string, delete corresponding entries.\n\n        Parameters\n        ----------\n        restriction: str\n            Optional restriction to apply before deletion from master/part\n            tables. If not provided, delete all entries.\n        kwargs: dict\n            Additional keyword arguments for DataJoint delete.\n\n        Example\n        -------\n            &gt;&gt;&gt; MergeTable.merge_delete(\"field = 1\")\n        \"\"\"\n        uuids = [\n            {k: v}\n            for entry in cls.merge_restrict(restriction).fetch(\"KEY\")\n            for k, v in entry.items()\n            if k == cls()._reserved_pk\n        ]\n        (cls() &amp; uuids).delete(**kwargs)\n\n    @classmethod\n    def merge_delete_parent(\n        cls, restriction: str = True, dry_run=True, **kwargs\n    ) -&gt; list:\n\"\"\"Delete entries from merge master, part, and respective part parents\n\n        Note: Clears merge entries from their respective parents.\n\n        Parameters\n        ----------\n        restriction: str\n            Optional restriction to apply before deletion from parents. If not\n            provided, delete all entries present in Merge Table.\n        dry_run: bool\n            Default True. If true, return list of tables with entries that would be\n            deleted. Otherwise, table entries.\n        kwargs: dict\n            Additional keyword arguments for DataJoint delete.\n        \"\"\"\n\n        part_parents = cls._merge_restrict_parents(\n            restriction=restriction, as_objects=True, return_empties=False\n        )\n\n        if dry_run:\n            return part_parents\n\n        super().delete(cls(), **kwargs)\n        for part_parent in part_parents:\n            super().delete(part_parent, **kwargs)\n\n    def fetch_nwb(self, *attrs, **kwargs):\n        part_parents = self._merge_restrict_parents(\n            restriction=self.restriction, return_empties=False\n        )\n\n        if len(part_parents) == 1:\n            return fetch_nwb(\n                part_parents[0],\n                (AnalysisNwbfile, \"analysis_file_abs_path\"),\n                *attrs,\n                **kwargs,\n            )\n        else:\n            raise ValueError(\n                f\"{len(part_parents)} possible sources found in Merge Table\"\n                + part_parents\n            )\n\n    def merge_get_part(\n        self, restriction: str = True, join_master: bool = False\n    ) -&gt; dj.Table:\n\"\"\"Retrieve part table from a restricted Merge table\n\n        Parameters\n        ----------\n        restriction: str\n            Optional restriction to apply before determining part to return.\n            Default True.\n        join_master: bool\n            Default False. Joint part with Merge master to show uuid and source\n\n        Example\n        -------\n            &gt;&gt;&gt; (MergeTable &amp; restriction).get_part_table()\n            &gt;&gt;&gt; MergeTable().merge_get_part(restriction, join_master=True)\n\n        Raises\n        ------\n        ValueError\n            If multiple sources are found, lists and suggests restricting\n        \"\"\"\n        # Note: `get_part_table`-&gt;`merge_get_part` to keep prefix on methods\n        sources = self.restrict(restriction).fetch(self._reserved_sk)\n\n        if len(sources) != 1:\n            raise ValueError(\n                f\"Found multiple potential parts: {sources}\\n\\t\"\n                + \"Try adding a restriction before invoking `get_part`.\"\n            )\n\n        if False:  # Original: &amp; here does nothing bc no unique pk on master\n            return getattr(self, sources[0]) &amp; self\n\n        if join_master:  # Alt: Master * Part shows source\n            return self * getattr(self, sources[0])\n        else:  # Current default aligns with func name\n            return getattr(self, sources[0])()\n\n    @classmethod\n    def merge_fetch(cls, *attrs, **kwargs) -&gt; list:\n\"\"\"Perform a fetch across all parts. If &gt;1 result, return as a list.\n\n        Parameters\n        ----------\n        attrs, kwargs\n            arguments passed to DataJoint `fetch` call\n\n        Returns\n        -------\n        Union[ List[np.array], List[dict], List[pd.DataFrame] ]\n            Table contents, with type determined by kwargs\n        \"\"\"\n        results = []\n        parts = cls()._merge_restrict_parts(\n            restriction=cls._restriction, return_empties=False\n        )\n\n        for part in parts:\n            try:\n                results.extend(part.fetch(*attrs, **kwargs))\n            except DataJointError as e:\n                print(\n                    f\"WARNING: {e.args[0]} Skipping \"\n                    + to_camel_case(part.table_name.split(\"__\")[-1])\n                )\n\n        # Note: this could collapse results like merge_view, but user may call\n        # for recarray, pd.DataFrame, or dict, and fetched contents differ if\n        # attrs or \"KEY\" called. Intercept format, merge, and then transform?\n\n        return results[0] if len(results) == 1 else results\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.insert", "title": "<code>insert(rows, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Merges table specific insert</p> <p>Ensuring db integrity and mutual exclusivity</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>list</code> <p>An iterable where an element is a dictionary.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If rows is not a list of dicts</p> <code>ValueError</code> <p>If entry already exists, mutual exclusivity errors If data doesn't exist in part parents, integrity error</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef insert(cls, rows: list, **kwargs):\n\"\"\"Merges table specific insert\n\n    Ensuring db integrity and mutual exclusivity\n\n    Parameters\n    ---------\n    rows: List[dict]\n        An iterable where an element is a dictionary.\n\n    Raises\n    ------\n    TypeError\n        If rows is not a list of dicts\n    ValueError\n        If entry already exists, mutual exclusivity errors\n        If data doesn't exist in part parents, integrity error\n    \"\"\"\n    cls._merge_insert(rows, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_view", "title": "<code>merge_view(restriction=True)</code>  <code>classmethod</code>", "text": "<p>Prints merged view, including null entries for unique columns.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Restriction to apply to the merged view</p> <code>True</code> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_view(cls, restriction: str = True):\n\"\"\"Prints merged view, including null entries for unique columns.\n\n    Parameters\n    ---------\n    restriction: str, optional\n        Restriction to apply to the merged view\n    \"\"\"\n\n    # If we overwrite `preview`, we then encounter issues with operators\n    # getting passed a `Union`, which doesn't have a method we can\n    # intercept to manage master/parts\n\n    return pprint(cls._merge_repr(restriction=restriction))\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_html", "title": "<code>merge_html(restriction=True)</code>  <code>classmethod</code>", "text": "<p>Displays HTML in notebooks.</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_html(cls, restriction: str = True):\n\"\"\"Displays HTML in notebooks.\"\"\"\n\n    return HTML(repr_html(cls._merge_repr(restriction=restriction)))\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_restrict", "title": "<code>merge_restrict(restriction=True)</code>  <code>classmethod</code>", "text": "<p>Given a restriction string, return a merged view with restriction applied.</p>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_restrict--example", "title": "Example", "text": "<pre><code>&gt;&gt;&gt; MergeTable.merge_restrict(\"field = 1\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Restriction one would apply if <code>merge_view</code> was a real table.</p> <code>True</code> <p>Returns:</p> Type Description <code>datajoint.Union</code> <p>Merged view with restriction applied.</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_restrict(cls, restriction: str = True) -&gt; dj.U:\n\"\"\"Given a restriction string, return a merged view with restriction applied.\n\n    Example\n    -------\n        &gt;&gt;&gt; MergeTable.merge_restrict(\"field = 1\")\n\n    Parameters\n    ----------\n    restriction: str\n        Restriction one would apply if `merge_view` was a real table.\n\n    Returns\n    -------\n    datajoint.Union\n        Merged view with restriction applied.\n    \"\"\"\n    return cls._merge_repr(restriction=restriction)\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_delete", "title": "<code>merge_delete(restriction=True, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Given a restriction string, delete corresponding entries.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Optional restriction to apply before deletion from master/part tables. If not provided, delete all entries.</p> <code>True</code> <code>kwargs</code> <p>Additional keyword arguments for DataJoint delete.</p> <code>{}</code>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_delete--example", "title": "Example", "text": "<pre><code>&gt;&gt;&gt; MergeTable.merge_delete(\"field = 1\")\n</code></pre> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_delete(cls, restriction: str = True, **kwargs):\n\"\"\"Given a restriction string, delete corresponding entries.\n\n    Parameters\n    ----------\n    restriction: str\n        Optional restriction to apply before deletion from master/part\n        tables. If not provided, delete all entries.\n    kwargs: dict\n        Additional keyword arguments for DataJoint delete.\n\n    Example\n    -------\n        &gt;&gt;&gt; MergeTable.merge_delete(\"field = 1\")\n    \"\"\"\n    uuids = [\n        {k: v}\n        for entry in cls.merge_restrict(restriction).fetch(\"KEY\")\n        for k, v in entry.items()\n        if k == cls()._reserved_pk\n    ]\n    (cls() &amp; uuids).delete(**kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_delete_parent", "title": "<code>merge_delete_parent(restriction=True, dry_run=True, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Delete entries from merge master, part, and respective part parents</p> <p>Note: Clears merge entries from their respective parents.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Optional restriction to apply before deletion from parents. If not provided, delete all entries present in Merge Table.</p> <code>True</code> <code>dry_run</code> <p>Default True. If true, return list of tables with entries that would be deleted. Otherwise, table entries.</p> <code>True</code> <code>kwargs</code> <p>Additional keyword arguments for DataJoint delete.</p> <code>{}</code> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_delete_parent(\n    cls, restriction: str = True, dry_run=True, **kwargs\n) -&gt; list:\n\"\"\"Delete entries from merge master, part, and respective part parents\n\n    Note: Clears merge entries from their respective parents.\n\n    Parameters\n    ----------\n    restriction: str\n        Optional restriction to apply before deletion from parents. If not\n        provided, delete all entries present in Merge Table.\n    dry_run: bool\n        Default True. If true, return list of tables with entries that would be\n        deleted. Otherwise, table entries.\n    kwargs: dict\n        Additional keyword arguments for DataJoint delete.\n    \"\"\"\n\n    part_parents = cls._merge_restrict_parents(\n        restriction=restriction, as_objects=True, return_empties=False\n    )\n\n    if dry_run:\n        return part_parents\n\n    super().delete(cls(), **kwargs)\n    for part_parent in part_parents:\n        super().delete(part_parent, **kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_get_part", "title": "<code>merge_get_part(restriction=True, join_master=False)</code>", "text": "<p>Retrieve part table from a restricted Merge table</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>str</code> <p>Optional restriction to apply before determining part to return. Default True.</p> <code>True</code> <code>join_master</code> <code>bool</code> <p>Default False. Joint part with Merge master to show uuid and source</p> <code>False</code>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_get_part--example", "title": "Example", "text": "<pre><code>&gt;&gt;&gt; (MergeTable &amp; restriction).get_part_table()\n&gt;&gt;&gt; MergeTable().merge_get_part(restriction, join_master=True)\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If multiple sources are found, lists and suggests restricting</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>def merge_get_part(\n    self, restriction: str = True, join_master: bool = False\n) -&gt; dj.Table:\n\"\"\"Retrieve part table from a restricted Merge table\n\n    Parameters\n    ----------\n    restriction: str\n        Optional restriction to apply before determining part to return.\n        Default True.\n    join_master: bool\n        Default False. Joint part with Merge master to show uuid and source\n\n    Example\n    -------\n        &gt;&gt;&gt; (MergeTable &amp; restriction).get_part_table()\n        &gt;&gt;&gt; MergeTable().merge_get_part(restriction, join_master=True)\n\n    Raises\n    ------\n    ValueError\n        If multiple sources are found, lists and suggests restricting\n    \"\"\"\n    # Note: `get_part_table`-&gt;`merge_get_part` to keep prefix on methods\n    sources = self.restrict(restriction).fetch(self._reserved_sk)\n\n    if len(sources) != 1:\n        raise ValueError(\n            f\"Found multiple potential parts: {sources}\\n\\t\"\n            + \"Try adding a restriction before invoking `get_part`.\"\n        )\n\n    if False:  # Original: &amp; here does nothing bc no unique pk on master\n        return getattr(self, sources[0]) &amp; self\n\n    if join_master:  # Alt: Master * Part shows source\n        return self * getattr(self, sources[0])\n    else:  # Current default aligns with func name\n        return getattr(self, sources[0])()\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.Merge.merge_fetch", "title": "<code>merge_fetch(*attrs, **kwargs)</code>  <code>classmethod</code>", "text": "<p>Perform a fetch across all parts. If &gt;1 result, return as a list.</p> <p>Parameters:</p> Name Type Description Default <code>attrs</code> <p>arguments passed to DataJoint <code>fetch</code> call</p> <code>()</code> <code>kwargs</code> <p>arguments passed to DataJoint <code>fetch</code> call</p> <code>()</code> <p>Returns:</p> Type Description <code>Union[List[np.array], List[dict], List[pd.DataFrame]]</code> <p>Table contents, with type determined by kwargs</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>@classmethod\ndef merge_fetch(cls, *attrs, **kwargs) -&gt; list:\n\"\"\"Perform a fetch across all parts. If &gt;1 result, return as a list.\n\n    Parameters\n    ----------\n    attrs, kwargs\n        arguments passed to DataJoint `fetch` call\n\n    Returns\n    -------\n    Union[ List[np.array], List[dict], List[pd.DataFrame] ]\n        Table contents, with type determined by kwargs\n    \"\"\"\n    results = []\n    parts = cls()._merge_restrict_parts(\n        restriction=cls._restriction, return_empties=False\n    )\n\n    for part in parts:\n        try:\n            results.extend(part.fetch(*attrs, **kwargs))\n        except DataJointError as e:\n            print(\n                f\"WARNING: {e.args[0]} Skipping \"\n                + to_camel_case(part.table_name.split(\"__\")[-1])\n            )\n\n    # Note: this could collapse results like merge_view, but user may call\n    # for recarray, pd.DataFrame, or dict, and fetched contents differ if\n    # attrs or \"KEY\" called. Intercept format, merge, and then transform?\n\n    return results[0] if len(results) == 1 else results\n</code></pre>"}, {"location": "api/src/spyglass/utils/dj_merge_tables/#src.spyglass.utils.dj_merge_tables.delete_downstream_merge", "title": "<code>delete_downstream_merge(table, restriction=True, dry_run=True, **kwargs)</code>", "text": "<p>Given a table/restriction, id or delete relevant downstream merge entries</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>dj.Table</code> <p>DataJoint table or restriction thereof</p> required <code>restriction</code> <code>str</code> <p>Optional restriction to apply before deletion from merge/part tables. If not provided, delete all downstream entries.</p> <code>True</code> <code>dry_run</code> <p>Default True. If true, return list of tuples, merge/part tables downstream of table input. Otherwise, delete merge/part table entries.</p> <code>True</code> <code>kwargs</code> <p>Additional keyword arguments for DataJoint delete.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Tuple[dj.Table, dj.Table]]</code> <p>Entries in merge/part tables downstream of table input.</p> Source code in <code>src/spyglass/utils/dj_merge_tables.py</code> <pre><code>def delete_downstream_merge(\n    table: dj.Table, restriction: str = True, dry_run=True, **kwargs\n) -&gt; list:\n\"\"\"Given a table/restriction, id or delete relevant downstream merge entries\n\n    Parameters\n    ----------\n    table: dj.Table\n        DataJoint table or restriction thereof\n    restriction: str\n        Optional restriction to apply before deletion from merge/part\n        tables. If not provided, delete all downstream entries.\n    dry_run: bool\n        Default True. If true, return list of tuples, merge/part tables\n        downstream of table input. Otherwise, delete merge/part table entries.\n    kwargs: dict\n        Additional keyword arguments for DataJoint delete.\n\n    Returns\n    -------\n    List[Tuple[dj.Table, dj.Table]]\n        Entries in merge/part tables downstream of table input.\n    \"\"\"\n    if not restriction:\n        restriction = True\n\n    # Adapted from Spyglass PR 535\n    # dj.utils.get_master could maybe help here, but it uses names, not objs\n    merge_pairs = [  # get each merge/part table\n        (master, descendant.restrict(restriction))\n        for descendant in table.descendants(as_objects=True)  # given tbl desc\n        for master in descendant.parents(as_objects=True)  # and those parents\n        # if is a part table (using a dunder not immediately after schema name)\n        if \"__\" in descendant.full_table_name.replace(\"`.`__\", \"\")\n        # and it is not in not in direct descendants\n        and master.full_table_name not in table.descendants(as_objects=False)\n        # and it uses our reserved primary key in attributes\n        and RESERVED_PRIMARY_KEY in master.heading.attributes.keys()\n    ]\n\n    # restrict the merge table based on uuids in part\n    merge_pairs = [\n        (merge &amp; uuids, part)  # don't need part for del, but show on dry_run\n        for merge, part in merge_pairs\n        for uuids in part.fetch(RESERVED_PRIMARY_KEY, as_dict=True)\n    ]\n\n    if dry_run:\n        return merge_pairs\n\n    for merge_table, _ in merge_pairs:\n        merge_table.delete(**kwargs)\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/", "title": "nwb_helper_fn.py", "text": "<p>NWB helper functions for finding processing modules and data interfaces.</p>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_nwb_file", "title": "<code>get_nwb_file(nwb_file_path)</code>", "text": "<p>Return an NWBFile object with the given file path in read mode.    If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Path to the NWB file.</p> required <p>Returns:</p> Name Type Description <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>NWB file object for the given path opened in read mode.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_file(nwb_file_path):\n\"\"\"Return an NWBFile object with the given file path in read mode.\n       If the file is not found locally, this will check if it has been shared with kachery and if so, download it and open it.\n\n    Parameters\n    ----------\n    nwb_file_path : str\n        Path to the NWB file.\n\n    Returns\n    -------\n    nwbfile : pynwb.NWBFile\n        NWB file object for the given path opened in read mode.\n    \"\"\"\n    _, nwbfile = __open_nwb_files.get(nwb_file_path, (None, None))\n    nwb_uri = None\n    nwb_raw_uri = None\n    if nwbfile is None:\n        # check to see if the file exists\n        if not os.path.exists(nwb_file_path):\n            print(\n                f\"NWB file {nwb_file_path} does not exist locally; checking kachery\"\n            )\n            # first try the analysis files\n            from ..sharing.sharing_kachery import AnalysisNwbfileKachery\n\n            # the download functions assume just the filename, so we need to get that from the path\n            if not AnalysisNwbfileKachery.download_file(\n                os.path.basename(nwb_file_path)\n            ):\n                return None\n        # now open the file\n        io = pynwb.NWBHDF5IO(\n            path=nwb_file_path, mode=\"r\", load_namespaces=True\n        )  # keep file open\n        nwbfile = io.read()\n        __open_nwb_files[nwb_file_path] = (io, nwbfile)\n\n    return nwbfile\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_config", "title": "<code>get_config(nwb_file_path)</code>", "text": "<p>Return a dictionary of config settings for the given NWB file. If the file does not exist, return an empty dict.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_file_path</code> <code>str</code> <p>Absolute path to the NWB file.</p> required <code>Returns</code> required <code>d</code> <code>dict</code> <p>Dictionary of configuration settings loaded from the corresponding YAML file</p> required Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_config(nwb_file_path):\n\"\"\"Return a dictionary of config settings for the given NWB file.\n    If the file does not exist, return an empty dict.\n    Parameters\n    ----------\n    nwb_file_path : str\n        Absolute path to the NWB file.\n    Returns\n    -------\n    d : dict\n        Dictionary of configuration settings loaded from the corresponding YAML file\n    \"\"\"\n    if nwb_file_path in __configs:  # load from cache if exists\n        return __configs[nwb_file_path]\n\n    p = Path(nwb_file_path)\n    # NOTE use p.stem[:-1] to remove the underscore that was added to the file\n    config_path = p.parent / (p.stem[:-1] + \"_spyglass_config.yaml\")\n    if not os.path.exists(config_path):\n        print(f\"No config found at file path {config_path}\")\n        return dict()\n    with open(config_path, \"r\") as stream:\n        d = yaml.safe_load(stream)\n\n    # TODO write a JSON schema for the yaml file and validate the yaml file\n    __configs[nwb_file_path] = d  # store in cache\n    return d\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_data_interface", "title": "<code>get_data_interface(nwbfile, data_interface_name, data_interface_class=None)</code>", "text": "<p>Search for a specified NWBDataInterface or DynamicTable in the processing modules of an NWB file.</p> <p>Parameters:</p> Name Type Description Default <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>The NWB file object to search in.</p> required <code>data_interface_name</code> <code>str</code> <p>The name of the NWBDataInterface or DynamicTable to search for.</p> required <code>data_interface_class</code> <code>type</code> <p>The class (or superclass) to search for. This argument helps to prevent accessing an object with the same name but the incorrect type. Default: no restriction.</p> <code>None</code> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If multiple NWBDataInterface and DynamicTable objects with the matching name are found.</p> <p>Returns:</p> Name Type Description <code>data_interface</code> <code>NWBDataInterface</code> <p>The data interface object with the given name, or None if not found.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_data_interface(nwbfile, data_interface_name, data_interface_class=None):\n\"\"\"Search for a specified NWBDataInterface or DynamicTable in the processing modules of an NWB file.\n\n    Parameters\n    ----------\n    nwbfile : pynwb.NWBFile\n        The NWB file object to search in.\n    data_interface_name : str\n        The name of the NWBDataInterface or DynamicTable to search for.\n    data_interface_class : type, optional\n        The class (or superclass) to search for. This argument helps to prevent accessing an object with the same\n        name but the incorrect type. Default: no restriction.\n\n    Warns\n    -----\n    UserWarning\n        If multiple NWBDataInterface and DynamicTable objects with the matching name are found.\n\n    Returns\n    -------\n    data_interface : NWBDataInterface\n        The data interface object with the given name, or None if not found.\n    \"\"\"\n    ret = []\n    for module in nwbfile.processing.values():\n        match = module.data_interfaces.get(data_interface_name, None)\n        if match is not None:\n            if data_interface_class is not None and not isinstance(\n                match, data_interface_class\n            ):\n                continue\n            ret.append(match)\n    if len(ret) &gt; 1:\n        warnings.warn(\n            f\"Multiple data interfaces with name '{data_interface_name}' \"\n            f\"found in NWBFile with identifier {nwbfile.identifier}. Using the first one found. \"\n            \"Use the data_interface_class argument to restrict the search.\"\n        )\n    if len(ret) &gt;= 1:\n        return ret[0]\n    else:\n        return None\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_raw_eseries", "title": "<code>get_raw_eseries(nwbfile)</code>", "text": "<p>Return all ElectricalSeries in the acquisition group of an NWB file.</p> <p>ElectricalSeries found within LFP objects in the acquisition will also be returned.</p> <p>Parameters:</p> Name Type Description Default <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>The NWB file object to search in.</p> required <p>Returns:</p> Name Type Description <code>ret</code> <code>list</code> <p>A list of all ElectricalSeries in the acquisition group of an NWB file</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_raw_eseries(nwbfile):\n\"\"\"Return all ElectricalSeries in the acquisition group of an NWB file.\n\n    ElectricalSeries found within LFP objects in the acquisition will also be returned.\n\n    Parameters\n    ----------\n    nwbfile : pynwb.NWBFile\n        The NWB file object to search in.\n\n    Returns\n    -------\n    ret : list\n        A list of all ElectricalSeries in the acquisition group of an NWB file\n    \"\"\"\n    ret = []\n    for nwb_object in nwbfile.acquisition.values():\n        if isinstance(nwb_object, pynwb.ecephys.ElectricalSeries):\n            ret.append(nwb_object)\n        elif isinstance(nwb_object, pynwb.ecephys.LFP):\n            ret.extend(nwb_object.electrical_series.values())\n    return ret\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.estimate_sampling_rate", "title": "<code>estimate_sampling_rate(timestamps, multiplier)</code>", "text": "<p>Estimate the sampling rate given a list of timestamps.</p> <p>Assumes that the most common temporal differences between timestamps approximate the sampling rate. Note that this can fail for very high sampling rates and irregular timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>numpy.ndarray</code> <p>1D numpy array of timestamp values.</p> required <code>multiplier</code> <code>float or int</code> required <p>Returns:</p> Name Type Description <code>estimated_rate</code> <code>float</code> <p>The estimated sampling rate.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def estimate_sampling_rate(timestamps, multiplier):\n\"\"\"Estimate the sampling rate given a list of timestamps.\n\n    Assumes that the most common temporal differences between timestamps approximate the sampling rate. Note that this\n    can fail for very high sampling rates and irregular timestamps.\n\n    Parameters\n    ----------\n    timestamps : numpy.ndarray\n        1D numpy array of timestamp values.\n    multiplier : float or int\n\n    Returns\n    -------\n    estimated_rate : float\n        The estimated sampling rate.\n    \"\"\"\n\n    # approach:\n    # 1. use a box car smoother and a histogram to get the modal value\n    # 2. identify adjacent samples as those that have a time difference &lt; the multiplier * the modal value\n    # 3. average the time differences between adjacent samples\n    sample_diff = np.diff(timestamps[~np.isnan(timestamps)])\n    if len(sample_diff) &lt; 10:\n        raise ValueError(\n            f\"Only {len(sample_diff)} timestamps are valid. Check the data.\"\n        )\n    nsmooth = 10\n    smoother = np.ones(nsmooth) / nsmooth\n    smooth_diff = np.convolve(sample_diff, smoother, mode=\"same\")\n\n    # we histogram with 100 bins out to 3 * mean, which should be fine for any reasonable number of samples\n    hist, bins = np.histogram(\n        smooth_diff, bins=100, range=[0, 3 * np.mean(smooth_diff)]\n    )\n    mode = bins[np.where(hist == np.max(hist))]\n\n    adjacent = sample_diff &lt; mode[0] * multiplier\n    return np.round(1.0 / np.mean(sample_diff[adjacent]))\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_valid_intervals", "title": "<code>get_valid_intervals(timestamps, sampling_rate, gap_proportion, min_valid_len)</code>", "text": "<p>Finds the set of all valid intervals in a list of timestamps. Valid interval: (start time, stop time) during which there are no gaps (i.e. missing samples).</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>numpy.ndarray</code> <p>1D numpy array of timestamp values.</p> required <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data.</p> required <code>gap_proportion</code> <code>float, greater than 1; unit: samples</code> <p>Threshold for detecting a gap; i.e. if the difference (in samples) between consecutive timestamps exceeds gap_proportion, it is considered a gap</p> required <code>min_valid_len</code> <code>float</code> <p>Length of smallest valid interval.</p> required <p>Returns:</p> Name Type Description <code>valid_times</code> <code>np.ndarray</code> <p>Array of start and stop times of shape (N, 2) for valid data.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_valid_intervals(\n    timestamps, sampling_rate, gap_proportion, min_valid_len\n):\n\"\"\"Finds the set of all valid intervals in a list of timestamps.\n    Valid interval: (start time, stop time) during which there are\n    no gaps (i.e. missing samples).\n\n    Parameters\n    ----------\n    timestamps : numpy.ndarray\n        1D numpy array of timestamp values.\n    sampling_rate : float\n        Sampling rate of the data.\n    gap_proportion : float, greater than 1; unit: samples\n        Threshold for detecting a gap;\n        i.e. if the difference (in samples) between\n        consecutive timestamps exceeds gap_proportion,\n        it is considered a gap\n    min_valid_len : float\n        Length of smallest valid interval.\n\n    Returns\n    -------\n    valid_times : np.ndarray\n        Array of start and stop times of shape (N, 2) for valid data.\n    \"\"\"\n\n    eps = 0.0000001\n\n    # get rid of NaN elements\n    timestamps = timestamps[~np.isnan(timestamps)]\n    # find gaps\n    gap = np.diff(timestamps) &gt; 1.0 / sampling_rate * gap_proportion\n\n    # all true entries of gap represent gaps. Get the times bounding these intervals.\n    gapind = np.asarray(np.where(gap))\n    # The end of each valid interval are the indices of the gaps and the final value\n    valid_end = np.append(gapind, np.asarray(len(timestamps) - 1))\n\n    # the beginning of the gaps are the first element and gapind+1\n    valid_start = np.insert(gapind + 1, 0, 0)\n\n    valid_indices = np.vstack([valid_start, valid_end]).transpose()\n\n    valid_times = timestamps[valid_indices]\n    # adjust the times to deal with single valid samples\n    valid_times[:, 0] = valid_times[:, 0] - eps\n    valid_times[:, 1] = valid_times[:, 1] + eps\n\n    valid_intervals = (valid_times[:, 1] - valid_times[:, 0]) &gt; min_valid_len\n\n    return valid_times[valid_intervals, :]\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_electrode_indices", "title": "<code>get_electrode_indices(nwb_object, electrode_ids)</code>", "text": "<p>Given an NWB file or electrical series object, return the indices of the specified electrode_ids.</p> <p>If an ElectricalSeries is given, then the indices returned are relative to the selected rows in ElectricalSeries.electrodes. For example, if electricalseries.electrodes = [5], and row index 5 of nwbfile.electrodes has ID 10, then calling get_electrode_indices(electricalseries, 10) will return 0, the index of the matching electrode in electricalseries.electrodes.</p> <p>Indices for electrode_ids that are not in the electrical series are returned as np.nan</p> <p>If an NWBFile is given, then the row indices with the matching IDs in the file's electrodes table are returned.</p> <p>Parameters:</p> Name Type Description Default <code>nwb_object</code> <code>pynwb.NWBFile or pynwb.ecephys.ElectricalSeries</code> <p>The NWB file object or NWB electrical series object.</p> required <code>electrode_ids</code> <code>np.ndarray or list</code> <p>Array or list of electrode IDs.</p> required <p>Returns:</p> Name Type Description <code>electrode_indices</code> <code>list</code> <p>Array of indices of the specified electrode IDs.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_electrode_indices(nwb_object, electrode_ids):\n\"\"\"Given an NWB file or electrical series object, return the indices of the specified electrode_ids.\n\n    If an ElectricalSeries is given, then the indices returned are relative to the selected rows in\n    ElectricalSeries.electrodes. For example, if electricalseries.electrodes = [5], and row index 5 of\n    nwbfile.electrodes has ID 10, then calling get_electrode_indices(electricalseries, 10) will return 0, the\n    index of the matching electrode in electricalseries.electrodes.\n\n    Indices for electrode_ids that are not in the electrical series are returned as np.nan\n\n    If an NWBFile is given, then the row indices with the matching IDs in the file's electrodes table are returned.\n\n    Parameters\n    ----------\n    nwb_object : pynwb.NWBFile or pynwb.ecephys.ElectricalSeries\n        The NWB file object or NWB electrical series object.\n    electrode_ids : np.ndarray or list\n        Array or list of electrode IDs.\n\n    Returns\n    -------\n    electrode_indices : list\n        Array of indices of the specified electrode IDs.\n    \"\"\"\n    if isinstance(nwb_object, pynwb.ecephys.ElectricalSeries):\n        # electrodes is a DynamicTableRegion which may contain a subset of the rows in NWBFile.electrodes\n        # match against only the subset of electrodes referenced by this ElectricalSeries\n        electrode_table_indices = nwb_object.electrodes.data[:]\n        selected_elect_ids = [\n            nwb_object.electrodes.table.id[x] for x in electrode_table_indices\n        ]\n    elif isinstance(nwb_object, pynwb.NWBFile):\n        # electrodes is a DynamicTable that contains all electrodes\n        selected_elect_ids = list(nwb_object.electrodes.id[:])\n    else:\n        raise ValueError(\n            \"nwb_object must be of type ElectricalSeries or NWBFile\"\n        )\n\n    # for each electrode_id, find its index in selected_elect_ids and return that if it's there and invalid_electrode_index if not.\n    return [\n        selected_elect_ids.index(elect_id)\n        if elect_id in selected_elect_ids\n        else invalid_electrode_index\n        for elect_id in electrode_ids\n    ]\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_all_spatial_series", "title": "<code>get_all_spatial_series(nwbf, verbose=False)</code>", "text": "<p>Given an NWBFile, get the spatial series and interval lists from the file and return a dictionary by epoch.</p> <p>Parameters:</p> Name Type Description Default <code>nwbf</code> <code>pynwb.NWBFile</code> <p>The source NWB file object.</p> required <code>verbose</code> <code>bool</code> <p>Flag representing whether to print the sampling rate.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pos_data_dict</code> <code>dict</code> <p>Dict mapping indices to a dict with keys 'valid_times' and 'raw_position_object_id'. Returns None if there is no position data in the file. The 'raw_position_object_id' is the object ID of the SpatialSeries object.</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_all_spatial_series(nwbf, verbose=False):\n\"\"\"Given an NWBFile, get the spatial series and interval lists from the file and return a dictionary by epoch.\n\n    Parameters\n    ----------\n    nwbf : pynwb.NWBFile\n        The source NWB file object.\n    verbose : bool\n        Flag representing whether to print the sampling rate.\n\n    Returns\n    -------\n    pos_data_dict : dict\n        Dict mapping indices to a dict with keys 'valid_times' and 'raw_position_object_id'. Returns None if there\n        is no position data in the file. The 'raw_position_object_id' is the object ID of the SpatialSeries object.\n    \"\"\"\n    position = get_data_interface(nwbf, \"position\", pynwb.behavior.Position)\n    if position is None:\n        return None\n\n    # for some reason the spatial_series do not necessarily come out in order, so we need to figure out the right order\n    epoch_start_time = np.zeros(len(position.spatial_series.values()))\n    for pos_epoch, spatial_series in enumerate(\n        position.spatial_series.values()\n    ):\n        epoch_start_time[pos_epoch] = spatial_series.timestamps[0]\n\n    sorted_order = np.argsort(epoch_start_time)\n    pos_data_dict = dict()\n\n    for index, orig_epoch in enumerate(sorted_order):\n        spatial_series = list(position.spatial_series.values())[orig_epoch]\n        pos_data_dict[index] = dict()\n        # get the valid intervals for the position data\n        timestamps = np.asarray(spatial_series.timestamps)\n\n        # estimate the sampling rate\n        timestamps = np.asarray(spatial_series.timestamps)\n        sampling_rate = estimate_sampling_rate(timestamps, 1.75)\n        if sampling_rate &lt; 0:\n            raise ValueError(\n                f\"Error adding position data for position epoch {index}\"\n            )\n        if verbose:\n            print(\n                \"Processing raw position data. Estimated sampling rate: {} Hz\".format(\n                    sampling_rate\n                )\n            )\n        # add the valid intervals to the Interval list\n        pos_data_dict[index][\"valid_times\"] = get_valid_intervals(\n            timestamps,\n            sampling_rate,\n            gap_proportion=2.5,\n            min_valid_len=int(sampling_rate),\n        )\n        pos_data_dict[index][\n            \"raw_position_object_id\"\n        ] = spatial_series.object_id\n\n    return pos_data_dict\n</code></pre>"}, {"location": "api/src/spyglass/utils/nwb_helper_fn/#src.spyglass.utils.nwb_helper_fn.get_nwb_copy_filename", "title": "<code>get_nwb_copy_filename(nwb_file_name)</code>", "text": "<p>Get file name of copy of nwb file without the electrophys data</p> Source code in <code>src/spyglass/utils/nwb_helper_fn.py</code> <pre><code>def get_nwb_copy_filename(nwb_file_name):\n\"\"\"Get file name of copy of nwb file without the electrophys data\"\"\"\n\n    filename, file_extension = os.path.splitext(nwb_file_name)\n\n    return f\"{filename}_{file_extension}\"\n</code></pre>"}, {"location": "installation/", "title": "Installation", "text": ""}, {"location": "installation/#production", "title": "Production", "text": "<p>Install Production if you want to use the officially released version of spyglass. This is meant for regular end-users. Click here to for instructuons - Production</p>"}, {"location": "installation/#local", "title": "Local", "text": "<p>Install Local if you want to work on the development-version in order to add features. This is meant for anyone interested in making updates to the code base. Click here to for instructuons - Local</p>"}, {"location": "installation/local/", "title": "Local Installation", "text": ""}, {"location": "installation/local/#clone-repository", "title": "Clone Repository", "text": "<p>For local development, first pull down the code base -</p> <pre><code>git clone https://github.com/LorenFrankLab/spyglass.git\n</code></pre> <p>Set up and activate a conda environment from environment.yml:</p> <pre><code>cd spyglass\nconda env create -f environment.yml\nconda activate spyglass\n</code></pre> <p>Install this repository:</p> <pre><code>pip install -e .\n</code></pre>"}, {"location": "installation/local/#additional-packages", "title": "Additional Packages", "text": "<p>Some of the pipeline requires installation of additional packages. For example, the spike sorting pipeline relies on <code>spikeinterface</code>. We recommend installing it directly from the GitHub repo:</p> <pre><code>pip install spikeinterface[full,widgets]\n</code></pre> <p>You may also need to install individual sorting algorithms. For example, Loren Frank's lab at UCSF typically uses <code>mountainsort4</code>:</p> <pre><code>pip install mountainsort4\n</code></pre> <p>WARNING: If you are on an M1 Mac, you need to install <code>pyfftw</code> via <code>conda</code> BEFORE installing <code>ghostipy</code>:</p> <pre><code>conda install -c conda-forge pyfftw\n</code></pre> <p>The LFP pipeline uses <code>ghostipy</code>:</p> <pre><code>pip install ghostipy\n</code></pre>"}, {"location": "installation/local/#setting-up-database-access", "title": "Setting up database access", "text": "<ol> <li> <p>To use <code>spyglass</code>, you need to have access to a MySQL database. If your lab    already administers a database, connect to it by setting    DataJoint configurations. If you want to run    your own database, consult instructions in datajoint tutorial    and/or our tutorial notebook.</p> </li> <li> <p>Add the following environment variables (e.g. in <code>~/.bashrc</code>). The following    are specific to Frank lab so you may want to change <code>SPYGLASS_BASE_DIR</code>.</p> </li> </ol> <p><code>bash    export SPYGLASS_BASE_DIR=\"/stelmo/nwb\"    export SPYGLASS_RECORDING_DIR=\"$SPYGLASS_BASE_DIR/recording\"    export SPYGLASS_SORTING_DIR=\"$SPYGLASS_BASE_DIR/sorting\"    export SPYGLASS_VIDEO_DIR=\"$SPYGLASS_BASE_DIR/video\"    export SPYGLASS_WAVEFORMS_DIR=\"$SPYGLASS_BASE_DIR/waveforms\"    export SPYGLASS_TEMP_DIR=\"$SPYGLASS_BASE_DIR/tmp/spyglass\"    export DJ_SUPPORT_FILEPATH_MANAGEMENT=\"TRUE\"</code></p> <p>Note that a local <code>SPYGLASS_TEMP_DIR</code> (e.g. one on your machine) will speed    up spike sorting, but make sure it has enough free space (ideally at least    500GB)</p> <p>Before proceeding, run -</p> <p><code>bash    source ~/.bashrc</code></p> <p>in order to persist the changes.</p> <ol> <li>Set up <code>kachery-cloud</code>    (Frank Lab members only). Once you have initialized a <code>kachery-cloud</code>    directory, add the following environment variables (again, shown for Frank    lab).</li> </ol> <p><code>bash    export KACHERY_CLOUD_DIR=\"$SPYGLASS_BASE_DIR/.kachery-cloud\"    export KACHERY_TEMP_DIR=\"$SPYGLASS_BASE_DIR/tmp\"</code></p> <p>Before proceeding, run -</p> <p><code>bash    source ~/.bashrc</code></p> <p>in order to persist the changes.</p> <ol> <li>Configure DataJoint. To connect to the    DataJoint database, we have to specify    information about it such as the hostname and the port. You should also    change your password from the temporary one you were given. Go to the config    directory, and run    <code>dj_config.py</code>    in the config folder. Then run    <code>dj_config.py</code>    terminal with your username:</li> </ol> <p><code>bash    cd config # change to the config directory    python dj_config.py &lt;username&gt; # run the configuration script</code></p> <p>Finally, open up a python console (e.g. run <code>ipython</code> from terminal) and import <code>spyglass</code> to check that the installation has worked.</p>"}, {"location": "installation/production/", "title": "Production Installation", "text": ""}, {"location": "installation/production/#virtual-environment", "title": "Virtual Environment", "text": "<p>It is recommended you install a virtual environment. There are many options like conda and venv. This installation instruction will use conda.</p> <p>The instructions to install conda can be found at https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html. Conda can be used both for installing packages and for creating a virtual environment.</p> <p>To create the environment after conda is installed, run -</p> <pre><code>conda create -n spyglass-env python=3.9.0\n</code></pre> <p>The name selected here is <code>spyglass-env</code>. However, a different name can be used if desired or necessary.</p> <p>Now, it is time to activate the virtual environment. To start, run -</p> <pre><code>conda activate spyglass-env\n</code></pre>"}, {"location": "installation/production/#installing-spyglass", "title": "Installing Spyglass", "text": "<p><code>spyglass</code> can be installed via pip off of pypi:</p> <pre><code>pip install spyglass-neuro\n</code></pre>"}, {"location": "installation/production/#additional-packages", "title": "Additional Packages", "text": "<p>Some pipelines require installation of additional packages. For example, the spike sorting pipeline relies on <code>spikeinterface</code>. We recommend installing it directly from the GitHub repo:</p> <pre><code>pip install spikeinterface[full,widgets]\n</code></pre> <p>You may also need to install individual sorting algorithms. For example, Loren Frank's lab at UCSF typically uses <code>mountainsort4</code>:</p> <pre><code>pip install mountainsort4\n</code></pre> <p>WARNING: If you are on an M1 Mac, you need to install <code>pyfftw</code> via <code>conda</code> BEFORE installing <code>ghostipy</code>:</p> <pre><code>conda install -c conda-forge pyfftw\n</code></pre> <p>The LFP pipeline uses <code>ghostipy</code>:</p> <pre><code>pip install ghostipy\n</code></pre>"}, {"location": "installation/production/#setting-up-database-access", "title": "Setting up database access", "text": "<ol> <li> <p>To use <code>spyglass</code>, you need to have access to a MySQL database. If your lab    already administers a database, connect to it by setting    DataJoint configurations. If you want to run    your own database, consult instructions in    datajoint tutorial    and/or our tutorial notebook.</p> </li> <li> <p>Add the following environment variables (e.g. in <code>~/.bashrc</code>). The following    are specific to Frank lab so you may want to change <code>SPYGLASS_BASE_DIR</code>.</p> </li> </ol> <p><code>bash    export SPYGLASS_BASE_DIR=\"/stelmo/nwb\"    export SPYGLASS_RECORDING_DIR=\"$SPYGLASS_BASE_DIR/recording\"    export SPYGLASS_SORTING_DIR=\"$SPYGLASS_BASE_DIR/sorting\"    export SPYGLASS_VIDEO_DIR=\"$SPYGLASS_BASE_DIR/video\"    export SPYGLASS_WAVEFORMS_DIR=\"$SPYGLASS_BASE_DIR/waveforms\"    export SPYGLASS_TEMP_DIR=\"$SPYGLASS_BASE_DIR/tmp/spyglass\"    export DJ_SUPPORT_FILEPATH_MANAGEMENT=\"TRUE\"</code></p> <p>Note that a local <code>SPYGLASS_TEMP_DIR</code> (e.g. one on your machine) will speed    up spike sorting, but make sure it has enough free space (ideally at least    500GB)</p> <p>Before proceeding, run -</p> <p><code>bash    source ~/.bashrc</code></p> <p>in order to persist the changes.</p> <ol> <li>Set up <code>kachery-cloud</code>    (if you are in Frank lab, skip this step). Once you have initialized a    <code>kachery-cloud</code> directory, add the following environment variables (again,    shown for Frank lab).</li> </ol> <p><code>bash    export KACHERY_CLOUD_DIR=\"$SPYGLASS_BASE_DIR/.kachery-cloud\"    export KACHERY_TEMP_DIR=\"$SPYGLASS_BASE_DIR/tmp\"</code></p> <p>Before proceeding, run -</p> <p><code>bash    source ~/.bashrc</code></p> <p>in order to persist the changes.</p> <ol> <li>Configure DataJoint. To connect to the    DataJoint database, we have to specify    information about it such as the hostname and the port. You should also    change your password from the temporary one you were given. Download    <code>dj_config.py</code>    from    https://github.com/LorenFrankLab/spyglass/blob/master/config/dj_config.py    and save locally as <code>dj_config.py</code>, to any folder. Instructions on how to    download a single file from github can be found at    https://stackoverflow.com/a/13593430/178550.    Then run <code>dj_config.py</code> in a terminal with your username -</li> </ol> <p><code>bash    cd config # change to the config directory    python dj_config.py &lt;username&gt; # run the configuration script</code></p> <p>Finally, open up a python console (e.g. run <code>ipython</code> from terminal) and import <code>spyglass</code> to check that the installation has worked.</p>"}, {"location": "misc/figurl_views/", "title": "Creating figurl views", "text": ""}, {"location": "misc/figurl_views/#spike-sorting-recording-view", "title": "Spike sorting recording view", "text": "<pre><code>import spyglass.common as ndc\nimport spyglass.figurl_views as ndf\n\nquery = ...\n\n# To replace:\n# (ndf.SpikeSortingRecordingView &amp; query).delete()\n\nndf.SpikeSortingRecordingView.populate([(ndc.SpikeSortingRecording &amp; query).proj()])\n</code></pre>"}, {"location": "misc/figurl_views/#spike-sorting-view", "title": "Spike sorting view", "text": "<pre><code>import spyglass.common as ndc\nimport spyglass.figurl_views as ndf\n\nquery = ...\n\n# To replace:\n# (ndf.SpikeSortingView &amp; query).delete()\n\nndf.SpikeSortingView.populate([(ndc.SpikeSorting &amp; query).proj()])\n</code></pre>"}, {"location": "misc/insert_data/", "title": "How to insert data into <code>spyglass</code>", "text": "<p>In <code>spyglass</code>, every table corresponds to an object. An experimental session is defined as a collection of such objects. When an NWB file is ingested into <code>spyglass</code>, the information about these objects is first read and inserted into tables in the <code>common</code> module (e.g. <code>Institution</code>, <code>Lab</code>, <code>Electrode</code>, etc). However, not every NWB file has all the information required by <code>spyglass</code>. For example, many NWB files do not contain any information about the <code>DataAcquisitionDevice</code> or <code>Probe</code> because NWB does not yet have an official standard for specifying them. In addition, one might find that the information contained in the NWB file is incorrect and would like to modify it before inserting it into <code>spyglass</code> without having to go through the time-consuming process of re-generating the NWB file. For these cases, we provide an alternative approach to inserting data to <code>spyglass</code>.</p> <p>This alternate approach consists of two steps. First, the user must identify entries that they would like to add to the <code>spyglass</code> database that exist independently of any particular NWB file. For example, information about a particular probe is stored in the <code>ProbeType</code> and <code>Probe</code> tables of <code>spyglass.common</code>. The user can either:</p> <ol> <li> <p>create these entries programmatically using DataJoint <code>insert</code> commands, for    example:</p> <p><code>python sgc.ProbeType.insert1({ \"probe_type\": \"128c-4s6mm6cm-15um-26um-sl\", \"probe_description\": \"A Livermore flexible probe with 128 channels, 4 shanks, 6 mm shank length, 6 cm ribbon length. 15 um contact diameter, 26 um center-to-center distance (pitch), single-line configuration.\", \"manufacturer\": \"Lawrence Livermore National Lab\", \"num_shanks\": 4, }, skip_duplicates=True)</code></p> </li> <li> <p>define these entries in a special YAML file called <code>entries.yaml</code> that is    processed when <code>spyglass</code> is imported. One can think of <code>entries.yaml</code> as a    place to define information that the database should come pre-equipped prior    to ingesting any NWB files. The <code>entries.yaml</code> file should be placed in the    <code>spyglass</code> base directory. An example can be found in    <code>examples/config_yaml/entries.yaml</code>. It has the following structure:</p> <p><code>yaml TableName: - TableEntry1Field1: Value TableEntry1Field2: Value - TableEntry2Field1: Value TableEntry2Field2: Value</code></p> <p>For example,</p> <p><code>yaml ProbeType: - probe_type: 128c-4s6mm6cm-15um-26um-sl probe_description: A Livermore flexible probe with 128 channels, 4 shanks, 6 mm shank length, 6 cm ribbon length. 15 um contact diameter, 26 um center-to-center distance (pitch), single-line configuration. manufacturer: Lawrence Livermore National Lab num_shanks: 4</code></p> </li> </ol> <p>Using a YAML file over programmatically creating these entries in a notebook or script has the advantages that the YAML file maintains a record of what entries have been added that is easy to access, and the file is portable and can be shared alongside an NWB file or set of NWB files from a given experiment.</p> <p>Next, the user must associate the NWB file with entries defined in the database. This is done by cresqating a configuration file, which must: be in the same directory as the NWB file that it configures be in YAML format have the following naming convention: <code>&lt;name_of_nwb_file&gt;_spyglass_config.yaml</code>.</p> <p>Users can programmatically generate this configuration file. It is then read by spyglass when calling <code>insert_session</code> on the associated NWB file.</p> <p>An example of this can be found at <code>examples/config_yaml/\u200b\u200bsub-AppleBottom_ses-AppleBottom-DY20-g3_behavior+ecephys_spyglass_config.yaml</code>. This file is associated with the NWB file <code>sub-AppleBottom_ses-AppleBottom-DY20-g3_behavior+ecephys.nwb</code>.</p> <p>This is the general format for the config entry:</p> <pre><code>TableName:\n- primary_key1: value1\n</code></pre> <p>For example:</p> <pre><code>DataAcquisitionDevice:\n- data_acquisition_device_name: Neuropixels Recording Device\n</code></pre> <p>In this example, the NWB file that corresponds to this config YAML will become associated with the DataAcquisitionDevice with primary key data_acquisition_device_name: Neuropixels Recording Device. This entry must exist.</p>"}, {"location": "misc/merge_tables/", "title": "Merge Tables", "text": ""}, {"location": "misc/merge_tables/#why", "title": "Why", "text": "<p>A pipeline may diverge when we want to process the same data in different ways. Merge Tables allow us to join divergent pipelines together, and unify downstream processing steps. For a more in depth discussion, please refer to this notebook and related discussions here and here.</p> <p>Note: Deleting entries upstream of Merge Tables will throw errors related to deleting a part entry before the master. To circumvent this, you can add <code>force_parts=True</code> to the <code>delete</code> function call, but this will leave and orphaned primary key in the master. Instead, use <code>spyglass.utils.dj_merge_tables.delete_downstream_merge</code> to delete master/part pairs.</p>"}, {"location": "misc/merge_tables/#what", "title": "What", "text": "<p>A Merge Table is fundamentally a master table with one part for each divergent pipeline. By convention...</p> <ol> <li> <p>The master table has one primary key, <code>merge_id</code>, a    UUID, and one    secondary attribute, <code>source</code>, which gives the part table name. Both are    managed with the custom <code>insert</code> function of this class.</p> </li> <li> <p>Each part table has inherits the final table in its respective pipeline, and    shares the same name as this table.</p> </li> </ol> <pre><code>from spyglass.utils.dj_merge_tables import Merge\n\n@schema\nclass MergeTable(Merge):\n    definition = \"\"\"\n    merge_id: uuid\n    ---\n    source: varchar(32)\n    \"\"\"\n\n    class One(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        ---\n        -&gt; One\n        \"\"\"\n\n    class Two(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        ---\n        -&gt; Two\n        \"\"\"\n</code></pre>"}, {"location": "misc/merge_tables/#how", "title": "How", "text": "<p>The Merge class in Spyglass's utils is a subclass of DataJoint's Manual Table and adds functions to make the awkwardness of part tables more manageable. These functions are described in the API section, under <code>utils.dj_merge_tables</code>.</p> <p>One quirk of these utilities is that they take restrictions as arguments, rather than with operators. So <code>Table &amp; \"field='value'\"</code> becomes <code>MergeTable.merge_view(restriction=\"field='value'\"</code>)<code>. This is because</code>merge_view<code>is a</code>Union` rather than a true Table.</p>"}, {"location": "misc/merge_tables/#example", "title": "Example", "text": "<p>First, we'll import various items related to the LFP Merge Table...</p> <pre><code>from spyglass.utils.dj_merge_tables import delete_downstream_merge, Merge\nfrom spyglass.common.common_ephys import LFP as CommonLFP  # Upstream 1\nfrom spyglass.lfp.lfp_merge import LFPOutput  # Merge Table\nfrom spyglass.lfp.v1.lfp import LFPV1  # Upstream 2\n</code></pre> <p>Merge Tables have multiple custom methods that begin with <code>merge</code>. <code>help</code> can show us the docstring of each</p> <pre><code>merge_methods=[d for d in dir(Merge) if d.startswith('merge')]\nhelp(getattr(Merge,merge_methods[-1]))\n</code></pre> <p>We'll use this example to explore populating both <code>LFPV1</code> and the <code>LFPOutput</code> Merge Table.</p> <pre><code>nwb_file_dict = { # We'll use this later when fetching from the Merge Table\n    \"nwb_file_name\": \"tonks20211103_.nwb\",\n}\nlfpv1_key = {\n    **nwb_file_dict,\n    \"lfp_electrode_group_name\": \"CA1_test\",\n    \"target_interval_list_name\": \"test interval2\",\n    \"filter_name\": \"LFP 0-400 Hz\",\n    \"filter_sampling_rate\": 30000,\n}\nLFPV1.populate(lfpv1_key)  # Also populates LFPOutput\n</code></pre> <p>The Merge Table can also be populated with keys from <code>common_ephys.LFP</code>.</p> <pre><code>common_keys = CommonLFP.fetch(limit=3, as_dict=True)\nLFPOutput.insert1(common_keys[0], skip_duplicates=True)\nLFPOutput.insert(common_keys[1:], skip_duplicates=True)\n</code></pre> <p><code>merge_view</code> shows a union of the master and all part tables.</p> <pre><code>LFPOutput.merge_view()\nLFPOutput.merge_view(restriction=lfpv1_key)\n</code></pre> <p>UUIDs help retain unique entries across all part tables. We can fetch NWB file by referencing this or other features.</p> <pre><code>uuid_key = LFPOutput.fetch(limit=1, as_dict=True)[-1]\nrestrict = LFPOutput &amp; uuid_key\nresult1 = restrict.fetch_nwb()\n\nnwb_key = LFPOutput.merge_restrict(nwb_file_dict).fetch(as_dict=True)[0]\nresult2 = (LFPOutput &amp; nwb_key).fetch_nwb()\n</code></pre> <p>There are also functions for retrieving part table(s) and fetching data. This <code>fetch</code> will collect all relevant entries and return them as a list in the format specified by keyword arguments and one's DataJoint config.</p> <pre><code>result3 = (LFPOutput &amp; common_keys[0]).merge_get_part(join_master=True)\nresult4 = LFPOutput().merge_get_part(restriction=common_keys[0])\nresult5 = LFPOutput.merge_fetch(\"filter_name\", \"nwb_file_name\")\nresult6 = LFPOutput.merge_fetch(as_dict=True)\n</code></pre> <p>When deleting from Merge Tables, we can either...</p> <ol> <li>delete from the Merge Table itself with <code>merge_delete</code>_parent, deleting both    the master and part.</li> <li>use <code>merge_delete_parent</code> to delete from the parent sources, getting rid of    the entries in the source table they came from.</li> <li>use <code>delete_downstream_merge</code> to find Merge Tables downstream and get rid    full entries, avoiding orphaned master table entries.</li> </ol> <p>The two latter cases can be destructive, so we include an extra layer of protection with <code>dry_run</code>. When true (by default), these functions return a list of tables with the entries that would otherwise be deleted.</p> <pre><code>LFPOutput.merge_delete(common_keys[0])  # Delete from merge table\nLFPOutput.merge_delete_parent(restriction=nwb_file_dict, dry_run=True)\ndelete_downstream_merge(\n    table=CommonLFP, restriction=common_keys[0], dry_run=True\n)\n</code></pre>"}, {"location": "misc/session_groups/", "title": "Session groups", "text": "<p>A session group is a collection of sessions. Each group has a name (primary key) and a description.</p> <pre><code>from spyglass.common import SessionGroup\n\n# Create a new session group\nSessionGroup.add_group('test_group_1', 'Description of test group 1')\n\n# Get the table of session groups\nSessionGroup()\n\n# Add a session to the group\nSessionGroup.add_session_to_group('RN2_20191110_.nwb', 'test_group_1')\n\n# Remove a session from a group\n# SessionGroup.remove_session_from_group('RN2_20191110_.nwb', 'test_group_1')\n\n# Get all sessions in group\nSessionGroup.get_group_sessions('test_group_1')\n\n# Update the description of a session group\nSessionGroup.update_session_group_description('test_group_1', 'Test description')\n</code></pre>"}, {"location": "notebooks/00_intro/", "title": "Introduction", "text": "In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\n\nimport spyglass as sg\nimport datajoint as dj\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\nwarnings.simplefilter(\"ignore\", category=UserWarning)\n</pre> import os import numpy as np  import spyglass as sg import datajoint as dj  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) warnings.simplefilter(\"ignore\", category=UserWarning) In\u00a0[\u00a0]: Copied! <pre># We also import spyglass.common so we can call tables more easily\nimport spyglass.common as sgc\n\n# We import spyglass.data_import to allow for inserting an NWB file into the database\nimport spyglass.data_import as sdi\n</pre> # We also import spyglass.common so we can call tables more easily import spyglass.common as sgc  # We import spyglass.data_import to allow for inserting an NWB file into the database import spyglass.data_import as sdi In\u00a0[\u00a0]: Copied! <pre># Draw tables that are three levels below and one level above Session\ndj.ERD(sgc.Session) - 1 + 3\n</pre> # Draw tables that are three levels below and one level above Session dj.ERD(sgc.Session) - 1 + 3 <p>Now that we have a general idea of how our database is organized, we will try inserting new data to it. We assume that the data is a neural recording (along with other auxiliary data) that has already been converted to the NWB format. For the purposes of this tutorial, we will use <code>montague20200802.nwb</code>. If you're accessing the Frank lab database on the UCSF network, this file can be found in <code>/stelmo/nwb/raw</code> directory (assuming you have mounted <code>stelmo</code> at <code>/</code>). If you do not have access to Frank lab database, then download <code>montague20200802.nwb</code> from here - It is about 8 GB in size.</p> <p>Once you have the NWB file, you should copy it and rename the copy to something unique (e.g. <code>montague20200802_yourname.nwb</code>). This is because many people will be using this example file to practice inserting data, and a file can only be inserted once (the file name acts as a primary key in the <code>Session</code> table).</p> In\u00a0[\u00a0]: Copied! <pre># Define the name of the file that you copied and renamed; make sure it's something unique.\nnwb_file_name = \"montague20200802_tutorial.nwb\"\nfilename, file_extension = os.path.splitext(nwb_file_name)\n# This is a copy of the original nwb file, except it doesn't contain the raw data (for storage reasons)\nnwb_copy_file_name = filename + \"_\" + file_extension\n</pre> # Define the name of the file that you copied and renamed; make sure it's something unique. nwb_file_name = \"montague20200802_tutorial.nwb\" filename, file_extension = os.path.splitext(nwb_file_name) # This is a copy of the original nwb file, except it doesn't contain the raw data (for storage reasons) nwb_copy_file_name = filename + \"_\" + file_extension In\u00a0[\u00a0]: Copied! <pre>nwb_file_name\n</pre> nwb_file_name In\u00a0[\u00a0]: Copied! <pre>sdi.insert_sessions(nwb_file_name)\n</pre> sdi.insert_sessions(nwb_file_name) In\u00a0[\u00a0]: Copied! <pre>sgc.Lab()\n</pre> sgc.Lab() <p>There is only one attribute (<code>lab_name</code>) and two entries (<code>Loren Frank</code>, <code>Giocomo</code>) in this table. Note that even though <code>Lab</code> is one of the tables that has to be manually entered (i.e. the green rectangles in our ERD), <code>sg.insert_sessions</code> populates it from the NWB file.</p> <p>Let's look at a more interesting table: <code>Session</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sgc.Session()\n</pre> sgc.Session() <p>This session has multiple attributes. The attributes that make up the primary key are shown in bold. In this case, there is only one attribute in the primary key: <code>nwb_file_name</code>. Given that many downstream tables are connected to <code>Session</code> via bold lines, you can use this attribute to uniquely define data entries in many tables in our database.</p> In\u00a0[\u00a0]: Copied! <pre># can also look at the docstring\nsgc.Session.describe()\n</pre> # can also look at the docstring sgc.Session.describe() <p>To look at specific entries from a table, include the appropriate condition with the <code>&amp;</code> operator. The condition must be in the form of a key-value pair, or a dictionary. As an example, let's view only the entry whose <code>nwb_file_name</code> is our example NWB file in <code>Session</code>:</p> In\u00a0[\u00a0]: Copied! <pre>sgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> sgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name} <p>Now let's go to a downstream table: <code>Raw</code>. This table is connected to <code>Session</code> table with a bold line, so it has the same primary key.</p> In\u00a0[\u00a0]: Copied! <pre>sgc.Raw &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> sgc.Raw &amp; {\"nwb_file_name\": nwb_copy_file_name} <p><code>IntervalList</code> table is connected to <code>Session</code> table with a solid line, as it inherits <code>nwb_file_name</code> as one of the two attributes that make up the primary key. This means that you need to know both <code>nwb_file_name</code> and <code>interval_list_name</code> to uniquely identify an entry.</p> In\u00a0[\u00a0]: Copied! <pre>sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name} <p>As you can see, some of the data is shown in the table (e.g. <code>interval_list_name</code>) while others are obscured (e.g. <code>valid_times</code> are shown as <code>=BLOB=</code>). To inspect the data, use the <code>fetch</code> (for getting multiple entries) or <code>fetch1</code> (for getting just one entry) methods. The following query returns <code>valid_times</code> of an interval list called <code>04_r2</code>.</p> In\u00a0[\u00a0]: Copied! <pre>(\n    sgc.IntervalList\n    &amp; {\"nwb_file_name\": nwb_copy_file_name, \"interval_list_name\": \"04_r2\"}\n).fetch1(\"valid_times\")\n</pre> (     sgc.IntervalList     &amp; {\"nwb_file_name\": nwb_copy_file_name, \"interval_list_name\": \"04_r2\"} ).fetch1(\"valid_times\") <p>Query supports many operations and conditions - just make sure the conditions are in the form of a dictionary. We have already seen that <code>&amp;</code> represents the set-theoretic intersection. <code>-</code>, on the other hand, is like the set-theoretic complement. The following query returns all <code>interval_list_name</code> that is not <code>01_s1</code> or <code>04_r2</code>.</p> In\u00a0[\u00a0]: Copied! <pre>(\n    (\n        (sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name})\n        - {\"interval_list_name\": \"01_s1\"}\n    )\n    - {\"interval_list_name\": \"04_r2\"}\n).fetch(\"interval_list_name\")\n</pre> (     (         (sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name})         - {\"interval_list_name\": \"01_s1\"}     )     - {\"interval_list_name\": \"04_r2\"} ).fetch(\"interval_list_name\") <p>Other available operations are described here. Also feel free to inspect other tables and learn what kind of data they contain.</p> In\u00a0[\u00a0]: Copied! <pre># our data is currently in Session table\nsgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> # our data is currently in Session table sgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name} In\u00a0[\u00a0]: Copied! <pre># Type `yes` when prompted to delete\n(sgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name}).delete()\n</pre> # Type `yes` when prompted to delete (sgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name}).delete() In\u00a0[\u00a0]: Copied! <pre># Check that delete worked\nsgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> # Check that delete worked sgc.Session &amp; {\"nwb_file_name\": nwb_copy_file_name} <p><code>delete</code> method is useful when you need to re-do something. We discourage editing an entry because this will affect the associated entries in downstream tables and violate data integrity. Instead it is recommended that you just delete and re-enter it. This is easy to do, as most data entry is automated in our pipeline.</p> In\u00a0[\u00a0]: Copied! <pre># Entries are also gone from downstream tables, e.g. IntervalList\nsgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> # Entries are also gone from downstream tables, e.g. IntervalList sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name} <p>We're not quite done yet. Not only should we delete our entry from <code>Session</code>, we should also delete the associated entry in <code>Nwbfile</code>. Recall from the entity relationship diagram that <code>Nwbfile</code> is upstream of <code>Session</code>. That means getting rid of our entry from <code>Session</code> doesn't affect the corresponding entry in <code>Nwbfile</code> - this has to be manually removed. To do so, we again use the <code>delete</code> method, but in the case of <code>Nwbfile</code> there is an extra step: we need to remove the NWB file itself as well as the entry from <code>Nwbfile</code> table. To remove the files, we run the <code>cleanup</code> method with the <code>delete_files</code> argument as <code>True</code>.</p> <p>Note that the same idea applies to deleting files from <code>AnalysisNwbfile</code> table.</p> In\u00a0[\u00a0]: Copied! <pre># Check out the Nwb file\nsgc.Nwbfile &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> # Check out the Nwb file sgc.Nwbfile &amp; {\"nwb_file_name\": nwb_copy_file_name} In\u00a0[\u00a0]: Copied! <pre># Let's delete the entry\n(sgc.Nwbfile &amp; {\"nwb_file_name\": nwb_copy_file_name}).delete()\n</pre> # Let's delete the entry (sgc.Nwbfile &amp; {\"nwb_file_name\": nwb_copy_file_name}).delete() In\u00a0[\u00a0]: Copied! <pre># Note that the file (ends with _.nwb) has not been deleted, even though the entry is\n!ls $SPYGLASS_BASE_DIR/raw\n</pre> # Note that the file (ends with _.nwb) has not been deleted, even though the entry is !ls $SPYGLASS_BASE_DIR/raw In\u00a0[\u00a0]: Copied! <pre># We clean it up\nsgc.Nwbfile().cleanup(delete_files=True)\n</pre> # We clean it up sgc.Nwbfile().cleanup(delete_files=True) In\u00a0[\u00a0]: Copied! <pre># Now the file is gone as well\n!ls $SPYGLASS_BASE_DIR/raw\n</pre> # Now the file is gone as well !ls $SPYGLASS_BASE_DIR/raw In\u00a0[\u00a0]: Copied! <pre># take a look at the lab members\nsgc.LabMember()\n</pre> # take a look at the lab members sgc.LabMember() In\u00a0[\u00a0]: Copied! <pre># LabMember also has a Parts table called LabMemberInfo\nsgc.LabMember.LabMemberInfo()\n</pre> # LabMember also has a Parts table called LabMemberInfo sgc.LabMember.LabMemberInfo() In\u00a0[\u00a0]: Copied! <pre># these are the existing lab teams\nsgc.LabTeam()\n</pre> # these are the existing lab teams sgc.LabTeam() In\u00a0[\u00a0]: Copied! <pre># create a new team\n# change team_name to something unique\n# change team_members\n# team_description is optional\nsgc.LabTeam().create_new_team(\n    team_name=\"Beans\", team_members=[\"Alison Comrie\"], team_description=\"test\"\n)\n</pre> # create a new team # change team_name to something unique # change team_members # team_description is optional sgc.LabTeam().create_new_team(     team_name=\"Beans\", team_members=[\"Alison Comrie\"], team_description=\"test\" ) In\u00a0[\u00a0]: Copied! <pre># add info about the team members\n# add your name and your google account\nsgc.LabMember.LabMemberInfo.insert(\n    [[\"Alison Comrie\", \"comrie.alison@gmail.com\", \"alison\"]],\n    skip_duplicates=True,\n)\n</pre> # add info about the team members # add your name and your google account sgc.LabMember.LabMemberInfo.insert(     [[\"Alison Comrie\", \"comrie.alison@gmail.com\", \"alison\"]],     skip_duplicates=True, ) In\u00a0[\u00a0]: Copied! <pre>sgc.LabMember.LabMemberInfo()\n</pre> sgc.LabMember.LabMemberInfo()"}, {"location": "notebooks/00_intro/#spyglass-tutorial-0", "title": "Spyglass tutorial 0\u00b6", "text": "<p>Note: make a copy of this notebook and run the copy to avoid git conflicts in the future</p> <p>This is the zeroth in a multi-part tutorial on the Spyglass pipeline used in Loren Frank's lab, UCSF. It gives a general introduction to Datajoint and shows how to insert an NWB file into the Datajoint database, inspect and query the data, and delete it.</p> <p>Let's start by importing the <code>spyglass</code> package, along with a few others.</p>"}, {"location": "notebooks/00_intro/#visualizing-the-database", "title": "Visualizing the database\u00b6", "text": "<p>Datajoint enables users to use Python to build and interact with a Relational Database. A Relational Database stores and provides access to data stored in row-features and column-features. Each row provide information about different features of an individivial object while each column provide information about a single feature for a collection of objects. It supports the ability to create a new object, read an existing object, update an existing object and delete an object.</p> <p>The following diagram (called entity relationship diagram) shows all the tables in our database and their relationships.</p> <p>Polygons are tables:</p> <ul> <li>Blue oval: tables whose entries are imported from external files (e.g. NWB file).</li> <li>Green rectangle: tables whose entries are entered manually.</li> <li>Red circle: tables whose entries are computed from entries of other tables.</li> <li>No shape (only text): tables whose entries are part of the table upstream</li> </ul> <p>Lines are dependencies between tables:</p> <ul> <li>Bold lines: downstream table inherits the primary key ^ of upstream table as its sole primary key</li> <li>Solid lines: downstream table inherits the primary key of upstream table as part of its primary key</li> <li>Dashed lines: downstream table inherits the primary key of upstream table as non-primary key</li> </ul> <p>^ Primary key: a set of attributes (i.e. column names) used to uniquely define an entry (i.e. a row)</p>"}, {"location": "notebooks/00_intro/#example-data", "title": "Example data\u00b6", "text": ""}, {"location": "notebooks/00_intro/#inserting-data", "title": "Inserting data\u00b6", "text": "<p>As you may know, the NWB file contains a lot of information, such as information about the experimenter (e.g. who did the experiment, where was it done, etc); the animal's behavior (e.g. video recording of the animal's position in an environment during the experiment); the neural activity (extracellular recording of multiple brain areas) etc. We wish to enter this information into the tables of our Datajoint database so that we can easily access them later. If we have an NWB file that has been properly generated, this is straightforward: just run the <code>sg.insert_session</code> function, which will populate many of the tables automatically. Let's do this for our example NWB file.</p> <p>Note: this may take a while because it makes a copy of the NWB file.</p>"}, {"location": "notebooks/00_intro/#inspecting-and-querying-data", "title": "Inspecting and querying data\u00b6", "text": "<p>To look at the tables, just call it (don't forget the <code>()</code> at the end, as tables are like Python classes). Let's try calling the <code>Lab</code> table.</p>"}, {"location": "notebooks/00_intro/#deleting-data", "title": "Deleting data\u00b6", "text": "<p>Another neat feature of Datajoint is that it automatically maintains the integrity of your data. For example, if we were to delete our entry in the <code>Session</code> table, the associated entries in all of the downstream tables (e.g. <code>Raw</code>, <code>IntervalList</code>, <code>ElectrodeGroup</code> to name a few) will also be deleted. That way, there is no 'orphan' data whose origin cannot be tracked. We will do this now using the <code>delete</code> method.</p>"}, {"location": "notebooks/00_intro/#creating-a-lab-team", "title": "Creating a Lab Team\u00b6", "text": "<p>Before we end this tutorial, there is one last thing we must do: create a lab team. A lab team is a set of lab members who own a set of NWB files and the associated information in our Datajoint database. Think of this as a \"subgroup\" within the lab that collaborates on the same projects. Only the members of a lab team will be able to delete table entries they made (this permission system is not yet implemented; we're working on it). Right now we need to set a lab team so that the permission for manual curation of spike sorted data can be set (this will be covered in tutorial 2).</p> <p>The <code>LabMember</code> table contains the list of lab members. It has a parts table called <code>LabMemberInfo</code>, where the Google account of each member can be found (for authentication purposes). Similarly, <code>LabTeam</code> table contains the list of lab teams and has a parts table called <code>LabTeamMember</code> which specifies which of the lab members (as entered in <code>LabMember</code>) belongs to each lab team. Both <code>LabMember</code> and <code>LabTeam</code> are <code>dj.Manual</code> tables, which means the data can be entered manually (although when we add our NWB file with <code>nd.insert_sessions(nwb_file_name)</code>, the owner of the NWB file (as specified in the <code>experimenter</code> field) is automatically entered into <code>LabMember</code> table).</p> <p>To proceed, we will create a new team using the <code>create_new_team</code> method of <code>LabTeam</code>. You will give this team a name, and add yourself (and your Google account) to this team. Later, when we do spike sorting, we will specify the team the sorting belongs to. This will give everyone in the team (in this case, just you) the permission to add curation labels.</p>"}, {"location": "notebooks/01_spikesorting/", "title": "Spike Sorting", "text": "<p>Let's start by importing tables from spyglass along with some other useful packages</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport datajoint as dj\nimport spyglass as sg\nimport spyglass.common as sgc\nimport spyglass.spikesorting as sgss\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import os import numpy as np import datajoint as dj import spyglass as sg import spyglass.common as sgc import spyglass.spikesorting as sgss  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) <p>Now let's make sure that you're a part of the LorenLab <code>LabTeam</code>, so you'll have the right permissions for this tutorial.Replace <code>your_name</code>, <code>your_email</code>, and <code>datajoint_username</code>, with your information.</p> In\u00a0[\u00a0]: Copied! <pre>your_name = \"FirstName LastName\"\nyour_email = \"gmail@gmail.com\"\ndatajoint_username = \"user\"\n</pre> your_name = \"FirstName LastName\" your_email = \"gmail@gmail.com\" datajoint_username = \"user\" In\u00a0[\u00a0]: Copied! <pre>lorenlab_team_members = (\n    (sgc.LabTeam().LabTeamMember() &amp; {\"team_name\": \"LorenLab\"})\n    .fetch(\"lab_member_name\")\n    .tolist()\n)\nsgc.LabMember.insert_from_name(your_name)\nsgc.LabMember.LabMemberInfo.insert1(\n    [your_name, your_email, datajoint_username], skip_duplicates=True\n)\nsgc.LabTeam.LabTeamMember.insert1(\n    {\"team_name\": \"LorenLab\", \"lab_member_name\": your_name},\n    skip_duplicates=True,\n)\n</pre> lorenlab_team_members = (     (sgc.LabTeam().LabTeamMember() &amp; {\"team_name\": \"LorenLab\"})     .fetch(\"lab_member_name\")     .tolist() ) sgc.LabMember.insert_from_name(your_name) sgc.LabMember.LabMemberInfo.insert1(     [your_name, your_email, datajoint_username], skip_duplicates=True ) sgc.LabTeam.LabTeamMember.insert1(     {\"team_name\": \"LorenLab\", \"lab_member_name\": your_name},     skip_duplicates=True, ) <p>Now try and use the <code>fetch</code> command and the example above to complete the code below and query the <code>LabTeam</code> table to double-check that you are a part of the <code>LorenLab</code> team.</p> In\u00a0[\u00a0]: Copied! <pre>if your_name in (sgc.LabTeam._____() &amp; {___: ____}).fetch(______).tolist():\n    print(\"You made it in!\")\n</pre> if your_name in (sgc.LabTeam._____() &amp; {___: ____}).fetch(______).tolist():     print(\"You made it in!\") In\u00a0[\u00a0]: Copied! <pre>import spyglass.data_import as sdi\n\nsdi.insert_sessions(\"montague20200802_tutorial.nwb\")\nnwb_file_name = \"montague20200802_tutorial_.nwb\"\n</pre> import spyglass.data_import as sdi  sdi.insert_sessions(\"montague20200802_tutorial.nwb\") nwb_file_name = \"montague20200802_tutorial_.nwb\" <p></p> In\u00a0[\u00a0]: Copied! <pre># answer 'yes' when prompted\nsgss.SortGroup().set_group_by_shank(nwb_file_name)\n</pre> # answer 'yes' when prompted sgss.SortGroup().set_group_by_shank(nwb_file_name) <p>Each electrode will have an <code>electrode_id</code> and be associated with an <code>electrode_group_name</code>, which will correspond with a <code>sort_group_id</code>. In this case, the data was recorded from a 32 tetrode (128 channel) drive, and thus results in 128 unique <code>electrode_id</code>, 32 unique <code>electrode_group_name</code>, and 32 unique <code>sort_group_id</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sgss.SortGroup.SortGroupElectrode &amp; {\"nwb_file_name\": nwb_file_name}\n</pre> sgss.SortGroup.SortGroupElectrode &amp; {\"nwb_file_name\": nwb_file_name} In\u00a0[\u00a0]: Copied! <pre>sgc.IntervalList &amp; {\"nwb_file_name\": nwb_file_name}\n</pre> sgc.IntervalList &amp; {\"nwb_file_name\": nwb_file_name} <p>For our example, let's choose to start with the first run interval (<code>02_r1</code>) as our sort interval. We first fetch <code>valid_times</code> for this interval.</p> In\u00a0[\u00a0]: Copied! <pre>interval_list_name = \"02_r1\"\ninterval_list = (\n    sgc.IntervalList\n    &amp; {\"nwb_file_name\": nwb_file_name, \"interval_list_name\": interval_list_name}\n).fetch1(\"valid_times\")\nprint(\n    f\"IntervalList begins as a {np.round((interval_list[0][1] - interval_list[0][0]) / 60,0):g} min long epoch\"\n)\n</pre> interval_list_name = \"02_r1\" interval_list = (     sgc.IntervalList     &amp; {\"nwb_file_name\": nwb_file_name, \"interval_list_name\": interval_list_name} ).fetch1(\"valid_times\") print(     f\"IntervalList begins as a {np.round((interval_list[0][1] - interval_list[0][0]) / 60,0):g} min long epoch\" ) In\u00a0[\u00a0]: Copied! <pre>sort_interval = interval_list[0]\nsort_interval_name = interval_list_name + \"_first180\"\nsort_interval = np.copy(interval_list[0])\nsort_interval[1] = sort_interval[0] + 180\n</pre> sort_interval = interval_list[0] sort_interval_name = interval_list_name + \"_first180\" sort_interval = np.copy(interval_list[0]) sort_interval[1] = sort_interval[0] + 180 <p>We can now add this <code>sort_interval</code> with the specified <code>sort_interval_name</code> <code>'02_r1_first180'</code> to the <code>SortInterval</code> table. The <code>SortInterval.insert()</code> function requires the arguments input as a dictionary with keys <code>nwb_file_name</code>, <code>sort_interval_name</code>, and <code>sort_interval</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sgss.SortInterval.insert1(\n    {\n        \"nwb_file_name\": nwb_file_name,\n        \"sort_interval_name\": sort_interval_name,\n        \"sort_interval\": sort_interval,\n    },\n    skip_duplicates=True,\n)\n</pre> sgss.SortInterval.insert1(     {         \"nwb_file_name\": nwb_file_name,         \"sort_interval_name\": sort_interval_name,         \"sort_interval\": sort_interval,     },     skip_duplicates=True, ) <p>Now that we've inserted the entry into <code>SortInterval</code> you can see that entry by querying <code>SortInterval</code> using the <code>nwb_file_name</code> and <code>sort_interval_name</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sgss.SortInterval &amp; {\n    \"nwb_file_name\": nwb_file_name,\n    \"sort_interval_name\": sort_interval_name,\n}\n</pre> sgss.SortInterval &amp; {     \"nwb_file_name\": nwb_file_name,     \"sort_interval_name\": sort_interval_name, } <p>Now using the <code>.fetch()</code> command, you can retrieve your user-defined sort interval from the <code>SortInterval</code> table.A quick double-check will show that it is indeed a 180 second segment.</p> In\u00a0[\u00a0]: Copied! <pre>fetched_sort_interval = (\n    sgss.SortInterval\n    &amp; {\"nwb_file_name\": nwb_file_name, \"sort_interval_name\": sort_interval_name}\n).fetch1(\"sort_interval\")\nprint(\n    f\"The sort interval goes from {fetched_sort_interval[0]} to {fetched_sort_interval[1]}, \\\nwhich is {(fetched_sort_interval[1] - fetched_sort_interval[0])} seconds. COOL!\"\n)\n</pre> fetched_sort_interval = (     sgss.SortInterval     &amp; {\"nwb_file_name\": nwb_file_name, \"sort_interval_name\": sort_interval_name} ).fetch1(\"sort_interval\") print(     f\"The sort interval goes from {fetched_sort_interval[0]} to {fetched_sort_interval[1]}, \\ which is {(fetched_sort_interval[1] - fetched_sort_interval[0])} seconds. COOL!\" ) In\u00a0[\u00a0]: Copied! <pre>sgss.SpikeSortingPreprocessingParameters()\n</pre> sgss.SpikeSortingPreprocessingParameters() <p>Now let's set the filtering parameters. Here we insert the default parameters, and then fetch the default parameter dictionary.</p> In\u00a0[\u00a0]: Copied! <pre>sgss.SpikeSortingPreprocessingParameters().insert_default()\npreproc_params = (\n    sgss.SpikeSortingPreprocessingParameters()\n    &amp; {\"preproc_params_name\": \"default\"}\n).fetch1(\"preproc_params\")\nprint(preproc_params)\n</pre> sgss.SpikeSortingPreprocessingParameters().insert_default() preproc_params = (     sgss.SpikeSortingPreprocessingParameters()     &amp; {\"preproc_params_name\": \"default\"} ).fetch1(\"preproc_params\") print(preproc_params) <p>Lets first adjust the <code>frequency_min</code> parameter to 600, which is the preference for hippocampal data. Now we can insert that into <code>SpikeSortingPreprocessingParameters</code> as a new set of filtering parameters for hippocampal data, named <code>'franklab_default_hippocampus'</code>.</p> In\u00a0[\u00a0]: Copied! <pre>preproc_params[\"frequency_min\"] = 600\nsgss.SpikeSortingPreprocessingParameters().insert1(\n    {\n        \"preproc_params_name\": \"franklab_default_hippocampus\",\n        \"preproc_params\": preproc_params,\n    },\n    skip_duplicates=True,\n)\n</pre> preproc_params[\"frequency_min\"] = 600 sgss.SpikeSortingPreprocessingParameters().insert1(     {         \"preproc_params_name\": \"franklab_default_hippocampus\",         \"preproc_params\": preproc_params,     },     skip_duplicates=True, ) In\u00a0[\u00a0]: Copied! <pre>key = dict()\nkey[\"nwb_file_name\"] = nwb_file_name\nkey[\"sort_group_id\"] = 10\nkey[\"sort_interval_name\"] = \"02_r1_first180\"\nkey[\"preproc_params_name\"] = \"franklab_default_hippocampus\"\nkey[\"interval_list_name\"] = \"02_r1\"\nkey[\"team_name\"] = \"LorenLab\"\n\nssr_key = key\n</pre> key = dict() key[\"nwb_file_name\"] = nwb_file_name key[\"sort_group_id\"] = 10 key[\"sort_interval_name\"] = \"02_r1_first180\" key[\"preproc_params_name\"] = \"franklab_default_hippocampus\" key[\"interval_list_name\"] = \"02_r1\" key[\"team_name\"] = \"LorenLab\"  ssr_key = key In\u00a0[\u00a0]: Copied! <pre>sgss.SpikeSortingRecordingSelection.insert1(ssr_key, skip_duplicates=True)\nsgss.SpikeSortingRecordingSelection() &amp; ssr_key\n</pre> sgss.SpikeSortingRecordingSelection.insert1(ssr_key, skip_duplicates=True) sgss.SpikeSortingRecordingSelection() &amp; ssr_key In\u00a0[\u00a0]: Copied! <pre>sgss.SpikeSortingRecording.populate(\n    [(sgss.SpikeSortingRecordingSelection &amp; ssr_key).proj()]\n)\n</pre> sgss.SpikeSortingRecording.populate(     [(sgss.SpikeSortingRecordingSelection &amp; ssr_key).proj()] ) In\u00a0[\u00a0]: Copied! <pre>sgss.SpikeSortingRecording() &amp; ssr_key\n</pre> sgss.SpikeSortingRecording() &amp; ssr_key In\u00a0[\u00a0]: Copied! <pre>sgss.ArtifactDetectionParameters().insert_default()\n</pre> sgss.ArtifactDetectionParameters().insert_default() In\u00a0[\u00a0]: Copied! <pre>artifact_key = (sgss.SpikeSortingRecording() &amp; ssr_key).fetch1(\"KEY\")\nartifact_key[\"artifact_params_name\"] = \"none\"\n</pre> artifact_key = (sgss.SpikeSortingRecording() &amp; ssr_key).fetch1(\"KEY\") artifact_key[\"artifact_params_name\"] = \"none\" <p>Now we can pair your choice of artifact detection parameters (as entered into <code>ArtifactParameters</code>) with the recording you just extracted through population of <code>SpikeSortingRecording</code> and insert into <code>ArtifactDetectionSelection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sgss.ArtifactDetectionSelection().insert1(artifact_key)\n</pre> sgss.ArtifactDetectionSelection().insert1(artifact_key) In\u00a0[\u00a0]: Copied! <pre>sgss.ArtifactDetectionSelection() &amp; artifact_key\n</pre> sgss.ArtifactDetectionSelection() &amp; artifact_key <p>Now that we've inserted into <code>ArtifactDetectionSelection</code> we're ready to populate the <code>ArtifactDetection</code> table, which will find periods where there are artifacts (as specified by your parameters) in the recording.</p> In\u00a0[\u00a0]: Copied! <pre>sgss.ArtifactDetection.populate(artifact_key)\n</pre> sgss.ArtifactDetection.populate(artifact_key) <p>Populating <code>ArtifactDetection</code> also inserts an entry into <code>ArtifactRemovedIntervalList</code>, which stores the interval without detected artifacts.</p> In\u00a0[\u00a0]: Copied! <pre>sgss.ArtifactRemovedIntervalList() &amp; artifact_key\n</pre> sgss.ArtifactRemovedIntervalList() &amp; artifact_key In\u00a0[\u00a0]: Copied! <pre>sgss.SpikeSorterParameters().insert_default()\n</pre> sgss.SpikeSorterParameters().insert_default() In\u00a0[\u00a0]: Copied! <pre># Let's look at the default params\nsorter_name = \"mountainsort4\"\nms4_default_params = (\n    sgss.SpikeSorterParameters\n    &amp; {\"sorter\": sorter_name, \"sorter_params_name\": \"default\"}\n).fetch1()\nprint(ms4_default_params)\n</pre> # Let's look at the default params sorter_name = \"mountainsort4\" ms4_default_params = (     sgss.SpikeSorterParameters     &amp; {\"sorter\": sorter_name, \"sorter_params_name\": \"default\"} ).fetch1() print(ms4_default_params) <p>Now we can change these default parameters to line up more closely with our preferences.</p> In\u00a0[\u00a0]: Copied! <pre>param_dict = ms4_default_params[\"sorter_params\"]\n# Detect downward going spikes (1 is for upward, 0 is for both up and down)\nparam_dict[\"detect_sign\"] = -1\n# We will sort electrodes together that are within 100 microns of each other\nparam_dict[\"adjacency_radius\"] = 100\n# Turn filter off since we will filter it prior to starting sort\nparam_dict[\"filter\"] = False\nparam_dict[\"freq_min\"] = 0\nparam_dict[\"freq_max\"] = 0\n# Turn whiten off since we will whiten it prior to starting sort\nparam_dict[\"whiten\"] = False\n# set num_workers to be the same number as the number of electrodes\nparam_dict[\"num_workers\"] = 4\nparam_dict[\"verbose\"] = True\n# set clip size as number of samples for 1.33 millisecond based on the sampling rate\nparam_dict[\"clip_size\"] = np.int(\n    1.33e-3\n    * (sgc.Raw &amp; {\"nwb_file_name\": nwb_file_name}).fetch1(\"sampling_rate\")\n)\nparam_dict\n</pre> param_dict = ms4_default_params[\"sorter_params\"] # Detect downward going spikes (1 is for upward, 0 is for both up and down) param_dict[\"detect_sign\"] = -1 # We will sort electrodes together that are within 100 microns of each other param_dict[\"adjacency_radius\"] = 100 # Turn filter off since we will filter it prior to starting sort param_dict[\"filter\"] = False param_dict[\"freq_min\"] = 0 param_dict[\"freq_max\"] = 0 # Turn whiten off since we will whiten it prior to starting sort param_dict[\"whiten\"] = False # set num_workers to be the same number as the number of electrodes param_dict[\"num_workers\"] = 4 param_dict[\"verbose\"] = True # set clip size as number of samples for 1.33 millisecond based on the sampling rate param_dict[\"clip_size\"] = np.int(     1.33e-3     * (sgc.Raw &amp; {\"nwb_file_name\": nwb_file_name}).fetch1(\"sampling_rate\") ) param_dict <p>Let's go ahead and insert a new <code>sorter_params_name</code> and <code>sorter_params</code> dict into the <code>SpikeSorterParameters</code> table named <code>franklab_hippocampus_tutorial</code>.</p> In\u00a0[\u00a0]: Copied! <pre>parameter_set_name = \"franklab_hippocampus_tutorial\"\n</pre> parameter_set_name = \"franklab_hippocampus_tutorial\" <p>Now we insert our parameters for use by the spike sorter into <code>SpikeSorterParameters</code> and double-check that it made it in to the table.</p> In\u00a0[\u00a0]: Copied! <pre>sgss.SpikeSorterParameters.insert1(\n    {\n        \"sorter\": sorter_name,\n        \"sorter_params_name\": parameter_set_name,\n        \"sorter_params\": param_dict,\n    },\n    skip_duplicates=True,\n)\n# Check that insert was successful\np = (\n    sgss.SpikeSorterParameters\n    &amp; {\"sorter\": sorter_name, \"sorter_params_name\": parameter_set_name}\n).fetch1()\np\n</pre> sgss.SpikeSorterParameters.insert1(     {         \"sorter\": sorter_name,         \"sorter_params_name\": parameter_set_name,         \"sorter_params\": param_dict,     },     skip_duplicates=True, ) # Check that insert was successful p = (     sgss.SpikeSorterParameters     &amp; {\"sorter\": sorter_name, \"sorter_params_name\": parameter_set_name} ).fetch1() p In\u00a0[\u00a0]: Copied! <pre>ss_key = (sgss.ArtifactDetection &amp; ssr_key).fetch1(\"KEY\")\nss_key.update((sgss.ArtifactRemovedIntervalList() &amp; key).fetch1(\"KEY\"))\nss_key[\"sorter\"] = sorter_name\nss_key[\"sorter_params_name\"] = parameter_set_name\ndel ss_key[\"artifact_params_name\"]\nsgss.SpikeSortingSelection.insert1(ss_key, skip_duplicates=True)\n(sgss.SpikeSortingSelection &amp; ss_key)\n</pre> ss_key = (sgss.ArtifactDetection &amp; ssr_key).fetch1(\"KEY\") ss_key.update((sgss.ArtifactRemovedIntervalList() &amp; key).fetch1(\"KEY\")) ss_key[\"sorter\"] = sorter_name ss_key[\"sorter_params_name\"] = parameter_set_name del ss_key[\"artifact_params_name\"] sgss.SpikeSortingSelection.insert1(ss_key, skip_duplicates=True) (sgss.SpikeSortingSelection &amp; ss_key) In\u00a0[\u00a0]: Copied! <pre>sgss.SpikeSorting.populate([(sgss.SpikeSortingSelection &amp; ss_key).proj()])\n</pre> sgss.SpikeSorting.populate([(sgss.SpikeSortingSelection &amp; ss_key).proj()]) In\u00a0[\u00a0]: Copied! <pre>sgss.SpikeSorting() &amp; ss_key\n</pre> sgss.SpikeSorting() &amp; ss_key"}, {"location": "notebooks/01_spikesorting/#spyglass-spike-sorting-tutorial", "title": "Spyglass Spike Sorting Tutorial\u00b6", "text": "<p>Note: make a copy of this notebook and run the copy to avoid git conflicts in the future</p> <p>This is the second in a multi-part tutorial on the Spyglass pipeline used in Loren Frank's lab, UCSF. It demonstrates how to run spike sorting and curate units within the pipeline.</p> <p>If you have not done tutorial 0 yet, make sure to do so before proceeding. It is also recommended that you complete the datajoint tutorials before starting this.</p> <p>Note 2: Make sure you are running this within the spyglass Conda environment)</p>"}, {"location": "notebooks/01_spikesorting/#spike-sorting-overview-with-associated-tables", "title": "Spike Sorting Overview - With associated tables\u00b6", "text": "Extracting the recording from the NWB file <ol> <li>Specifying your NWB file.</li> <li>Specifying which electrodes involved in the recording to sort data from. - <code>SortGroup</code></li> <li>Specifying the time segment of the recording we want to sort. - <code>IntervalList</code>, <code>SortInterval</code></li> <li>Specifying the parameters to use for filtering the recording. - <code>SpikeSortingPreprocessingParameters</code></li> <li>Combining these parameters. - <code>SpikeSortingRecordingSelection</code></li> <li>Extracting the recording. - <code>SpikeSortingRecording</code></li> <li>Specifying the parameters to apply for artifact detection/removal. -<code>ArtifactDetectionParameters</code></li> </ol> Spike sorting the extracted recording <ol> <li>Specify the spike sorter and parameters to use. - <code>SpikeSorterParameters</code></li> <li>Combine these parameters. - <code>SpikeSortingSelection</code></li> <li>Spike sort the extracted recording according to chose parameter set. - <code>SpikeSorting</code></li> </ol>"}, {"location": "notebooks/01_spikesorting/#specifying-your-nwb-filename", "title": "Specifying your NWB filename\u00b6", "text": "<p>NWB filenames take the form of an animal name plus the date of the recording.For this tutorial, we will use the nwb file <code>'montague20200802_tutorial_.nwb'</code>. The animal name is <code>montague</code> and the date of the recording is <code>20200802</code>, the <code>tutorial</code> is unique to this setting :-).We'll first re-insert the NWB file that you deleted at the end of <code>0_intro</code>. This file may already be in the table, in which case there will be a warning, that's ok!Note: If you're not on the Frank Lab server, this file will be accessible as <code>montague20200802.nwb</code> on DANDI through this URL: insert-url</p>"}, {"location": "notebooks/01_spikesorting/#note-this-is-a-common-file-for-this-tutorial-table-entries-may-change-during-use-if-others-are-also-working-through", "title": "NOTE: This is a common file for this tutorial, table entries may change during use if others are also working through.\u00b6", "text": ""}, {"location": "notebooks/01_spikesorting/#extracting-the-recording-from-the-nwb-file", "title": "Extracting the recording from the NWB file\u00b6", "text": ""}, {"location": "notebooks/01_spikesorting/#sortgroup", "title": "<code>SortGroup</code>\u00b6", "text": "<p>For each NWB file there will be multiple electrodes available to sort spikes from.We commonly sort over multiple electrodes at a time, also referred to as a <code>SortGroup</code>.This is accomplished by grouping electrodes according to what tetrode or shank of a probe they were on.NOTE: answer 'yes' when prompted</p>"}, {"location": "notebooks/01_spikesorting/#intervallist", "title": "<code>IntervalList</code>\u00b6", "text": "<p>Next, we make a decision about the time interval for our spike sorting. Let's re-examine <code>IntervalList</code>.</p>"}, {"location": "notebooks/01_spikesorting/#sortinterval", "title": "<code>SortInterval</code>\u00b6", "text": "<p>For brevity's sake, we'll select only the first 180 seconds of that 90 minute epoch as our sort interval. To do so, we define our new sort interval as the start time of <code>interval_list</code> from the previous cell, plus 180 seconds.</p>"}, {"location": "notebooks/01_spikesorting/#spikesortingpreprocessingparameters", "title": "<code>SpikeSortingPreprocessingParameters</code>\u00b6", "text": "<p>Let's first take a look at the <code>SpikeSortingPreprocessingParameters</code> table, which contains the parameters used to filter the recorded data in the spike band prior to spike sorting it.</p>"}, {"location": "notebooks/01_spikesorting/#setting-a-key", "title": "Setting a key\u00b6", "text": "<p>Now we set up the parameters of the recording we are interested in, and make a dictionary to hold all these values, which will make querying and inserting into tables all the easier moving forward.We'll assign this to <code>ssr_key</code> as these values are relvant to the recording we'll use to spike sort, also referred to as the spike sorting recording (ssr) :-)The <code>sort_group_id</code> refers back to the <code>SortGroup</code> table we populated at the beginning of the tutorial. We'll use <code>sort_group_id</code> 10 here. Our <code>sort_interval_name</code> is the same as above: <code>'02_r1_first600'</code>.Our <code>preproc_params_name</code> is the same ones we just inserted into <code>SpikeSortingPreprocessingParameters</code>.The <code>interval_list</code> was also set above as <code>'02_r1'</code>. Unlike <code>sort_interval_name</code>, which reflects our subsection of the recording, we keep <code>interval_list</code> unchanged from the original epoch name.</p>"}, {"location": "notebooks/01_spikesorting/#spikesortingrecordingselection", "title": "<code>SpikeSortingRecordingSelection</code>\u00b6", "text": "<p>We now insert all of these parameters into the <code>SpikeSortingRecordingSelection</code> table, which we will use to specify what time/tetrode/etc of the recording we want to extract.</p>"}, {"location": "notebooks/01_spikesorting/#spikesortingrecording", "title": "<code>SpikeSortingRecording</code>\u00b6", "text": "<p>And now we're ready to extract the recording! We use the <code>.proj()</code> command to pass along all of the primary keys from the <code>SpikeSortingRecordingSelection</code> table to the <code>SpikeSortingRecording</code> table, so it knows exactly what to extract.Note: This step might take a bit with longer duration sort intervals</p>"}, {"location": "notebooks/01_spikesorting/#now-we-can-see-our-recording-in-the-table-e-x-c-i-t-i-n-g", "title": "Now we can see our recording in the table. E x c i t i n g !\u00b6", "text": ""}, {"location": "notebooks/01_spikesorting/#artifactdetectionparameters", "title": "<code>ArtifactDetectionParameters</code>\u00b6", "text": "<p>Similarly, we set up the <code>ArtifactDetectionParameters</code>, which can allow us to remove artifacts from the data.Specifically, we want to target artifact signal that is within the frequency band of our filter (600Hz-6KHz), and thus will not get removed by filtering.For the moment we just set up a <code>\"none\"</code> parameter set, which will do nothing when used.</p>"}, {"location": "notebooks/01_spikesorting/#spike-sorting-the-extracted-recording", "title": "Spike sorting the extracted recording\u00b6", "text": ""}, {"location": "notebooks/01_spikesorting/#spikesorterparameters", "title": "<code>SpikeSorterParameters</code>\u00b6", "text": "<p>For our example, we will be using <code>mountainsort4</code>. There are already some default parameters in the <code>SpikeSorterParameters</code> table we'll <code>fetch</code>.</p>"}, {"location": "notebooks/01_spikesorting/#spikesortingselection", "title": "<code>SpikeSortingSelection</code>\u00b6", "text": ""}, {"location": "notebooks/01_spikesorting/#gearing-up-to-spike-sort", "title": "Gearing up to Spike Sort!\u00b6", "text": "<p>We now collect all the decisions we made up to here and put it into the <code>SpikeSortingSelection</code> table, which is specific to this recording and eventual sorting segment.We'll add in a few parameters to our key and call it <code>ss_key</code>.(note: the spike sorter parameters defined above are for the sorter, <code>mountainsort4</code> in this case.)</p>"}, {"location": "notebooks/01_spikesorting/#spikesorting", "title": "<code>SpikeSorting</code>\u00b6", "text": "<p>Now we can run spike sorting. It's nothing more than populating a table (<code>SpikeSorting</code>) based on the entries of <code>SpikeSortingSelection</code>.Note: This will take a little bit with longer data sets.</p>"}, {"location": "notebooks/01_spikesorting/#check-to-make-sure-the-table-populated", "title": "Check to make sure the table populated\u00b6", "text": ""}, {"location": "notebooks/01_spikesorting/#congratulations-youve-spike-sorted-see-2_curation-for-the-next-tutorial", "title": "Congratulations, you've spike sorted! See 2_curation for the next tutorial\u00b6", "text": ""}, {"location": "notebooks/02_curation/", "title": "Curation", "text": "In\u00a0[1]: Copied! <pre>import os\nimport numpy as np\n\nimport spyglass as nd\n\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import os import numpy as np  import spyglass as nd  import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) <pre>[2022-08-01 17:21:49,989][INFO]: Connecting zoldello@lmf-db.cin.ucsf.edu:3306\n[2022-08-01 17:21:50,035][INFO]: Connected zoldello@lmf-db.cin.ucsf.edu:3306\n</pre> <pre>/home/zoldello/anaconda3/envs/spyglass/lib/python3.9/site-packages/position_tools/core.py:3: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n  from scipy.ndimage.filters import gaussian_filter1d\n</pre> In\u00a0[14]: Copied! <pre># We also import a bunch of tables so that we can call them easily\nfrom spyglass.common import (\n    RawPosition,\n    HeadDir,\n    Speed,\n    LinPos,\n    StateScriptFile,\n    VideoFile,\n    DataAcquisitionDevice,\n    CameraDevice,\n    Probe,\n    DIOEvents,\n    ElectrodeGroup,\n    Electrode,\n    Raw,\n    SampleCount,\n    LFPSelection,\n    LFP,\n    LFPBandSelection,\n    LFPBand,\n    FirFilterParameters,\n    IntervalList,\n    Lab,\n    LabMember,\n    LabTeam,\n    Institution,\n    BrainRegion,\n    SensorData,\n    Session,\n    ExperimenterList,\n    Subject,\n    Task,\n    TaskEpoch,\n    Nwbfile,\n    AnalysisNwbfile,\n    NwbfileKachery,\n    AnalysisNwbfileKachery,\n)\n\nfrom spyglass.spikesorting import (\n    ArtifactDetection,\n    ArtifactDetectionParameters,\n    ArtifactDetectionSelection,\n    ArtifactRemovedIntervalList,\n    SortGroup,\n    SortInterval,\n    SpikeSortingPreprocessingParameters,\n    SpikeSortingRecording,\n    SpikeSortingRecordingSelection,\n    SpikeSorterParameters,\n    SpikeSorting,\n    SpikeSortingSelection,\n    SortInterval,\n    SpikeSorting,\n    AutomaticCuration,\n    AutomaticCurationParameters,\n    AutomaticCurationSelection,\n    CuratedSpikeSorting,\n    CuratedSpikeSortingSelection,\n    Curation,\n    MetricParameters,\n    MetricSelection,\n    QualityMetrics,\n    UnitInclusionParameters,\n    WaveformParameters,\n    Waveforms,\n    WaveformSelection,\n)\n</pre> # We also import a bunch of tables so that we can call them easily from spyglass.common import (     RawPosition,     HeadDir,     Speed,     LinPos,     StateScriptFile,     VideoFile,     DataAcquisitionDevice,     CameraDevice,     Probe,     DIOEvents,     ElectrodeGroup,     Electrode,     Raw,     SampleCount,     LFPSelection,     LFP,     LFPBandSelection,     LFPBand,     FirFilterParameters,     IntervalList,     Lab,     LabMember,     LabTeam,     Institution,     BrainRegion,     SensorData,     Session,     ExperimenterList,     Subject,     Task,     TaskEpoch,     Nwbfile,     AnalysisNwbfile,     NwbfileKachery,     AnalysisNwbfileKachery, )  from spyglass.spikesorting import (     ArtifactDetection,     ArtifactDetectionParameters,     ArtifactDetectionSelection,     ArtifactRemovedIntervalList,     SortGroup,     SortInterval,     SpikeSortingPreprocessingParameters,     SpikeSortingRecording,     SpikeSortingRecordingSelection,     SpikeSorterParameters,     SpikeSorting,     SpikeSortingSelection,     SortInterval,     SpikeSorting,     AutomaticCuration,     AutomaticCurationParameters,     AutomaticCurationSelection,     CuratedSpikeSorting,     CuratedSpikeSortingSelection,     Curation,     MetricParameters,     MetricSelection,     QualityMetrics,     UnitInclusionParameters,     WaveformParameters,     Waveforms,     WaveformSelection, ) In\u00a0[7]: Copied! <pre># Define the name of the file that you copied and renamed from previous tutorials\nnwb_file_name = \"beans20190718.nwb\"\nfilename, file_extension = os.path.splitext(nwb_file_name)\nnwb_file_name2 = filename + \"_\" + file_extension\n</pre> # Define the name of the file that you copied and renamed from previous tutorials nwb_file_name = \"beans20190718.nwb\" filename, file_extension = os.path.splitext(nwb_file_name) nwb_file_name2 = filename + \"_\" + file_extension <p>First, make sure that the results of your sorting from tutorial 1 are stored in <code>SpikeSorting</code> table.</p> In\u00a0[13]: Copied! <pre>SpikeSorting &amp; {\"nwb_file_name\": nwb_file_name2}\n</pre> SpikeSorting &amp; {\"nwb_file_name\": nwb_file_name2} Out[13]: <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sort_interval_name</p> name for this interval <p>preproc_params_name</p> <p>team_name</p> <p>sorter</p> <p>sorter_params_name</p> <p>artifact_removed_interval_list_name</p> <p>sorting_path</p> <p>time_of_sort</p> in Unix time, to the nearest second CH6120211203_.nwb 1 raw data valid times franklab_tetrode_hippocampus Michael Coulter clusterless_thresholder default_clusterless CH6120211203_.nwb_raw data valid times_1_franklab_tetrode_hippocampus_ampl_1500_prop_001_5ms_artifact_removed_valid_times /stelmo/nwb/sorting/CH6120211203_.nwb_raw data valid times_1_franklab_tetrode_hippocampus_d02e4cc4_spikesorting 1652421556CH6120211203_.nwb 1 raw data valid times franklab_tetrode_hippocampus Michael Coulter mountainsort4 franklab_tetrode_hippocampus_30KHz CH6120211203_.nwb_raw data valid times_1_franklab_tetrode_hippocampus_ampl_1500_prop_001_5ms_artifact_removed_valid_times /stelmo/nwb/sorting/CH6120211203_.nwb_raw data valid times_1_franklab_tetrode_hippocampus_3ce905c4_spikesorting 1652390802CH6120211203_.nwb 2 raw data valid times franklab_tetrode_hippocampus Michael Coulter clusterless_thresholder default_clusterless CH6120211203_.nwb_raw data valid times_2_franklab_tetrode_hippocampus_ampl_1500_prop_001_5ms_artifact_removed_valid_times /stelmo/nwb/sorting/CH6120211203_.nwb_raw data valid times_2_franklab_tetrode_hippocampus_3236edfe_spikesorting 1652421602CH6120211203_.nwb 2 raw data valid times franklab_tetrode_hippocampus Michael Coulter mountainsort4 franklab_tetrode_hippocampus_30KHz CH6120211203_.nwb_raw data valid times_2_franklab_tetrode_hippocampus_ampl_1500_prop_001_5ms_artifact_removed_valid_times /stelmo/nwb/sorting/CH6120211203_.nwb_raw data valid times_2_franklab_tetrode_hippocampus_175d4957_spikesorting 1652391136CH6120211203_.nwb 3 raw data valid times franklab_tetrode_hippocampus Michael Coulter clusterless_thresholder default_clusterless CH6120211203_.nwb_raw data valid times_3_franklab_tetrode_hippocampus_ampl_1500_prop_001_5ms_artifact_removed_valid_times /stelmo/nwb/sorting/CH6120211203_.nwb_raw data valid times_3_franklab_tetrode_hippocampus_0aa65847_spikesorting 1652421651CH6120211203_.nwb 3 raw data valid times franklab_tetrode_hippocampus Michael Coulter mountainsort4 franklab_tetrode_hippocampus_30KHz CH6120211203_.nwb_raw data valid times_3_franklab_tetrode_hippocampus_ampl_1500_prop_001_5ms_artifact_removed_valid_times /stelmo/nwb/sorting/CH6120211203_.nwb_raw data valid times_3_franklab_tetrode_hippocampus_b3b560dc_spikesorting 1652391465CH6120211203_.nwb 4 raw data valid times franklab_tetrode_hippocampus Michael Coulter clusterless_thresholder default_clusterless CH6120211203_.nwb_raw data valid times_4_franklab_tetrode_hippocampus_ampl_1500_prop_001_5ms_artifact_removed_valid_times /stelmo/nwb/sorting/CH6120211203_.nwb_raw data valid times_4_franklab_tetrode_hippocampus_a248a29c_spikesorting 1652421729CH6120211203_.nwb 4 raw data valid times franklab_tetrode_hippocampus Michael Coulter mountainsort4 franklab_tetrode_hippocampus_30KHz CH6120211203_.nwb_raw data valid times_4_franklab_tetrode_hippocampus_ampl_1500_prop_001_5ms_artifact_removed_valid_times /stelmo/nwb/sorting/CH6120211203_.nwb_raw data valid times_4_franklab_tetrode_hippocampus_52ff93db_spikesorting 1652391770CH6120211203_.nwb 5 raw data valid times franklab_tetrode_hippocampus Michael Coulter clusterless_thresholder default_clusterless CH6120211203_.nwb_raw data valid times_5_franklab_tetrode_hippocampus_ampl_1500_prop_001_5ms_artifact_removed_valid_times /stelmo/nwb/sorting/CH6120211203_.nwb_raw data valid times_5_franklab_tetrode_hippocampus_c75bf7f7_spikesorting 1652421799CH6120211203_.nwb 5 raw data valid times franklab_tetrode_hippocampus Michael Coulter mountainsort4 franklab_tetrode_hippocampus_30KHz CH6120211203_.nwb_raw data valid times_5_franklab_tetrode_hippocampus_ampl_1500_prop_001_5ms_artifact_removed_valid_times /stelmo/nwb/sorting/CH6120211203_.nwb_raw data valid times_5_franklab_tetrode_hippocampus_962a4aa4_spikesorting 1652392074CH6120211203_.nwb 6 raw data valid times franklab_tetrode_hippocampus Michael Coulter clusterless_thresholder default_clusterless CH6120211203_.nwb_raw data valid times_6_franklab_tetrode_hippocampus_ampl_1500_prop_001_5ms_artifact_removed_valid_times /stelmo/nwb/sorting/CH6120211203_.nwb_raw data valid times_6_franklab_tetrode_hippocampus_11b507a6_spikesorting 1652421841CH6120211203_.nwb 6 raw data valid times franklab_tetrode_hippocampus Michael Coulter mountainsort4 franklab_tetrode_hippocampus_30KHz CH6120211203_.nwb_raw data valid times_6_franklab_tetrode_hippocampus_ampl_1500_prop_001_5ms_artifact_removed_valid_times /stelmo/nwb/sorting/CH6120211203_.nwb_raw data valid times_6_franklab_tetrode_hippocampus_ca67c53d_spikesorting 1652392496 <p>...</p> <p>Total: 4702</p> <p>Currently (June 2021) the manual curation of spike sorted data is done on the <code>sortingview</code> web app. To proceed, click on the link printed at the end of spike sorting in tutorial 1. Alternatively, run the following cell and click on the URL output.</p> In\u00a0[17]: Copied! <pre># workspace_uri = (SpikeSorting &amp; {'nwb_file_name': nwb_file_name2}).fetch1('curation_feed_uri')\n# print(f'https://sortingview.vercel.app/workspace?workspace={workspace_uri}&amp;channel=franklab')\n</pre> # workspace_uri = (SpikeSorting &amp; {'nwb_file_name': nwb_file_name2}).fetch1('curation_feed_uri') # print(f'https://sortingview.vercel.app/workspace?workspace={workspace_uri}&amp;channel=franklab') <p>This will take you to a workspace on the <code>sortingview</code> app. The workspace, which you can think of as a list of recording and associated sorting objects, was created at the end of spike sorting. On the workspace view, you will see a set of recordings that have been added to the workspace.</p> <p></p> <p>Clicking on a recording then takes you to a page that gives you information about the recording as well as the associated sorting objects.</p> <p></p> <p>Click on a sorting to see the curation view. Try exploring the many visualization widgets.</p> <p></p> <p>The most important is the <code>Units Table</code> and the <code>Curation</code> menu, which allows you to give labels to the units. The curation labels will persist even if you suddenly lose connection to the app; this is because the curaiton actions are appended to the workspace as soon as they are created. Note that if you are not logged in with your Google account, <code>Curation</code> menu may not be visible. Log in and refresh the page to access this feature.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/02_curation/#curation", "title": "Curation\u00b6", "text": "<p>Note: make a copy of this notebook and run the copy to avoid git conflicts in the future</p> <p>This is the second in a multi-part tutorial on the NWB-Datajoint pipeline used in Loren Frank's lab, UCSF. It demonstrates how to curate the results of spike sorting.</p> <p>Finish tutorial 0 and tutorial 1 before proceeding.</p> <p>Let's start by importing the <code>spyglass</code> package, along with a few others.</p>"}, {"location": "notebooks/02_curation/#sortingview-web-app", "title": "<code>sortingview</code> web app\u00b6", "text": ""}, {"location": "notebooks/03_lfp/", "title": "LFP", "text": "In\u00a0[\u00a0]: Copied! <pre>import pynwb\nimport os\nimport numpy as np\n\n# DataJoint and DataJoint schema\nimport datajoint as dj\n\ndj.config[\"filepath_checksum_size_limit\"] = 1 * 1024**2\n\n## We also import a bunch of tables so that we can call them easily\nfrom spyglass.common import (\n    RawPosition,\n    StateScriptFile,\n    VideoFile,\n    DataAcquisitionDevice,\n    CameraDevice,\n    Probe,\n    DIOEvents,\n    ElectrodeGroup,\n    Electrode,\n    Raw,\n    SampleCount,\n    FirFilterParameters,\n    IntervalList,\n    Lab,\n    LabMember,\n    LabTeam,\n    Institution,\n    BrainRegion,\n    SensorData,\n    Session,\n    Subject,\n    Task,\n    TaskEpoch,\n    Nwbfile,\n    AnalysisNwbfile,\n    NwbfileKachery,\n    AnalysisNwbfileKachery,\n    interval_list_contains,\n    interval_list_contains_ind,\n    interval_list_excludes,\n    interval_list_excludes_ind,\n    interval_list_intersect,\n    get_electrode_indices,\n)\n\nfrom spyglass.lfp.v1 import (\n    LFPElectrodeGroup,\n    LFPSelection,\n    LFP,\n    LFPOutput,\n    LFPBandSelection,\n    LFPBand,\n)\n\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import pynwb import os import numpy as np  # DataJoint and DataJoint schema import datajoint as dj  dj.config[\"filepath_checksum_size_limit\"] = 1 * 1024**2  ## We also import a bunch of tables so that we can call them easily from spyglass.common import (     RawPosition,     StateScriptFile,     VideoFile,     DataAcquisitionDevice,     CameraDevice,     Probe,     DIOEvents,     ElectrodeGroup,     Electrode,     Raw,     SampleCount,     FirFilterParameters,     IntervalList,     Lab,     LabMember,     LabTeam,     Institution,     BrainRegion,     SensorData,     Session,     Subject,     Task,     TaskEpoch,     Nwbfile,     AnalysisNwbfile,     NwbfileKachery,     AnalysisNwbfileKachery,     interval_list_contains,     interval_list_contains_ind,     interval_list_excludes,     interval_list_excludes_ind,     interval_list_intersect,     get_electrode_indices, )  from spyglass.lfp.v1 import (     LFPElectrodeGroup,     LFPSelection,     LFP,     LFPOutput,     LFPBandSelection,     LFPBand, )  import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) In\u00a0[\u00a0]: Copied! <pre>nwb_file_names = Nwbfile().fetch(\"nwb_file_name\")\n# take the first one for this demonstration\nnwb_file_name = nwb_file_names[0]\nprint(nwb_file_name)\n\n# test:\nnwb_file_name = \"tonks20211103_.nwb\"\n</pre> nwb_file_names = Nwbfile().fetch(\"nwb_file_name\") # take the first one for this demonstration nwb_file_name = nwb_file_names[0] print(nwb_file_name)  # test: nwb_file_name = \"tonks20211103_.nwb\" In\u00a0[\u00a0]: Copied! <pre>FirFilterParameters().create_standard_filters()\n</pre> FirFilterParameters().create_standard_filters() In\u00a0[\u00a0]: Copied! <pre>LFPOutput().LFP()\n</pre> LFPOutput().LFP() In\u00a0[\u00a0]: Copied! <pre>electrode_ids = (Electrode &amp; {\"nwb_file_name\": nwb_file_name}).fetch(\n    \"electrode_id\"\n)\n# electrode_indexes = [0, 4, 8]\n# lfp_electrode_ids = electrode_ids[electrode_indexes]\nlfp_electrode_ids = [28, 32, 40]\nlfp_electrode_group_name = \"test_group\"\n\n\nlfp_eg_key = {\n    \"nwb_file_name\": nwb_file_name,\n    \"lfp_electrode_group_name\": lfp_electrode_group_name,\n}\n# Delete the old test group if it exists (uncomment the line below if so) and then insert the new one\n# (LFPElectrodeGroup &amp; lfp_eg_key).delete(force_parts=True)\nLFPElectrodeGroup.create_lfp_electrode_group(\n    nwb_file_name=nwb_file_name,\n    group_name=lfp_electrode_group_name,\n    electrode_list=lfp_electrode_ids,\n)\n</pre> electrode_ids = (Electrode &amp; {\"nwb_file_name\": nwb_file_name}).fetch(     \"electrode_id\" ) # electrode_indexes = [0, 4, 8] # lfp_electrode_ids = electrode_ids[electrode_indexes] lfp_electrode_ids = [28, 32, 40] lfp_electrode_group_name = \"test_group\"   lfp_eg_key = {     \"nwb_file_name\": nwb_file_name,     \"lfp_electrode_group_name\": lfp_electrode_group_name, } # Delete the old test group if it exists (uncomment the line below if so) and then insert the new one # (LFPElectrodeGroup &amp; lfp_eg_key).delete(force_parts=True) LFPElectrodeGroup.create_lfp_electrode_group(     nwb_file_name=nwb_file_name,     group_name=lfp_electrode_group_name,     electrode_list=lfp_electrode_ids, ) In\u00a0[\u00a0]: Copied! <pre>LFPElectrodeGroup().LFPElectrode() &amp; {\"nwb_file_name\": nwb_file_name}\n</pre> LFPElectrodeGroup().LFPElectrode() &amp; {\"nwb_file_name\": nwb_file_name} In\u00a0[\u00a0]: Copied! <pre># we choose the first run period and the standard LFP filter for 30KHz data and add a new short interval for this demonstration\norig_interval_list_name = \"02_r1\"\nvalid_times = (\n    IntervalList\n    &amp; {\n        \"nwb_file_name\": nwb_file_name,\n        \"interval_list_name\": orig_interval_list_name,\n    }\n).fetch1(\"valid_times\")\nnew_valid_times = np.asarray([[valid_times[0, 0], valid_times[0, 0] + 100]])\ninterval_list_name = \"test interval\"\nIntervalList.insert1(\n    {\n        \"nwb_file_name\": nwb_file_name,\n        \"interval_list_name\": interval_list_name,\n        \"valid_times\": new_valid_times,\n    },\n    skip_duplicates=True,\n)\n\nfilter_name = \"LFP 0-400 Hz\"\nfilter_sampling_rate = 30000\n</pre> # we choose the first run period and the standard LFP filter for 30KHz data and add a new short interval for this demonstration orig_interval_list_name = \"02_r1\" valid_times = (     IntervalList     &amp; {         \"nwb_file_name\": nwb_file_name,         \"interval_list_name\": orig_interval_list_name,     } ).fetch1(\"valid_times\") new_valid_times = np.asarray([[valid_times[0, 0], valid_times[0, 0] + 100]]) interval_list_name = \"test interval\" IntervalList.insert1(     {         \"nwb_file_name\": nwb_file_name,         \"interval_list_name\": interval_list_name,         \"valid_times\": new_valid_times,     },     skip_duplicates=True, )  filter_name = \"LFP 0-400 Hz\" filter_sampling_rate = 30000 In\u00a0[\u00a0]: Copied! <pre>lfp_s_key = lfp_eg_key.copy()\nlfp_s_key[\"target_interval_list_name\"] = interval_list_name\nlfp_s_key[\"filter_name\"] = filter_name\nlfp_s_key[\"filter_sampling_rate\"] = filter_sampling_rate\nLFPSelection.insert1(lfp_s_key, skip_duplicates=True)\n</pre> lfp_s_key = lfp_eg_key.copy() lfp_s_key[\"target_interval_list_name\"] = interval_list_name lfp_s_key[\"filter_name\"] = filter_name lfp_s_key[\"filter_sampling_rate\"] = filter_sampling_rate LFPSelection.insert1(lfp_s_key, skip_duplicates=True) In\u00a0[\u00a0]: Copied! <pre>LFP().populate(lfp_s_key)\nLFPOutput()\n</pre> LFP().populate(lfp_s_key) LFPOutput() In\u00a0[\u00a0]: Copied! <pre>lfp_sampling_rate = (LFP() &amp; {\"nwb_file_name\": nwb_file_name}).fetch1(\n    \"lfp_sampling_rate\"\n)\nfilter_name = \"Theta 5-11 Hz\"\nFirFilterParameters().add_filter(\n    filter_name,\n    lfp_sampling_rate,\n    \"bandpass\",\n    [4, 5, 11, 12],\n    \"theta filter for 1 Khz data\",\n)\n</pre> lfp_sampling_rate = (LFP() &amp; {\"nwb_file_name\": nwb_file_name}).fetch1(     \"lfp_sampling_rate\" ) filter_name = \"Theta 5-11 Hz\" FirFilterParameters().add_filter(     filter_name,     lfp_sampling_rate,     \"bandpass\",     [4, 5, 11, 12],     \"theta filter for 1 Khz data\", ) <p>Next we add an entry for the LFP Band and the electrodes we want to filter</p> In\u00a0[\u00a0]: Copied! <pre># assume that we've filtered these electrodes; change this if not\nlfp_band_electrode_ids = [28, 32]\n\n# set the interval list name for this band; here we use the same interval as above\ninterval_list_name = \"test interval\"\n\n# set the reference to -1 to indicate no reference for all channels\nref_elect = [-1]\n\n# desired sampling rate\nlfp_band_sampling_rate = 100\n\n# we also need the uuid for the LFP object\nlfp_id = (LFPOutput.LFP &amp; lfp_s_key).fetch1(\"lfp_id\")\n</pre> # assume that we've filtered these electrodes; change this if not lfp_band_electrode_ids = [28, 32]  # set the interval list name for this band; here we use the same interval as above interval_list_name = \"test interval\"  # set the reference to -1 to indicate no reference for all channels ref_elect = [-1]  # desired sampling rate lfp_band_sampling_rate = 100  # we also need the uuid for the LFP object lfp_id = (LFPOutput.LFP &amp; lfp_s_key).fetch1(\"lfp_id\") In\u00a0[\u00a0]: Copied! <pre>LFPBandSelection().set_lfp_band_electrodes(\n    nwb_file_name=nwb_file_name,\n    lfp_id=lfp_id,\n    electrode_list=lfp_band_electrode_ids,\n    filter_name=filter_name,\n    interval_list_name=interval_list_name,\n    reference_electrode_list=ref_elect,\n    lfp_band_sampling_rate=lfp_band_sampling_rate,\n)\nlfp_b_key = (\n    LFPBandSelection &amp; {\"lfp_id\": lfp_id, \"filter_name\": filter_name}\n).fetch1(\"KEY\")\n</pre> LFPBandSelection().set_lfp_band_electrodes(     nwb_file_name=nwb_file_name,     lfp_id=lfp_id,     electrode_list=lfp_band_electrode_ids,     filter_name=filter_name,     interval_list_name=interval_list_name,     reference_electrode_list=ref_elect,     lfp_band_sampling_rate=lfp_band_sampling_rate, ) lfp_b_key = (     LFPBandSelection &amp; {\"lfp_id\": lfp_id, \"filter_name\": filter_name} ).fetch1(\"KEY\") <p>Check to make sure it worked</p> In\u00a0[\u00a0]: Copied! <pre>(LFPBandSelection() &amp; {\"nwb_file_name\": nwb_file_name})\n</pre> (LFPBandSelection() &amp; {\"nwb_file_name\": nwb_file_name}) In\u00a0[\u00a0]: Copied! <pre>LFPBand().populate(LFPBandSelection() &amp; {\"nwb_file_name\": nwb_file_name})\nLFPBand()\n</pre> LFPBand().populate(LFPBandSelection() &amp; {\"nwb_file_name\": nwb_file_name}) LFPBand() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n</pre> import matplotlib.pyplot as plt import numpy as np In\u00a0[\u00a0]: Copied! <pre># get the three electrical series objects and the indeces of the electrodes we band pass filtered\norig_eseries = (Raw() &amp; {\"nwb_file_name\": nwb_file_name}).fetch_nwb()[0][\"raw\"]\norig_elect_indeces = get_electrode_indices(orig_eseries, lfp_band_electrode_ids)\norig_timestamps = np.asarray(orig_eseries.timestamps)\n\nlfp_eseries = (LFP() &amp; lfp_s_key).fetch_nwb()[0][\"lfp\"]\nlfp_elect_indeces = get_electrode_indices(lfp_eseries, lfp_band_electrode_ids)\nlfp_timestamps = np.asarray(lfp_eseries.timestamps)\n\nlfp_band_eseries = (LFPBand() &amp; lfp_b_key).fetch_nwb()[0][\"filtered_data\"]\nlfp_band_elect_indeces = get_electrode_indices(\n    lfp_band_eseries, lfp_band_electrode_ids\n)\nlfp_band_timestamps = np.asarray(lfp_band_eseries.timestamps)\n</pre> # get the three electrical series objects and the indeces of the electrodes we band pass filtered orig_eseries = (Raw() &amp; {\"nwb_file_name\": nwb_file_name}).fetch_nwb()[0][\"raw\"] orig_elect_indeces = get_electrode_indices(orig_eseries, lfp_band_electrode_ids) orig_timestamps = np.asarray(orig_eseries.timestamps)  lfp_eseries = (LFP() &amp; lfp_s_key).fetch_nwb()[0][\"lfp\"] lfp_elect_indeces = get_electrode_indices(lfp_eseries, lfp_band_electrode_ids) lfp_timestamps = np.asarray(lfp_eseries.timestamps)  lfp_band_eseries = (LFPBand() &amp; lfp_b_key).fetch_nwb()[0][\"filtered_data\"] lfp_band_elect_indeces = get_electrode_indices(     lfp_band_eseries, lfp_band_electrode_ids ) lfp_band_timestamps = np.asarray(lfp_band_eseries.timestamps) In\u00a0[\u00a0]: Copied! <pre># get a list of times for the first run epoch and then select a 2 second interval 100 seconds from the beginning\n# run1times = (\n#    IntervalList &amp; {\"nwb_file_name\": nwb_file_name, \"interval_list_name\": \"02_r1\"}\n# ).fetch1(\"valid_times\")\nplottimes = [new_valid_times[0][0] + 10, new_valid_times[0][0] + 12]\n</pre> # get a list of times for the first run epoch and then select a 2 second interval 100 seconds from the beginning # run1times = ( #    IntervalList &amp; {\"nwb_file_name\": nwb_file_name, \"interval_list_name\": \"02_r1\"} # ).fetch1(\"valid_times\") plottimes = [new_valid_times[0][0] + 10, new_valid_times[0][0] + 12] In\u00a0[\u00a0]: Copied! <pre># get the time indeces for each dataset\norig_time_ind = np.where(\n    np.logical_and(\n        orig_timestamps &gt; plottimes[0], orig_timestamps &lt; plottimes[1]\n    )\n)[0]\n\nlfp_time_ind = np.where(\n    np.logical_and(lfp_timestamps &gt; plottimes[0], lfp_timestamps &lt; plottimes[1])\n)[0]\nlfp_band_time_ind = np.where(\n    np.logical_and(\n        lfp_band_timestamps &gt; plottimes[0],\n        lfp_band_timestamps &lt; plottimes[1],\n    )\n)[0]\n</pre> # get the time indeces for each dataset orig_time_ind = np.where(     np.logical_and(         orig_timestamps &gt; plottimes[0], orig_timestamps &lt; plottimes[1]     ) )[0]  lfp_time_ind = np.where(     np.logical_and(lfp_timestamps &gt; plottimes[0], lfp_timestamps &lt; plottimes[1]) )[0] lfp_band_time_ind = np.where(     np.logical_and(         lfp_band_timestamps &gt; plottimes[0],         lfp_band_timestamps &lt; plottimes[1],     ) )[0] In\u00a0[\u00a0]: Copied! <pre>plt.plot(\n    orig_eseries.timestamps[orig_time_ind],\n    orig_eseries.data[orig_time_ind, orig_elect_indeces[0]],\n    \"k-\",\n)\nplt.plot(\n    lfp_eseries.timestamps[lfp_time_ind],\n    lfp_eseries.data[lfp_time_ind, lfp_elect_indeces[0]],\n    \"b-\",\n)\nplt.plot(\n    lfp_band_eseries.timestamps[lfp_band_time_ind],\n    lfp_band_eseries.data[lfp_band_time_ind, lfp_band_elect_indeces[0]],\n    \"r-\",\n)\nplt.xlabel(\"Time (sec)\")\nplt.ylabel(\"Amplitude (AD units)\")\n\nplt.show()\n</pre> plt.plot(     orig_eseries.timestamps[orig_time_ind],     orig_eseries.data[orig_time_ind, orig_elect_indeces[0]],     \"k-\", ) plt.plot(     lfp_eseries.timestamps[lfp_time_ind],     lfp_eseries.data[lfp_time_ind, lfp_elect_indeces[0]],     \"b-\", ) plt.plot(     lfp_band_eseries.timestamps[lfp_band_time_ind],     lfp_band_eseries.data[lfp_band_time_ind, lfp_band_elect_indeces[0]],     \"r-\", ) plt.xlabel(\"Time (sec)\") plt.ylabel(\"Amplitude (AD units)\")  plt.show() In\u00a0[\u00a0]: Copied! <pre>LFPOutput.delete({\"lfp_id\": lfp_id})\nLFPElectrodeGroup.delete(lfp_eg_key)\n</pre> LFPOutput.delete({\"lfp_id\": lfp_id}) LFPElectrodeGroup.delete(lfp_eg_key)"}, {"location": "notebooks/03_lfp/#lfp-extraction", "title": "LFP Extraction\u00b6", "text": ""}, {"location": "notebooks/03_lfp/#next-we-select-the-nwb-file-which-corresponds-to-the-dataset-we-want-to-extract-lfp-from", "title": "Next we select the NWB file, which corresponds to the dataset we want to extract LFP from\u00b6", "text": ""}, {"location": "notebooks/03_lfp/#create-the-standard-lfp-filters-this-only-needs-to-be-done-once", "title": "Create the standard LFP Filters. This only needs to be done once.\u00b6", "text": ""}, {"location": "notebooks/03_lfp/#now-we-create-an-lfp-electrode-group-that-defines-the-set-of-electrodes-we-want-to-filter-for-lfp-data", "title": "Now we create an lfp electrode group that defines the set of electrodes we want to filter for lfp data.\u00b6", "text": "<p>In this case we'll take the three electrode with indexes 0, 4, and 8.</p>"}, {"location": "notebooks/03_lfp/#we-now-look-at-the-list-of-electrodes-that-are-part-of-this-lfp-electrode-group-to-verify-that-we-got-the-right-ones", "title": "We now look at the list of electrodes that are part of this lfp electrode group to verify that we got the right ones\u00b6", "text": ""}, {"location": "notebooks/03_lfp/#next-we-need-to-select-an-interval-list-and-the-lfp-filter-we-want-to-use", "title": "Next we need to select an interval list and the lfp filter we want to use\u00b6", "text": "<p>You might need to run (IntervalList &amp; {\"nwb_file_name\": nwb_file_name})  to see the list of intervals and similarly FirFilterParameters()  to see the list of defined filters</p>"}, {"location": "notebooks/03_lfp/#now-we-create-the-lfpselection-entry-to-combine-the-data-interval-list-and-filter", "title": "Now we create the LFPSelection entry to combine the data, interval list and filter\u00b6", "text": ""}, {"location": "notebooks/03_lfp/#populate-the-lfp-table-note-that-this-takes-2-hours-or-so-on-a-laptop-if-you-use-all-electrodes", "title": "Populate the LFP table. Note that this takes 2 hours or so on a laptop if you use all electrodes\u00b6", "text": "<p>Note here that populating the LFP table also inserts an LFP entry into LFPOutput, a table that allows us to merge computed and imported lfps</p>"}, {"location": "notebooks/03_lfp/#now-that-weve-created-the-lfp-object-we-can-perform-a-second-level-of-filtering-for-a-band-of-interest-in-this-case-the-theta-band", "title": "Now that we've created the LFP object we can perform a second level of filtering for a band of interest, in this case the theta band\u00b6", "text": "<p>We first need to create the filter</p>"}, {"location": "notebooks/03_lfp/#now-we-can-plot-the-original-signal-the-lfp-filtered-trace-and-the-theta-filtered-trace-together", "title": "Now we can plot the original signal, the LFP filtered trace, and the theta filtered trace together.\u00b6", "text": "<p>Much of the code below could be replaced by a function calls that would return the data from each electrical series</p>"}, {"location": "notebooks/03_lfp/#now-we-delete-the-tutorial-entries-we-added-to-clean-up-the-database", "title": "Now we delete the tutorial entries we added to clean up the database\u00b6", "text": ""}, {"location": "notebooks/04_Trodes_position/", "title": "Pipeline", "text": "In\u00a0[27]: Copied! <pre>import matplotlib.pyplot as plt\nimport datajoint as dj\nimport spyglass.position as sgp\nimport spyglass.common as sgc\n</pre> import matplotlib.pyplot as plt import datajoint as dj import spyglass.position as sgp import spyglass.common as sgc In\u00a0[15]: Copied! <pre>dj.config[\"filepath_checksum_size_limit\"] = 1 * 1024**3\ndj.config.save_global()\n</pre> dj.config[\"filepath_checksum_size_limit\"] = 1 * 1024**3 dj.config.save_global() <p>This data pipeline takes the 2D position (in video pixels) of the green and red LEDs tracked by Trodes, and computes:</p> <ul> <li>head position (in cm)</li> <li>head orientation (in radians)</li> <li>head velocity (in cm/s)</li> <li>head speed (in cm/s)</li> </ul> <p>We can then check the quality of the head position and direction by plotting the them on the video along with the oringal green and red LEDs.</p> <p>This notebook will take you through this process for one dataset.</p> In\u00a0[22]: Copied! <pre>nwb_file_name = \"chimi20200216_new.nwb\"\nnwb_copy_file_name = sgc.nwb_helper_fn.get_nwb_copy_filename(nwb_file_name)\nsgc.RawPosition() &amp; {\"nwb_file_name\": nwb_copy_file_name}\n</pre> nwb_file_name = \"chimi20200216_new.nwb\" nwb_copy_file_name = sgc.nwb_helper_fn.get_nwb_copy_filename(nwb_file_name) sgc.RawPosition() &amp; {\"nwb_file_name\": nwb_copy_file_name} Out[22]: <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>raw_position_object_id</p> the object id of the spatial series for this epoch in the NWB file chimi20200216_new_.nwb pos 0 valid times 9cdefc88-4d2f-4e52-ad6e-d4f7592e4446chimi20200216_new_.nwb pos 1 valid times db43201b-285b-478c-ae38-7ab5f4dbff71chimi20200216_new_.nwb pos 2 valid times d35cbc4d-e3d9-488f-a851-93723635f76achimi20200216_new_.nwb pos 3 valid times 39f017a8-2a22-45a0-b888-0217c910571echimi20200216_new_.nwb pos 4 valid times 6a07a237-a0d5-4ca5-8255-17a816c2a9cachimi20200216_new_.nwb pos 5 valid times 2090528b-c186-4824-8fce-37424ef0a4abchimi20200216_new_.nwb pos 6 valid times c8e09ed9-848c-44b9-82b6-f01569c72017 <p>Total: 7</p> In\u00a0[7]: Copied! <pre>print(f\"accepted parameters:\\n{sgp.TrodesPosParams.get_accepted_params()}\")\nprint(f\"default parameters:\\n{sgp.TrodesPosParams.get_default()['params']}\")\n</pre> print(f\"accepted parameters:\\n{sgp.TrodesPosParams.get_accepted_params()}\") print(f\"default parameters:\\n{sgp.TrodesPosParams.get_default()['params']}\") <pre>accepted parameters:\n['max_separation', 'max_speed', 'position_smoothing_duration', 'speed_smoothing_std_dev', 'orient_smoothing_std_dev', 'led1_is_front', 'is_upsampled', 'upsampling_sampling_rate', 'upsampling_interpolation_method']\ndefault parameters:\n{'max_separation': 9.0, 'max_speed': 300.0, 'position_smoothing_duration': 0.125, 'speed_smoothing_std_dev': 0.1, 'orient_smoothing_std_dev': 0.001, 'led1_is_front': 1, 'is_upsampled': 0, 'upsampling_sampling_rate': None, 'upsampling_interpolation_method': 'linear'}\n</pre> <p>Now we pair the parameters we chose with an interval from our specified NWB file and insert into <code>TrodesPosSelection</code>. We can define the interval we want to use via its <code>interval_list_name</code>, which we can see in the <code>IntervalList</code> table.</p> <pre>sgc.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name\n</pre> <p>A cool trick doctors don't want you to know: <code>interval_list_name</code> = 'pos (epoch# - 1) valid times' (e.g. epoch 3's interval_list_name: 'pos 2 valid times')</p> <p>Let's choose the interval corresponding to <code>pos 1 valid times</code> in <code>chimi20200216_new_.nwb</code>.</p> <p>We first look at the \"raw\" position data now to see the input into the pipeline. The raw position is in the <code>RawPosition</code> table and corresponds to the inferred position of the red and green LEDs from the video (using an algorithm in Trodes). It is in units of pixels. The number of time points corresponds to when the position tracking was turned on and off (and so may not have the same number of time points as the video itself).</p> <p>We can retrieve the data in the <code>RawPosition</code> table for a given interval using a special method called <code>fetch1_dataframe</code>. It returns the position of the red and green LEDs as a pandas dataframe where time is the index. The columns of the dataframe are:</p> <ul> <li><code>xloc</code>, <code>yloc</code> are the x- and y-position of one of the LEDs</li> <li><code>xloc2</code>, <code>yloc2</code> are the x- and y-position of the other LEDs.</li> </ul> In\u00a0[24]: Copied! <pre>interval_list_name = \"pos 1 valid times\"\nraw_position_df = (\n    sgc.RawPosition()\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": interval_list_name,\n    }\n).fetch1_dataframe()\nraw_position_df\n</pre> interval_list_name = \"pos 1 valid times\" raw_position_df = (     sgc.RawPosition()     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": interval_list_name,     } ).fetch1_dataframe() raw_position_df Out[24]: xloc yloc xloc2 yloc2 time 1.581887e+09 264 638 285 635 1.581887e+09 264 672 269 635 1.581887e+09 264 638 280 642 1.581887e+09 268 635 284 636 1.581887e+09 268 674 283 636 ... ... ... ... ... 1.581888e+09 556 598 543 616 1.581888e+09 555 598 543 616 1.581888e+09 556 598 543 616 1.581888e+09 555 598 543 616 1.581888e+09 556 598 542 615 <p>39340 rows \u00d7 4 columns</p> <p>Let's just quickly plot the two LEDs to get a sense of the inputs to the pipeline:</p> In\u00a0[28]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(raw_position_df.xloc, raw_position_df.yloc, color=\"green\")\nax.plot(raw_position_df.xloc2, raw_position_df.yloc2, color=\"red\")\nax.set_xlabel(\"x-position [pixels]\", fontsize=18)\nax.set_ylabel(\"y-position [pixels]\", fontsize=18)\nax.set_title(\"Raw Position\", fontsize=28)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(raw_position_df.xloc, raw_position_df.yloc, color=\"green\") ax.plot(raw_position_df.xloc2, raw_position_df.yloc2, color=\"red\") ax.set_xlabel(\"x-position [pixels]\", fontsize=18) ax.set_ylabel(\"y-position [pixels]\", fontsize=18) ax.set_title(\"Raw Position\", fontsize=28) Out[28]: <pre>Text(0.5, 1.0, 'Raw Position')</pre> <p>Okay, now that we understand what the inputs to the pipeline are, let's associate a set of parameters with a given interval. To associate parameters with a given interval, we insert them into the <code>TrodesPosSelection</code> table. Here we associate the <code>default</code> Trodes position parameters with the interval <code>pos 1 valid times</code>:</p> In\u00a0[30]: Copied! <pre>sgp.TrodesPosSelection.insert1(\n    {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"trodes_pos_params_name\": \"default\",\n    },\n    skip_duplicates=True,\n)\n</pre> sgp.TrodesPosSelection.insert1(     {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"trodes_pos_params_name\": \"default\",     },     skip_duplicates=True, ) <p>Now let's check to see if we've inserted the parameters correctly:</p> In\u00a0[32]: Copied! <pre>sgp.TrodesPosSelection()\ntrodes_key = (\n    sgp.TrodesPosSelection()\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"trodes_pos_params_name\": \"default\",\n    }\n).fetch1(\"KEY\")\n</pre> sgp.TrodesPosSelection() trodes_key = (     sgp.TrodesPosSelection()     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"trodes_pos_params_name\": \"default\",     } ).fetch1(\"KEY\") In\u00a0[33]: Copied! <pre>sgp.TrodesPos.populate(trodes_key)\n</pre> sgp.TrodesPos.populate(trodes_key) <pre>Computing position for: {'nwb_file_name': 'chimi20200216_new_.nwb', 'interval_list_name': 'pos 1 valid times', 'trodes_pos_params_name': 'default'}\nWriting new NWB file chimi20200216_new_GP6D6C8SDF.nwb\n</pre> <pre>\n---------------------------------------------------------------------------\nOSError                                   Traceback (most recent call last)\n/home/dgramling/Src/spyglass/notebooks/04_Trodes_position.ipynb Cell 19 in &lt;cell line: 1&gt;()\n----&gt; &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22426f7265616c6973227d/home/dgramling/Src/spyglass/notebooks/04_Trodes_position.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'&gt;1&lt;/a&gt; sgp.TrodesPos.populate(trodes_key)\n\nFile ~/anaconda3/envs/spyglass/lib/python3.8/site-packages/datajoint/autopopulate.py:229, in AutoPopulate.populate(self, suppress_errors, return_exception_objects, reserve_jobs, order, limit, max_calls, display_progress, processes, make_kwargs, *restrictions)\n    225 if processes == 1:\n    226     for key in (\n    227         tqdm(keys, desc=self.__class__.__name__) if display_progress else keys\n    228     ):\n--&gt; 229         error = self._populate1(key, jobs, **populate_kwargs)\n    230         if error is not None:\n    231             error_list.append(error)\n\nFile ~/anaconda3/envs/spyglass/lib/python3.8/site-packages/datajoint/autopopulate.py:281, in AutoPopulate._populate1(self, key, jobs, suppress_errors, return_exception_objects, make_kwargs)\n    279 self.__class__._allow_insert = True\n    280 try:\n--&gt; 281     make(dict(key), **(make_kwargs or {}))\n    282 except (KeyboardInterrupt, SystemExit, Exception) as error:\n    283     try:\n\nFile ~/Src/spyglass/src/spyglass/position/position_trodes_position.py:109, in TrodesPos.make(self, key)\n    107 def make(self, key):\n    108     print(f\"Computing position for: {key}\")\n--&gt; 109     key[\"analysis_file_name\"] = AnalysisNwbfile().create(key[\"nwb_file_name\"])\n    110     raw_position = (RawPosition() &amp; key).fetch_nwb()[0]\n    111     position_info_parameters = (TrodesPosParams() &amp; key).fetch1(\"params\")\n\nFile ~/Src/spyglass/src/spyglass/common/common_nwbfile.py:177, in AnalysisNwbfile.create(self, nwb_file_name)\n    175     analysis_file_abs_path = AnalysisNwbfile.get_abs_path(analysis_file_name)\n    176     # export the new NWB file\n--&gt; 177     with pynwb.NWBHDF5IO(\n    178         path=analysis_file_abs_path, mode=\"w\", manager=io.manager\n    179     ) as export_io:\n    180         export_io.export(io, nwbf)\n    182 # change the permissions to only allow owner to write\n\nFile ~/anaconda3/envs/spyglass/lib/python3.8/site-packages/hdmf/utils.py:583, in docval.&lt;locals&gt;.dec.&lt;locals&gt;.func_call(*args, **kwargs)\n    581 def func_call(*args, **kwargs):\n    582     pargs = _check_args(args, kwargs)\n--&gt; 583     return func(args[0], **pargs)\n\nFile ~/anaconda3/envs/spyglass/lib/python3.8/site-packages/pynwb/__init__.py:246, in NWBHDF5IO.__init__(self, **kwargs)\n    244     elif manager is None:\n    245         manager = get_manager()\n--&gt; 246 super(NWBHDF5IO, self).__init__(path, manager=manager, mode=mode, file=file_obj, comm=comm, driver=driver)\n\nFile ~/anaconda3/envs/spyglass/lib/python3.8/site-packages/hdmf/utils.py:583, in docval.&lt;locals&gt;.dec.&lt;locals&gt;.func_call(*args, **kwargs)\n    581 def func_call(*args, **kwargs):\n    582     pargs = _check_args(args, kwargs)\n--&gt; 583     return func(args[0], **pargs)\n\nFile ~/anaconda3/envs/spyglass/lib/python3.8/site-packages/hdmf/backends/hdf5/h5tools.py:79, in HDF5IO.__init__(self, **kwargs)\n     77 self.__mode = mode\n     78 self.__file = file_obj\n---&gt; 79 super().__init__(manager, source=path)\n     80 self.__built = dict()       # keep track of each builder for each dataset/group/link for each file\n     81 self.__read = dict()        # keep track of which files have been read. Key is the filename value is the builder\n\nFile ~/anaconda3/envs/spyglass/lib/python3.8/site-packages/hdmf/utils.py:583, in docval.&lt;locals&gt;.dec.&lt;locals&gt;.func_call(*args, **kwargs)\n    581 def func_call(*args, **kwargs):\n    582     pargs = _check_args(args, kwargs)\n--&gt; 583     return func(args[0], **pargs)\n\nFile ~/anaconda3/envs/spyglass/lib/python3.8/site-packages/hdmf/backends/io.py:22, in HDMFIO.__init__(self, **kwargs)\n     20 self.__built = dict()\n     21 self.__source = source\n---&gt; 22 self.open()\n\nFile ~/anaconda3/envs/spyglass/lib/python3.8/site-packages/hdmf/backends/hdf5/h5tools.py:774, in HDF5IO.open(self)\n    771 if self.driver is not None:\n    772     kwargs.update(driver=self.driver)\n--&gt; 774 self.__file = File(self.source, open_flag, **kwargs)\n\nFile ~/anaconda3/envs/spyglass/lib/python3.8/site-packages/h5py/_hl/files.py:406, in File.__init__(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\n    404 with phil:\n    405     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0, **kwds)\n--&gt; 406     fid = make_fid(name, mode, userblock_size,\n    407                    fapl, fcpl=make_fcpl(track_order=track_order),\n    408                    swmr=swmr)\n    410 if isinstance(libver, tuple):\n    411     self._libver = libver\n\nFile ~/anaconda3/envs/spyglass/lib/python3.8/site-packages/h5py/_hl/files.py:179, in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)\n    177     fid = h5f.create(name, h5f.ACC_EXCL, fapl=fapl, fcpl=fcpl)\n    178 elif mode == 'w':\n--&gt; 179     fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl)\n    180 elif mode == 'a':\n    181     # Open in append mode (read/write).\n    182     # If that fails, create a new file only if it won't clobber an\n    183     # existing one (ACC_EXCL)\n    184     try:\n\nFile h5py/_objects.pyx:54, in h5py._objects.with_phil.wrapper()\n\nFile h5py/_objects.pyx:55, in h5py._objects.with_phil.wrapper()\n\nFile h5py/h5f.pyx:108, in h5py.h5f.create()\n\nOSError: Unable to create file (unable to open file: name = '/stelmo/nwb/analysis/chimi20200216_new/chimi20200216_new_GP6D6C8SDF.nwb', errno = 13, error message = 'Permission denied', flags = 13, o_flags = 242)</pre> <p>We can see that each NWB file, interval, and parameter set is now associated with a newly created analysis NWB file and object IDs that correspond to our newly computed data. This isn't as useful to work with so we will use another method below to actually retrieve the data for a given interval.</p> In\u00a0[34]: Copied! <pre>sgp.TrodesPos()\n</pre> sgp.TrodesPos() Out[34]: <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>trodes_pos_params_name</p> name for this set of parameters <p>analysis_file_name</p> name of the file <p>position_object_id</p> <p>orientation_object_id</p> <p>velocity_object_id</p> <p>Total: 0</p> <p>In order to retrieve the results of the computation, we use a special method called <code>fetch1_dataframe</code> from the <code>IntervalPositionInfo</code> table that will retrieve the position pipeline results as a pandas DataFrame. Time is set as the index of the dataframe.</p> <p>This will only work for a single interval so we need to specify the NWB file and the interval.</p> <p>This dataframe has the following columns:</p> <ul> <li><code>head_position_x</code>, <code>head_position_y</code>: the x,y position of the head position (in cm).</li> <li><code>head_orientation</code>: The direction of the head relative to the bottom left corner (in radians)</li> <li><code>head_velocity_x</code>, <code>head_velocity_y</code>: the directional change in head position over time (in cm/s)</li> <li><code>head_speed</code>: the magnitude of the change in head position over time (in cm/s)</li> </ul> In\u00a0[13]: Copied! <pre>position_info = (\n    IntervalPositionInfo()\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"position_info_param_name\": \"default\",\n    }\n).fetch1_dataframe()\nposition_info\n</pre> position_info = (     IntervalPositionInfo()     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"position_info_param_name\": \"default\",     } ).fetch1_dataframe() position_info <pre>/stelmo/nwb/analysis/chimi20200216_new_ZF24K2JHC1.nwb\n</pre> Out[13]: head_position_x head_position_y head_orientation head_velocity_x head_velocity_y head_speed time 1.581887e+09 91.051650 211.127050 2.999696 1.387074 2.848838 3.168573 1.581887e+09 90.844337 211.417287 3.078386 3.123201 3.411111 4.624939 1.581887e+09 90.637025 211.707525 -3.114572 5.431643 4.089597 6.799085 1.581887e+09 90.802875 211.596958 -3.033109 8.097753 4.979262 9.506138 1.581887e+09 91.288579 211.482443 -3.062550 10.840482 6.071373 12.424880 ... ... ... ... ... ... ... 1.581888e+09 182.158583 201.452467 -0.986926 0.348276 0.218575 0.411182 1.581888e+09 182.158583 201.397183 -0.978610 0.279135 -0.058413 0.285182 1.581888e+09 182.213867 201.341900 -0.957589 0.193798 -0.283200 0.343162 1.581888e+09 182.158583 201.341900 -0.970083 0.110838 -0.417380 0.431846 1.581888e+09 182.158583 201.286617 -0.936414 0.045190 -0.453966 0.456209 <p>39340 rows \u00d7 6 columns</p> <p>If you are not familiar with pandas, the time variable is set as the index. It can be accessed using <code>.index</code> on the dataframe.</p> In\u00a0[14]: Copied! <pre>position_info.index\n</pre> position_info.index Out[14]: <pre>Float64Index([1581886916.3153033, 1581886916.3486283, 1581886916.3819742,\n              1581886916.4152992,  1581886916.448645, 1581886916.4819698,\n              1581886916.5152948, 1581886916.5486405, 1581886916.5819652,\n              1581886916.6152902,\n              ...\n              1581888227.3021932,  1581888227.335518,  1581888227.368864,\n              1581888227.4021888,  1581888227.435535, 1581888227.4688597,\n              1581888227.5021844, 1581888227.5355306, 1581888227.5688553,\n                1581888227.60218],\n             dtype='float64', name='time', length=39340)</pre> In\u00a0[15]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(position_info.head_position_x, position_info.head_position_y)\nax.set_xlabel(\"x-position [cm]\", fontsize=18)\nax.set_ylabel(\"y-position [cm]\", fontsize=18)\nax.set_title(\"Head Position\", fontsize=28)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(position_info.head_position_x, position_info.head_position_y) ax.set_xlabel(\"x-position [cm]\", fontsize=18) ax.set_ylabel(\"y-position [cm]\", fontsize=18) ax.set_title(\"Head Position\", fontsize=28) Out[15]: <pre>Text(0.5, 1.0, 'Head Position')</pre> In\u00a0[16]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(position_info.head_velocity_x, position_info.head_velocity_y)\nax.set_xlabel(\"x-velocity [cm/s]\", fontsize=18)\nax.set_ylabel(\"y-velocity [cm/s]\", fontsize=18)\nax.set_title(\"Head Velocity\", fontsize=28)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(position_info.head_velocity_x, position_info.head_velocity_y) ax.set_xlabel(\"x-velocity [cm/s]\", fontsize=18) ax.set_ylabel(\"y-velocity [cm/s]\", fontsize=18) ax.set_title(\"Head Velocity\", fontsize=28) Out[16]: <pre>Text(0.5, 1.0, 'Head Velocity')</pre> In\u00a0[17]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(25, 3))\nax.plot(position_info.index, position_info.head_speed)\nax.set_xlabel(\"Time\", fontsize=18)\nax.set_ylabel(\"Speed [cm/s]\", fontsize=18)\nax.set_title(\"Head Speed\", fontsize=28)\nax.set_xlim((position_info.index.min(), position_info.index.max()))\n</pre> fig, ax = plt.subplots(1, 1, figsize=(25, 3)) ax.plot(position_info.index, position_info.head_speed) ax.set_xlabel(\"Time\", fontsize=18) ax.set_ylabel(\"Speed [cm/s]\", fontsize=18) ax.set_title(\"Head Speed\", fontsize=28) ax.set_xlim((position_info.index.min(), position_info.index.max())) Out[17]: <pre>(1581886916.3153033, 1581888227.60218)</pre> In\u00a0[24]: Copied! <pre>from spyglass.common.common_position import PositionVideo\n\nPositionVideo().make(\n    {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"position_info_param_name\": \"default\",\n    }\n)\n</pre> from spyglass.common.common_position import PositionVideo  PositionVideo().make(     {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"position_info_param_name\": \"default\",     } ) <pre>[2022-08-04 15:05:53,003][WARNING]: Skipped checksum for file with hash: 050179d6-42a0-9236-45ee-6069346d0196, and path: /stelmo/nwb/raw/chimi20200216_new_.nwb\n</pre> <pre>Loading position data...\nLoading video data...\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nInput In [24], in &lt;cell line: 3&gt;()\n      1 from spyglass.common.common_position import PositionVideo\n----&gt; 3 PositionVideo().make({'nwb_file_name': nwb_copy_file_name,\n      4 'interval_list_name': 'pos 1 valid times',\n      5 'position_info_param_name': 'default'\n      6 })\n\nFile ~/Documents/code/spyglass/src/spyglass/common/common_position.py:631, in PositionVideo.make(self, key)\n    628 io = pynwb.NWBHDF5IO('/stelmo/nwb/raw/' +\n    629                      video_info['nwb_file_name'], 'r')\n    630 nwb_file = io.read()\n--&gt; 631 nwb_video = nwb_file.objects[video_info['video_file_object_id']]\n    632 video_filename = nwb_video.external_file.value[0]\n    634 nwb_base_filename = key['nwb_file_name'].replace('.nwb', '')\n\nFile ~/anaconda3/envs/spyglass/lib/python3.9/site-packages/hdmf/utils.py:932, in LabelledDict.__getitem__(self, args)\n    930         return super().__getitem__(val)\n    931 else:\n--&gt; 932     return super().__getitem__(key)\n\nKeyError: '3234c1e5-c992-4fb4-9c82-d1603d90bce7'</pre> In\u00a0[25]: Copied! <pre>PositionInfoParameters.insert1(\n    {\n        \"position_info_param_name\": \"default_decoding\",\n        \"is_upsampled\": 1,\n        \"upsampling_sampling_rate\": 500,\n    },\n    skip_duplicates=True,\n)\n\nPositionInfoParameters()\n</pre> PositionInfoParameters.insert1(     {         \"position_info_param_name\": \"default_decoding\",         \"is_upsampled\": 1,         \"upsampling_sampling_rate\": 500,     },     skip_duplicates=True, )  PositionInfoParameters() Out[25]: <p>position_info_param_name</p> name for this set of parameters <p>max_separation</p> max distance (in cm) between head LEDs <p>max_speed</p> max speed (in cm / s) of animal <p>position_smoothing_duration</p> size of moving window (in seconds) <p>speed_smoothing_std_dev</p> smoothing standard deviation (in seconds) <p>head_orient_smoothing_std_dev</p> smoothing std deviation (in seconds) <p>led1_is_front</p> first LED is front LED and second is back LED, else first LED is back <p>is_upsampled</p> upsample the position to higher sampling rate <p>upsampling_sampling_rate</p> The rate to be upsampled to <p>upsampling_interpolation_method</p> see pandas.DataFrame.interpolation for list of methods default 9.0 300.0 0.125 0.1 0.001 1 0 nan lineardefault_decoding 9.0 300.0 0.125 0.1 0.001 1 1 500.0 lineardefault_lfp 9.0 300.0 0.125 0.1 0.001 1 1 1000.0 linear <p>Total: 3</p> In\u00a0[26]: Copied! <pre>IntervalPositionInfoSelection.insert1(\n    {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"position_info_param_name\": \"default_decoding\",\n    },\n    skip_duplicates=True,\n)\n\nIntervalPositionInfoSelection()\n</pre> IntervalPositionInfoSelection.insert1(     {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"position_info_param_name\": \"default_decoding\",     },     skip_duplicates=True, )  IntervalPositionInfoSelection() Out[26]: <p>position_info_param_name</p> name for this set of parameters <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list default_decoding CH6120211203_.nwb pos 0 valid timesdefault_decoding CH6520211125_.nwb pos 0 valid timesdefault_decoding CH6520211201_.nwb pos 0 valid timesdefault chimi20200216_new_.nwb pos 1 valid timesdefault_decoding chimi20200216_new_.nwb pos 1 valid timesdefault_lfp chimi20200216_new_.nwb pos 1 valid timesdefault fern20211007_.nwb pos 0 valid timesdefault_decoding fern20211007_.nwb pos 0 valid timesdefault fern20211007_.nwb pos 1 valid timesdefault_decoding fern20211007_.nwb pos 1 valid timesdefault fern20211007_.nwb pos 10 valid timesdefault_decoding fern20211007_.nwb pos 10 valid times <p>...</p> <p>Total: 1815</p> In\u00a0[27]: Copied! <pre>IntervalPositionInfo.populate()\n</pre> IntervalPositionInfo.populate() In\u00a0[28]: Copied! <pre>upsampled_position_info = (\n    IntervalPositionInfo()\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"position_info_param_name\": \"default_decoding\",\n    }\n).fetch1_dataframe()\n\nupsampled_position_info\n</pre> upsampled_position_info = (     IntervalPositionInfo()     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"position_info_param_name\": \"default_decoding\",     } ).fetch1_dataframe()  upsampled_position_info <pre>/stelmo/nwb/analysis/chimi20200216_new_6YC9LPAR7S.nwb\n</pre> Out[28]: head_position_x head_position_y head_orientation head_velocity_x head_velocity_y head_speed time 1.581887e+09 91.051650 211.127050 2.680048 1.741550 2.301478 2.886139 1.581887e+09 91.039455 211.144123 3.003241 1.827555 2.333931 2.964320 1.581887e+09 91.027260 211.161196 3.008398 1.915800 2.366668 3.044898 1.581887e+09 91.015065 211.178268 3.012802 2.006286 2.399705 3.127901 1.581887e+09 91.002871 211.195341 3.017242 2.099012 2.433059 3.213352 ... ... ... ... ... ... ... 1.581888e+09 182.158583 201.299625 -0.944304 0.057520 -0.356012 0.360629 1.581888e+09 182.158583 201.296373 -0.942329 0.053954 -0.356343 0.360404 1.581888e+09 182.158583 201.293121 -0.940357 0.050477 -0.356407 0.359964 1.581888e+09 182.158583 201.289869 -0.953059 0.047091 -0.356212 0.359312 1.581888e+09 182.158583 201.286617 -0.588081 0.043796 -0.355764 0.358450 <p>655645 rows \u00d7 6 columns</p> In\u00a0[24]: Copied! <pre>fig, axes = plt.subplots(\n    1, 2, figsize=(20, 10), sharex=True, sharey=True, constrained_layout=True\n)\naxes[0].plot(position_info.head_position_x, position_info.head_position_y)\naxes[0].set_xlabel(\"x-position [cm]\", fontsize=18)\naxes[0].set_ylabel(\"y-position [cm]\", fontsize=18)\naxes[0].set_title(\"Head Position\", fontsize=28)\n\naxes[1].plot(\n    upsampled_position_info.head_position_x,\n    upsampled_position_info.head_position_y,\n)\naxes[1].set_xlabel(\"x-position [cm]\", fontsize=18)\naxes[1].set_ylabel(\"y-position [cm]\", fontsize=18)\naxes[1].set_title(\"Upsampled Head Position\", fontsize=28)\n</pre> fig, axes = plt.subplots(     1, 2, figsize=(20, 10), sharex=True, sharey=True, constrained_layout=True ) axes[0].plot(position_info.head_position_x, position_info.head_position_y) axes[0].set_xlabel(\"x-position [cm]\", fontsize=18) axes[0].set_ylabel(\"y-position [cm]\", fontsize=18) axes[0].set_title(\"Head Position\", fontsize=28)  axes[1].plot(     upsampled_position_info.head_position_x,     upsampled_position_info.head_position_y, ) axes[1].set_xlabel(\"x-position [cm]\", fontsize=18) axes[1].set_ylabel(\"y-position [cm]\", fontsize=18) axes[1].set_title(\"Upsampled Head Position\", fontsize=28) Out[24]: <pre>Text(0.5, 1.0, 'Upsampled Head Position')</pre> In\u00a0[25]: Copied! <pre>fig, axes = plt.subplots(\n    2, 1, figsize=(25, 6), sharex=True, sharey=True, constrained_layout=True\n)\naxes[0].plot(position_info.index, position_info.head_speed)\naxes[0].set_xlabel(\"Time\", fontsize=18)\naxes[0].set_ylabel(\"Speed [cm/s]\", fontsize=18)\naxes[0].set_title(\"Head Speed\", fontsize=28)\naxes[0].set_xlim((position_info.index.min(), position_info.index.max()))\n\naxes[1].plot(upsampled_position_info.index, upsampled_position_info.head_speed)\naxes[1].set_xlabel(\"Time\", fontsize=18)\naxes[1].set_ylabel(\"Speed [cm/s]\", fontsize=18)\naxes[1].set_title(\"Upsampled Head Speed\", fontsize=28)\n</pre> fig, axes = plt.subplots(     2, 1, figsize=(25, 6), sharex=True, sharey=True, constrained_layout=True ) axes[0].plot(position_info.index, position_info.head_speed) axes[0].set_xlabel(\"Time\", fontsize=18) axes[0].set_ylabel(\"Speed [cm/s]\", fontsize=18) axes[0].set_title(\"Head Speed\", fontsize=28) axes[0].set_xlim((position_info.index.min(), position_info.index.max()))  axes[1].plot(upsampled_position_info.index, upsampled_position_info.head_speed) axes[1].set_xlabel(\"Time\", fontsize=18) axes[1].set_ylabel(\"Speed [cm/s]\", fontsize=18) axes[1].set_title(\"Upsampled Head Speed\", fontsize=28) Out[25]: <pre>Text(0.5, 1.0, 'Upsampled Head Speed')</pre> In\u00a0[26]: Copied! <pre>fig, axes = plt.subplots(\n    1, 2, figsize=(20, 10), sharex=True, sharey=True, constrained_layout=True\n)\naxes[0].plot(position_info.head_velocity_x, position_info.head_velocity_y)\naxes[0].set_xlabel(\"x-velocity [cm/s]\", fontsize=18)\naxes[0].set_ylabel(\"y-velocity [cm/s]\", fontsize=18)\naxes[0].set_title(\"Head Velocity\", fontsize=28)\n\naxes[1].plot(\n    upsampled_position_info.head_velocity_x,\n    upsampled_position_info.head_velocity_y,\n)\naxes[1].set_xlabel(\"x-velocity [cm/s]\", fontsize=18)\naxes[1].set_ylabel(\"y-velocity [cm/s]\", fontsize=18)\naxes[1].set_title(\"Upsampled Head Velocity\", fontsize=28)\n</pre> fig, axes = plt.subplots(     1, 2, figsize=(20, 10), sharex=True, sharey=True, constrained_layout=True ) axes[0].plot(position_info.head_velocity_x, position_info.head_velocity_y) axes[0].set_xlabel(\"x-velocity [cm/s]\", fontsize=18) axes[0].set_ylabel(\"y-velocity [cm/s]\", fontsize=18) axes[0].set_title(\"Head Velocity\", fontsize=28)  axes[1].plot(     upsampled_position_info.head_velocity_x,     upsampled_position_info.head_velocity_y, ) axes[1].set_xlabel(\"x-velocity [cm/s]\", fontsize=18) axes[1].set_ylabel(\"y-velocity [cm/s]\", fontsize=18) axes[1].set_title(\"Upsampled Head Velocity\", fontsize=28) Out[26]: <pre>Text(0.5, 1.0, 'Upsampled Head Velocity')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/04_Trodes_position/#position-using-trodes-tracking-from-nwb-file", "title": "Position using Trodes Tracking from NWB file\u00b6", "text": "<p>Note: make a copy of this notebook and run the copy to avoid git conflicts in the future</p> <p>This is a tutorial on how to process position that was extracted through Trodes Tracking (online or offline) using the Spyglass pipeline used in Loren Frank's lab, UCSF. It will walk through defining your parameters and processing the raw position to extract a centroid and orientation, and inserting the resulting information into the <code>IntervalPositionInfo</code> table. -&gt; This tutorial assumes you've completed tutorial 0 Note 2: Make sure you are running this within the spyglass Conda environment</p>"}, {"location": "notebooks/04_Trodes_position/#1-loading-the-session-data", "title": "1. Loading the session data\u00b6", "text": "<p>First let us make sure that the session we want to analyze is inserted into the <code>RawPosition</code> table</p>"}, {"location": "notebooks/04_Trodes_position/#2-setting-the-parameters-for-running-the-position-pipeline", "title": "2. Setting the parameters for running the position pipeline\u00b6", "text": "<p>The parameters for the position pipeline are set by the <code>TrodesPosParams</code> table. <code>default</code> is the name of the standard set of parameters. As usual, if you want to change the parameters, you can insert your own into the table.</p> <p>The parameters are as follows:</p> <ul> <li><code>max_separation</code> is the maxmium acceptable distance (in cm) between the red and green LEDs. When the distance between the LEDs becomes greater than this number, the times are marked as NaNs and inferred by interpolation. This is useful parameter when the inferred red or green LED position tracks a reflection instead of the true red or green LED position. It is set to 9.0 cm by default.</li> <li><code>max_speed</code> is the maximum plausible speed (in cm/s) the animal can move at. Times when the speed is greater than this threshold are marked as NaNs and inferred by interpolation. This can be useful in preventing big, sudden jumps in position. It is set to 300.0 cm/s by default.</li> <li><code>position_smoothing_duration</code> controls how much the red and green LEDs are smoothed before computing the average of their position to get the head position. It is in units of seconds.</li> <li><code>speed_smoothing_std_dev</code> controls how much the head speed is smoothed. It corresponds to the standard deviation of the Gaussian kernel used to smooth the speed and is in units of seconds. It is set to 0.100 seconds by default.</li> <li><code>front_led1</code> is either 1 or 0 indicating True or False. It controls which LED is treated as the front LED and the back LED, which is important for calculating the head direction.<ul> <li>1 indicates that the LED corresponding to <code>xloc</code>, <code>yloc</code> in the <code>RawPosition</code> table as the front LED and the LED corresponding to <code>xloc2</code>, <code>yloc2</code> as the back LED.</li> <li>0 indicates that <code>xloc</code>, <code>yloc</code> are treated as the back LED and <code>xloc2</code>, <code>yloc2</code> are treated as the front LED.</li> </ul> </li> </ul> <p>We can get a list of potential parameters using the method <code>TrodesPosParams.get_accepted_params()</code> And view the default setting for the parameters using <code>get_default</code>.</p>"}, {"location": "notebooks/04_Trodes_position/#3-running-the-position-pipeline-and-retrieving-the-results", "title": "3. Running the position pipeline and retrieving the results\u00b6", "text": "<p>Now that we have associated the parameters with the interval we want to run, we can finally run the pipeline for that interval.</p> <p>We run the pipeline using the <code>populate</code> method on the <code>TrodesPos</code> table.</p>"}, {"location": "notebooks/04_Trodes_position/#4-examining-the-results", "title": "4. Examining the results\u00b6", "text": "<p>We should always spot check our results to verify that the pipeline worked correctly.</p>"}, {"location": "notebooks/04_Trodes_position/#plots", "title": "Plots\u00b6", "text": "<p>Let's plot some of the variables first:</p>"}, {"location": "notebooks/04_Trodes_position/#video", "title": "Video\u00b6", "text": "<p>These look reasonable but it is better to evaluate these variables by plotting the results on the video where we can see how they correspond.</p> <p>The video will appear in the current working directory when it is done.</p>"}, {"location": "notebooks/04_Trodes_position/#4-upsampling-position-data", "title": "4. Upsampling position data\u00b6", "text": "<p>Sometimes you need the position data to be in a different rate than it is sampled in, such as when decoding in 2 ms time bins. You can use the upsampling parameters to get this data:</p> <ul> <li><code>is_upsampled</code> controls whether upsampling happens. If it is 1 then there is upsampling, and if it is 0 then upsampling does not happen.</li> <li><code>upsampling_sampling_rate</code> is the rate you want to upsample to. For example position is typically recorded at 33 frames per seconds and you may want to upsample up to 500 frames per second.</li> <li><code>upsampling_interpolation_method</code> is the interpolation method used for upsampling. It is set to linear by default. See the methods available for pandas.DataFrame.interpolate to get a list of the methods.</li> </ul>"}, {"location": "notebooks/05_DLC_from_scratch/", "title": "From Scratch", "text": "In\u00a0[1]: Copied! <pre>from pathlib import Path, PosixPath, PurePath\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport pynwb\nimport datajoint as dj\nimport spyglass.common as sgc\nimport spyglass.position.v1 as sgp\nfrom spyglass.position import FinalPosition\n</pre> from pathlib import Path, PosixPath, PurePath import os import glob import numpy as np import pandas as pd import pynwb import datajoint as dj import spyglass.common as sgc import spyglass.position.v1 as sgp from spyglass.position import FinalPosition <pre>[2023-04-20 14:51:05,338][INFO]: Connecting dgramling@lmf-db.cin.ucsf.edu:3306\n[2023-04-20 14:51:05,410][INFO]: Connected dgramling@lmf-db.cin.ucsf.edu:3306\n</pre> Here is a schematic showing the tables used in this notebook. Note: The cells within the <code>DLCProject</code> step need to be performed in a local Jupyter notebook to allow for use of the frame labeling GUI. <p>Let us begin with visualizing the contents of the BodyPart table. This table will store standard names of body parts used within DLC models throughout the lab with a concise description.</p> Please do not add to this table unless necessary. In\u00a0[2]: Copied! <pre>sgp.BodyPart()\n</pre> sgp.BodyPart() Out[2]: <p>bodypart</p> <p>bodypart_description</p> greenLED green LED on implant LED ringredLED redLEDredLED_C center red LED on implant LED ringredLED_L left red LED on implant LED ringredLED_R right red LED on implant LED ringtailBase base of the tail on subjectwhiteLED white LED on headstage <p>Total: 7</p>  If the bodyparts you plan to use in your model are not yet in the table, here is code to add bodyparts:  <pre>\n</pre> <p>sgp.BodyPart.insert([{'bodypart': 'bodypart_1', 'bodypart_description': 'concise description of bodypart'}, {'bodypart': 'bodypart_2', 'bodypart_description': 'concise description of bodypart'},], skip_duplicates=True)</p> <pre><code></code></pre> <p>Next we want to construct a list of videos from which we will extract frames to train the model.The list can either contain dictionaries identifying behavioral videos for NWB files that have already been added to Spyglass, or absolute file paths to the videos you want to use.For this tutorial, we'll use two videos for which we already have frames labeled.</p> In\u00a0[3]: Copied! <pre>video_list = [\n    {\"nwb_file_name\": \"J1620210529_.nwb\", \"epoch\": 2},\n    {\"nwb_file_name\": \"peanut20201103_.nwb\", \"epoch\": 4},\n]\n</pre> video_list = [     {\"nwb_file_name\": \"J1620210529_.nwb\", \"epoch\": 2},     {\"nwb_file_name\": \"peanut20201103_.nwb\", \"epoch\": 4}, ] <p>Before creating our project, we need to define a few variables.</p> <p>First, we want to set a team name to one that exists in the <code>LabTeam</code> table to ensure proper permission are set.In this case we'll use \"LorenLab\", as all Frank Lab members are a part of this team. We also need to define a <code>project_name</code>, which should be a unique identifier for this project. For the tutorial we'll set it as \"tutorial_scratch_yourinitials\"Next, we need to define a list of <code>bodyparts</code> for which we want to extract position. The pre-labeled frames we're using include the bodyparts listed below, but please modify as needed for your own project.We also want to define how many frames we want to extract and eventually label from each video we're using. I will typically use 200 <code>frames_per_video</code>, but we'll keep it to 100 for efficiency's sake.</p> In\u00a0[4]: Copied! <pre>team_name = \"LorenLab\"\nproject_name = \"tutorial_scratch_DG\"\nframes_per_video = 100\nbodyparts = [\"redLED_C\", \"greenLED\", \"redLED_L\", \"redLED_R\", \"tailBase\"]\nproject_key = sgp.DLCProject.insert_new_project(\n    project_name=project_name,\n    bodyparts=bodyparts,\n    lab_team=team_name,\n    frames_per_video=frames_per_video,\n    video_list=video_list,\n    skip_duplicates=True,\n)\n</pre> team_name = \"LorenLab\" project_name = \"tutorial_scratch_DG\" frames_per_video = 100 bodyparts = [\"redLED_C\", \"greenLED\", \"redLED_L\", \"redLED_R\", \"tailBase\"] project_key = sgp.DLCProject.insert_new_project(     project_name=project_name,     bodyparts=bodyparts,     lab_team=team_name,     frames_per_video=frames_per_video,     video_list=video_list,     skip_duplicates=True, ) <pre>project name: tutorial_scratch_DG is already in use.\n</pre> <p>Now that we've intialized our project we'll need to extract and label frames.While this has already been done for this tutorial, here are the commands in order to pull up the DLC GUI to perform these actions:</p> <pre>\n</pre> <p>sgp.DLCProject().run_extract_frames(project_key) sgp.DLCProject().run_label_frames(project_key)</p> <pre><code></code></pre> <p>Typically, in order to use pre-labeled frames to your project you'll need to change the values in the labeled-data files. You can do that using the <code>import_labeled_frames</code> method.</p> This function expects the `project_key` from your new projectThe absolute path to the project you want to import the labeled frames fromThe filename (without file extension) of the videos from which you want the frames. Here we'll use the path to a pre-existing project from tutorial 06    In\u00a0[5]: Copied! <pre>sgp.DLCProject.import_labeled_frames(\n    project_key.copy(),\n    import_project_path=\"/nimbus/deeplabcut/projects/tutorial_model-LorenLab-2022-07-15/\",\n    video_filenames=[\"20201103_peanut_04_r2\", \"20210529_J16_02_r1\"],\n    skip_duplicates=True,\n)\n</pre> sgp.DLCProject.import_labeled_frames(     project_key.copy(),     import_project_path=\"/nimbus/deeplabcut/projects/tutorial_model-LorenLab-2022-07-15/\",     video_filenames=[\"20201103_peanut_04_r2\", \"20210529_J16_02_r1\"],     skip_duplicates=True, ) <pre>/home/dgramling/Src/spyglass/src/spyglass/position/v1/position_dlc_project.py:451: FutureWarning: inplace is deprecated and will be removed in a future version.\n  dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n/home/dgramling/Src/spyglass/src/spyglass/position/v1/position_dlc_project.py:451: FutureWarning: inplace is deprecated and will be removed in a future version.\n  dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n2023-04-20 14:51:08.706450: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-20 14:51:08.897194: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n</pre> <pre>Loading DLC 2.2.3...\nOpenCV is built with OpenMP support. This usually results in poor performance. For details, see https://github.com/tensorpack/benchmarks/blob/master/ImageNet/benchmark-opencv-resize.py\n</pre> This step and beyond should be run on a GPU-enabled machine. In\u00a0[7]: Copied! <pre>sgp.dlc_utils.get_gpu_memory()\n</pre> sgp.dlc_utils.get_gpu_memory() Out[7]: <pre>{0: 80383, 1: 35, 2: 35, 3: 35, 4: 35, 5: 35, 6: 35, 7: 35, 8: 35, 9: 35}</pre>  Set GPU core here In\u00a0[8]: Copied! <pre>gputouse = 1  ## 1-9\n</pre> gputouse = 1  ## 1-9 <p>Now let's define the rest of our parameters and insert the entry. (If you want to see all possible parameters that you can pass, checkout the line below):</p> <pre>\n</pre> <p>sgp.DLCModelTrainingParams.get_accepted_params()</p> <pre><code></code></pre> In\u00a0[9]: Copied! <pre>training_params_name = \"tutorial\"\nsgp.DLCModelTrainingParams.insert_new_params(\n    paramset_name=training_params_name,\n    params={\n        \"trainingsetindex\": 0,\n        \"shuffle\": 1,\n        \"gputouse\": gputouse,\n        \"net_type\": \"resnet_50\",\n        \"augmenter_type\": \"imgaug\",\n    },\n    skip_duplicates=True,\n)\n</pre> training_params_name = \"tutorial\" sgp.DLCModelTrainingParams.insert_new_params(     paramset_name=training_params_name,     params={         \"trainingsetindex\": 0,         \"shuffle\": 1,         \"gputouse\": gputouse,         \"net_type\": \"resnet_50\",         \"augmenter_type\": \"imgaug\",     },     skip_duplicates=True, ) <pre>New param set not added\nA param set with name: tutorial already exists\n</pre> <p>Next we'll modify the <code>project_key</code> from above to include the necessary entries for <code>DLCModelTraining</code></p> In\u00a0[19]: Copied! <pre># project_key['project_path'] = os.path.dirname(project_key['config_path'])\nif \"config_path\" in project_key:\n    del project_key[\"config_path\"]\n</pre> # project_key['project_path'] = os.path.dirname(project_key['config_path']) if \"config_path\" in project_key:     del project_key[\"config_path\"] <p>And here we can insert an entry into <code>DLCModelTrainingSelection</code> and populate <code>DLCModelTraining</code> with that entry, which will run training for us. Note: You can stop training at any point using <code>I + I</code> or interrupt the Kernel</p> In\u00a0[18]: Copied! <pre>sgp.DLCModelTrainingSelection.heading\n</pre> sgp.DLCModelTrainingSelection.heading Out[18]: <pre># Specification for a DLC model training instance\nproject_name         : varchar(100)                 # name of DLC project\ndlc_training_params_name : varchar(50)                  # descriptive name of parameter set\ntraining_id          : int                          # unique integer,\n---\nmodel_prefix=\"\"      : varchar(32)                  # </pre> In\u00a0[22]: Copied! <pre>sgp.DLCModelTrainingSelection().insert1(\n    {\n        **project_key,\n        \"dlc_training_params_name\": training_params_name,\n        \"training_id\": 0,\n        \"model_prefix\": \"\",\n    }\n)\nmodel_training_key = (\n    sgp.DLCModelTrainingSelection\n    &amp; {\n        **project_key,\n        \"dlc_training_params_name\": training_params_name,\n    }\n).fetch1(\"KEY\")\nsgp.DLCModelTraining.populate(model_training_key)\n</pre> sgp.DLCModelTrainingSelection().insert1(     {         **project_key,         \"dlc_training_params_name\": training_params_name,         \"training_id\": 0,         \"model_prefix\": \"\",     } ) model_training_key = (     sgp.DLCModelTrainingSelection     &amp; {         **project_key,         \"dlc_training_params_name\": training_params_name,     } ).fetch1(\"KEY\") sgp.DLCModelTraining.populate(model_training_key) <pre>[20-Apr-23 15:00:16] in /home/dgramling/Src/spyglass/src/spyglass/position/v1/position_dlc_training.py, line 179: creating training dataset\nINFO:DLC_project_{project_name}_training:creating training dataset\n</pre> <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[22], line 7\n      1 sgp.DLCModelTrainingSelection().insert1({**project_key,\n      2                                          \"dlc_training_params_name\": training_params_name,\n      3                                          \"training_id\": 0,\n      4                                          \"model_prefix\": '',})\n      5 model_training_key = (sgp.DLCModelTrainingSelection &amp; {**project_key,\n      6                                                        \"dlc_training_params_name\":training_params_name,}).fetch1(\"KEY\")\n----&gt; 7 sgp.DLCModelTraining.populate(model_training_key)\n\nFile ~/anaconda3/envs/spyglass-position/lib/python3.9/site-packages/datajoint/autopopulate.py:230, in AutoPopulate.populate(self, suppress_errors, return_exception_objects, reserve_jobs, order, limit, max_calls, display_progress, processes, make_kwargs, *restrictions)\n    226 if processes == 1:\n    227     for key in (\n    228         tqdm(keys, desc=self.__class__.__name__) if display_progress else keys\n    229     ):\n--&gt; 230         error = self._populate1(key, jobs, **populate_kwargs)\n    231         if error is not None:\n    232             error_list.append(error)\n\nFile ~/anaconda3/envs/spyglass-position/lib/python3.9/site-packages/datajoint/autopopulate.py:281, in AutoPopulate._populate1(self, key, jobs, suppress_errors, return_exception_objects, make_kwargs)\n    279 self.__class__._allow_insert = True\n    280 try:\n--&gt; 281     make(dict(key), **(make_kwargs or {}))\n    282 except (KeyboardInterrupt, SystemExit, Exception) as error:\n    283     try:\n\nFile ~/Src/spyglass/src/spyglass/position/v1/position_dlc_training.py:180, in DLCModelTraining.make(self, key)\n    176 training_dataset_kwargs = {\n    177     k: v for k, v in dlc_config.items() if k in training_dataset_input_args\n    178 }\n    179 logger.logger.info(\"creating training dataset\")\n--&gt; 180 create_training_dataset(dlc_cfg_filepath, **training_dataset_kwargs)\n    181 # ---- Trigger DLC model training job ----\n    182 train_network_input_args = list(inspect.signature(train_network).parameters)\n\nFile ~/anaconda3/envs/spyglass-position/lib/python3.9/site-packages/deeplabcut/generate_training_dataset/trainingsetmanipulation.py:992, in create_training_dataset(config, num_shuffles, Shuffles, windows2linux, userfeedback, trainIndices, testIndices, net_type, augmenter_type, posecfg_template)\n    982 (\n    983     datafilename,\n    984     metadatafilename,\n    985 ) = auxiliaryfunctions.get_data_and_metadata_filenames(\n    986     trainingsetfolder, trainFraction, shuffle, cfg\n    987 )\n    989 ################################################################################\n    990 # Saving data file (convert to training file for deeper cut (*.mat))\n    991 ################################################################################\n--&gt; 992 data, MatlabData = format_training_data(\n    993 Data, trainIndices, nbodyparts, project_path\n    994 )\n    995 sio.savemat(\n    996     os.path.join(project_path, datafilename), {\"dataset\": MatlabData}\n    997 )\n    999 ################################################################################\n   1000 # Saving metadata (Pickle file)\n   1001 ################################################################################\n\nFile ~/anaconda3/envs/spyglass-position/lib/python3.9/site-packages/deeplabcut/generate_training_dataset/trainingsetmanipulation.py:685, in format_training_data(df, train_inds, nbodyparts, project_path)\n    683 filename = df.index[i]\n    684 data[\"image\"] = filename\n--&gt; 685 img_shape = read_image_shape_fast(os.path.join(project_path, *filename))\n    686 data[\"size\"] = img_shape\n    687 temp = df.iloc[i].values.reshape(-1, 2)\n\nFile ~/anaconda3/envs/spyglass-position/lib/python3.9/site-packages/deeplabcut/generate_training_dataset/trainingsetmanipulation.py:667, in read_image_shape_fast(path)\n    664 @lru_cache(maxsize=None)\n    665 def read_image_shape_fast(path):\n    666     # Blazing fast and does not load the image into memory\n--&gt; 667     with Image.open(path) as img:\n    668         width, height = img.size\n    669         return len(img.getbands()), height, width\n\nFile ~/anaconda3/envs/spyglass-position/lib/python3.9/site-packages/PIL/Image.py:3227, in open(fp, mode, formats)\n   3224     filename = fp\n   3226 if filename:\n-&gt; 3227     fp = builtins.open(filename, \"rb\")\n   3228     exclusive_fp = True\n   3230 try:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/nimbus/deeplabcut/projects/tutorial_scratch_DG-LorenLab-2022-11-03/labeled-data/20201103_peanut_04_r2/img22800.png'</pre> <p>Here we'll make sure that the entry made it into the table properly!</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelTraining() &amp; model_training_key\n</pre> sgp.DLCModelTraining() &amp; model_training_key <p>Populating <code>DLCModelTraining</code> automatically inserts the entry into <code>DLCModelSource</code>.  <code>DLCModelSource</code> is a table that is used to switch between the models we train using Spyglass and pre-existing projects.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelSource() &amp; model_training_key\n</pre> sgp.DLCModelSource() &amp; model_training_key <p>Notice the <code>source</code> field in the table above. It will only accept \"FromImport\" or \"FromUpstream\" as entries. Let's checkout the <code>FromUpstream</code> part table attached to <code>DLCModelSource</code> below.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelSource.FromUpstream() &amp; model_training_key\n</pre> sgp.DLCModelSource.FromUpstream() &amp; model_training_key In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelParams.get_default()\n</pre> sgp.DLCModelParams.get_default() <p>Here is the syntax to add your own parameter set:</p> <pre>dlc_model_params_name = \"make_this_yours\"\nparams = {\n            \"params\": {},\n            \"shuffle\": 1,\n            \"trainingsetindex\": 0,\n            \"model_prefix\": \"\",\n        }\nsgp.DLCModelParams.insert1({\"dlc_model_params_name\": dlc_model_params_name, \"params\": params}, skip_duplicates=True)\n</pre> <pre><code></code></pre> <p>Now that we've defined a set of parameters and inserted into <code>DLCModelParams</code>, we can insert an entry into <code>DLCModelSelection</code> and populate <code>DLCModel</code>.</p> In\u00a0[\u00a0]: Copied! <pre>temp_model_key = (sgp.DLCModelSource &amp; model_training_key).fetch1(\"KEY\")\nsgp.DLCModelSelection().insert1({\n    **temp_model_key,\n    \"dlc_model_params_name\": \"default\"},\n    skip_duplicates=True)\nmodel_key = (sgp.DLCModelSelection &amp; ).fetch1(\"KEY\")\nsgp.DLCModel.populate(model_key)\n</pre> temp_model_key = (sgp.DLCModelSource &amp; model_training_key).fetch1(\"KEY\") sgp.DLCModelSelection().insert1({     **temp_model_key,     \"dlc_model_params_name\": \"default\"},     skip_duplicates=True) model_key = (sgp.DLCModelSelection &amp; ).fetch1(\"KEY\") sgp.DLCModel.populate(model_key) <p>Again, let's make sure that everything looks correct in <code>DLCModel</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModel() &amp; model_key\n</pre> sgp.DLCModel() &amp; model_key In\u00a0[\u00a0]: Copied! <pre>nwb_file_name = \"J1620210604_.nwb\"\nepoch = 14\n</pre> nwb_file_name = \"J1620210604_.nwb\" epoch = 14 In\u00a0[\u00a0]: Copied! <pre>sgc.VideoFile() &amp; {\"nwb_file_name\": nwb_file_name, \"epoch\": epoch}\n</pre> sgc.VideoFile() &amp; {\"nwb_file_name\": nwb_file_name, \"epoch\": epoch} <p>To set up pose estimation, we need to make sure a few things are in order. Using <code>insert_estimation_task</code> will take care of these steps for us!Briefly, it will convert out video to be in .mp4 format (DLC struggles with .h264) and determine the directory in which we'll store the pose estimation results.</p> <p><code>task_mode</code> determines whether or not populating <code>DLCPoseEstimation</code> runs a new pose estimation, or loads an existing. Use 'trigger' unless you've already run this specific pose estimation.<code>video_file_num</code> will be 0 in almost all cases.<code>gputouse</code> has already been set above during the training step. It may be a good idea to make sure that core is still free before moving forward.</p> In\u00a0[\u00a0]: Copied! <pre>pose_estimation_key = sgp.DLCPoseEstimationSelection.insert_estimation_task(\n    {\n        \"nwb_file_name\": nwb_file_name,\n        \"epoch\": epoch,\n        \"video_file_num\": 0,\n        **model_key,\n    },\n    task_mode=\"trigger\",\n    params={\"gputouse\": gputouse, \"videotype\": \"mp4\"},\n)\n</pre> pose_estimation_key = sgp.DLCPoseEstimationSelection.insert_estimation_task(     {         \"nwb_file_name\": nwb_file_name,         \"epoch\": epoch,         \"video_file_num\": 0,         **model_key,     },     task_mode=\"trigger\",     params={\"gputouse\": gputouse, \"videotype\": \"mp4\"}, ) <p>And now we populate <code>DLCPoseEstimation</code>! This might take a bit...</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPoseEstimation().populate(pose_estimation_key)\n</pre> sgp.DLCPoseEstimation().populate(pose_estimation_key) <p>Let's visualize the output from Pose Estimation</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCPoseEstimation() &amp; pose_estimation_key).fetch_dataframe()\n</pre> (sgp.DLCPoseEstimation() &amp; pose_estimation_key).fetch_dataframe() <p>Now that we've completed pose estimation, it's time to interpolate over low likelihood periods and smooth the resulting positions.First we need to define some parameters for smoothing and interpolation. We can see the default parameter set below.</p> In\u00a0[\u00a0]: Copied! <pre>print(sgp.DLCSmoothInterpParams.get_default())\nsi_params_name = \"default\"\n</pre> print(sgp.DLCSmoothInterpParams.get_default()) si_params_name = \"default\" <p>If you'd like to change any of these parameters, here is the syntax to do that</p> <pre>si_params_name = 'your_unique_param_name'\nparams = {\n    \"smoothing_params\": {\n        \"smoothing_duration\": 0.##,\n        \"smooth_method\": \"moving_avg\",\n    },\n    \"interp_params\": {\n        \"likelihood_thresh\": 0.##,\n    },\n    \"max_plausible_speed\": ###,\n    \"speed_smoothing_std_dev\": 0.###,\n}\nsgp.DLCSmoothInterpParams().insert1(\n    {\n        'dlc_si_params_name': si_params_name,\n        \"params\": params,\n    },\n    skip_duplicates=True)\n</pre> <pre><code></code></pre> <p>Here we'll create a dictionary with the correct set of keys for the <code>DLCSmoothInterpSelection</code> table</p> In\u00a0[\u00a0]: Copied! <pre>si_key = pose_estimation_key.copy()\nfields = list(sgp.DLCSmoothInterpSelection.fetch().dtype.fields.keys())\nsi_key = {key: val for key, val in si_key.items() if key in fields}\nsi_key\n</pre> si_key = pose_estimation_key.copy() fields = list(sgp.DLCSmoothInterpSelection.fetch().dtype.fields.keys()) si_key = {key: val for key, val in si_key.items() if key in fields} si_key <p>And now we can insert all of the bodyparts we want to process into <code>DLCSmoothInterpSelection</code> First lets visualize the bodyparts we have available to us.</p> In\u00a0[\u00a0]: Copied! <pre>print((sgp.DLCPoseEstimation.BodyPart &amp; pose_estimation_key).fetch(\"bodypart\"))\n</pre> print((sgp.DLCPoseEstimation.BodyPart &amp; pose_estimation_key).fetch(\"bodypart\")) <p>We can use <code>insert1</code> to insert a single bodypart, but would suggest using <code>insert</code> to insert a list of keys with different bodyparts.</p> <p>Syntax to insert a single bodypart</p> <pre>sgp.DLCSmoothInterpSelection.insert1(\n    {\n        **si_key,\n        'bodypart': 'greenLED',\n        'dlc_si_params_name': si_params_name,\n    },\n    skip_duplicates=True)\n</pre> <pre><code></code></pre> <p>Lets set a list of bodyparts we want to insert and then insert them into <code>DLCSmoothInterpSelection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>bodyparts = [\"greenLED\", \"redLED_C\"]\nsgp.DLCSmoothInterpSelection.insert(\n    [\n        {\n            **si_key,\n            \"bodypart\": bodypart,\n            \"dlc_si_params_name\": si_params_name,\n        }\n        for bodypart in bodyparts\n    ],\n    skip_duplicates=True,\n)\n</pre> bodyparts = [\"greenLED\", \"redLED_C\"] sgp.DLCSmoothInterpSelection.insert(     [         {             **si_key,             \"bodypart\": bodypart,             \"dlc_si_params_name\": si_params_name,         }         for bodypart in bodyparts     ],     skip_duplicates=True, ) <p>And to make sure that all of the bodyparts we want made it into the the selection table, we can visualize the table below.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCSmoothInterpSelection() &amp; si_key\n</pre> sgp.DLCSmoothInterpSelection() &amp; si_key <p>Now we can populate <code>DLCSmoothInterp</code>, which will perform smoothing and interpolation on all of the bodyparts we specified.We can limit the populate using <code>si_key</code> since it is bodypart agnostic.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCSmoothInterp().populate(si_key)\n</pre> sgp.DLCSmoothInterp().populate(si_key) <p>And let's visualize the resulting position data using a scatter plot</p> In\u00a0[\u00a0]: Copied! <pre>(\n    sgp.DLCSmoothInterp() &amp; {**si_key, \"bodypart\": bodyparts[0]}\n).fetch1_dataframe().plot.scatter(x=\"x\", y=\"y\", s=1, figsize=(5, 5))\n</pre> (     sgp.DLCSmoothInterp() &amp; {**si_key, \"bodypart\": bodyparts[0]} ).fetch1_dataframe().plot.scatter(x=\"x\", y=\"y\", s=1, figsize=(5, 5)) <p>Now that we've smoothed and interpolated our position data for each bodypart, we need to form a set of bodyparts from which we want to derive a centroid and orientation (or potentially a second set for orientation). This is the goal of the <code>DLCSmoothInterpCohort</code> table.</p> <p>First, let's make a key that represents the 'cohort' we want to form.</p> <p>We'll set the <code>dlc_si_cohort_selection_name</code> to a concise nameWe'll also form a dictionary with the bodypart name as the key and the smoothing/interpolation parameter name used for that bodypart as the value.</p> In\u00a0[\u00a0]: Copied! <pre>cohort_key = si_key.copy()\nif \"bodypart\" in cohort_key:\n    del cohort_key[\"bodypart\"]\nif \"dlc_si_params_name\" in cohort_key:\n    del cohort_key[\"dlc_si_params_name\"]\ncohort_key[\"dlc_si_cohort_selection_name\"] = \"green_red_led\"\ncohort_key[\"bodyparts_params_dict\"] = {\n    \"greenLED\": si_params_name,\n    \"redLED_C\": si_params_name,\n}\nprint(cohort_key)\n</pre> cohort_key = si_key.copy() if \"bodypart\" in cohort_key:     del cohort_key[\"bodypart\"] if \"dlc_si_params_name\" in cohort_key:     del cohort_key[\"dlc_si_params_name\"] cohort_key[\"dlc_si_cohort_selection_name\"] = \"green_red_led\" cohort_key[\"bodyparts_params_dict\"] = {     \"greenLED\": si_params_name,     \"redLED_C\": si_params_name, } print(cohort_key) <p>Here we'll insert the cohort into the <code>DLCSmoothInterpCohortSelection</code> table..and populate <code>DLCSmoothInterpCohort</code>, which collates the separately smoothed and interpolated bodyparts into a single entry.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCSmoothInterpCohortSelection().insert1(cohort_key, skip_duplicates=True)\nsgp.DLCSmoothInterpCohort.populate(cohort_key)\n</pre> sgp.DLCSmoothInterpCohortSelection().insert1(cohort_key, skip_duplicates=True) sgp.DLCSmoothInterpCohort.populate(cohort_key) <p>And of course, let's make sure that the table populated correctly.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCSmoothInterpCohort.BodyPart() &amp; cohort_key\n</pre> sgp.DLCSmoothInterpCohort.BodyPart() &amp; cohort_key <p>We now have a cohort of smoothed and interpolated bodyparts from which to determine a centroid!To start, we'll need a set of parameters to use for determining the centroid. For this tutorial, we can use the default.</p> In\u00a0[\u00a0]: Copied! <pre># Here is the default set\nprint(sgp.DLCCentroidParams.get_default())\ncentroid_params_name = \"default\"\n</pre> # Here is the default set print(sgp.DLCCentroidParams.get_default()) centroid_params_name = \"default\" <p>Here is the syntax to add your own parameters:</p> <pre>centroid_params = {\n    'centroid_method': 'two_pt_centroid',\n    'points' : {\n        'greenLED': 'greenLED',\n        'redLED_C': 'redLED_C',},\n    'speed_smoothing_std_dev': 0.100,\n}\ncentroid_params_name = 'your_unique_param_name'\nsgp.DLCCentroidParams.insert1({'dlc_centroid_params_name': centroid_params_name,\n                                'params': centroid_params},\n                                skip_duplicates=True)\n</pre> <pre><code></code></pre> <p>And now let's make a key to insert into <code>DLCCentroidSelection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>centroid_key = cohort_key.copy()\nfields = list(sgp.DLCCentroidSelection.fetch().dtype.fields.keys())\ncentroid_key = {key: val for key, val in centroid_key.items() if key in fields}\ncentroid_key[\"dlc_centroid_params_name\"] = centroid_params_name\nprint(centroid_key)\n</pre> centroid_key = cohort_key.copy() fields = list(sgp.DLCCentroidSelection.fetch().dtype.fields.keys()) centroid_key = {key: val for key, val in centroid_key.items() if key in fields} centroid_key[\"dlc_centroid_params_name\"] = centroid_params_name print(centroid_key) <p>Let's insert it into <code>DLCCentroidSelection</code> and then populate <code>DLCCentroid</code> !</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCCentroidSelection.insert1(centroid_key, skip_duplicates=True)\nsgp.DLCCentroid.populate(centroid_key)\n</pre> sgp.DLCCentroidSelection.insert1(centroid_key, skip_duplicates=True) sgp.DLCCentroid.populate(centroid_key) <p>Here we can visualize the resulting centroid position</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCCentroid() &amp; centroid_key).fetch1_dataframe().plot.scatter(\n    x=\"position_x\",\n    y=\"position_y\",\n    c=\"speed\",\n    colormap=\"viridis\",\n    alpha=0.5,\n    s=0.5,\n    figsize=(10, 10),\n)\n</pre> (sgp.DLCCentroid() &amp; centroid_key).fetch1_dataframe().plot.scatter(     x=\"position_x\",     y=\"position_y\",     c=\"speed\",     colormap=\"viridis\",     alpha=0.5,     s=0.5,     figsize=(10, 10), ) <p>We'll now go through a similar process to identify the orientation!To start, we'll need a set of parameters to use for determining the orientation. For this tutorial, we can use the default.</p> In\u00a0[\u00a0]: Copied! <pre>print(sgp.DLCOrientationParams.get_default())\ndlc_orientation_params_name = \"default\"\n</pre> print(sgp.DLCOrientationParams.get_default()) dlc_orientation_params_name = \"default\" <p>Here we'll prune the <code>cohort_key</code> we used above and add our <code>dlc_orientation_params_name</code> to make it suitable for <code>DLCOrientationSelection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>fields = list(sgp.DLCOrientationSelection.fetch().dtype.fields.keys())\norient_key = {key: val for key, val in cohort_key.items() if key in fields}\norient_key[\"dlc_orientation_params_name\"] = dlc_orientation_params_name\nprint(orient_key)\n</pre> fields = list(sgp.DLCOrientationSelection.fetch().dtype.fields.keys()) orient_key = {key: val for key, val in cohort_key.items() if key in fields} orient_key[\"dlc_orientation_params_name\"] = dlc_orientation_params_name print(orient_key) <p>And now let's insert into <code>DLCOrientationSelection</code> and populate <code>DLCOrientation</code> to determine the orientation!</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCOrientationSelection().insert1(orient_key, skip_duplicates=True)\nsgp.DLCOrientation().populate(orient_key)\n</pre> sgp.DLCOrientationSelection().insert1(orient_key, skip_duplicates=True) sgp.DLCOrientation().populate(orient_key) <p>We can fetch the output of <code>DLCOrientation</code> as a dataframe to make sure everything looks appropriate.</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCOrientation() &amp; orient_key).fetch1_dataframe()\n</pre> (sgp.DLCOrientation() &amp; orient_key).fetch1_dataframe() <p>Ok, we're now done with processing the position data! We just have to do some table manipulations to make sure everything ends up in the same format and same location. To summarize, we brought in a pretrained DLC project, used that model to run pose estimation on a new behavioral video, smoothed and interpolated the result, formed a cohort of bodyparts, and determined the centroid and orientation of this cohort. Whew! Now let's populate <code>DLCPos</code> with our centroid and orientation entries from above.---- To begin, we'll make a key that combines the cohort names we used for the orientation and centroid as well as the params names for both.</p> In\u00a0[\u00a0]: Copied! <pre>fields = list(sgp.DLCPos.fetch().dtype.fields.keys())\ndlc_key = {key: val for key, val in centroid_key.items() if key in fields}\ndlc_key[\"dlc_si_cohort_centroid\"] = centroid_key[\"dlc_si_cohort_selection_name\"]\ndlc_key[\"dlc_si_cohort_orientation\"] = orient_key[\n    \"dlc_si_cohort_selection_name\"\n]\ndlc_key[\"dlc_orientation_params_name\"] = orient_key[\n    \"dlc_orientation_params_name\"\n]\nprint(dlc_key)\n</pre> fields = list(sgp.DLCPos.fetch().dtype.fields.keys()) dlc_key = {key: val for key, val in centroid_key.items() if key in fields} dlc_key[\"dlc_si_cohort_centroid\"] = centroid_key[\"dlc_si_cohort_selection_name\"] dlc_key[\"dlc_si_cohort_orientation\"] = orient_key[     \"dlc_si_cohort_selection_name\" ] dlc_key[\"dlc_orientation_params_name\"] = orient_key[     \"dlc_orientation_params_name\" ] print(dlc_key) <p>Now we can insert into <code>DLCPosSelection</code> and populate <code>DLCPos</code> with our <code>dlc_key</code></p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPosSelection().insert1(dlc_key, skip_duplicates=True)\nsgp.DLCPos().populate(dlc_key)\n</pre> sgp.DLCPosSelection().insert1(dlc_key, skip_duplicates=True) sgp.DLCPos().populate(dlc_key) <p>We can also make sure that all of our data made it through by fetching the dataframe attached to this entry.We should expect 8 columns:</p> <p>timevideo_frame_indposition_xposition_yorientationvelocity_xvelocity_yspeed</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCPos() &amp; dlc_key).fetch1_dataframe()\n</pre> (sgp.DLCPos() &amp; dlc_key).fetch1_dataframe() <p>And even more, we can fetch the <code>pose_eval_result</code> that is calculated during this step. This field contains the percentage of frames that each bodypart was below the likelihood threshold of 0.95 as a means of assessing the quality of the pose estimation.</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCPos() &amp; dlc_key).fetch1(\"pose_eval_result\")\n</pre> (sgp.DLCPos() &amp; dlc_key).fetch1(\"pose_eval_result\") <p>Here we can create a video with the centroid and orientation overlaid on the animal's behavioral video. This will also plot the likelihood of each bodypart used in the cohort. This is completely optional, but a good idea to make sure everything looks correct.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPosVideoParams.insert_default()\n</pre> sgp.DLCPosVideoParams.insert_default() In\u00a0[\u00a0]: Copied! <pre>params = {\n    \"percent_frames\": 0.05,\n    \"incl_likelihood\": True,\n}\nsgp.DLCPosVideoParams.insert1(\n    {\"dlc_pos_video_params_name\": \"five_percent\", \"params\": params},\n    skip_duplicates=True,\n)\n</pre> params = {     \"percent_frames\": 0.05,     \"incl_likelihood\": True, } sgp.DLCPosVideoParams.insert1(     {\"dlc_pos_video_params_name\": \"five_percent\", \"params\": params},     skip_duplicates=True, ) In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPosVideoSelection.insert1(\n    {**dlc_key, \"dlc_pos_video_params_name\": \"five_percent\"},\n    skip_duplicates=True,\n)\n</pre> sgp.DLCPosVideoSelection.insert1(     {**dlc_key, \"dlc_pos_video_params_name\": \"five_percent\"},     skip_duplicates=True, ) In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPosVideo().populate(dlc_key)\n</pre> sgp.DLCPosVideo().populate(dlc_key) <p><code>PositionOutput</code> is the final table of the position pipeline and is automatically populated when we populate <code>DLCPosV1</code>! Let's make sure that our entry made it in.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.PositionOutput() &amp; dlc_key\n</pre> sgp.PositionOutput() &amp; dlc_key <p><code>PositionOutput</code> also has a part table, similar to the <code>DLCModelSource</code> table above. Let's check that out as well.</p> In\u00a0[\u00a0]: Copied! <pre>PositionOutput.DLCPosV1() &amp; dlc_key\n</pre> PositionOutput.DLCPosV1() &amp; dlc_key In\u00a0[\u00a0]: Copied! <pre>(PositionOutput.DLCPosV1() &amp; dlc_key).fetch1_dataframe()\n</pre> (PositionOutput.DLCPosV1() &amp; dlc_key).fetch1_dataframe() <p>Bonus points if you made it this far... We can use the <code>PositionVideo</code> table to create a video that overlays just the centroid and orientation (regardless of upstream source) on the behavioral video. This table uses the parameter <code>plot</code> to determine whether to plot the entry deriving from the DLC arm or from the Trodes arm of the position pipeline. This parameter also accepts 'all', which will plot both (if they exist) in order to compare results.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.PositionVideoSelection().insert1(\n    {\n        \"nwb_file_name\": \"J1620210604_.nwb\",\n        \"interval_list_name\": \"pos 13 valid times\",\n        \"trodes_position_id\": 0,\n        \"dlc_position_id\": 1,\n        \"plot\": \"DLC\",\n        \"output_dir\": \"/home/dgramling/Src/\",\n    }\n)\n</pre> sgp.PositionVideoSelection().insert1(     {         \"nwb_file_name\": \"J1620210604_.nwb\",         \"interval_list_name\": \"pos 13 valid times\",         \"trodes_position_id\": 0,         \"dlc_position_id\": 1,         \"plot\": \"DLC\",         \"output_dir\": \"/home/dgramling/Src/\",     } ) In\u00a0[\u00a0]: Copied! <pre>sgp.PositionVideo.populate({\"plot\": \"DLC\"})\n</pre> sgp.PositionVideo.populate({\"plot\": \"DLC\"}) Return To Table of Contents In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/05_DLC_from_scratch/#position-using-deeplabcut-from-scratch", "title": "Position using DeepLabCut from Scratch\u00b6", "text": "<p>Note: make a copy of this notebook and run the copy to avoid git conflicts in the future</p> <p>This is a tutorial on how to extract position via DeepLabCut (DLC) using the Spyglass pipeline used in Loren Frank's lab, UCSF. It will walk through creating your DLC project, extracting and labeling frames, training your model, executing pose estimation on a novel behavioral video, processing the pose estimation output to extract a centroid and orientation, and inserting the resulting information into the <code>IntervalPositionInfo</code> table. -&gt; This tutorial assumes you've completed tutorial 0 Note 2: Make sure you are running this within the spyglass Conda environment</p>"}, {"location": "notebooks/05_DLC_from_scratch/#table-of-contents", "title": "Table of Contents\u00b6", "text": "<p><code>DLCProject</code> <code>DLCModelTraining</code> <code>DLCModel</code> <code>DLCPoseEstimation</code> <code>DLCSmoothInterp</code> <code>DLCCentroid</code> <code>DLCOrientation</code> <code>DLCPos</code> <code>DLCPosVideo</code> <code>PosSource</code> <code>IntervalPositionInfo</code></p>"}, {"location": "notebooks/05_DLC_from_scratch/#dlcproject", "title": "DLCProject \u00b6", "text": "<p>You can click on any header to return to the Table of Contents</p>"}, {"location": "notebooks/05_DLC_from_scratch/#dlcmodeltraining", "title": "DLCModelTraining\u00b6", "text": "<p>Please make sure you're running this notebook on a GPU-enabled machine. Now that we've imported existing frames, we can get ready to train our model. First, we'll need to define a set of parameters for <code>DLCModelTrainingParams</code>, which will get used by DeepLabCut during training Let's start with <code>gputouse</code></p> <code>gputouse</code> determines which GPU core to use for pose estimation. Run the cell below to determine which core has space and set the <code>gputouse</code> variable accordingly."}, {"location": "notebooks/05_DLC_from_scratch/#dlcmodel", "title": "DLCModel \u00b6", "text": "<p>Next we'll get ready to populate the <code>DLCModel</code> table, which holds all the relevant information for both pre-trained models and models trained within Spyglass.First we'll need to determine a set of parameters for our model to select the correct model file.We can visualize a default set below:</p>"}, {"location": "notebooks/05_DLC_from_scratch/#dlcposeestimation", "title": "DLCPoseEstimation \u00b6", "text": "<p>Alright, now that we've trained model and populated the <code>DLCModel</code> table, we're ready to set-up Pose Estimation on a behavioral video of your choice.For this tutorial, you can choose to use an epoch of your choice, we can also use the one specified below. If you'd like to use your own video, just specify the <code>nwb_file_name</code> and <code>epoch</code> number and make sure it's in the <code>VideoFile</code> table!</p>"}, {"location": "notebooks/05_DLC_from_scratch/#dlcsmoothinterp", "title": "DLCSmoothInterp \u00b6", "text": ""}, {"location": "notebooks/05_DLC_from_scratch/#dlcsmoothinterpcohort", "title": "DLCSmoothInterpCohort \u00b6", "text": ""}, {"location": "notebooks/05_DLC_from_scratch/#dlccentroid", "title": "DLCCentroid \u00b6", "text": ""}, {"location": "notebooks/05_DLC_from_scratch/#dlcorientation", "title": "DLCOrientation \u00b6", "text": ""}, {"location": "notebooks/05_DLC_from_scratch/#dlcpos", "title": "DLCPos \u00b6", "text": ""}, {"location": "notebooks/05_DLC_from_scratch/#dlcposvideo", "title": "DLCPosVideo \u00b6", "text": ""}, {"location": "notebooks/05_DLC_from_scratch/#positionoutput", "title": "PositionOutput \u00b6", "text": ""}, {"location": "notebooks/05_DLC_from_scratch/#positionvideo", "title": "PositionVideo\u00b6", "text": ""}, {"location": "notebooks/05_DLC_from_scratch/#congratulations", "title": "CONGRATULATIONS!!\u00b6", "text": "<p>Please treat yourself to a nice tea break :-)</p>"}, {"location": "notebooks/06_DLC_from_dir/", "title": "From Pre-Trained", "text": "In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path, PosixPath, PurePath\nimport os\nimport numpy as np\nimport pandas as pd\nimport pynwb\nimport datajoint as dj\nimport spyglass.common as sgc\nimport spyglass.position.v1 as sgp\nfrom spyglass.position import PositionOutput\n</pre> from pathlib import Path, PosixPath, PurePath import os import numpy as np import pandas as pd import pynwb import datajoint as dj import spyglass.common as sgc import spyglass.position.v1 as sgp from spyglass.position import PositionOutput Here is a schematic showing the tables used in this notebook. <p>Let us begin with visualizing the contents of the BodyPart table. This table will store standard names of body parts used within DLC models throughout the lab with a concise description.</p> <p>Please do not add to this table unless necessary.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.BodyPart()\n</pre> sgp.BodyPart() <p>To use an existing DLC project we can use the <code>insert_existing_project</code> method on the <code>DLCProject</code> table.This function will return a dictionary that can be used to query <code>DLCProject</code> in the future and expects:</p> <p><code>project_name</code>: a short, unique, descriptive name of your project that will be referenced throughout the pipeline<code>lab_team</code>: the name of your team from the Spyglass table <code>LabTeam</code><code>config_path</code>: string of the path to your existing DLC project's config.yaml<code>bodyparts</code>: a list of bodyparts used in your project (optional)<code>frames_per_video</code>: number of frames to extract for training from each video (optional)</p> In\u00a0[\u00a0]: Copied! <pre>project_name = \"tutorial_DG\"\nlab_team = \"LorenLab\"\nproject_key = sgp.DLCProject.insert_existing_project(\n    project_name=project_name,\n    lab_team=lab_team,\n    config_path=\"/nimbus/deeplabcut/projects/tutorial_model-LorenLab-2022-07-15/config.yaml\",\n    bodyparts=[\"redLED_C\", \"greenLED\", \"redLED_L\", \"redLED_R\", \"tailBase\"],\n    frames_per_video=200,\n    skip_duplicates=True,\n)\n</pre> project_name = \"tutorial_DG\" lab_team = \"LorenLab\" project_key = sgp.DLCProject.insert_existing_project(     project_name=project_name,     lab_team=lab_team,     config_path=\"/nimbus/deeplabcut/projects/tutorial_model-LorenLab-2022-07-15/config.yaml\",     bodyparts=[\"redLED_C\", \"greenLED\", \"redLED_L\", \"redLED_R\", \"tailBase\"],     frames_per_video=200,     skip_duplicates=True, ) In\u00a0[\u00a0]: Copied! <pre>sgp.DLCProject() &amp; {\"project_name\": project_name}\n</pre> sgp.DLCProject() &amp; {\"project_name\": project_name} <p>Lets take a look at the <code>DLCModelInput</code> table next. This table has <code>dlc_model_name</code> and <code>project_name</code> as primary keys and <code>project_path</code> as a secondary key.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelInput()\n</pre> sgp.DLCModelInput() <p>Next we can modify the <code>project_key</code> to replace <code>config_path</code> with <code>project_path</code> to fit with the fields in <code>DLCModelInput</code></p> In\u00a0[\u00a0]: Copied! <pre>print(f\"current project_key:\\n{project_key}\")\nif not \"project_path\" in project_key:\n    project_key[\"project_path\"] = os.path.dirname(project_key[\"config_path\"])\n    del project_key[\"config_path\"]\n    print(f\"updated project_key:\\n{project_key}\")\n</pre> print(f\"current project_key:\\n{project_key}\") if not \"project_path\" in project_key:     project_key[\"project_path\"] = os.path.dirname(project_key[\"config_path\"])     del project_key[\"config_path\"]     print(f\"updated project_key:\\n{project_key}\") <p>Here we can set a unique name for our model using the <code>dlc_model_name</code> variable.We then combine this with the updated <code>project_key</code> to insert into <code>DLCModelInput</code>.</p> In\u00a0[\u00a0]: Copied! <pre>dlc_model_name = \"tutorial_model_DG\"\nsgp.DLCModelInput().insert1(\n    {\"dlc_model_name\": dlc_model_name, **project_key}, skip_duplicates=True\n)\nsgp.DLCModelInput()\n</pre> dlc_model_name = \"tutorial_model_DG\" sgp.DLCModelInput().insert1(     {\"dlc_model_name\": dlc_model_name, **project_key}, skip_duplicates=True ) sgp.DLCModelInput() <p>Inserting an entry into <code>DLCModelInput</code> will also populate <code>DLCModelSource</code>. <code>DLCModelSource</code> is a table that is used to switch between models trained using Spyglass and pre-existing projects.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelSource() &amp; project_key\n</pre> sgp.DLCModelSource() &amp; project_key <p>Notice the <code>source</code> field in the table above. It will only accept \"FromImport\" or \"FromUpstream\" as entries. Let's checkout the <code>FromImport</code> part table attached to <code>DLCModelSource</code> below.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelSource.FromImport() &amp; project_key\n</pre> sgp.DLCModelSource.FromImport() &amp; project_key <p>Next we'll get ready to populate the <code>DLCModel</code> table, which holds all the relevant information for both pre-trained models and models trained within Spyglass.First we'll need to determine a set of parameters for our model to select the correct model file.We can visualize a default set below:</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelParams.get_default()\n</pre> sgp.DLCModelParams.get_default() <p>Here is the syntax to add your own parameter set:</p> <pre>dlc_model_params_name = \"make_this_yours\"\nparams = {\n            \"params\": {},\n            \"shuffle\": 1,\n            \"trainingsetindex\": 0,\n            \"model_prefix\": \"\",\n        }\nsgp.DLCModelParams.insert1({\"dlc_model_params_name\": dlc_model_params_name, \"params\": params}, skip_duplicates=True)\n</pre> <pre><code></code></pre> <p>Now let's fetch the primary keys from <code>DLCModelSource</code> to make our lives a bit easier when we insert into <code>DLCModelSelection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>temp_model_key = (sgp.DLCModelSource.FromImport() &amp; project_key).fetch1(\"KEY\")\n</pre> temp_model_key = (sgp.DLCModelSource.FromImport() &amp; project_key).fetch1(\"KEY\") <p>And insert into <code>DLCModelSelection</code> to allow for population of <code>DLCModel</code></p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModelSelection().insert1(\n    {**temp_model_key, \"dlc_model_params_name\": \"default\"}, skip_duplicates=True\n)\n</pre> sgp.DLCModelSelection().insert1(     {**temp_model_key, \"dlc_model_params_name\": \"default\"}, skip_duplicates=True ) <p>Let's populate <code>DLCModel</code>!!</p> In\u00a0[\u00a0]: Copied! <pre>model_key = (sgp.DLCModelSelection &amp; temp_model_key).fetch1(\"KEY\")\nsgp.DLCModel.populate(model_key)\n</pre> model_key = (sgp.DLCModelSelection &amp; temp_model_key).fetch1(\"KEY\") sgp.DLCModel.populate(model_key) <p>And of course make sure it populated correctly</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCModel() &amp; model_key\n</pre> sgp.DLCModel() &amp; model_key  The following steps should be run on a GPU cluster <p>Alright, now that we brought our trained model into Spyglass we're ready to set-up Pose Estimation on a behavioral video of your choice.For this tutorial, you can choose to use an epoch of your choice, we can also use the one specified below. If you'd like to use your own video, just specify the <code>nwb_file_name</code> and <code>epoch</code> number and make sure it's in the <code>VideoFile</code> table!</p> In\u00a0[\u00a0]: Copied! <pre>nwb_file_name = \"J1620210529_.nwb\"\nepoch = 2\n</pre> nwb_file_name = \"J1620210529_.nwb\" epoch = 2 In\u00a0[\u00a0]: Copied! <pre>sgc.VideoFile() &amp; {\"nwb_file_name\": nwb_file_name, \"epoch\": epoch}\n</pre> sgc.VideoFile() &amp; {\"nwb_file_name\": nwb_file_name, \"epoch\": epoch} Setting up Pose Estimation <code>gputouse</code> determines which GPU core to use for pose estimation. Run the cell below to determine which core has space and set the <code>gputouse</code> variable accordingly.    In\u00a0[\u00a0]: Copied! <pre>sgp.dlc_utils.get_gpu_memory()\n</pre> sgp.dlc_utils.get_gpu_memory()  Set GPU core here In\u00a0[\u00a0]: Copied! <pre>gputouse = 0  ## 0-9\n</pre> gputouse = 0  ## 0-9 <p>To set up pose estimation, we need to make sure a few things are in order. Using <code>insert_estimation_task</code> will take care of these steps for us!Briefly, it will convert out video to be in .mp4 format (DLC struggles with .h264) and determine the directory in which we'll store the pose estimation results.</p> <p><code>task_mode</code> determines whether or not populating <code>DLCPoseEstimation</code> runs a new pose estimation, or loads an existing. Use 'trigger' unless you've already run this specific pose estimation.<code>video_file_num</code> will be 0 in almost all cases.<code>check_crop</code> is a boolean True/False and will trigger a prompt for the user to enter the cropping coordinates. A frame of the video with coordinates will be provided for reference.</p> When prompted for crop, the behavior takes place on the left-hand maze. The coordinates I used were: <code>50, 500, 50, 800</code>. Feel free to play around with these! In\u00a0[\u00a0]: Copied! <pre>pose_estimation_key = sgp.DLCPoseEstimationSelection.insert_estimation_task(\n    {\n        \"nwb_file_name\": nwb_file_name,\n        \"epoch\": epoch,\n        \"video_file_num\": 0,\n        **model_key,\n    },\n    task_mode=\"trigger\",\n    params={\"gputouse\": gputouse, \"videotype\": \"mp4\", \"cropping\": None},\n    check_crop=True,\n)\n</pre> pose_estimation_key = sgp.DLCPoseEstimationSelection.insert_estimation_task(     {         \"nwb_file_name\": nwb_file_name,         \"epoch\": epoch,         \"video_file_num\": 0,         **model_key,     },     task_mode=\"trigger\",     params={\"gputouse\": gputouse, \"videotype\": \"mp4\", \"cropping\": None},     check_crop=True, ) <p>And now we populate <code>DLCPoseEstimation</code>! This might take a bit...</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPoseEstimation().populate(pose_estimation_key)\n</pre> sgp.DLCPoseEstimation().populate(pose_estimation_key) <p>Let's visualize the output from Pose Estimation</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCPoseEstimation() &amp; pose_estimation_key).fetch_dataframe()\n</pre> (sgp.DLCPoseEstimation() &amp; pose_estimation_key).fetch_dataframe() <p>Now that we've completed pose estimation, it's time to identify NaNs and optionally interpolate over low likelihood periods and smooth the resulting positions.First we need to define some parameters for smoothing and interpolation. We can see the default parameter set below.Note: it is recommended to use the <code>just_nan</code> parameters here and save interpolation and smoothing for the centroid step as this provides for a better end result.</p> In\u00a0[\u00a0]: Copied! <pre># The default parameter set to interpolate and smooth over each LED individually\nprint(sgp.DLCSmoothInterpParams.get_default())\n</pre> # The default parameter set to interpolate and smooth over each LED individually print(sgp.DLCSmoothInterpParams.get_default()) In\u00a0[\u00a0]: Copied! <pre># The just_nan parameter set that identifies NaN indices and leaves smoothing and interpolation to the centroid step\nprint(sgp.DLCSmoothInterpParams.get_nan_params())\nsi_params_name = \"just_nan\"\n</pre> # The just_nan parameter set that identifies NaN indices and leaves smoothing and interpolation to the centroid step print(sgp.DLCSmoothInterpParams.get_nan_params()) si_params_name = \"just_nan\" <p>If you'd like to change any of these parameters, here is the syntax to do that</p> <pre>si_params_name = 'your_unique_param_name'\nparams = {\n    \"smoothing_params\": {\n        \"smoothing_duration\": 0.##,\n        \"smooth_method\": \"moving_avg\",\n    },\n    \"interp_params\": {\n        \"likelihood_thresh\": 0.##,\n    },\n    \"max_plausible_speed\": ###,\n    \"speed_smoothing_std_dev\": 0.###,\n}\nsgp.DLCSmoothInterpParams().insert1(\n    {\n        'dlc_si_params_name': si_params_name,\n        \"params\": params,\n    },\n    skip_duplicates=True)\n</pre> <pre><code></code></pre> <p>Here we'll create a dictionary with the correct set of keys for the <code>DLCSmoothInterpSelection</code> table</p> In\u00a0[\u00a0]: Copied! <pre>si_key = pose_estimation_key.copy()\nfields = list(sgp.DLCSmoothInterpSelection.fetch().dtype.fields.keys())\nsi_key = {key: val for key, val in si_key.items() if key in fields}\nsi_key\n</pre> si_key = pose_estimation_key.copy() fields = list(sgp.DLCSmoothInterpSelection.fetch().dtype.fields.keys()) si_key = {key: val for key, val in si_key.items() if key in fields} si_key <p>And now we can insert all of the bodyparts we want to process into <code>DLCSmoothInterpSelection</code> First lets visualize the bodyparts we have available to us.</p> In\u00a0[\u00a0]: Copied! <pre>print((sgp.DLCPoseEstimation.BodyPart &amp; pose_estimation_key).fetch(\"bodypart\"))\n</pre> print((sgp.DLCPoseEstimation.BodyPart &amp; pose_estimation_key).fetch(\"bodypart\")) <p>We can use <code>insert1</code> to insert a single bodypart, but would suggest using <code>insert</code> to insert a list of keys with different bodyparts.</p> <p>Syntax to insert a single bodypart</p> <pre>sgp.DLCSmoothInterpSelection.insert1(\n    {\n        **si_key,\n        'bodypart': 'greenLED',\n        'dlc_si_params_name': si_params_name,\n    },\n    skip_duplicates=True)\n</pre> <pre><code></code></pre> <p>Lets set a list of bodyparts we want to insert and then insert them into <code>DLCSmoothInterpSelection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>bodyparts = [\"greenLED\", \"redLED_C\"]\nsgp.DLCSmoothInterpSelection.insert(\n    [\n        {\n            **si_key,\n            \"bodypart\": bodypart,\n            \"dlc_si_params_name\": si_params_name,\n        }\n        for bodypart in bodyparts\n    ],\n    skip_duplicates=True,\n)\n</pre> bodyparts = [\"greenLED\", \"redLED_C\"] sgp.DLCSmoothInterpSelection.insert(     [         {             **si_key,             \"bodypart\": bodypart,             \"dlc_si_params_name\": si_params_name,         }         for bodypart in bodyparts     ],     skip_duplicates=True, ) <p>And to make sure that all of the bodyparts we want made it into the the selection table, we can visualize the table below.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCSmoothInterpSelection() &amp; si_key\n</pre> sgp.DLCSmoothInterpSelection() &amp; si_key <p>Now we can populate <code>DLCSmoothInterp</code>, which will perform smoothing and interpolation on all of the bodyparts we specified.We can limit the populate using <code>si_key</code> since it is bodypart agnostic.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCSmoothInterp().populate(si_key)\n</pre> sgp.DLCSmoothInterp().populate(si_key) <p>And let's visualize the resulting position data using a scatter plot</p> In\u00a0[\u00a0]: Copied! <pre>(\n    sgp.DLCSmoothInterp() &amp; {**si_key, \"bodypart\": bodyparts[0]}\n).fetch1_dataframe().plot.scatter(x=\"x\", y=\"y\", s=1, figsize=(5, 5))\n</pre> (     sgp.DLCSmoothInterp() &amp; {**si_key, \"bodypart\": bodyparts[0]} ).fetch1_dataframe().plot.scatter(x=\"x\", y=\"y\", s=1, figsize=(5, 5)) <p>Now that we've smoothed and interpolated our position data for each bodypart, we need to form a set of bodyparts from which we want to derive a centroid and orientation (or potentially a second set for orientation). This is the goal of the <code>DLCSmoothInterpCohort</code> table.</p> <p>First, let's make a key that represents the 'cohort' we want to form.</p> <p>We'll set the <code>dlc_si_cohort_selection_name</code> to a concise nameWe'll also form a dictionary with the bodypart name as the key and the smoothing/interpolation parameter name used for that bodypart as the value.</p> In\u00a0[\u00a0]: Copied! <pre>cohort_key = si_key.copy()\nif \"bodypart\" in cohort_key:\n    del cohort_key[\"bodypart\"]\nif \"dlc_si_params_name\" in cohort_key:\n    del cohort_key[\"dlc_si_params_name\"]\ncohort_key[\"dlc_si_cohort_selection_name\"] = \"green_red_led\"\ncohort_key[\"bodyparts_params_dict\"] = {\n    \"greenLED\": si_params_name,\n    \"redLED_C\": si_params_name,\n}\nprint(cohort_key)\n</pre> cohort_key = si_key.copy() if \"bodypart\" in cohort_key:     del cohort_key[\"bodypart\"] if \"dlc_si_params_name\" in cohort_key:     del cohort_key[\"dlc_si_params_name\"] cohort_key[\"dlc_si_cohort_selection_name\"] = \"green_red_led\" cohort_key[\"bodyparts_params_dict\"] = {     \"greenLED\": si_params_name,     \"redLED_C\": si_params_name, } print(cohort_key) <p>Here we'll insert the cohort into the <code>DLCSmoothInterpCohortSelection</code> table..and populate <code>DLCSmoothInterpCohort</code>, which collates the separately smoothed and interpolated bodyparts into a single entry.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCSmoothInterpCohortSelection().insert1(cohort_key, skip_duplicates=True)\nsgp.DLCSmoothInterpCohort.populate(cohort_key)\n</pre> sgp.DLCSmoothInterpCohortSelection().insert1(cohort_key, skip_duplicates=True) sgp.DLCSmoothInterpCohort.populate(cohort_key) <p>And of course, let's make sure that the table populated correctly.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCSmoothInterpCohort.BodyPart() &amp; cohort_key\n</pre> sgp.DLCSmoothInterpCohort.BodyPart() &amp; cohort_key <p>We now have a cohort of smoothed and interpolated bodyparts from which to determine a centroid!To start, we'll need a set of parameters to use for determining the centroid. For this tutorial, we can use the default.</p> In\u00a0[\u00a0]: Copied! <pre># Here is the default set\nprint(sgp.DLCCentroidParams.get_default())\ncentroid_params_name = \"default\"\n</pre> # Here is the default set print(sgp.DLCCentroidParams.get_default()) centroid_params_name = \"default\" <p>Here is the syntax to add your own parameters:</p> <pre>centroid_params = {\n    'centroid_method': 'two_pt_centroid',\n    'points' : {\n        'point1': 'greenLED',\n        'point2': 'redLED_C',},\n    'speed_smoothing_std_dev': 0.100,\n}\ncentroid_params_name = 'your_unique_param_name'\nsgp.DLCCentroidParams.insert1({'dlc_centroid_params_name': centroid_params_name,\n                                'params': centroid_params},\n                                skip_duplicates=True)\n</pre> <pre><code></code></pre> <p>And now let's make a key to insert into <code>DLCCentroidSelection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>centroid_key = cohort_key.copy()\nfields = list(sgp.DLCCentroidSelection.fetch().dtype.fields.keys())\ncentroid_key = {key: val for key, val in centroid_key.items() if key in fields}\ncentroid_key[\"dlc_centroid_params_name\"] = centroid_params_name\nprint(centroid_key)\n</pre> centroid_key = cohort_key.copy() fields = list(sgp.DLCCentroidSelection.fetch().dtype.fields.keys()) centroid_key = {key: val for key, val in centroid_key.items() if key in fields} centroid_key[\"dlc_centroid_params_name\"] = centroid_params_name print(centroid_key) <p>Let's insert it into <code>DLCCentroidSelection</code> and then populate <code>DLCCentroid</code> !</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCCentroidSelection.insert1(centroid_key, skip_duplicates=True)\nsgp.DLCCentroid.populate(centroid_key)\n</pre> sgp.DLCCentroidSelection.insert1(centroid_key, skip_duplicates=True) sgp.DLCCentroid.populate(centroid_key) <p>Here we can visualize the resulting centroid position</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCCentroid() &amp; centroid_key).fetch1_dataframe().plot.scatter(\n    x=\"position_x\",\n    y=\"position_y\",\n    c=\"speed\",\n    colormap=\"viridis\",\n    alpha=0.5,\n    s=0.5,\n    figsize=(10, 10),\n)\n</pre> (sgp.DLCCentroid() &amp; centroid_key).fetch1_dataframe().plot.scatter(     x=\"position_x\",     y=\"position_y\",     c=\"speed\",     colormap=\"viridis\",     alpha=0.5,     s=0.5,     figsize=(10, 10), ) <p>We'll now go through a similar process to identify the orientation!To start, we'll need a set of parameters to use for determining the orientation. For this tutorial, we can use the default.</p> In\u00a0[\u00a0]: Copied! <pre>print(sgp.DLCOrientationParams.get_default())\ndlc_orientation_params_name = \"default\"\n</pre> print(sgp.DLCOrientationParams.get_default()) dlc_orientation_params_name = \"default\" <p>Here we'll prune the <code>cohort_key</code> we used above and add our <code>dlc_orientation_params_name</code> to make it suitable for <code>DLCOrientationSelection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>fields = list(sgp.DLCOrientationSelection.fetch().dtype.fields.keys())\norient_key = {key: val for key, val in cohort_key.items() if key in fields}\norient_key[\"dlc_orientation_params_name\"] = dlc_orientation_params_name\nprint(orient_key)\n</pre> fields = list(sgp.DLCOrientationSelection.fetch().dtype.fields.keys()) orient_key = {key: val for key, val in cohort_key.items() if key in fields} orient_key[\"dlc_orientation_params_name\"] = dlc_orientation_params_name print(orient_key) <p>And now let's insert into <code>DLCOrientationSelection</code> and populate <code>DLCOrientation</code> to determine the orientation!</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCOrientationSelection().insert1(orient_key, skip_duplicates=True)\nsgp.DLCOrientation().populate(orient_key)\n</pre> sgp.DLCOrientationSelection().insert1(orient_key, skip_duplicates=True) sgp.DLCOrientation().populate(orient_key) <p>We can fetch the output of <code>DLCOrientation</code> as a dataframe to make sure everything looks appropriate.</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCOrientation() &amp; orient_key).fetch1_dataframe()\n</pre> (sgp.DLCOrientation() &amp; orient_key).fetch1_dataframe() <p>Ok, we're now done with processing the position data! We just have to do some table manipulations to make sure everything ends up in the same format and same location. To summarize, we brought in a pretrained DLC project, used that model to run pose estimation on a new behavioral video, smoothed and interpolated the result, formed a cohort of bodyparts, and determined the centroid and orientation of this cohort. Whew! Now let's populate <code>DLCPos</code> with our centroid and orientation entries from above.---- To begin, we'll make a key that combines the cohort names we used for the orientation and centroid as well as the params names for both.</p> In\u00a0[\u00a0]: Copied! <pre>fields = list(sgp.DLCPosV1.fetch().dtype.fields.keys())\ndlc_key = {key: val for key, val in centroid_key.items() if key in fields}\ndlc_key[\"dlc_si_cohort_centroid\"] = centroid_key[\"dlc_si_cohort_selection_name\"]\ndlc_key[\"dlc_si_cohort_orientation\"] = orient_key[\n    \"dlc_si_cohort_selection_name\"\n]\ndlc_key[\"dlc_orientation_params_name\"] = orient_key[\n    \"dlc_orientation_params_name\"\n]\nprint(dlc_key)\n</pre> fields = list(sgp.DLCPosV1.fetch().dtype.fields.keys()) dlc_key = {key: val for key, val in centroid_key.items() if key in fields} dlc_key[\"dlc_si_cohort_centroid\"] = centroid_key[\"dlc_si_cohort_selection_name\"] dlc_key[\"dlc_si_cohort_orientation\"] = orient_key[     \"dlc_si_cohort_selection_name\" ] dlc_key[\"dlc_orientation_params_name\"] = orient_key[     \"dlc_orientation_params_name\" ] print(dlc_key) <p>Now we can insert into <code>DLCPosSelection</code> and populate <code>DLCPos</code> with our <code>dlc_key</code></p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPosSelection().insert1(dlc_key, skip_duplicates=True)\nsgp.DLCPosV1().populate(dlc_key)\n</pre> sgp.DLCPosSelection().insert1(dlc_key, skip_duplicates=True) sgp.DLCPosV1().populate(dlc_key) <p>We can also make sure that all of our data made it through by fetching the dataframe attached to this entry.We should expect 8 columns:</p> <p>timevideo_frame_indposition_xposition_yorientationvelocity_xvelocity_yspeed</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCPosV1() &amp; dlc_key).fetch1_dataframe()\n</pre> (sgp.DLCPosV1() &amp; dlc_key).fetch1_dataframe() <p>And even more, we can fetch the <code>pose_eval_result</code> that is calculated during this step. This field contains the percentage of frames that each bodypart was below the likelihood threshold of 0.95 as a means of assessing the quality of the pose estimation.</p> In\u00a0[\u00a0]: Copied! <pre>(sgp.DLCPosV1() &amp; dlc_key).fetch1(\"pose_eval_result\")\n</pre> (sgp.DLCPosV1() &amp; dlc_key).fetch1(\"pose_eval_result\") <p>Here we can create a video with the centroid and orientation overlaid on the animal's behavioral video. This will also plot the likelihood of each bodypart used in the cohort. This is completely optional, but a good idea to make sure everything looks correct.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPosVideoParams.insert_default()\n</pre> sgp.DLCPosVideoParams.insert_default() In\u00a0[\u00a0]: Copied! <pre>params = {\n    \"percent_frames\": 0.05,\n    \"incl_likelihood\": True,\n}\nsgp.DLCPosVideoParams.insert1(\n    {\"dlc_pos_video_params_name\": \"five_percent\", \"params\": params},\n    skip_duplicates=True,\n)\n</pre> params = {     \"percent_frames\": 0.05,     \"incl_likelihood\": True, } sgp.DLCPosVideoParams.insert1(     {\"dlc_pos_video_params_name\": \"five_percent\", \"params\": params},     skip_duplicates=True, ) In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPosVideoSelection.insert1(\n    {**dlc_key, \"dlc_pos_video_params_name\": \"five_percent\"},\n    skip_duplicates=True,\n)\n</pre> sgp.DLCPosVideoSelection.insert1(     {**dlc_key, \"dlc_pos_video_params_name\": \"five_percent\"},     skip_duplicates=True, ) In\u00a0[\u00a0]: Copied! <pre>sgp.DLCPosVideo().populate(dlc_key)\n</pre> sgp.DLCPosVideo().populate(dlc_key) <p><code>PositionOutput</code> is the final table of the position pipeline and is automatically populated when we populate <code>DLCPos</code>! Let's make sure that our entry made it in.</p> In\u00a0[\u00a0]: Copied! <pre>PositionOutput() &amp; dlc_key\n</pre> PositionOutput() &amp; dlc_key <p><code>PositionOutput</code> also has a part table, similar to the <code>DLCModelSource</code> table above. Let's check that out as well.</p> In\u00a0[\u00a0]: Copied! <pre>PositionOutput.DLCPosV1() &amp; dlc_key\n</pre> PositionOutput.DLCPosV1() &amp; dlc_key <p>Bonus points if you made it this far... We can use the <code>PositionVideo</code> table to create a video that overlays just the centroid and orientation (regardless of upstream source) on the behavioral video. This table uses the parameter <code>plot</code> to determine whether to plot the entry deriving from the DLC arm or from the Trodes arm of the position pipeline. This parameter also accepts 'all', which will plot both (if they exist) in order to compare results.</p> In\u00a0[\u00a0]: Copied! <pre>sgp.PositionVideoSelection().insert1(\n    {\n        \"nwb_file_name\": \"J1620210604_.nwb\",\n        \"interval_list_name\": \"pos 13 valid times\",\n        \"trodes_position_id\": 0,\n        \"dlc_position_id\": 1,\n        \"plot\": \"DLC\",\n        \"output_dir\": \"/home/dgramling/Src/\",\n    }\n)\n</pre> sgp.PositionVideoSelection().insert1(     {         \"nwb_file_name\": \"J1620210604_.nwb\",         \"interval_list_name\": \"pos 13 valid times\",         \"trodes_position_id\": 0,         \"dlc_position_id\": 1,         \"plot\": \"DLC\",         \"output_dir\": \"/home/dgramling/Src/\",     } ) In\u00a0[\u00a0]: Copied! <pre>sgp.PositionVideo.populate({\"plot\": \"DLC\"})\n</pre> sgp.PositionVideo.populate({\"plot\": \"DLC\"}) <code>Return To Table of Contents</code>"}, {"location": "notebooks/06_DLC_from_dir/#position-using-deeplabcut-from-a-pre-trained-dlc-project", "title": "Position using DeepLabCut from a Pre-Trained DLC Project\u00b6", "text": "<p>Note: make a copy of this notebook and run the copy to avoid git conflicts in the future</p> <p>This is a tutorial on how to extract position given a pre-trained DeepLabCut (DLC) model using the Spyglass pipeline used in Loren Frank's lab, UCSF. It will walk through adding your DLC model to Spyglass, executing pose estimation on a novel behavioral video, processing the pose estimation output to extract a centroid and orientation, and inserting the resulting information into the <code>IntervalPositionInfo</code> table. -&gt; This tutorial assumes you've completed tutorial 0 Note 2: Make sure you are running this within the spyglass Conda environment</p>"}, {"location": "notebooks/06_DLC_from_dir/#table-of-contents", "title": "Table of Contents\u00b6", "text": "<p><code>DLCProject</code> <code>DLCModel</code> <code>DLCPoseEstimation</code> <code>DLCSmoothInterp</code> <code>DLCCentroid</code> <code>DLCOrientation</code> <code>DLCPos</code> <code>DLCPosVideo</code> <code>PositionOutput</code></p>"}, {"location": "notebooks/06_DLC_from_dir/#dlcproject", "title": "DLCProject \u00b6", "text": "<p>You can click on any header to return to the Table of Contents</p>"}, {"location": "notebooks/06_DLC_from_dir/#dlcmodel", "title": "DLCModel \u00b6", "text": ""}, {"location": "notebooks/06_DLC_from_dir/#dlcposeestimation", "title": "DLCPoseEstimation \u00b6", "text": ""}, {"location": "notebooks/06_DLC_from_dir/#dlcsmoothinterp", "title": "DLCSmoothInterp \u00b6", "text": ""}, {"location": "notebooks/06_DLC_from_dir/#dlcsmoothinterpcohort", "title": "DLCSmoothInterpCohort \u00b6", "text": ""}, {"location": "notebooks/06_DLC_from_dir/#dlccentroid", "title": "DLCCentroid \u00b6", "text": ""}, {"location": "notebooks/06_DLC_from_dir/#dlcorientation", "title": "DLCOrientation \u00b6", "text": ""}, {"location": "notebooks/06_DLC_from_dir/#dlcpos", "title": "DLCPos \u00b6", "text": ""}, {"location": "notebooks/06_DLC_from_dir/#dlcposvideo", "title": "DLCPosVideo \u00b6", "text": ""}, {"location": "notebooks/06_DLC_from_dir/#positionoutput", "title": "PositionOutput \u00b6", "text": ""}, {"location": "notebooks/06_DLC_from_dir/#positionvideo", "title": "PositionVideo\u00b6", "text": ""}, {"location": "notebooks/06_DLC_from_dir/#congratulations", "title": "CONGRATULATIONS!!\u00b6", "text": "<p>Please treat yourself to a nice tea break :-)</p>"}, {"location": "notebooks/07_linearization/", "title": "Linearization Pipeline", "text": "In\u00a0[\u00a0]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\n</pre> %reload_ext autoreload %autoreload 2 <p>This pipeline takes 2D position data from the <code>IntervalPositionInfo</code> table and \"linearizes\" it to 1D position.</p> In\u00a0[2]: Copied! <pre>from spyglass.common.nwb_helper_fn import get_nwb_copy_filename\nimport spyglass as nd\n\nnwb_file_name = \"chimi20200216_new.nwb\"\nnwb_copy_file_name = get_nwb_copy_filename(nwb_file_name)\nnwb_copy_file_name\n</pre> from spyglass.common.nwb_helper_fn import get_nwb_copy_filename import spyglass as nd  nwb_file_name = \"chimi20200216_new.nwb\" nwb_copy_file_name = get_nwb_copy_filename(nwb_file_name) nwb_copy_file_name <pre>[2022-08-04 16:16:35,902][INFO]: Connecting zoldello@lmf-db.cin.ucsf.edu:3306\n[2022-08-04 16:16:35,953][INFO]: Connected zoldello@lmf-db.cin.ucsf.edu:3306\n</pre> <pre>/home/zoldello/anaconda3/envs/spyglass/lib/python3.9/site-packages/position_tools/core.py:3: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n  from scipy.ndimage.filters import gaussian_filter1d\n</pre> Out[2]: <pre>'chimi20200216_new_.nwb'</pre> <p>Now we can specify which interval we want to look at (and which parameters we used if we've run it with more than one set of parameters).</p> <p>We will fetch the pandas dataframe from the <code>IntervalPositionInfo</code> table for easy plotting.</p> <p>(We probably want a way to specify what type of position we are using in case we use deep lab cut...)</p> In\u00a0[3]: Copied! <pre>from spyglass.common.common_position import IntervalPositionInfo\n\n\nposition_info = (\n    IntervalPositionInfo()\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"position_info_param_name\": \"default\",\n    }\n).fetch1_dataframe()\nposition_info\n</pre> from spyglass.common.common_position import IntervalPositionInfo   position_info = (     IntervalPositionInfo()     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"position_info_param_name\": \"default\",     } ).fetch1_dataframe() position_info <pre>/stelmo/nwb/analysis/chimi20200216_new_ZF24K2JHC1.nwb\n</pre> Out[3]: head_position_x head_position_y head_orientation head_velocity_x head_velocity_y head_speed time 1.581887e+09 91.051650 211.127050 2.999696 1.387074 2.848838 3.168573 1.581887e+09 90.844337 211.417287 3.078386 3.123201 3.411111 4.624939 1.581887e+09 90.637025 211.707525 -3.114572 5.431643 4.089597 6.799085 1.581887e+09 90.802875 211.596958 -3.033109 8.097753 4.979262 9.506138 1.581887e+09 91.288579 211.482443 -3.062550 10.840482 6.071373 12.424880 ... ... ... ... ... ... ... 1.581888e+09 182.158583 201.452467 -0.986926 0.348276 0.218575 0.411182 1.581888e+09 182.158583 201.397183 -0.978610 0.279135 -0.058413 0.285182 1.581888e+09 182.213867 201.341900 -0.957589 0.193798 -0.283200 0.343162 1.581888e+09 182.158583 201.341900 -0.970083 0.110838 -0.417380 0.431846 1.581888e+09 182.158583 201.286617 -0.936414 0.045190 -0.453966 0.456209 <p>39340 rows \u00d7 6 columns</p> <p>Let's linearize the head position. We will plot the head position to get a sense of the data.</p> In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(\n    position_info.head_position_x,\n    position_info.head_position_y,\n    color=\"lightgrey\",\n)\nax.set_xlabel(\"x-position [cm]\", fontsize=18)\nax.set_ylabel(\"y-position [cm]\", fontsize=18)\nax.set_title(\"Head Position\", fontsize=28)\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(     position_info.head_position_x,     position_info.head_position_y,     color=\"lightgrey\", ) ax.set_xlabel(\"x-position [cm]\", fontsize=18) ax.set_ylabel(\"y-position [cm]\", fontsize=18) ax.set_title(\"Head Position\", fontsize=28) Out[4]: <pre>Text(0.5, 1.0, 'Head Position')</pre> In\u00a0[5]: Copied! <pre>import numpy as np\n\nnode_positions = np.array(\n    [\n        (79.910, 216.720),  # top left well 0\n        (132.031, 187.806),  # top middle intersection 1\n        (183.718, 217.713),  # top right well 2\n        (132.544, 132.158),  # middle intersection 3\n        (87.202, 101.397),  # bottom left intersection 4\n        (31.340, 126.110),  # middle left well 5\n        (180.337, 104.799),  # middle right intersection 6\n        (92.693, 42.345),  # bottom left well 7\n        (183.784, 45.375),  # bottom right well 8\n        (231.338, 136.281),  # middle right well 9\n    ]\n)\n\nedges = np.array(\n    [\n        (0, 1),\n        (1, 2),\n        (1, 3),\n        (3, 4),\n        (4, 5),\n        (3, 6),\n        (6, 9),\n        (4, 7),\n        (6, 8),\n    ]\n)\n\nlinear_edge_order = [\n    (3, 6),\n    (6, 8),\n    (6, 9),\n    (3, 1),\n    (1, 2),\n    (1, 0),\n    (3, 4),\n    (4, 5),\n    (4, 7),\n]\nlinear_edge_spacing = 15\n</pre> import numpy as np  node_positions = np.array(     [         (79.910, 216.720),  # top left well 0         (132.031, 187.806),  # top middle intersection 1         (183.718, 217.713),  # top right well 2         (132.544, 132.158),  # middle intersection 3         (87.202, 101.397),  # bottom left intersection 4         (31.340, 126.110),  # middle left well 5         (180.337, 104.799),  # middle right intersection 6         (92.693, 42.345),  # bottom left well 7         (183.784, 45.375),  # bottom right well 8         (231.338, 136.281),  # middle right well 9     ] )  edges = np.array(     [         (0, 1),         (1, 2),         (1, 3),         (3, 4),         (4, 5),         (3, 6),         (6, 9),         (4, 7),         (6, 8),     ] )  linear_edge_order = [     (3, 6),     (6, 8),     (6, 9),     (3, 1),     (1, 2),     (1, 0),     (3, 4),     (4, 5),     (4, 7), ] linear_edge_spacing = 15 <p>Once we have these variables, we must name the track graph (<code>track_graph_name</code>). We also can specify the environment it corresponds to (<code>environment</code>)</p> In\u00a0[6]: Copied! <pre>from spyglass.common.common_position import TrackGraph\n\nTrackGraph.insert1(\n    {\n        \"track_graph_name\": \"6 arm\",\n        \"environment\": \"6 arm\",\n        \"node_positions\": node_positions,\n        \"edges\": edges,\n        \"linear_edge_order\": linear_edge_order,\n        \"linear_edge_spacing\": linear_edge_spacing,\n    },\n    skip_duplicates=True,\n)\n\ngraph = TrackGraph() &amp; {\"track_graph_name\": \"6 arm\"}\ngraph\n</pre> from spyglass.common.common_position import TrackGraph  TrackGraph.insert1(     {         \"track_graph_name\": \"6 arm\",         \"environment\": \"6 arm\",         \"node_positions\": node_positions,         \"edges\": edges,         \"linear_edge_order\": linear_edge_order,         \"linear_edge_spacing\": linear_edge_spacing,     },     skip_duplicates=True, )  graph = TrackGraph() &amp; {\"track_graph_name\": \"6 arm\"} graph Out[6]: <p>track_graph_name</p> <p>environment</p> Type of Environment <p>node_positions</p> 2D position of track_graph nodes, shape (n_nodes, 2) <p>edges</p> shape (n_edges, 2) <p>linear_edge_order</p> order of track graph edges in the linear space, shape (n_edges, 2) <p>linear_edge_spacing</p> amount of space between edges in the linear space, shape (n_edges,) <p>linear_edge_specialty</p> denote what edges (denote by 5) are going to be lumped to what edge (denote by 1), shape (n_edges,) 6 arm 6 arm =BLOB= =BLOB= =BLOB= =BLOB= =BLOB= <p>Total: 1</p> <p>The <code>TrackGraph</code> has several convenient methods for visualizing the graph in 2D and 1D space. Here we use the method <code>plot_track_graph</code> to plot the graph in 2D. Notice we give it the name of the track_graph. It is important to plot the track graph in 2D over the position to make sure our layout makes sense.</p> In\u00a0[7]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(\n    position_info.head_position_x,\n    position_info.head_position_y,\n    color=\"lightgrey\",\n    alpha=0.7,\n    zorder=0,\n)\nax.set_xlabel(\"x-position [cm]\", fontsize=18)\nax.set_ylabel(\"y-position [cm]\", fontsize=18)\ngraph.plot_track_graph(ax=ax)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(     position_info.head_position_x,     position_info.head_position_y,     color=\"lightgrey\",     alpha=0.7,     zorder=0, ) ax.set_xlabel(\"x-position [cm]\", fontsize=18) ax.set_ylabel(\"y-position [cm]\", fontsize=18) graph.plot_track_graph(ax=ax) <p>We can also look at how this will translate to 1D space by using the <code>plot_track_graph_as_1D</code> method</p> In\u00a0[8]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(20, 1))\ngraph.plot_track_graph_as_1D(ax=ax)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(20, 1)) graph.plot_track_graph_as_1D(ax=ax) In\u00a0[9]: Copied! <pre>from spyglass.common.common_position import LinearizationParameters\n\nLinearizationParameters.insert1(\n    {\"linearization_param_name\": \"default\"}, skip_duplicates=True\n)\nLinearizationParameters()\n</pre> from spyglass.common.common_position import LinearizationParameters  LinearizationParameters.insert1(     {\"linearization_param_name\": \"default\"}, skip_duplicates=True ) LinearizationParameters() Out[9]: <p>linearization_param_name</p> name for this set of parameters <p>use_hmm</p> use HMM to determine linearization <p>route_euclidean_distance_scaling</p> How much to prefer route distances between successive time points that are closer to the euclidean distance. Smaller numbers mean the route distance is more likely to be close to the euclidean distance. <p>sensor_std_dev</p> Uncertainty of position sensor (in cm). <p>diagonal_bias</p> Biases the transition matrix to prefer the current track segment. default 0 1.0 5.0 0.5 <p>Total: 1</p> <p>Once we have some linearization parameters, like with the 2D position, we specify the corresponding position interval we wish to use those parameters with.</p> In\u00a0[10]: Copied! <pre>from spyglass.common.common_position import IntervalLinearizationSelection\n\n\nIntervalLinearizationSelection.insert1(\n    {\n        \"position_info_param_name\": \"default\",\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"track_graph_name\": \"6 arm\",\n        \"linearization_param_name\": \"default\",\n    },\n    skip_duplicates=True,\n)\n\nIntervalLinearizationSelection()\n</pre> from spyglass.common.common_position import IntervalLinearizationSelection   IntervalLinearizationSelection.insert1(     {         \"position_info_param_name\": \"default\",         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"track_graph_name\": \"6 arm\",         \"linearization_param_name\": \"default\",     },     skip_duplicates=True, )  IntervalLinearizationSelection() Out[10]: <p>position_info_param_name</p> name for this set of parameters <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>track_graph_name</p> <p>linearization_param_name</p> name for this set of parameters default chimi20200216_new_.nwb pos 1 valid times 6 arm default <p>Total: 1</p> <p>And then we can run the linearization by populating the <code>IntervalLinearizedPosition</code> table.</p> In\u00a0[11]: Copied! <pre>from spyglass.common.common_position import IntervalLinearizedPosition\n\n\nIntervalLinearizedPosition().populate()\nIntervalLinearizedPosition()\n</pre> from spyglass.common.common_position import IntervalLinearizedPosition   IntervalLinearizedPosition().populate() IntervalLinearizedPosition() <pre>Computing linear position for: {'position_info_param_name': 'default', 'nwb_file_name': 'chimi20200216_new_.nwb', 'interval_list_name': 'pos 1 valid times', 'track_graph_name': '6 arm', 'linearization_param_name': 'default'}\nWriting new NWB file chimi20200216_new_GBGCXYMIWB.nwb\n</pre> Out[11]: <p>position_info_param_name</p> name for this set of parameters <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>track_graph_name</p> <p>linearization_param_name</p> name for this set of parameters <p>analysis_file_name</p> name of the file <p>linearized_position_object_id</p> default chimi20200216_new_.nwb pos 1 valid times 6 arm default chimi20200216_new_GBGCXYMIWB.nwb 8d132da2-c1e4-402f-ba3a-4b8725a6c87a <p>Total: 1</p> In\u00a0[12]: Copied! <pre>linear_position_df = (\n    IntervalLinearizedPosition()\n    &amp; {\n        \"position_info_param_name\": \"default\",\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"track_graph_name\": \"6 arm\",\n        \"linearization_param_name\": \"default\",\n    }\n).fetch1_dataframe()\nlinear_position_df\n</pre> linear_position_df = (     IntervalLinearizedPosition()     &amp; {         \"position_info_param_name\": \"default\",         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"track_graph_name\": \"6 arm\",         \"linearization_param_name\": \"default\",     } ).fetch1_dataframe() linear_position_df Out[12]: linear_position track_segment_id projected_x_position projected_y_position time 1.581887e+09 412.042773 0 90.802281 210.677533 1.581887e+09 412.364853 0 90.520636 210.833775 1.581887e+09 412.686934 0 90.238990 210.990018 1.581887e+09 412.488270 0 90.412714 210.893645 1.581887e+09 412.007991 0 90.832697 210.660660 ... ... ... ... ... 1.581888e+09 340.401589 1 175.500994 212.958497 1.581888e+09 340.373902 1 175.477029 212.944630 1.581888e+09 340.394065 1 175.494481 212.954729 1.581888e+09 340.346214 1 175.453064 212.930764 1.581888e+09 340.318527 1 175.429100 212.916898 <p>39340 rows \u00d7 4 columns</p> <p>Let's plot the linearized position over time colored by the particular edge. As a reference, we can put the 1D layout of the track graph on the y-axis.</p> In\u00a0[13]: Copied! <pre>fig, ax = plt.subplots(figsize=(20, 13))\nax.scatter(\n    linear_position_df.index,\n    linear_position_df.linear_position,\n    c=linear_position_df.track_segment_id,\n    s=1,\n)\ngraph.plot_track_graph_as_1D(\n    ax=ax, axis=\"y\", other_axis_start=linear_position_df.index[-1] + 10\n)\n\nax.set_xlabel(\"Time [s]\", fontsize=18)\nax.set_ylabel(\"Linear Position [cm]\", fontsize=18)\nax.set_title(\"Linear Position\", fontsize=28)\n</pre> fig, ax = plt.subplots(figsize=(20, 13)) ax.scatter(     linear_position_df.index,     linear_position_df.linear_position,     c=linear_position_df.track_segment_id,     s=1, ) graph.plot_track_graph_as_1D(     ax=ax, axis=\"y\", other_axis_start=linear_position_df.index[-1] + 10 )  ax.set_xlabel(\"Time [s]\", fontsize=18) ax.set_ylabel(\"Linear Position [cm]\", fontsize=18) ax.set_title(\"Linear Position\", fontsize=28) Out[13]: <pre>Text(0.5, 1.0, 'Linear Position')</pre> <p>We can also plot the 2D position projected to the track graph</p> In\u00a0[14]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(\n    position_info.head_position_x,\n    position_info.head_position_y,\n    color=\"lightgrey\",\n    alpha=0.7,\n    zorder=0,\n)\nax.set_xlabel(\"x-position [cm]\", fontsize=18)\nax.set_ylabel(\"y-position [cm]\", fontsize=18)\nax.plot(\n    linear_position_df.projected_x_position,\n    linear_position_df.projected_y_position,\n)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(     position_info.head_position_x,     position_info.head_position_y,     color=\"lightgrey\",     alpha=0.7,     zorder=0, ) ax.set_xlabel(\"x-position [cm]\", fontsize=18) ax.set_ylabel(\"y-position [cm]\", fontsize=18) ax.plot(     linear_position_df.projected_x_position,     linear_position_df.projected_y_position, ) Out[14]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f5d0807e760&gt;]</pre> In\u00a0[15]: Copied! <pre>%matplotlib widget\n\nfrom spyglass.common.common_position import NodePicker\nimport pynwb\n\n\nkey = {\n    \"nwb_file_name\": nwb_copy_file_name,\n    \"interval_list_name\": \"pos 1 valid times\",\n}\n\nepoch = (\n    int(\n        key[\"interval_list_name\"]\n        .replace(\"pos \", \"\")\n        .replace(\" valid times\", \"\")\n    )\n    + 1\n)\nvideo_info = (\n    nd.common.common_behav.VideoFile()\n    &amp; {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": epoch}\n).fetch1()\n\nio = pynwb.NWBHDF5IO(\"/stelmo/nwb/raw/\" + video_info[\"nwb_file_name\"], \"r\")\nnwb_file = io.read()\nnwb_video = nwb_file.objects[video_info[\"video_file_object_id\"]]\nvideo_filename = nwb_video.external_file.value[0]\n\nfig, ax = plt.subplots(figsize=(8, 8))\npicker = NodePicker(ax=ax, video_filename=video_filename)\n</pre> %matplotlib widget  from spyglass.common.common_position import NodePicker import pynwb   key = {     \"nwb_file_name\": nwb_copy_file_name,     \"interval_list_name\": \"pos 1 valid times\", }  epoch = (     int(         key[\"interval_list_name\"]         .replace(\"pos \", \"\")         .replace(\" valid times\", \"\")     )     + 1 ) video_info = (     nd.common.common_behav.VideoFile()     &amp; {\"nwb_file_name\": key[\"nwb_file_name\"], \"epoch\": epoch} ).fetch1()  io = pynwb.NWBHDF5IO(\"/stelmo/nwb/raw/\" + video_info[\"nwb_file_name\"], \"r\") nwb_file = io.read() nwb_video = nwb_file.objects[video_info[\"video_file_object_id\"]] video_filename = nwb_video.external_file.value[0]  fig, ax = plt.subplots(figsize=(8, 8)) picker = NodePicker(ax=ax, video_filename=video_filename) <pre>Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous \u2026</pre> <p>Once the nodes and edges have been selected, we can retrieve them using the <code>node_positions</code> and <code>edges</code> attributes</p> In\u00a0[16]: Copied! <pre>picker.node_positions\n</pre> picker.node_positions Out[16]: <pre>array([], dtype=float64)</pre> In\u00a0[17]: Copied! <pre>picker.edges\n</pre> picker.edges Out[17]: <pre>[[]]</pre> In\u00a0[31]: Copied! <pre>%matplotlib widget\n\nfrom spyglass.common.common_position import SelectFromCollection\n\nfig, ax = plt.subplots(figsize=(8, 8))\nselector = SelectFromCollection(ax, video_filename)\n\nprint(\"Select points in the figure by enclosing them within a polygon.\")\nprint(\"Press the 'esc' key to start a new polygon.\")\nprint(\"Try holding the 'shift' key to move all of the vertices.\")\nprint(\"Try holding the 'ctrl' key to move a single vertex.\")\n</pre> %matplotlib widget  from spyglass.common.common_position import SelectFromCollection  fig, ax = plt.subplots(figsize=(8, 8)) selector = SelectFromCollection(ax, video_filename)  print(\"Select points in the figure by enclosing them within a polygon.\") print(\"Press the 'esc' key to start a new polygon.\") print(\"Try holding the 'shift' key to move all of the vertices.\") print(\"Try holding the 'ctrl' key to move a single vertex.\") <pre>Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous \u2026</pre> <pre>Select points in the figure by enclosing them within a polygon.\nPress the 'esc' key to start a new polygon.\nTry holding the 'shift' key to move all of the vertices.\nTry holding the 'ctrl' key to move a single vertex.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/07_linearization/#linearization-pipeline", "title": "Linearization Pipeline\u00b6", "text": ""}, {"location": "notebooks/07_linearization/#1-retrieving-2d-position-from-the-intervalpositioninfo-table", "title": "1. Retrieving 2D position from the <code>IntervalPositionInfo</code> table\u00b6", "text": "<p>To retrieve 2D position data, we need to specify a nwb file, an position time interval, and the set of parameters used to compute the position info.</p> <p>First we specify the nwb file and get the name of the copied version (without the ephys data) that is used as a key.</p>"}, {"location": "notebooks/07_linearization/#2-specifying-the-track-graph-for-linearization", "title": "2. Specifying the track graph for linearization\u00b6", "text": "<p>In order to linearize the data, we must build a graph of nodes and edges that specify the geometry of the track in 1D and 2D. This will be referred to as the <code>TrackGraph</code>.</p> <p>We do this by specifying four variables:</p> <ol> <li><code>node_positions</code> are the 2D positions of the graph (in cm)</li> <li><code>edges</code> specify how the nodes are connected. Each edge consists of a pair of nodes. Each node is labeled by their respective index in <code>node_positions</code>. For example, (79.910, 216.720) is the 2D position of node 0 and (183.784, 45.375) is the 2D position of node 8. So specifying (0, 8) means there is an edge between node 0 and node 8</li> <li><code>linear_edge_order</code> specifies how the edges are laid out in linear space in order. As before, each edge consists of a pair of nodes, which are labeled by their index. The order of the nodes controls their order in 1D space. So edge (0, 1) connects node 0 and node 1 and node 0 will be placed in linear position space before node 1. Specifying edge (1, 0) would reverse the linear positions for that edge.</li> <li><code>linear_edge_spacing</code> specifies the spacing between each edge. This can either by a single number or be an array the length of the number of gaps between edges. If it is a single number, all edges will have a gap between them (15 cm in this example). If it is an array, then the spacing between edges can be individually controlled. You may want to have gaps between edges if they are not spatially connected in 2D space.</li> </ol> <p>For more examples, see this notebook: https://github.com/LorenFrankLab/track_linearization/blob/master/notebooks/</p>"}, {"location": "notebooks/07_linearization/#3-setting-up-the-parameters-for-linearization", "title": "3. Setting up the parameters for linearization\u00b6", "text": "<p>There are several other parameters we can set for linearization. They are only relevant if you choose to use the HMM method of linearization.</p> <p>By default, linearization assigns each 2D position to its nearest point on the track graph. This is then translated into 1D space.</p> <p>If <code>use_hmm</code> is selected, then an HMM is used to assign these points. The HMM can be useful because it takes into account the prior position and edge the animal is on. This can keep the position from suddenly jumping to another edge such as in the case of an intersection or if the reward wells are close to each other and the animal's head position swings closer to the other reward well (even though it is physically on another edge of the track).</p>"}, {"location": "notebooks/07_linearization/#4-examining-the-data", "title": "4. Examining the data\u00b6", "text": "<p>After populating the table, we can use the <code>fetch1_dataframe</code> method to retreive the linear position data.</p> <p>The dataframe has several variables:</p> <ul> <li><code>linear_position</code> is the 1D linearized position</li> <li><code>track_segment_id</code> is the index number of the edges given to track graph</li> <li><code>projected_x_position</code>, <code>projected_y_position</code> is the 2D position projected to the track graph</li> </ul> <p>Time is set as the index of the dataframe</p>"}, {"location": "notebooks/07_linearization/#5-interactively-selecting-the-track-graph-nodes-and-edges-work-in-progress", "title": "5. Interactively selecting the track graph nodes and edges [Work in Progress]\u00b6", "text": ""}, {"location": "notebooks/07_linearization/#nodepicker", "title": "NodePicker\u00b6", "text": "<p>The linearization heavily depends on how you specify the track graph. Setting the node positions and edges can be diffcult. To help simplify this process, we can use the <code>NodePicker</code> to interactively set the node positions and edges based on the video of the track.</p>"}, {"location": "notebooks/07_linearization/#selector", "title": "Selector\u00b6", "text": "<p>We can also draw a 2d polygon around the track and attempt to recover the graph.</p>"}, {"location": "notebooks/08_Extract_Mark_indicators/", "title": "Mark Indicators", "text": "In\u00a0[\u00a0]: Copied! <pre># ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\nwarnings.simplefilter(\"ignore\", category=UserWarning)\n\nnwb_copy_file_name = \"J1620210531_.nwb\"\n</pre> # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) warnings.simplefilter(\"ignore\", category=UserWarning)  nwb_copy_file_name = \"J1620210531_.nwb\" <p>In order to use clusterless decoding, you need to populate the <code>UnitMarksIndicator</code> table. This involves populating a series of tables (<code>SpikeSorting</code> -&gt; <code>CuratedSpikeSorting</code> -&gt; <code>UnitMarks</code> -&gt; <code>UnitMarkIndicators</code>)</p> <p>This may seem confusing at first, as the point of clusterless decoding is to avoid spike sorting. However, in order to keep the pipeline for spike sorting and clusterless mark extraction similar as possible, we use some shared steps. However, the \"spike sorting\" step involves just a simple thresholding operation (sorter: clusterless_thresholder) and the curation step is essentially skipped (although the entry still needs to be populated). It also make sense to only extract the mark indicator functions time periods where there is position data.</p> <p>In order to simplify some of these steps, there is a function called <code>populate_mark_indicators</code>. In order to use it, you must first populate <code>SpikeSortingSelection</code> and <code>IntervalPositionInfo</code>.</p> <p>Note that <code>SpikeSortingSelection</code> itself depends on:</p> <ul> <li><code>SpikeSortingRecording</code></li> <li><code>SpikeSorterParameters</code></li> <li><code>ArtifactRemovedIntervalList</code>.</li> </ul> <p>and <code>SpikeSortingRecording</code> itself depends on:</p> <ul> <li><code>SortGroup</code></li> <li><code>SortInterval</code></li> <li><code>SpikeSortingPreprocessingParameters</code></li> <li><code>LabTeam</code></li> </ul> <p>You will have to define/populate all these things before getting to the \"spike sorting\" step. In order to read up on these steps, it is best to refer to tutorial notebook <code>1_spikesorting.ipynb</code> for the spikesorting related steps and <code>4_position_info.ipynb</code> for populating <code>IntervalPositionInfo</code>.</p> <p>For the purposes of this notebook, let's assume you've done these steps and now simply need to populate <code>UnitMarkIndicators</code> via <code>SpikeSorting</code>. Let's import the function and look at its docstring, which will tell us what inputs the function expects:</p> In\u00a0[2]: Copied! <pre>from spyglass.decoding.clusterless import populate_mark_indicators\n\n\n?populate_mark_indicators\n</pre> from spyglass.decoding.clusterless import populate_mark_indicators   ?populate_mark_indicators <pre>/home/edeno/miniconda3/envs/spyglass/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(mpl.__version__) &gt;= \"3.0\":\n/home/edeno/miniconda3/envs/spyglass/lib/python3.8/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n</pre> <pre>Connecting edeno@lmf-db.cin.ucsf.edu:3306\n</pre> <pre>Signature:\npopulate_mark_indicators(\n    spikesorting_selection_keys: list,\n    mark_param_name='default',\n    position_info_param_name='default_decoding',\n)\nDocstring: &lt;no docstring&gt;\nFile:      /stelmo/edeno/nwb_datajoint/src/spyglass/decoding/clusterless.py\nType:      function\n</pre> <p>From the docstring, we see that the main thing the function expects is <code>spikesorting_selection_keys</code>. This is a list of dictionaries, where each dictionary specifies the primary keys that <code>SpikeSorting</code> requires to run. It is a list of dictionaries because we typically want to extract the marks for more than one electrode at a time. Here are the primary keys required by <code>SpikeSorting</code>:</p> In\u00a0[3]: Copied! <pre>from spyglass.spikesorting import SpikeSorting\n\nSpikeSorting.primary_key\n</pre> from spyglass.spikesorting import SpikeSorting  SpikeSorting.primary_key Out[3]: <pre>['nwb_file_name',\n 'sort_group_id',\n 'sort_interval_name',\n 'preproc_params_name',\n 'team_name',\n 'sorter',\n 'sorter_params_name',\n 'artifact_removed_interval_list_name']</pre> <p>Here is an example of what <code>spikesorting_selection_keys</code> should look like:</p> In\u00a0[4]: Copied! <pre>spikesorting_selections = [\n    {\n        \"nwb_file_name\": \"J1620210531_.nwb\",\n        \"sort_group_id\": 0,\n        \"sort_interval_name\": \"raw data valid times no premaze no home\",\n        \"preproc_params_name\": \"franklab_tetrode_hippocampus\",\n        \"team_name\": \"JG_DG\",\n        \"sorter\": \"clusterless_thresholder\",\n        \"sorter_params_name\": \"clusterless_fixed\",\n        \"artifact_removed_interval_list_name\": \"J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times\",\n    },\n    {\n        \"nwb_file_name\": \"J1620210531_.nwb\",\n        \"sort_group_id\": 1,\n        \"sort_interval_name\": \"raw data valid times no premaze no home\",\n        \"preproc_params_name\": \"franklab_tetrode_hippocampus\",\n        \"team_name\": \"JG_DG\",\n        \"sorter\": \"clusterless_thresholder\",\n        \"sorter_params_name\": \"clusterless_fixed\",\n        \"artifact_removed_interval_list_name\": \"J1620210531_.nwb_1_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times\",\n    },\n]\n</pre> spikesorting_selections = [     {         \"nwb_file_name\": \"J1620210531_.nwb\",         \"sort_group_id\": 0,         \"sort_interval_name\": \"raw data valid times no premaze no home\",         \"preproc_params_name\": \"franklab_tetrode_hippocampus\",         \"team_name\": \"JG_DG\",         \"sorter\": \"clusterless_thresholder\",         \"sorter_params_name\": \"clusterless_fixed\",         \"artifact_removed_interval_list_name\": \"J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times\",     },     {         \"nwb_file_name\": \"J1620210531_.nwb\",         \"sort_group_id\": 1,         \"sort_interval_name\": \"raw data valid times no premaze no home\",         \"preproc_params_name\": \"franklab_tetrode_hippocampus\",         \"team_name\": \"JG_DG\",         \"sorter\": \"clusterless_thresholder\",         \"sorter_params_name\": \"clusterless_fixed\",         \"artifact_removed_interval_list_name\": \"J1620210531_.nwb_1_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times\",     }, ] <p>WARNING: Remember that this will only work if you have populated both <code>SpikeSortingSelection</code> and <code>IntervalPositionInfo</code>. You can check this by running the following and making sure there are table entries:</p> In\u00a0[5]: Copied! <pre>from spyglass.spikesorting import SpikeSortingSelection\n\nSpikeSortingSelection &amp; spikesorting_selections\n</pre> from spyglass.spikesorting import SpikeSortingSelection  SpikeSortingSelection &amp; spikesorting_selections Out[5]: Table for holding selection of recording and parameters for each spike sorting run <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sort_interval_name</p> name for this interval <p>preproc_params_name</p> <p>team_name</p> <p>sorter</p> <p>sorter_params_name</p> <p>artifact_removed_interval_list_name</p> <p>import_path</p> optional path to previous curated sorting output J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times J1620210531_.nwb 1 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_1_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times <p>Total: 2</p> <p>Remember to replace the nwb_file_name with your own nwb file name</p> In\u00a0[6]: Copied! <pre>from spyglass.common import IntervalPositionInfo\n\nIntervalPositionInfo &amp; {\n    \"nwb_file_name\": \"J1620210531_.nwb\",\n    \"position_info_param_name\": \"default_decoding\",\n}\n</pre> from spyglass.common import IntervalPositionInfo  IntervalPositionInfo &amp; {     \"nwb_file_name\": \"J1620210531_.nwb\",     \"position_info_param_name\": \"default_decoding\", } Out[6]: <p>position_info_param_name</p> name for this set of parameters <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>analysis_file_name</p> name of the file <p>head_position_object_id</p> <p>head_orientation_object_id</p> <p>head_velocity_object_id</p> default_decoding J1620210531_.nwb pos 0 valid times J1620210531_XOKZ4G53LE.nwb 200c54a9-57dc-4ed4-8e48-85cc9387b922 26f4ec71-2f5f-4ef2-82f1-ee3a89048168 7f9ffe2f-31e5-47ac-bad9-558440cd5f1ddefault_decoding J1620210531_.nwb pos 1 valid times J1620210531_B257TS35XJ.nwb c164cdfd-c328-49b2-b6b4-f8663a17f5d6 e1f26617-c44b-4464-942d-5e6755606789 f3c69ac6-5f0a-4622-827b-44a769ce4f77default_decoding J1620210531_.nwb pos 10 valid times J1620210531_1RICS57YYG.nwb 2394ec2d-c076-4b11-a40b-91e4b51b81fa 005080ee-b93f-4b3c-bcd0-595b46fedab2 47158c4f-3632-4ce1-9f1e-043f1ea5a5ecdefault_decoding J1620210531_.nwb pos 11 valid times J1620210531_HZSI1BGTE7.nwb 295aa187-81cd-4209-b96d-163f12a716ed 19a95d85-8cb9-4e06-82ee-26bc6df7c8c8 9abb39ce-f0e2-4637-b9e9-5f7b6891201ddefault_decoding J1620210531_.nwb pos 12 valid times J1620210531_M0UMC7TZ49.nwb ae4c015c-9d5f-4ca3-bbcd-049c1e1ae50b 1bbc244c-4d85-4e63-9d69-10815e37276c be8f6ba5-3cc9-4ac3-96af-5c3e84b9851cdefault_decoding J1620210531_.nwb pos 13 valid times J1620210531_TREA1SOT03.nwb c4bfe562-f523-495c-99e5-681524661264 65023b5e-321b-4d6e-b975-ec03cbd46247 ce391291-6107-420a-a73d-9a18d343a47adefault_decoding J1620210531_.nwb pos 14 valid times J1620210531_93PXV7F4CX.nwb 8521f813-635d-41cd-8f10-b00f26a7f957 75d08124-57f7-481b-ae44-0752a5912c18 84ce4054-8c11-49f2-8295-7d8173cb7eb7default_decoding J1620210531_.nwb pos 15 valid times J1620210531_FUJ1B1OKOA.nwb e72e5fcd-3d80-463a-a9d1-305e6e99f8db 6e8dc0e2-ab02-4305-bf2e-e2d208bae77a b7589719-49ba-48e7-a20b-88a911c3f8a5default_decoding J1620210531_.nwb pos 16 valid times J1620210531_76HL0SNP59.nwb b3124712-55db-4b0b-9bdc-77f03a0b8c98 08bfd82c-1e7c-4aa7-bd9e-256577b26e9c 85ffd435-3dc4-49ab-adea-a3327817a51ddefault_decoding J1620210531_.nwb pos 17 valid times J1620210531_5F8VRT0OC4.nwb 78b8f7e4-fb90-438d-8551-d15a4d97484a 7b96d15c-9721-44ea-9ecd-d8a2b60711d6 12336768-227e-4c0c-9ade-b3c7ac6c2de3default_decoding J1620210531_.nwb pos 18 valid times J1620210531_PP7XK1G26S.nwb 39ab3238-a0dd-4906-95c5-8e534458e371 ca0f939d-f0a7-4df4-9946-2f605ef5ab76 571277d0-885e-4d0e-b802-6ad8047ee3f1default_decoding J1620210531_.nwb pos 19 valid times J1620210531_AQZHSP2IDM.nwb b5a21053-3239-4943-aee1-3227b4915d5f 9f0c61c8-90ed-4a74-873c-e250aa872a2a 714f6ceb-6d70-474c-8c42-6f9fa3ff77e1 <p>...</p> <p>Total: 21</p> <p>Now that we've checked those, we can run the function:</p> In\u00a0[7]: Copied! <pre>populate_mark_indicators(spikesorting_selections)\n</pre> populate_mark_indicators(spikesorting_selections) <p>We can verify that this worked by importing the <code>UnitMarksIndicator</code> table:</p> In\u00a0[8]: Copied! <pre>from spyglass.decoding import UnitMarksIndicator\n\nUnitMarksIndicator &amp; spikesorting_selections\n</pre> from spyglass.decoding import UnitMarksIndicator  UnitMarksIndicator &amp; spikesorting_selections Out[8]: <p>curation_id</p> a number correponding to the index of this curation <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sort_interval_name</p> name for this interval <p>preproc_params_name</p> <p>team_name</p> <p>sorter</p> <p>sorter_params_name</p> <p>artifact_removed_interval_list_name</p> <p>mark_param_name</p> a name for this set of parameters <p>interval_list_name</p> descriptive name of this interval list <p>sampling_rate</p> <p>analysis_file_name</p> name of the file <p>marks_indicator_object_id</p> 0 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 0 valid times 500.0 J1620210531_NS19JB65RU.nwb d0de79cc-f2be-4e86-b4e8-e58a8aef1c020 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 1 valid times 500.0 J1620210531_YP9MXAPE2G.nwb 7598dc69-a024-4140-8b37-d1342a0e78290 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 10 valid times 500.0 J1620210531_AGOI7DXRQX.nwb 12a60beb-3ac2-41ae-a091-eebd0b14672c0 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 11 valid times 500.0 J1620210531_VXGQCG8UYO.nwb d0d40c96-e5a1-494f-b8a7-bf475304d6400 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 12 valid times 500.0 J1620210531_NF8RWZGCSG.nwb 2b6476a7-143f-4760-bafa-e4959f0e735a0 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 13 valid times 500.0 J1620210531_F7OHK7UMO3.nwb 84aea599-dbf1-4d11-bca2-00aa6ae9967a0 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 14 valid times 500.0 J1620210531_MPXHIH43Y1.nwb 78224e49-0118-458d-9aee-727db98fc3060 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 15 valid times 500.0 J1620210531_YFG1894358.nwb 721526f8-df6d-442e-84ee-bf7dd6ff8c190 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 16 valid times 500.0 J1620210531_VJ5V0LRJKU.nwb 39258f97-4508-4068-9b2b-ce419a5d718d0 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 17 valid times 500.0 J1620210531_NACXLH30KV.nwb bf312f88-af31-4247-83dc-9633d191401a0 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 18 valid times 500.0 J1620210531_LFKSWYVVRS.nwb af89e3e1-71f7-4c7c-a5ef-94d3cc7a12030 J1620210531_.nwb 0 raw data valid times no premaze no home franklab_tetrode_hippocampus JG_DG clusterless_thresholder clusterless_fixed J1620210531_.nwb_0_raw data valid times no premaze no home_franklab_tetrode_hippocampus_JG_DG_group_0.8_2000_8_1_artifact_removed_valid_times default pos 19 valid times 500.0 J1620210531_R9V8JD4IQA.nwb 3616a7b7-fca2-43bb-9c72-44eb3c22af80 <p>...</p> <p>Total: 42</p>"}, {"location": "notebooks/08_Extract_Mark_indicators/#mark-indicators", "title": "Mark Indicators\u00b6", "text": ""}, {"location": "notebooks/09_Decoding_with_GPUs_on_the_GPU_cluster/", "title": "GPU", "text": "In\u00a0[1]: Copied! <pre>import logging\n\nimport cupy as cp\nimport numpy as np\n\nfrom replay_trajectory_classification import SortedSpikesClassifier\nfrom replay_trajectory_classification.sorted_spikes_simulation import (\n    make_simulated_run_data,\n    make_continuous_replay,\n)\nfrom replay_trajectory_classification.environments import Environment\nfrom replay_trajectory_classification.continuous_state_transitions import (\n    RandomWalk,\n    Uniform,\n    Identity,\n    estimate_movement_var,\n)\n\n# Set up logging message formatting\nlogging.basicConfig(\n    level=\"INFO\", format=\"%(asctime)s %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\"\n)\n\n\n# Create simulated data\n(\n    time,\n    position,\n    sampling_frequency,\n    spikes,\n    place_fields,\n) = make_simulated_run_data()\n\nreplay_time, test_spikes = make_continuous_replay()\n\n# Set up classifier\nmovement_var = estimate_movement_var(position, sampling_frequency)\n\nenvironment = Environment(place_bin_size=np.sqrt(movement_var))\ncontinuous_transition_types = [\n    [RandomWalk(movement_var=movement_var * 120), Uniform(), Identity()],\n    [Uniform(), Uniform(), Uniform()],\n    [RandomWalk(movement_var=movement_var * 120), Uniform(), Identity()],\n]\nclassifier = SortedSpikesClassifier(\n    environments=environment,\n    continuous_transition_types=continuous_transition_types,\n    sorted_spikes_algorithm=\"spiking_likelihood_kde_gpu\",  # specify GPU enabled algorithm for the likelihood\n    sorted_spikes_algorithm_params={\"position_std\": 3.0},\n)\nstate_names = [\"continuous\", \"fragmented\", \"stationary\"]\n\n# Use GPU #6\nGPU_ID = 5\n\n# use context manager to specify which GPU (device)\nwith cp.cuda.Device(GPU_ID):\n    # Fit the model place fields\n    classifier.fit(position, spikes)\n\n    # Run the model on the simulated replay\n    results = classifier.predict(\n        test_spikes,\n        time=replay_time,\n        state_names=state_names,\n        use_gpu=True,  # Also need to specify use of GPU for the computation of the causal and acausal posterior\n    )\n</pre> import logging  import cupy as cp import numpy as np  from replay_trajectory_classification import SortedSpikesClassifier from replay_trajectory_classification.sorted_spikes_simulation import (     make_simulated_run_data,     make_continuous_replay, ) from replay_trajectory_classification.environments import Environment from replay_trajectory_classification.continuous_state_transitions import (     RandomWalk,     Uniform,     Identity,     estimate_movement_var, )  # Set up logging message formatting logging.basicConfig(     level=\"INFO\", format=\"%(asctime)s %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\" )   # Create simulated data (     time,     position,     sampling_frequency,     spikes,     place_fields, ) = make_simulated_run_data()  replay_time, test_spikes = make_continuous_replay()  # Set up classifier movement_var = estimate_movement_var(position, sampling_frequency)  environment = Environment(place_bin_size=np.sqrt(movement_var)) continuous_transition_types = [     [RandomWalk(movement_var=movement_var * 120), Uniform(), Identity()],     [Uniform(), Uniform(), Uniform()],     [RandomWalk(movement_var=movement_var * 120), Uniform(), Identity()], ] classifier = SortedSpikesClassifier(     environments=environment,     continuous_transition_types=continuous_transition_types,     sorted_spikes_algorithm=\"spiking_likelihood_kde_gpu\",  # specify GPU enabled algorithm for the likelihood     sorted_spikes_algorithm_params={\"position_std\": 3.0}, ) state_names = [\"continuous\", \"fragmented\", \"stationary\"]  # Use GPU #6 GPU_ID = 5  # use context manager to specify which GPU (device) with cp.cuda.Device(GPU_ID):     # Fit the model place fields     classifier.fit(position, spikes)      # Run the model on the simulated replay     results = classifier.predict(         test_spikes,         time=replay_time,         state_names=state_names,         use_gpu=True,  # Also need to specify use of GPU for the computation of the causal and acausal posterior     ) <pre>04-Jun-22 09:36:07 Fitting initial conditions...\n04-Jun-22 09:36:07 Fitting continuous state transition...\n04-Jun-22 09:36:07 Fitting discrete state transition\n04-Jun-22 09:36:07 Fitting place fields...\n</pre> <pre>04-Jun-22 09:36:08 Estimating likelihood...\n</pre> <pre>04-Jun-22 09:36:08 Estimating causal posterior...\n04-Jun-22 09:36:10 Estimating acausal posterior...\n</pre> In\u00a0[2]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Sat Jun  4 09:37:07 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A100 80G...  On   | 00000000:4F:00.0 Off |                    0 |\n| N/A   30C    P0    42W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   1  NVIDIA A100 80G...  On   | 00000000:52:00.0 Off |                    0 |\n| N/A   32C    P0    43W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   2  NVIDIA A100 80G...  On   | 00000000:53:00.0 Off |                    0 |\n| N/A   32C    P0    62W / 300W |  32271MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   3  NVIDIA A100 80G...  On   | 00000000:56:00.0 Off |                    0 |\n| N/A   29C    P0    41W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   4  NVIDIA A100 80G...  On   | 00000000:57:00.0 Off |                    0 |\n| N/A   30C    P0    44W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   5  NVIDIA A100 80G...  On   | 00000000:CE:00.0 Off |                    0 |\n| N/A   32C    P0    64W / 300W |    855MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   6  NVIDIA A100 80G...  On   | 00000000:D1:00.0 Off |                    0 |\n| N/A   31C    P0    42W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   7  NVIDIA A100 80G...  On   | 00000000:D2:00.0 Off |                    0 |\n| N/A   29C    P0    43W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   8  NVIDIA A100 80G...  On   | 00000000:D5:00.0 Off |                    0 |\n| N/A   31C    P0    42W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   9  NVIDIA A100 80G...  On   | 00000000:D6:00.0 Off |                    0 |\n| N/A   32C    P0    44W / 300W |     38MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    1   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    2   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    2   N/A  N/A   3646915      C   .../envs/spyglass/bin/python    32199MiB |\n|    3   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    4   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    5   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    5   N/A  N/A   3645824      C   .../envs/spyglass/bin/python      817MiB |\n|    6   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    7   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    8   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n|    9   N/A  N/A      4198      G   /usr/lib/xorg/Xorg                 35MiB |\n+-----------------------------------------------------------------------------+\n</pre> <p>You can also monitor their usage by using <code>watch -n 0.1 nvidia-smi</code> which will update <code>nvidia-smi</code> every 100 ms. This is best to monitor in the terminal because jupyter notebook won't display the updates.</p> <p>Other ways to monitor GPU usage are:</p> <ul> <li>https://github.com/rapidsai/jupyterlab-nvdashboard: a jupyter widget by nvidia to monitor GPU usage in the notebook</li> <li>https://github.com/peci1/nvidia-htop: a terminal program like nvidia-smi with more information about which users are utilizing the GPUs and which GPUs are being utilized.</li> </ul> In\u00a0[3]: Copied! <pre>from dask_cuda import LocalCUDACluster\nfrom dask.distributed import Client\n\ncluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=[4, 5, 6])\nclient = Client(cluster)\n\nclient\n</pre> from dask_cuda import LocalCUDACluster from dask.distributed import Client  cluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=[4, 5, 6]) client = Client(cluster)  client <pre>2022-05-18 13:50:10,288 - distributed.diskutils - INFO - Found stale lock file and directory '/stelmo/edeno/nwb_datajoint/notebooks/dask-worker-space/worker-ly7bpyy1', purging\n2022-05-18 13:50:10,296 - distributed.diskutils - INFO - Found stale lock file and directory '/stelmo/edeno/nwb_datajoint/notebooks/dask-worker-space/worker-n6nteep3', purging\n2022-05-18 13:50:10,302 - distributed.diskutils - INFO - Found stale lock file and directory '/stelmo/edeno/nwb_datajoint/notebooks/dask-worker-space/worker-okcse855', purging\n2022-05-18 13:50:10,305 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n2022-05-18 13:50:10,313 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n2022-05-18 13:50:10,319 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n</pre> Out[3]: Client <p>Client-1c4b4a29-d6ec-11ec-9abe-3cecef615bf0</p> Connection method: Cluster object Cluster type: dask_cuda.LocalCUDACluster Dashboard:  http://127.0.0.1:8787/status Cluster Info LocalCUDACluster <p>096db37c</p> Dashboard: http://127.0.0.1:8787/status Workers: 3                  Total threads: 3                  Total memory: 3.94 TiB                  Status: running Using processes: True Scheduler Info Scheduler <p>Scheduler-9287d258-c1ff-4d5a-8e3c-2a66ca163173</p> Comm: tcp://127.0.0.1:37725                      Workers: 3                      Dashboard: http://127.0.0.1:8787/status Total threads: 3                      Started: Just now                      Total memory: 3.94 TiB                      Workers Worker: 4 Comm:  tcp://127.0.0.1:35731                          Total threads:  1                          Dashboard:  http://127.0.0.1:46067/status Memory:  1.31 TiB                          Nanny:  tcp://127.0.0.1:34151                          Local directory:  /stelmo/edeno/nwb_datajoint/notebooks/dask-worker-space/worker-n17o941r                          GPU: NVIDIA A100 80GB PCIe                          GPU memory:  80.00 GiB                          Worker: 5 Comm:  tcp://127.0.0.1:39549                          Total threads:  1                          Dashboard:  http://127.0.0.1:42725/status Memory:  1.31 TiB                          Nanny:  tcp://127.0.0.1:41769                          Local directory:  /stelmo/edeno/nwb_datajoint/notebooks/dask-worker-space/worker-ppy7ls9e                          GPU: NVIDIA A100 80GB PCIe                          GPU memory:  80.00 GiB                          Worker: 6 Comm:  tcp://127.0.0.1:39335                          Total threads:  1                          Dashboard:  http://127.0.0.1:34189/status Memory:  1.31 TiB                          Nanny:  tcp://127.0.0.1:46373                          Local directory:  /stelmo/edeno/nwb_datajoint/notebooks/dask-worker-space/worker-m7thw3ne                          GPU: NVIDIA A100 80GB PCIe                          GPU memory:  80.00 GiB                          <p>This uses three GPUs with IDS 4, 5, and 6.</p> <p>Finally, to run the code, you need to wrap the function that you want to run on each GPU with the <code>dask.delayed</code> decorator.</p> <p>For example, say we want to run the function <code>test_gpu</code> on each item of <code>data</code> where each item is processed on a different GPU.</p> In\u00a0[4]: Copied! <pre>import cupy as cp\nimport dask\nimport logging\n\n\ndef setup_logger(name_logfile, path_logfile):\n\"\"\"Sets up a logger for each function that outputs\n    to the console and to a file\"\"\"\n    logger = logging.getLogger(name_logfile)\n    formatter = logging.Formatter(\n        \"%(asctime)s %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\"\n    )\n    fileHandler = logging.FileHandler(path_logfile, mode=\"w\")\n    fileHandler.setFormatter(formatter)\n    streamHandler = logging.StreamHandler()\n    streamHandler.setFormatter(formatter)\n\n    logger.setLevel(logging.INFO)\n    logger.addHandler(fileHandler)\n    logger.addHandler(streamHandler)\n\n    return logger\n\n\n# This uses the dask.delayed decorator on the test_gpu function\n@dask.delayed\ndef test_gpu(x, ind):\n    # Create a log file for this run of the function\n    logger = setup_logger(\n        name_logfile=f\"test_{ind}\", path_logfile=f\"test_{ind}.log\"\n    )\n\n    # Test to see if these go into different log files\n    logger.info(f\"This is a test of {ind}\")\n    logger.info(\"This should be in a unique file\")\n\n    # Run a GPU computation\n    return cp.asnumpy(cp.mean(x[:, None] @ x[:, None].T, axis=0))\n\n\n# Make up 10 fake datasets\nx = cp.random.normal(size=10_000, dtype=cp.float32)\ndata = [x + i for i in range(10)]\n\n# Append the result of the computation into a results list\nresults = [test_gpu(x, ind) for ind, x in enumerate(data)]\n\n# Run `dask.compute` on the results list for the code to run\ndask.compute(*results)\n</pre> import cupy as cp import dask import logging   def setup_logger(name_logfile, path_logfile):     \"\"\"Sets up a logger for each function that outputs     to the console and to a file\"\"\"     logger = logging.getLogger(name_logfile)     formatter = logging.Formatter(         \"%(asctime)s %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\"     )     fileHandler = logging.FileHandler(path_logfile, mode=\"w\")     fileHandler.setFormatter(formatter)     streamHandler = logging.StreamHandler()     streamHandler.setFormatter(formatter)      logger.setLevel(logging.INFO)     logger.addHandler(fileHandler)     logger.addHandler(streamHandler)      return logger   # This uses the dask.delayed decorator on the test_gpu function @dask.delayed def test_gpu(x, ind):     # Create a log file for this run of the function     logger = setup_logger(         name_logfile=f\"test_{ind}\", path_logfile=f\"test_{ind}.log\"     )      # Test to see if these go into different log files     logger.info(f\"This is a test of {ind}\")     logger.info(\"This should be in a unique file\")      # Run a GPU computation     return cp.asnumpy(cp.mean(x[:, None] @ x[:, None].T, axis=0))   # Make up 10 fake datasets x = cp.random.normal(size=10_000, dtype=cp.float32) data = [x + i for i in range(10)]  # Append the result of the computation into a results list results = [test_gpu(x, ind) for ind, x in enumerate(data)]  # Run `dask.compute` on the results list for the code to run dask.compute(*results) <pre>18-May-22 13:50:12 This is a test of 4\n18-May-22 13:50:12 This should be in a unique file\n18-May-22 13:50:12 This is a test of 3\n18-May-22 13:50:12 This should be in a unique file\n18-May-22 13:50:12 This is a test of 1\n18-May-22 13:50:12 This should be in a unique file\n18-May-22 13:50:13 This is a test of 9\n18-May-22 13:50:13 This should be in a unique file\n18-May-22 13:50:13 This is a test of 6\n18-May-22 13:50:13 This should be in a unique file\n18-May-22 13:50:13 This is a test of 5\n18-May-22 13:50:13 This should be in a unique file\n18-May-22 13:50:13 This is a test of 0\n18-May-22 13:50:13 This should be in a unique file\n18-May-22 13:50:13 This is a test of 2\n18-May-22 13:50:13 This should be in a unique file\n18-May-22 13:50:13 This is a test of 7\n18-May-22 13:50:13 This should be in a unique file\n18-May-22 13:50:13 This is a test of 8\n18-May-22 13:50:13 This should be in a unique file\n</pre> Out[4]: <pre>(array([ 0.00106875, -0.0032616 , -0.00759213, ..., -0.00612375,\n         0.0059738 , -0.01329288], dtype=float32),\n array([ 1.1191896 ,  0.6762629 ,  0.23331782, ...,  0.38350907,\n         1.6208985 , -0.34978032], dtype=float32),\n array([4.23731  , 3.3557875, 2.4742277, ..., 2.7731416, 5.235823 ,\n        1.3137323], dtype=float32),\n array([ 9.3554325,  8.035313 ,  6.7151375, ...,  7.1627736, 10.850748 ,\n         4.9772453], dtype=float32),\n array([16.47355 , 14.714837, 12.956048, ..., 13.552408, 18.465672,\n        10.640757], dtype=float32),\n array([25.591675, 23.39436 , 21.196953, ..., 21.942041, 28.0806  ,\n        18.304268], dtype=float32),\n array([36.7098  , 34.073883, 31.437868, ..., 32.331676, 39.695522,\n        27.967781], dtype=float32),\n array([49.827915, 46.753407, 43.67878 , ..., 44.721313, 53.31045 ,\n        39.631294], dtype=float32),\n array([64.946045, 61.432938, 57.9197  , ..., 59.11094 , 68.92538 ,\n        53.2948  ], dtype=float32),\n array([82.064156, 78.11245 , 74.16061 , ..., 75.50058 , 86.5403  ,\n        68.95831 ], dtype=float32))</pre> <p>This example also shows how to create a log file for each item in data with the <code>setup_logger</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/09_Decoding_with_GPUs_on_the_GPU_cluster/#gpu", "title": "GPU\u00b6", "text": ""}, {"location": "notebooks/09_Decoding_with_GPUs_on_the_GPU_cluster/#how-do-i-access-the-gpu-clusters", "title": "How do I access the GPU clusters?\u00b6", "text": "<ul> <li>We have two GPU clusters:<ul> <li><code>breeze</code></li> <li><code>zephyr</code></li> </ul> </li> <li>You can ssh to them using the following commands: <code>ssh username@breeze.cin.ucsf.edu</code> or <code>ssh username@zephyr.cin.ucsf.edu</code><ul> <li>NOTE: You don\u2019t need to specify the port like on the virgas (it is the default port 22)</li> </ul> </li> <li>There are currently 10 available GPUs and each GPU can be referred to by their IDs which starts at 0 and end at 9</li> <li>Each GPU has 80 GB of RAM</li> </ul>"}, {"location": "notebooks/09_Decoding_with_GPUs_on_the_GPU_cluster/#how-do-i-select-a-gpu-to-use", "title": "How do I select a GPU to use?\u00b6", "text": "<p>To use GPU decoding, you first must install <code>cupy</code>. The best way to do this is via conda because it takes care of the installing the correct cuda-toolkit:</p> <pre>conda install cupy\n</pre> <p>Next you will want to select a single GPU for decoding. You can accomplish this using  <code>cp.cuda.Device(GPU_ID)</code> with a context manager (the <code>with</code> statement). Warning: if you don't use the context manager, cupy will default to using GPU 0.</p> <p>For example, This runs the indented code on GPU #6.</p>"}, {"location": "notebooks/09_Decoding_with_GPUs_on_the_GPU_cluster/#which-gpu-should-i-use", "title": "Which GPU should I use?\u00b6", "text": "<p>You can see which GPUs are occupied by running <code>nvidia-smi</code> in the command line or alternatively <code>!nvidia-smi</code> in a jupyter notebook. Pick a GPU with low memory usage. For example, in the below output, GPUs 1, 4, 6, and 7 are probably not in use because they have low memory utilization. In addition, the power should be around 42W if not in use:</p>"}, {"location": "notebooks/09_Decoding_with_GPUs_on_the_GPU_cluster/#how-do-i-use-multiple-gpus-at-once", "title": "How do I use multiple GPUs at once?\u00b6", "text": "<p>To use multiple GPUs, you need to install the <code>dask_cuda</code> package with conda:</p> <pre>conda install -c rapidsai -c nvidia -c conda-forge dask-cuda\n</pre> <p>Next you will set up a client, which controls which GPUs will be used. By default, this will use all the GPUs you have, but if you want to use specific GPUs, you accomplish this using the <code>CUDA_VISIBLE_DEVICES</code> argument like so:</p>"}, {"location": "notebooks/10_1D_Clusterless_Decoding/", "title": "1D Clusterless Decoding", "text": "In\u00a0[\u00a0]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\n</pre> %reload_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre>import spyglass as nd\n\n# ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> import spyglass as nd  # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport xarray as xr\nimport logging\n\nFORMAT = \"%(asctime)s %(message)s\"\n\nlogging.basicConfig(level=\"INFO\", format=FORMAT, datefmt=\"%d-%b-%y %H:%M:%S\")\n</pre> import matplotlib.pyplot as plt import numpy as np import xarray as xr import logging  FORMAT = \"%(asctime)s %(message)s\"  logging.basicConfig(level=\"INFO\", format=FORMAT, datefmt=\"%d-%b-%y %H:%M:%S\") In\u00a0[4]: Copied! <pre>nwb_copy_file_name = \"chimi20200216_new_.nwb\"\n</pre> nwb_copy_file_name = \"chimi20200216_new_.nwb\" In\u00a0[10]: Copied! <pre>import pandas as pd\nfrom spyglass.decoding.clusterless import UnitMarksIndicator\n\n\nmarks = (\n    UnitMarksIndicator\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"sort_interval_name\": \"runs_noPrePostTrialTimes raw data valid times\",\n        \"filter_parameter_set_name\": \"franklab_default_hippocampus\",\n        \"unit_inclusion_param_name\": \"all2\",\n        \"mark_param_name\": \"default\",\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"sampling_rate\": 500,\n    }\n).fetch_xarray()\n\nmarks\n</pre> import pandas as pd from spyglass.decoding.clusterless import UnitMarksIndicator   marks = (     UnitMarksIndicator     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"sort_interval_name\": \"runs_noPrePostTrialTimes raw data valid times\",         \"filter_parameter_set_name\": \"franklab_default_hippocampus\",         \"unit_inclusion_param_name\": \"all2\",         \"mark_param_name\": \"default\",         \"interval_list_name\": \"pos 1 valid times\",         \"sampling_rate\": 500,     } ).fetch_xarray()  marks <pre>/stelmo/nwb/analysis/chimi20200216_new_7M0E8ERPE7.nwb\n/stelmo/nwb/analysis/chimi20200216_new_6WW86B509M.nwb\n/stelmo/nwb/analysis/chimi20200216_new_TLD0MCIC5H.nwb\n/stelmo/nwb/analysis/chimi20200216_new_7BEQDOTX3E.nwb\n/stelmo/nwb/analysis/chimi20200216_new_F8QVNUMVJS.nwb\n/stelmo/nwb/analysis/chimi20200216_new_BVZKYWREUE.nwb\n/stelmo/nwb/analysis/chimi20200216_new_3HMJON557D.nwb\n/stelmo/nwb/analysis/chimi20200216_new_QGMZ5ESFVA.nwb\n/stelmo/nwb/analysis/chimi20200216_new_1KRVBBCP2N.nwb\n/stelmo/nwb/analysis/chimi20200216_new_9E2Z0R6TLO.nwb\n/stelmo/nwb/analysis/chimi20200216_new_ALRF0STB1P.nwb\n/stelmo/nwb/analysis/chimi20200216_new_F2TDZW8LRY.nwb\n/stelmo/nwb/analysis/chimi20200216_new_LTEU71Z21T.nwb\n/stelmo/nwb/analysis/chimi20200216_new_KT4E4LIYAI.nwb\n/stelmo/nwb/analysis/chimi20200216_new_KOIRLX6R6X.nwb\n/stelmo/nwb/analysis/chimi20200216_new_4S01EA6NVN.nwb\n/stelmo/nwb/analysis/chimi20200216_new_ATQO860QOB.nwb\n/stelmo/nwb/analysis/chimi20200216_new_H3E2HYMEJA.nwb\n/stelmo/nwb/analysis/chimi20200216_new_4KJ4XVBKW3.nwb\n/stelmo/nwb/analysis/chimi20200216_new_0V98T6HQHX.nwb\n/stelmo/nwb/analysis/chimi20200216_new_A5FBXFDZMD.nwb\n/stelmo/nwb/analysis/chimi20200216_new_A5ELOH1L7Y.nwb\n</pre> Out[10]: <pre>&lt;xarray.DataArray (time: 655645, marks: 4, electrodes: 22)&gt;\narray([[[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       ...,\n\n       [[ -99.,   nan,   nan, ...,   nan,   nan,   nan],\n        [-100.,   nan,   nan, ...,   nan,   nan,   nan],\n        [ -94.,   nan,   nan, ...,   nan,   nan,   nan],\n        [-104.,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]]])\nCoordinates:\n  * time        (time) float64 1.582e+09 1.582e+09 ... 1.582e+09 1.582e+09\n  * marks       (marks) &lt;U14 'amplitude_0000' ... 'amplitude_0003'\n  * electrodes  (electrodes) int64 0 1 2 3 5 6 7 8 9 ... 15 16 17 18 19 21 22 23</pre>xarray.DataArray<ul><li>time: 655645</li><li>marks: 4</li><li>electrodes: 22</li></ul><ul><li>nan nan nan nan nan -170.0 nan nan ... nan nan nan nan nan nan nan nan<pre>array([[[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       ...,\n\n       [[ -99.,   nan,   nan, ...,   nan,   nan,   nan],\n        [-100.,   nan,   nan, ...,   nan,   nan,   nan],\n        [ -94.,   nan,   nan, ...,   nan,   nan,   nan],\n        [-104.,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]]])</pre></li><li>Coordinates: (3)<ul><li>time(time)float641.582e+09 1.582e+09 ... 1.582e+09<pre>array([1.581887e+09, 1.581887e+09, 1.581887e+09, ..., 1.581888e+09,\n       1.581888e+09, 1.581888e+09])</pre></li><li>marks(marks)&lt;U14'amplitude_0000' ... 'amplitude_...<pre>array(['amplitude_0000', 'amplitude_0001', 'amplitude_0002', 'amplitude_0003'],\n      dtype='&lt;U14')</pre></li><li>electrodes(electrodes)int640 1 2 3 5 6 7 ... 17 18 19 21 22 23<pre>array([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19, 21, 22, 23])</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>After you get the marks, it is important to visualize them to make sure they look right. We can use the <code>plot_all_marks</code> method of UnitMarksIndicator to quickly plot each mark feature against the other for each electrode.</p> <p>Here it is important to look for things that look overly correlated (strong diagonal on the off-diagonal plots) and for extreme amplitudes.</p> In\u00a0[11]: Copied! <pre>UnitMarksIndicator.plot_all_marks(marks)\n</pre> UnitMarksIndicator.plot_all_marks(marks) In\u00a0[12]: Copied! <pre>from spyglass.common.common_position import IntervalPositionInfo\n\n\nposition_key = {\n    \"nwb_file_name\": nwb_copy_file_name,\n    \"interval_list_name\": \"pos 1 valid times\",\n    \"position_info_param_name\": \"default_decoding\",\n}\n\nposition_info = (IntervalPositionInfo() &amp; position_key).fetch1_dataframe()\n\nposition_info\n</pre> from spyglass.common.common_position import IntervalPositionInfo   position_key = {     \"nwb_file_name\": nwb_copy_file_name,     \"interval_list_name\": \"pos 1 valid times\",     \"position_info_param_name\": \"default_decoding\", }  position_info = (IntervalPositionInfo() &amp; position_key).fetch1_dataframe()  position_info <pre>/stelmo/nwb/analysis/chimi20200216_new_6YC9LPAR7S.nwb\n</pre> Out[12]: head_position_x head_position_y head_orientation head_velocity_x head_velocity_y head_speed time 1.581887e+09 91.051650 211.127050 2.680048 1.741550 2.301478 2.886139 1.581887e+09 91.039455 211.144123 3.003241 1.827555 2.333931 2.964320 1.581887e+09 91.027260 211.161196 3.008398 1.915800 2.366668 3.044898 1.581887e+09 91.015065 211.178268 3.012802 2.006286 2.399705 3.127901 1.581887e+09 91.002871 211.195341 3.017242 2.099012 2.433059 3.213352 ... ... ... ... ... ... ... 1.581888e+09 182.158583 201.299625 -0.944304 0.057520 -0.356012 0.360629 1.581888e+09 182.158583 201.296373 -0.942329 0.053954 -0.356343 0.360404 1.581888e+09 182.158583 201.293121 -0.940357 0.050477 -0.356407 0.359964 1.581888e+09 182.158583 201.289869 -0.953059 0.047091 -0.356212 0.359312 1.581888e+09 182.158583 201.286617 -0.588081 0.043796 -0.355764 0.358450 <p>655645 rows \u00d7 6 columns</p> <p>It is important to visualize the 2D position to make sure there are no weird values.</p> In\u00a0[11]: Copied! <pre>plt.figure(figsize=(7, 6))\nplt.plot(position_info.head_position_x, position_info.head_position_y)\n</pre> plt.figure(figsize=(7, 6)) plt.plot(position_info.head_position_x, position_info.head_position_y) Out[11]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f6bfabcd280&gt;]</pre> <p>Next we load the linearized position tables. Refer to the notebook <code>5_linearization.ipynb</code> for more information on how to create the linear position information.</p> In\u00a0[13]: Copied! <pre>from spyglass.common.common_position import IntervalLinearizedPosition\n\nlinearization_key = {\n    \"position_info_param_name\": \"default_decoding\",\n    \"nwb_file_name\": nwb_copy_file_name,\n    \"interval_list_name\": \"pos 1 valid times\",\n    \"track_graph_name\": \"6 arm\",\n    \"linearization_param_name\": \"default\",\n}\n\nlinear_position_df = (\n    IntervalLinearizedPosition() &amp; linearization_key\n).fetch1_dataframe()\n\nlinear_position_df\n</pre> from spyglass.common.common_position import IntervalLinearizedPosition  linearization_key = {     \"position_info_param_name\": \"default_decoding\",     \"nwb_file_name\": nwb_copy_file_name,     \"interval_list_name\": \"pos 1 valid times\",     \"track_graph_name\": \"6 arm\",     \"linearization_param_name\": \"default\", }  linear_position_df = (     IntervalLinearizedPosition() &amp; linearization_key ).fetch1_dataframe()  linear_position_df <pre>/stelmo/nwb/analysis/chimi20200216_new_0YEJJLYTSM.nwb\n</pre> Out[13]: linear_position track_segment_id projected_x_position projected_y_position time 1.581887e+09 412.042773 0 90.802281 210.677533 1.581887e+09 412.061718 0 90.785714 210.686724 1.581887e+09 412.080664 0 90.769147 210.695914 1.581887e+09 412.099610 0 90.752579 210.705105 1.581887e+09 412.118556 0 90.736012 210.714296 ... ... ... ... ... 1.581888e+09 340.325042 1 175.434739 212.920160 1.581888e+09 340.323413 1 175.433329 212.919345 1.581888e+09 340.321785 1 175.431919 212.918529 1.581888e+09 340.320156 1 175.430509 212.917713 1.581888e+09 340.318527 1 175.429100 212.916898 <p>655645 rows \u00d7 4 columns</p> <p>We should also quickly visualize the linear position in order to sanity check the values. Here we plot the 2D position projected to its corresponding 1D segment.</p> In\u00a0[14]: Copied! <pre>plt.figure(figsize=(7, 6))\nplt.scatter(\n    linear_position_df.projected_x_position,\n    linear_position_df.projected_y_position,\n    c=linear_position_df.track_segment_id,\n    cmap=\"tab20\",\n    s=1,\n)\n</pre> plt.figure(figsize=(7, 6)) plt.scatter(     linear_position_df.projected_x_position,     linear_position_df.projected_y_position,     c=linear_position_df.track_segment_id,     cmap=\"tab20\",     s=1, ) Out[14]: <pre>&lt;matplotlib.collections.PathCollection at 0x7fb80565b5e0&gt;</pre> <p>We should also plot the linearized position itself to make sure it is okay.</p> In\u00a0[15]: Copied! <pre>plt.figure(figsize=(20, 10))\nplt.scatter(\n    linear_position_df.index,\n    linear_position_df.linear_position,\n    s=1,\n    c=linear_position_df.track_segment_id,\n    cmap=\"tab20\",\n)\n</pre> plt.figure(figsize=(20, 10)) plt.scatter(     linear_position_df.index,     linear_position_df.linear_position,     s=1,     c=linear_position_df.track_segment_id,     cmap=\"tab20\", ) Out[15]: <pre>&lt;matplotlib.collections.PathCollection at 0x7fb8055de0a0&gt;</pre> <p>Okay now that we've looked at the data, we should quickly verify that all our data is the same size. It may not be due to the valid intervals of the neural and position data.</p> In\u00a0[17]: Copied! <pre>position_info.shape, marks.shape, linear_position_df.shape\n</pre> position_info.shape, marks.shape, linear_position_df.shape Out[17]: <pre>((655645, 6), (655645, 4, 22), (655645, 4))</pre> <p>We also want to make sure we have valid ephys data and valid position data for decoding. Here we only have one valid time interval, but if we had more than one, we should decode on each interval separately.</p> In\u00a0[18]: Copied! <pre>from spyglass.common.common_interval import interval_list_intersect\nfrom spyglass.common import IntervalList\n\nkey = {}\nkey[\"interval_list_name\"] = \"02_r1\"\nkey[\"nwb_file_name\"] = nwb_copy_file_name\n\ninterval = (\n    IntervalList\n    &amp; {\n        \"nwb_file_name\": key[\"nwb_file_name\"],\n        \"interval_list_name\": key[\"interval_list_name\"],\n    }\n).fetch1(\"valid_times\")\n\nvalid_ephys_times = (\n    IntervalList\n    &amp; {\n        \"nwb_file_name\": key[\"nwb_file_name\"],\n        \"interval_list_name\": \"raw data valid times\",\n    }\n).fetch1(\"valid_times\")\nposition_interval_names = (\n    IntervalPositionInfo\n    &amp; {\n        \"nwb_file_name\": key[\"nwb_file_name\"],\n        \"position_info_param_name\": \"default_decoding\",\n    }\n).fetch(\"interval_list_name\")\nvalid_pos_times = [\n    (\n        IntervalList\n        &amp; {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": pos_interval_name,\n        }\n    ).fetch1(\"valid_times\")\n    for pos_interval_name in position_interval_names\n]\n\nintersect_interval = interval_list_intersect(\n    interval_list_intersect(interval, valid_ephys_times), valid_pos_times[0]\n)\nvalid_time_slice = slice(intersect_interval[0][0], intersect_interval[0][1])\nvalid_time_slice\n</pre> from spyglass.common.common_interval import interval_list_intersect from spyglass.common import IntervalList  key = {} key[\"interval_list_name\"] = \"02_r1\" key[\"nwb_file_name\"] = nwb_copy_file_name  interval = (     IntervalList     &amp; {         \"nwb_file_name\": key[\"nwb_file_name\"],         \"interval_list_name\": key[\"interval_list_name\"],     } ).fetch1(\"valid_times\")  valid_ephys_times = (     IntervalList     &amp; {         \"nwb_file_name\": key[\"nwb_file_name\"],         \"interval_list_name\": \"raw data valid times\",     } ).fetch1(\"valid_times\") position_interval_names = (     IntervalPositionInfo     &amp; {         \"nwb_file_name\": key[\"nwb_file_name\"],         \"position_info_param_name\": \"default_decoding\",     } ).fetch(\"interval_list_name\") valid_pos_times = [     (         IntervalList         &amp; {             \"nwb_file_name\": key[\"nwb_file_name\"],             \"interval_list_name\": pos_interval_name,         }     ).fetch1(\"valid_times\")     for pos_interval_name in position_interval_names ]  intersect_interval = interval_list_intersect(     interval_list_intersect(interval, valid_ephys_times), valid_pos_times[0] ) valid_time_slice = slice(intersect_interval[0][0], intersect_interval[0][1]) valid_time_slice Out[18]: <pre>slice(1581886916.3153033, 1581888227.5987928, None)</pre> In\u00a0[19]: Copied! <pre>linear_position_df = linear_position_df.loc[valid_time_slice]\nmarks = marks.sel(time=valid_time_slice)\nposition_info = position_info.loc[valid_time_slice]\n</pre> linear_position_df = linear_position_df.loc[valid_time_slice] marks = marks.sel(time=valid_time_slice) position_info = position_info.loc[valid_time_slice] In\u00a0[20]: Copied! <pre>position_info.shape, marks.shape, linear_position_df.shape\n</pre> position_info.shape, marks.shape, linear_position_df.shape Out[20]: <pre>((655643, 6), (655643, 4, 22), (655643, 4))</pre> In\u00a0[21]: Copied! <pre>from replay_trajectory_classification.environments import Environment\nfrom spyglass.common.common_position import TrackGraph\nfrom spyglass.decoding.clusterless import ClusterlessClassifierParameters\n\nimport pprint\n\n\nparameters = (\n    ClusterlessClassifierParameters()\n    &amp; {\"classifier_param_name\": \"default_decoding_gpu\"}\n).fetch1()\n\ntrack_graph = (\n    TrackGraph() &amp; {\"track_graph_name\": \"6 arm\"}\n).get_networkx_track_graph()\ntrack_graph_params = (TrackGraph() &amp; {\"track_graph_name\": \"6 arm\"}).fetch1()\n\nparameters[\"classifier_params\"][\"environments\"] = [\n    Environment(\n        track_graph=track_graph,\n        edge_order=track_graph_params[\"linear_edge_order\"],\n        edge_spacing=track_graph_params[\"linear_edge_spacing\"],\n    )\n]\n\nparameters[\"classifier_params\"][\n    \"clusterless_algorithm\"\n] = \"multiunit_likelihood_integer_gpu\"\nparameters[\"classifier_params\"][\"clusterless_algorithm_params\"] = {\n    \"mark_std\": 24.0,\n    \"position_std\": 6.0,\n    \"block_size\": 2**12,\n}\n\npprint.pprint(parameters)\n</pre> from replay_trajectory_classification.environments import Environment from spyglass.common.common_position import TrackGraph from spyglass.decoding.clusterless import ClusterlessClassifierParameters  import pprint   parameters = (     ClusterlessClassifierParameters()     &amp; {\"classifier_param_name\": \"default_decoding_gpu\"} ).fetch1()  track_graph = (     TrackGraph() &amp; {\"track_graph_name\": \"6 arm\"} ).get_networkx_track_graph() track_graph_params = (TrackGraph() &amp; {\"track_graph_name\": \"6 arm\"}).fetch1()  parameters[\"classifier_params\"][\"environments\"] = [     Environment(         track_graph=track_graph,         edge_order=track_graph_params[\"linear_edge_order\"],         edge_spacing=track_graph_params[\"linear_edge_spacing\"],     ) ]  parameters[\"classifier_params\"][     \"clusterless_algorithm\" ] = \"multiunit_likelihood_integer_gpu\" parameters[\"classifier_params\"][\"clusterless_algorithm_params\"] = {     \"mark_std\": 24.0,     \"position_std\": 6.0,     \"block_size\": 2**12, }  pprint.pprint(parameters) <pre>{'classifier_param_name': 'default_decoding_gpu',\n 'classifier_params': {'clusterless_algorithm': 'multiunit_likelihood_integer_gpu',\n                       'clusterless_algorithm_params': {'block_size': 4096,\n                                                        'mark_std': 24.0,\n                                                        'position_std': 6.0},\n                       'continuous_transition_types': [[RandomWalk(environment_name='', movement_var=6.0, movement_mean=0.0, use_diffusion=False),\n                                                        Uniform(environment_name='', environment2_name=None)],\n                                                       [Uniform(environment_name='', environment2_name=None),\n                                                        Uniform(environment_name='', environment2_name=None)]],\n                       'discrete_transition_type': DiagonalDiscrete(diagonal_value=0.98),\n                       'environments': [Environment(environment_name='', place_bin_size=2.0, track_graph=&lt;networkx.classes.graph.Graph object at 0x7fb807560cd0&gt;, edge_order=[(3, 6), (6, 8), (6, 9), (3, 1), (1, 2), (1, 0), (3, 4), (4, 5), (4, 7)], edge_spacing=15, is_track_interior=None, position_range=None, infer_track_interior=True, fill_holes=False, dilate=False)],\n                       'infer_track_interior': True,\n                       'initial_conditions_type': UniformInitialConditions(),\n                       'observation_models': None},\n 'fit_params': {},\n 'predict_params': {'is_compute_acausal': True,\n                    'state_names': ['Continuous', 'Uniform'],\n                    'use_gpu': True}}\n</pre> <p>After we set the parameters, we can run the decoding. Here we are running this on the first GPU device with <code>cp.dua.Device(0)</code>. See the <code>Decoding_with_GPUs_on_the_GPU_cluster.ipynb</code> notebook for more information on how to use other GPUs.</p> In\u00a0[22]: Copied! <pre>from replay_trajectory_classification import ClusterlessClassifier\nimport cupy as cp\n\nwith cp.cuda.Device(0):\n    classifier = ClusterlessClassifier(**parameters[\"classifier_params\"])\n    classifier.fit(\n        position=linear_position_df.linear_position.values,\n        multiunits=marks.values,\n        **parameters[\"fit_params\"],\n    )\n    results = classifier.predict(\n        multiunits=marks.values,\n        time=linear_position_df.index,\n        **parameters[\"predict_params\"],\n    )\n    logging.info(\"Done!\")\n</pre> from replay_trajectory_classification import ClusterlessClassifier import cupy as cp  with cp.cuda.Device(0):     classifier = ClusterlessClassifier(**parameters[\"classifier_params\"])     classifier.fit(         position=linear_position_df.linear_position.values,         multiunits=marks.values,         **parameters[\"fit_params\"],     )     results = classifier.predict(         multiunits=marks.values,         time=linear_position_df.index,         **parameters[\"predict_params\"],     )     logging.info(\"Done!\") <pre>12-Sep-22 12:15:15 Fitting initial conditions...\n12-Sep-22 12:15:15 Fitting continuous state transition...\n12-Sep-22 12:15:15 Fitting discrete state transition\n12-Sep-22 12:15:15 Fitting multiunits...\n12-Sep-22 12:15:18 Estimating likelihood...\n</pre> <pre>12-Sep-22 12:15:32 Estimating causal posterior...\n12-Sep-22 12:18:49 Estimating acausal posterior...\n12-Sep-22 12:26:34 Done!\n</pre> In\u00a0[23]: Copied! <pre>from spyglass.decoding.visualization import (\n    create_interactive_1D_decoding_figurl,\n)\n\n\nview = create_interactive_1D_decoding_figurl(\n    position_info,\n    linear_position_df,\n    marks,\n    results,\n    position_name=\"linear_position\",\n    speed_name=\"head_speed\",\n    posterior_type=\"acausal_posterior\",\n    sampling_frequency=500,\n    view_height=800,\n)\n\nview.url(label=\"\")\n</pre> from spyglass.decoding.visualization import (     create_interactive_1D_decoding_figurl, )   view = create_interactive_1D_decoding_figurl(     position_info,     linear_position_df,     marks,     results,     position_name=\"linear_position\",     speed_name=\"head_speed\",     posterior_type=\"acausal_posterior\",     sampling_frequency=500,     view_height=800, )  view.url(label=\"\") <pre>WARNING: create_position_plot is deprecated. Instead use vv.PositionPlot(...). See tests/test_position_plot.py\nComputing sha1 of /stelmo/nwb/.kachery-cloud/tmp_WUd2b9mq/file.npy\n/stelmo/nwb/.kachery-cloud/tmp_VGviN8q9\nCreating segment/1/0\nCreating segment/1/1\nCreating segment/1/2\nCreating segment/1/3\nCreating segment/1/4\nCreating segment/1/5\nCreating segment/1/6\nCreating segment/3/0\nCreating segment/3/1\nCreating segment/3/2\nCreating segment/9/0\nCreating segment/27/0\nCreating segment/81/0\nCreating segment/243/0\nCreating segment/729/0\nCreating segment/2187/0\nCreating segment/6561/0\nCreating segment/19683/0\nCreating segment/59049/0\nCreating segment/177147/0\nCreating segment/531441/0\nComputing sha1 of /stelmo/nwb/.kachery-cloud/tmp_VGviN8q9/live_position_pdf_plot.h5\n</pre> Out[23]: <pre>'https://figurl.org/f?v=gs://figurl/spikesortingview-9&amp;d=sha1://319b25199f81cc30d84f8eed471309b419c9b95d&amp;project=lqqrbobsev&amp;label=test'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/10_1D_Clusterless_Decoding/#1d-clusterless-decoding", "title": "1D Clusterless Decoding\u00b6", "text": ""}, {"location": "notebooks/10_1D_Clusterless_Decoding/#unitmarksindicator", "title": "UnitMarksIndicator\u00b6", "text": "<p>The first thing we need are the marks indicators for the clusterless decoding. See the <code>Extract_Mark_indicators.ipynb</code> for information on how to populate this table. We will use the special method <code>fetch_xarray</code> to get a labeled array of shape (n_time, n_mark_features, n_electrodes). Time will be in 2 ms bins, where it will be NaN if no spike occurs and the value of the spike features if a spike occured.</p> <p>If more than one spike is in a single time bin from a single tetrode, we just average the marks. Technically this isn't ideal, we should use all the marks, but it doesn't seem to happen that often and the decodes seem robust to it.</p> <p>The UnitMarksIndicator table depends on an interval from the interval list and a sampling rate.</p>"}, {"location": "notebooks/10_1D_Clusterless_Decoding/#position", "title": "Position\u00b6", "text": "<p>After the marks look good, you'll need to load/populate the 2D position data. This comes from the <code>IntervalPositionInfo</code> table. Refer to the notebook <code>4_position_information.ipynb</code> for more information. Note that we will need to upsample the position data (which is done here via the <code>default_decoding</code> parameters) to match the sampling frequency that we intend to decode in (2 ms time bins or 500 Hz sampling rate)</p>"}, {"location": "notebooks/10_1D_Clusterless_Decoding/#decoding", "title": "Decoding\u00b6", "text": "<p>Okay, now having sanity checked the data, we can finally get to decoding. In the future this will be a pipeline, but for now it is manual as the table structure is still being prototyped.</p> <p>In order to set the parameters, we can fetch the default parameters and modify them.</p> <p>For 1D decoding, it is best to pass in the track graph and track graph parameters we used for linearization in order for the random walk to be handled properly. We can also set the amount of smoothing in the position and mark dimensions: <code>position_std</code> and <code>mark_std</code> respectively. Finally we set the <code>block_size</code>, which controls how many samples get processed at a time so that we don't run out of GPU memory.</p>"}, {"location": "notebooks/10_1D_Clusterless_Decoding/#visualization", "title": "Visualization\u00b6", "text": "<p>Finally, we can plot the decodes to make sure they make sense. We will use figurl to make an interactive figure. The function <code>create_interactive_1D_decoding_figurl</code> will return a URL that will lead you to the interactive figure. Note for this figure that you need to be running an interactive sorting view backend.</p>"}, {"location": "notebooks/11_2D_Clusterless_Decoding/", "title": "2D Clusterless Decoding", "text": "In\u00a0[\u00a0]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\n</pre> %reload_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre># ignore datajoint+jupyter async warnings\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=ResourceWarning)\n</pre> # ignore datajoint+jupyter async warnings import warnings  warnings.simplefilter(\"ignore\", category=DeprecationWarning) warnings.simplefilter(\"ignore\", category=ResourceWarning) In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport xarray as xr\nimport logging\n\nFORMAT = \"%(asctime)s %(message)s\"\n\nlogging.basicConfig(level=\"INFO\", format=FORMAT, datefmt=\"%d-%b-%y %H:%M:%S\")\n</pre> import matplotlib.pyplot as plt import numpy as np import xarray as xr import logging  FORMAT = \"%(asctime)s %(message)s\"  logging.basicConfig(level=\"INFO\", format=FORMAT, datefmt=\"%d-%b-%y %H:%M:%S\") In\u00a0[4]: Copied! <pre>nwb_copy_file_name = \"chimi20200216_new_.nwb\"\n</pre> nwb_copy_file_name = \"chimi20200216_new_.nwb\" In\u00a0[5]: Copied! <pre>from spyglass.decoding.clusterless import UnitMarksIndicator\n\n\nmarks = (\n    UnitMarksIndicator\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"sort_interval_name\": \"runs_noPrePostTrialTimes raw data valid times\",\n        \"filter_parameter_set_name\": \"franklab_default_hippocampus\",\n        \"unit_inclusion_param_name\": \"all2\",\n        \"mark_param_name\": \"default\",\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"sampling_rate\": 500,\n    }\n).fetch_xarray()\n\nmarks\n</pre> from spyglass.decoding.clusterless import UnitMarksIndicator   marks = (     UnitMarksIndicator     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"sort_interval_name\": \"runs_noPrePostTrialTimes raw data valid times\",         \"filter_parameter_set_name\": \"franklab_default_hippocampus\",         \"unit_inclusion_param_name\": \"all2\",         \"mark_param_name\": \"default\",         \"interval_list_name\": \"pos 1 valid times\",         \"sampling_rate\": 500,     } ).fetch_xarray()  marks <pre>/home/edeno/miniconda3/envs/spyglass/lib/python3.8/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(mpl.__version__) &gt;= \"3.0\":\n/home/edeno/miniconda3/envs/spyglass/lib/python3.8/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n13-Sep-22 15:42:32 Connected edeno@lmf-db.cin.ucsf.edu:3306\n</pre> <pre>Connecting edeno@lmf-db.cin.ucsf.edu:3306\n</pre> <pre>/stelmo/nwb/analysis/chimi20200216_new_7M0E8ERPE7.nwb\n/stelmo/nwb/analysis/chimi20200216_new_6WW86B509M.nwb\n/stelmo/nwb/analysis/chimi20200216_new_TLD0MCIC5H.nwb\n/stelmo/nwb/analysis/chimi20200216_new_7BEQDOTX3E.nwb\n/stelmo/nwb/analysis/chimi20200216_new_F8QVNUMVJS.nwb\n/stelmo/nwb/analysis/chimi20200216_new_BVZKYWREUE.nwb\n/stelmo/nwb/analysis/chimi20200216_new_3HMJON557D.nwb\n/stelmo/nwb/analysis/chimi20200216_new_QGMZ5ESFVA.nwb\n/stelmo/nwb/analysis/chimi20200216_new_1KRVBBCP2N.nwb\n/stelmo/nwb/analysis/chimi20200216_new_9E2Z0R6TLO.nwb\n/stelmo/nwb/analysis/chimi20200216_new_ALRF0STB1P.nwb\n/stelmo/nwb/analysis/chimi20200216_new_F2TDZW8LRY.nwb\n/stelmo/nwb/analysis/chimi20200216_new_LTEU71Z21T.nwb\n/stelmo/nwb/analysis/chimi20200216_new_KT4E4LIYAI.nwb\n/stelmo/nwb/analysis/chimi20200216_new_KOIRLX6R6X.nwb\n/stelmo/nwb/analysis/chimi20200216_new_4S01EA6NVN.nwb\n/stelmo/nwb/analysis/chimi20200216_new_ATQO860QOB.nwb\n/stelmo/nwb/analysis/chimi20200216_new_H3E2HYMEJA.nwb\n/stelmo/nwb/analysis/chimi20200216_new_4KJ4XVBKW3.nwb\n/stelmo/nwb/analysis/chimi20200216_new_0V98T6HQHX.nwb\n/stelmo/nwb/analysis/chimi20200216_new_A5FBXFDZMD.nwb\n/stelmo/nwb/analysis/chimi20200216_new_A5ELOH1L7Y.nwb\n</pre> Out[5]: <pre>&lt;xarray.DataArray (time: 655645, marks: 4, electrodes: 22)&gt;\narray([[[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       ...,\n\n       [[ -99.,   nan,   nan, ...,   nan,   nan,   nan],\n        [-100.,   nan,   nan, ...,   nan,   nan,   nan],\n        [ -94.,   nan,   nan, ...,   nan,   nan,   nan],\n        [-104.,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]]])\nCoordinates:\n  * time        (time) float64 1.582e+09 1.582e+09 ... 1.582e+09 1.582e+09\n  * marks       (marks) &lt;U14 'amplitude_0000' ... 'amplitude_0003'\n  * electrodes  (electrodes) int64 0 1 2 3 5 6 7 8 9 ... 15 16 17 18 19 21 22 23</pre>xarray.DataArray<ul><li>time: 655645</li><li>marks: 4</li><li>electrodes: 22</li></ul><ul><li>nan nan nan nan nan -170.0 nan nan ... nan nan nan nan nan nan nan nan<pre>array([[[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       ...,\n\n       [[ -99.,   nan,   nan, ...,   nan,   nan,   nan],\n        [-100.,   nan,   nan, ...,   nan,   nan,   nan],\n        [ -94.,   nan,   nan, ...,   nan,   nan,   nan],\n        [-104.,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]]])</pre></li><li>Coordinates: (3)<ul><li>time(time)float641.582e+09 1.582e+09 ... 1.582e+09<pre>array([1.581887e+09, 1.581887e+09, 1.581887e+09, ..., 1.581888e+09,\n       1.581888e+09, 1.581888e+09])</pre></li><li>marks(marks)&lt;U14'amplitude_0000' ... 'amplitude_...<pre>array(['amplitude_0000', 'amplitude_0001', 'amplitude_0002', 'amplitude_0003'],\n      dtype='&lt;U14')</pre></li><li>electrodes(electrodes)int640 1 2 3 5 6 7 ... 17 18 19 21 22 23<pre>array([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19, 21, 22, 23])</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[6]: Copied! <pre>plt.scatter(\n    marks.isel(electrodes=0).dropna(\"time\").isel(marks=0),\n    marks.isel(electrodes=0).dropna(\"time\").isel(marks=1),\n    s=1,\n)\n</pre> plt.scatter(     marks.isel(electrodes=0).dropna(\"time\").isel(marks=0),     marks.isel(electrodes=0).dropna(\"time\").isel(marks=1),     s=1, ) Out[6]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f2ad72ae3a0&gt;</pre> In\u00a0[7]: Copied! <pre>from spyglass.common.common_position import IntervalPositionInfo\n\n\nposition_key = {\n    \"nwb_file_name\": nwb_copy_file_name,\n    \"interval_list_name\": \"pos 1 valid times\",\n    \"position_info_param_name\": \"default_decoding\",\n}\n\nposition_info = (IntervalPositionInfo() &amp; position_key).fetch1_dataframe()\n\nposition_info\n</pre> from spyglass.common.common_position import IntervalPositionInfo   position_key = {     \"nwb_file_name\": nwb_copy_file_name,     \"interval_list_name\": \"pos 1 valid times\",     \"position_info_param_name\": \"default_decoding\", }  position_info = (IntervalPositionInfo() &amp; position_key).fetch1_dataframe()  position_info <pre>/stelmo/nwb/analysis/chimi20200216_new_6YC9LPAR7S.nwb\n</pre> Out[7]: head_position_x head_position_y head_orientation head_velocity_x head_velocity_y head_speed time 1.581887e+09 91.051650 211.127050 2.680048 1.741550 2.301478 2.886139 1.581887e+09 91.039455 211.144123 3.003241 1.827555 2.333931 2.964320 1.581887e+09 91.027260 211.161196 3.008398 1.915800 2.366668 3.044898 1.581887e+09 91.015065 211.178268 3.012802 2.006286 2.399705 3.127901 1.581887e+09 91.002871 211.195341 3.017242 2.099012 2.433059 3.213352 ... ... ... ... ... ... ... 1.581888e+09 182.158583 201.299625 -0.944304 0.057520 -0.356012 0.360629 1.581888e+09 182.158583 201.296373 -0.942329 0.053954 -0.356343 0.360404 1.581888e+09 182.158583 201.293121 -0.940357 0.050477 -0.356407 0.359964 1.581888e+09 182.158583 201.289869 -0.953059 0.047091 -0.356212 0.359312 1.581888e+09 182.158583 201.286617 -0.588081 0.043796 -0.355764 0.358450 <p>655645 rows \u00d7 6 columns</p> In\u00a0[8]: Copied! <pre>plt.plot(position_info.head_position_x, position_info.head_position_y)\n</pre> plt.plot(position_info.head_position_x, position_info.head_position_y) Out[8]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f2ad70abfd0&gt;]</pre> In\u00a0[9]: Copied! <pre>position_info.shape, marks.shape\n</pre> position_info.shape, marks.shape Out[9]: <pre>((655645, 6), (655645, 4, 22))</pre> In\u00a0[10]: Copied! <pre>from spyglass.common.common_interval import interval_list_intersect\nfrom spyglass.common import IntervalList\n\nkey = {}\nkey[\"interval_list_name\"] = \"02_r1\"\nkey[\"nwb_file_name\"] = nwb_copy_file_name\n\ninterval = (\n    IntervalList\n    &amp; {\n        \"nwb_file_name\": key[\"nwb_file_name\"],\n        \"interval_list_name\": key[\"interval_list_name\"],\n    }\n).fetch1(\"valid_times\")\n\nvalid_ephys_times = (\n    IntervalList\n    &amp; {\n        \"nwb_file_name\": key[\"nwb_file_name\"],\n        \"interval_list_name\": \"raw data valid times\",\n    }\n).fetch1(\"valid_times\")\nposition_interval_names = (\n    IntervalPositionInfo\n    &amp; {\n        \"nwb_file_name\": key[\"nwb_file_name\"],\n        \"position_info_param_name\": \"default_decoding\",\n    }\n).fetch(\"interval_list_name\")\nvalid_pos_times = [\n    (\n        IntervalList\n        &amp; {\n            \"nwb_file_name\": key[\"nwb_file_name\"],\n            \"interval_list_name\": pos_interval_name,\n        }\n    ).fetch1(\"valid_times\")\n    for pos_interval_name in position_interval_names\n]\n\nintersect_interval = interval_list_intersect(\n    interval_list_intersect(interval, valid_ephys_times), valid_pos_times[0]\n)\nvalid_time_slice = slice(intersect_interval[0][0], intersect_interval[0][1])\nvalid_time_slice\n</pre> from spyglass.common.common_interval import interval_list_intersect from spyglass.common import IntervalList  key = {} key[\"interval_list_name\"] = \"02_r1\" key[\"nwb_file_name\"] = nwb_copy_file_name  interval = (     IntervalList     &amp; {         \"nwb_file_name\": key[\"nwb_file_name\"],         \"interval_list_name\": key[\"interval_list_name\"],     } ).fetch1(\"valid_times\")  valid_ephys_times = (     IntervalList     &amp; {         \"nwb_file_name\": key[\"nwb_file_name\"],         \"interval_list_name\": \"raw data valid times\",     } ).fetch1(\"valid_times\") position_interval_names = (     IntervalPositionInfo     &amp; {         \"nwb_file_name\": key[\"nwb_file_name\"],         \"position_info_param_name\": \"default_decoding\",     } ).fetch(\"interval_list_name\") valid_pos_times = [     (         IntervalList         &amp; {             \"nwb_file_name\": key[\"nwb_file_name\"],             \"interval_list_name\": pos_interval_name,         }     ).fetch1(\"valid_times\")     for pos_interval_name in position_interval_names ]  intersect_interval = interval_list_intersect(     interval_list_intersect(interval, valid_ephys_times), valid_pos_times[0] ) valid_time_slice = slice(intersect_interval[0][0], intersect_interval[0][1]) valid_time_slice Out[10]: <pre>slice(1581886916.3153033, 1581888227.5987928, None)</pre> In\u00a0[11]: Copied! <pre>from replay_trajectory_classification import ClusterlessClassifier\nfrom replay_trajectory_classification.environments import Environment\nfrom replay_trajectory_classification.continuous_state_transitions import (\n    RandomWalk,\n    Uniform,\n)\nfrom spyglass.decoding.clusterless import ClusterlessClassifierParameters\n\nmarks = marks.sel(time=valid_time_slice)\nposition_info = position_info.loc[valid_time_slice]\n\nparameters = (\n    ClusterlessClassifierParameters()\n    &amp; {\"classifier_param_name\": \"default_decoding_gpu\"}\n).fetch1()\n\nparameters[\"classifier_params\"][\"clusterless_algorithm_params\"] = {\n    \"mark_std\": 24.0,\n    \"position_std\": 3.0,\n    \"block_size\": int(2**13),\n    \"disable_progress_bar\": False,\n    \"use_diffusion\": False,\n}\nparameters[\"classifier_params\"][\"environments\"][0] = Environment(\n    place_bin_size=3.0\n)\n\n\nimport cupy as cp\n\nwith cp.cuda.Device(0):\n    classifier = ClusterlessClassifier(**parameters[\"classifier_params\"])\n    classifier.fit(\n        position=position_info[[\"head_position_x\", \"head_position_y\"]].values,\n        multiunits=marks.values,\n        **parameters[\"fit_params\"],\n    )\n    results = classifier.predict(\n        multiunits=marks.values,\n        time=position_info.index,\n        **parameters[\"predict_params\"],\n    )\n    logging.info(\"Done!\")\n</pre> from replay_trajectory_classification import ClusterlessClassifier from replay_trajectory_classification.environments import Environment from replay_trajectory_classification.continuous_state_transitions import (     RandomWalk,     Uniform, ) from spyglass.decoding.clusterless import ClusterlessClassifierParameters  marks = marks.sel(time=valid_time_slice) position_info = position_info.loc[valid_time_slice]  parameters = (     ClusterlessClassifierParameters()     &amp; {\"classifier_param_name\": \"default_decoding_gpu\"} ).fetch1()  parameters[\"classifier_params\"][\"clusterless_algorithm_params\"] = {     \"mark_std\": 24.0,     \"position_std\": 3.0,     \"block_size\": int(2**13),     \"disable_progress_bar\": False,     \"use_diffusion\": False, } parameters[\"classifier_params\"][\"environments\"][0] = Environment(     place_bin_size=3.0 )   import cupy as cp  with cp.cuda.Device(0):     classifier = ClusterlessClassifier(**parameters[\"classifier_params\"])     classifier.fit(         position=position_info[[\"head_position_x\", \"head_position_y\"]].values,         multiunits=marks.values,         **parameters[\"fit_params\"],     )     results = classifier.predict(         multiunits=marks.values,         time=position_info.index,         **parameters[\"predict_params\"],     )     logging.info(\"Done!\") <pre>13-Sep-22 15:43:02 Fitting initial conditions...\n13-Sep-22 15:43:02 Fitting continuous state transition...\n13-Sep-22 15:43:04 Fitting discrete state transition\n13-Sep-22 15:43:04 Fitting multiunits...\n13-Sep-22 15:43:06 Estimating likelihood...\n</pre> <pre>13-Sep-22 15:44:52 Estimating causal posterior...\n13-Sep-22 15:48:34 Estimating acausal posterior...\n13-Sep-22 16:00:30 Done!\n</pre> In\u00a0[16]: Copied! <pre>from spyglass.decoding.visualization import create_interactive_2D_decoding_figurl\n\n\nview = create_interactive_2D_decoding_figurl(\n    position_info,\n    marks,\n    results,\n    classifier.environments[0].place_bin_size,,\n    position_name=[\"head_position_x\", \"head_position_y\"],\n    head_direction_name=\"head_orientation\",\n    speed_name=\"head_speed\",\n    posterior_type=\"acausal_posterior\",\n    sampling_frequency=500,\n    view_height=800,\n)\n</pre> from spyglass.decoding.visualization import create_interactive_2D_decoding_figurl   view = create_interactive_2D_decoding_figurl(     position_info,     marks,     results,     classifier.environments[0].place_bin_size,,     position_name=[\"head_position_x\", \"head_position_y\"],     head_direction_name=\"head_orientation\",     speed_name=\"head_speed\",     posterior_type=\"acausal_posterior\",     sampling_frequency=500,     view_height=800, ) <p>If you want to view the decode in the jupyter notebook, uncomment and run this next cell. This will create an interactive visualization locally:</p> In\u00a0[30]: Copied! <pre># view\n</pre> # view <p>If you would like to create a visualization shareable in the cloud, run this next cell:</p> In\u00a0[19]: Copied! <pre>view.url(label=\"2D Decode Example\")\n</pre> view.url(label=\"2D Decode Example\") Out[19]: <pre>'https://figurl.org/f?v=gs://figurl/spikesortingview-9&amp;d=sha1://7251ae9794d4dfd783fabd9b84826c86b1982f74&amp;label=2D%20Decode%20Example'</pre>"}, {"location": "notebooks/11_2D_Clusterless_Decoding/#2d-clusterless-decoding", "title": "2D Clusterless Decoding\u00b6", "text": ""}, {"location": "notebooks/12_Ripple_Detection/", "title": "Ripple Detection", "text": "In\u00a0[1]: Copied! <pre>from spyglass.common.common_ripple import (\n    RippleLFPSelection,\n    RippleParameters,\n    RippleTimes,\n)\nfrom spyglass.common import IntervalPositionInfo\n</pre> from spyglass.common.common_ripple import (     RippleLFPSelection,     RippleParameters,     RippleTimes, ) from spyglass.common import IntervalPositionInfo <pre>[2022-11-28 14:37:02,551][INFO]: Connecting edeno@lmf-db.cin.ucsf.edu:3306\n[2022-11-28 14:37:02,627][INFO]: Connected edeno@lmf-db.cin.ucsf.edu:3306\n/home/edeno/miniconda3/envs/spyglass/lib/python3.9/site-packages/position_tools/core.py:3: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n  from scipy.ndimage.filters import gaussian_filter1d\n</pre> In\u00a0[2]: Copied! <pre>?RippleLFPSelection.set_lfp_electrodes\n</pre> ?RippleLFPSelection.set_lfp_electrodes <pre>Signature:\nRippleLFPSelection.set_lfp_electrodes(\n    nwb_file_name,\n    electrode_list,\n    group_name='CA1',\n)\nDocstring:\nRemoves all electrodes for the specified nwb file and then adds back the electrodes in the list\n\nParameters\n----------\nnwb_file_name : str\n    The name of the nwb file for the desired session\nelectrode_list : list\n    list of electrodes to be used for LFP\ngroup_name : str, optional\nFile:      /cumulus/edeno/spyglass/src/spyglass/common/common_ripple.py\nType:      function\n</pre> <p>From the function parameters, we can see that we need the <code>nwb_file_name</code>, a list of electrodes (<code>electrode_list</code>), and to set a <code>group_name</code>.</p> In\u00a0[3]: Copied! <pre>nwb_file_name = \"chimi20200216_new_.nwb\"\n</pre> nwb_file_name = \"chimi20200216_new_.nwb\" <p>Now we can look at <code>electrode_id</code> in the <code>Electrode</code> table:</p> In\u00a0[4]: Copied! <pre>from spyglass.common import Electrode, BrainRegion\n\nelectrodes = (\n    (Electrode() &amp; {\"nwb_file_name\": nwb_file_name}) * BrainRegion\n).fetch(format=\"frame\")\nelectrodes\n</pre> from spyglass.common import Electrode, BrainRegion  electrodes = (     (Electrode() &amp; {\"nwb_file_name\": nwb_file_name}) * BrainRegion ).fetch(format=\"frame\") electrodes Out[4]: probe_type probe_shank probe_electrode name original_reference_electrode x y z filtering impedance bad_channel x_warped y_warped z_warped contacts region_name subregion_name subsubregion_name nwb_file_name electrode_group_name electrode_id region_id chimi20200216_new_.nwb 0 0 6 tetrode_12.5 0 0 0 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 1 6 tetrode_12.5 0 1 1 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 2 6 tetrode_12.5 0 2 2 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 3 6 tetrode_12.5 0 3 3 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 1 4 6 tetrode_12.5 0 0 4 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 8 35 6 tetrode_12.5 0 3 35 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 9 36 6 tetrode_12.5 0 0 36 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 37 6 tetrode_12.5 0 1 37 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 38 6 tetrode_12.5 0 2 38 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 39 6 tetrode_12.5 0 3 39 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus <p>224 rows \u00d7 18 columns</p> <p>For ripple detection, we want only tetrodes, and only the first good wire on each tetrode. We will assume that is the first wire on each tetrode. I will do this using pandas syntax but you could use datajoint to filter this table as well. Here is the filtered table.</p> In\u00a0[5]: Copied! <pre>electrodes.loc[\n    (electrodes.region_name == \"Hippocampus\")\n    &amp; (electrodes.probe_electrode == 0)\n]\n</pre> electrodes.loc[     (electrodes.region_name == \"Hippocampus\")     &amp; (electrodes.probe_electrode == 0) ] Out[5]: probe_type probe_shank probe_electrode name original_reference_electrode x y z filtering impedance bad_channel x_warped y_warped z_warped contacts region_name subregion_name subsubregion_name nwb_file_name electrode_group_name electrode_id region_id chimi20200216_new_.nwb 0 0 6 tetrode_12.5 0 0 0 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 1 4 6 tetrode_12.5 0 0 4 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 10 40 6 tetrode_12.5 0 0 40 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 11 44 6 tetrode_12.5 0 0 44 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 17 68 6 tetrode_12.5 0 0 68 81 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 18 72 6 tetrode_12.5 0 0 72 81 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 19 76 6 tetrode_12.5 0 0 76 81 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 2 8 6 tetrode_12.5 0 0 8 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 21 84 6 tetrode_12.5 0 0 84 81 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 22 88 6 tetrode_12.5 0 0 88 81 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 23 92 6 tetrode_12.5 0 0 92 81 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 3 12 6 tetrode_12.5 0 0 12 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 5 20 6 tetrode_12.5 0 0 20 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 6 24 6 tetrode_12.5 0 0 24 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 7 28 6 tetrode_12.5 0 0 28 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 8 32 6 tetrode_12.5 0 0 32 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus 9 36 6 tetrode_12.5 0 0 36 16 0.0 0.0 0.0 None 0.0 False 0.0 0.0 0.0 Hippocampus <p>We only want the electrode_id to put in the <code>electrode_list</code>:</p> In\u00a0[6]: Copied! <pre>electrode_list = (\n    electrodes.loc[\n        (electrodes.region_name == \"Hippocampus\")\n        &amp; (electrodes.probe_electrode == 0)\n    ]\n    .reset_index()\n    .electrode_id\n).tolist()\n\nelectrode_list\n</pre> electrode_list = (     electrodes.loc[         (electrodes.region_name == \"Hippocampus\")         &amp; (electrodes.probe_electrode == 0)     ]     .reset_index()     .electrode_id ).tolist()  electrode_list Out[6]: <pre>[0, 4, 40, 44, 68, 72, 76, 8, 84, 88, 92, 12, 20, 24, 28, 32, 36]</pre> <p>Now we can set <code>RippleLFPSelection</code>.</p> In\u00a0[7]: Copied! <pre>RippleLFPSelection.set_lfp_electrodes(\n    nwb_file_name,\n    electrode_list,\n    group_name=\"CA1\",\n)\n</pre> RippleLFPSelection.set_lfp_electrodes(     nwb_file_name,     electrode_list,     group_name=\"CA1\", ) In\u00a0[8]: Copied! <pre>RippleLFPSelection()\n</pre> RippleLFPSelection() Out[8]: <p>nwb_file_name</p> name of the NWB file <p>group_name</p> chimi20200216_new_.nwb CA1 <p>Total: 1</p> In\u00a0[10]: Copied! <pre>RippleParameters()\n</pre> RippleParameters() Out[10]: <p>ripple_param_name</p> a name for this set of parameters <p>ripple_param_dict</p> dictionary of parameters default =BLOB= <p>Total: 1</p> <p>Let's look at the default ripple parameters:</p> In\u00a0[14]: Copied! <pre>(RippleParameters() &amp; {\"ripple_param_name\": \"default\"}).fetch1()\n</pre> (RippleParameters() &amp; {\"ripple_param_name\": \"default\"}).fetch1() Out[14]: <pre>{'ripple_param_name': 'default',\n 'ripple_param_dict': {'filter_name': 'Ripple 150-250 Hz',\n  'speed_name': 'head_speed',\n  'ripple_detection_algorithm': 'Kay_ripple_detector',\n  'ripple_detection_params': {'speed_threshold': 4.0,\n   'minimum_duration': 0.015,\n   'zscore_threshold': 2.0,\n   'smoothing_sigma': 0.004,\n   'close_ripple_threshold': 0.0}}}</pre> <ul> <li><p><code>filter_name</code> refers to which bandpass filter is used</p> </li> <li><p><code>speed_name</code> refers to the name of the speed parameters in <code>IntervalPositionInfo</code></p> </li> <li><p>For the <code>Kay_ripple_detector</code> (options are currently Kay and Karlsson, see <code>ripple_detection</code> package for specifics) the parameters are:</p> <ul> <li><code>speed_threshold</code>: maxmimum speed the animal can move (in cm/s)</li> <li><code>minimum_duration</code>: minimum time above threshold (in seconds)</li> <li><code>zscore_threshold</code>: mimimum value to be considered a ripple (in standard deviations from mean)</li> <li><code>smoothing_sigma</code>: how much to smooth the signal in time (in seconds)</li> <li><code>close_ripple_threshold</code>: exclude ripples closer than this amount (in seconds)</li> </ul> </li> </ul> In\u00a0[23]: Copied! <pre>(\n    IntervalPositionInfo\n    &amp; {\n        \"nwb_file_name\": nwb_file_name,\n        \"position_info_param_name\": \"default\",\n        \"interval_list_name\": \"pos 1 valid times\",\n    }\n).fetch1_dataframe()\n</pre> (     IntervalPositionInfo     &amp; {         \"nwb_file_name\": nwb_file_name,         \"position_info_param_name\": \"default\",         \"interval_list_name\": \"pos 1 valid times\",     } ).fetch1_dataframe() Out[23]: head_position_x head_position_y head_orientation head_velocity_x head_velocity_y head_speed time 1.581887e+09 91.051650 211.127050 2.999696 1.387074 2.848838 3.168573 1.581887e+09 90.844337 211.417287 3.078386 3.123201 3.411111 4.624939 1.581887e+09 90.637025 211.707525 -3.114572 5.431643 4.089597 6.799085 1.581887e+09 90.802875 211.596958 -3.033109 8.097753 4.979262 9.506138 1.581887e+09 91.288579 211.482443 -3.062550 10.840482 6.071373 12.424880 ... ... ... ... ... ... ... 1.581888e+09 182.158583 201.452467 -0.986926 0.348276 0.218575 0.411182 1.581888e+09 182.158583 201.397183 -0.978610 0.279135 -0.058413 0.285182 1.581888e+09 182.213867 201.341900 -0.957589 0.193798 -0.283200 0.343162 1.581888e+09 182.158583 201.341900 -0.970083 0.110838 -0.417380 0.431846 1.581888e+09 182.158583 201.286617 -0.936414 0.045190 -0.453966 0.456209 <p>39340 rows \u00d7 6 columns</p> <p>We can see that <code>head_speed</code> exists. Remember this must be the same as set in the <code>RippleParameters</code>.</p> In\u00a0[25]: Copied! <pre>key = {\n    \"ripple_param_name\": \"default\",\n    \"nwb_file_name\": nwb_file_name,\n    \"group_name\": \"CA1\",\n    \"position_info_param_name\": \"default\",\n    \"interval_list_name\": \"pos 1 valid times\",\n}\nRippleTimes().populate(key)\n</pre> key = {     \"ripple_param_name\": \"default\",     \"nwb_file_name\": nwb_file_name,     \"group_name\": \"CA1\",     \"position_info_param_name\": \"default\",     \"interval_list_name\": \"pos 1 valid times\", } RippleTimes().populate(key) <pre>[2022-11-28 14:50:49,354][WARNING]: Skipped checksum for file with hash: 4bfb6228-ab9c-7765-6fec-3a4a84d0747f, and path: /stelmo/nwb/analysis/chimi20200216_new_QZ5RAQKFBU.nwb\n</pre> <pre>Computing ripple times for: {'ripple_param_name': 'default', 'nwb_file_name': 'chimi20200216_new_.nwb', 'group_name': 'CA1', 'position_info_param_name': 'default', 'interval_list_name': 'pos 1 valid times'}\nWriting new NWB file chimi20200216_new_52A55DER85.nwb\n</pre> <p>We can get the ripple times by using the method <code>fetch1_dataframe</code></p> In\u00a0[30]: Copied! <pre>ripple_times = (RippleTimes() &amp; key).fetch1_dataframe()\nripple_times\n</pre> ripple_times = (RippleTimes() &amp; key).fetch1_dataframe() ripple_times Out[30]: start_time end_time id 0 1.581887e+09 1.581887e+09 1 1.581887e+09 1.581887e+09 2 1.581887e+09 1.581887e+09 3 1.581887e+09 1.581887e+09 4 1.581887e+09 1.581887e+09 ... ... ... 672 1.581888e+09 1.581888e+09 673 1.581888e+09 1.581888e+09 674 1.581888e+09 1.581888e+09 675 1.581888e+09 1.581888e+09 676 1.581888e+09 1.581888e+09 <p>677 rows \u00d7 2 columns</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/12_Ripple_Detection/#ripple-detection", "title": "Ripple Detection\u00b6", "text": "<p>Ripple detection depends on a set of LFPs, the parameters used for detection and the speed of the animal. You will need <code>RippleLFPSelection</code>, <code>RippleParameters</code>, and <code>IntervalPositionInfo</code> to be populated accordingly. Let's import these:</p>"}, {"location": "notebooks/12_Ripple_Detection/#1-selecting-electrodes", "title": "1. Selecting Electrodes\u00b6", "text": "<p>First let's pick which electrodes we want to do ripple detection on. This involves the calling the method <code>set_lfp_electrodes</code> from <code>RippleLFPSelection</code></p>"}, {"location": "notebooks/12_Ripple_Detection/#how-to-set-the-group__name", "title": "How to set the <code>group__name</code>?\u00b6", "text": "<p>By default <code>group_name</code> is set to CA1 because this is what is commonly used to detect ripples. But if you had a different set of electrodes you wanted to detect on (say on PFC), you could set a different name.</p>"}, {"location": "notebooks/12_Ripple_Detection/#how-do-we-figure-out-the-electrode_list", "title": "How do we figure out the <code>electrode_list</code>?\u00b6", "text": "<p>You should already know which animal and session you want to choose, so the next question is how you set the <code>electrode_list</code>. Let's first define the nwb file we want to look at.</p>"}, {"location": "notebooks/12_Ripple_Detection/#2-setting-the-ripple-parameters", "title": "2. Setting the Ripple Parameters\u00b6", "text": "<p>Now that we know which electrodes to use, we can set up the ripple detection parameters:</p>"}, {"location": "notebooks/12_Ripple_Detection/#3-make-sure-the-speed-for-this-interval-exists", "title": "3. Make sure the speed for this interval exists\u00b6", "text": "<p>The speed for this interval should exist under the default position parameter set and for a given interval.</p>"}, {"location": "notebooks/12_Ripple_Detection/#4-running-ripple-detection-and-fetching-ripple-times", "title": "4. Running Ripple Detection and Fetching Ripple Times\u00b6", "text": "<p>Now we can put everything together.</p>"}, {"location": "notebooks/13_Theta_phase_and_power/", "title": "13 Theta phase and power", "text": "In\u00a0[1]: Copied! <pre>import numpy as np\nfrom spyglass.lfp.v1 import LFPBand\nimport matplotlib.pyplot as plt\n</pre> import numpy as np from spyglass.lfp.v1 import LFPBand import matplotlib.pyplot as plt <pre>[2023-05-24 19:29:32,670][INFO]: Connecting xulu@lmf-db.cin.ucsf.edu:3306\n[2023-05-24 19:29:32,722][INFO]: Connected xulu@lmf-db.cin.ucsf.edu:3306\n/home/lorenlab/anaconda3/envs/spyglass/lib/python3.9/site-packages/position_tools/core.py:3: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n  from scipy.ndimage.filters import gaussian_filter1d\n</pre> <p>In order to acquire theta band data from LFPBand, we need to define the correct keys.</p> <p>We can first take a look at the LFPBand table to understand what primary keys it contains:</p> In\u00a0[2]: Copied! <pre>LFPBand()\n</pre> LFPBand() Out[2]: <p>lfp_id</p> <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter <p>nwb_file_name</p> name of the NWB file <p>target_interval_list_name</p> descriptive name of this interval list <p>lfp_band_sampling_rate</p> the sampling rate for this band <p>analysis_file_name</p> name of the file <p>interval_list_name</p> descriptive name of this interval list <p>filtered_data_object_id</p> the NWB object ID for loading this object from the file 18ea201c-f121-11ed-aa77-f8f21e671294 theta_5_11 1000 chimi20200213_.nwb chimi20200213_.nwb_02_r1 noPrePostTrialTimes_LFP_none_artifact_removed_valid_times 1000 chimi20200213_M8CCMU7UYG.nwb chimi20200213_.nwb_02_r1 noPrePostTrialTimes_LFP_none_artifact_removed_valid_times lfp band 1000Hz 35275bb9-0509-4cdd-ba35-ae3d93387169784930ce-f091-11ed-9473-ac1f6b0f717e LFP 0-400 Hz 1000 pippin20210518_.nwb pippin20210518_.nwb_r1_r2_LFP_default_artifact_removed_valid_times 1000 pippin20210518_37BO9HWO2P.nwb pippin20210518_.nwb_r1_r2_LFP_default_artifact_removed_valid_times lfp band 1000Hz 64cb1c14-7635-432b-9273-c2214a410358784930ce-f091-11ed-9473-ac1f6b0f717e Ripple 150-250 Hz 1000 pippin20210518_.nwb pippin20210518_.nwb_r1_r2_LFP_default_artifact_removed_valid_times 1000 pippin20210518_VYWKS2LP8Y.nwb pippin20210518_.nwb_r1_r2_LFP_default_artifact_removed_valid_times lfp band 1000Hz 1d3c878b-b2a6-496c-b856-f6a0af11a05181cbe85a-f06c-11ed-a02d-04d9f580fe36 Theta 5-11 Hz 1000 j1620210710_.nwb test interval 1000 j1620210710_VEG6PIEL2F.nwb test interval lfp band 1000Hz a7a36a2e-3fa9-41db-853e-30a66a9e04f5976afb32-ed48-11ed-97e3-ac1f6b0f717e LFP 0-400 Hz 1000 molly20220401_.nwb molly20220401_.nwb_r2_r3_LFP_default_artifact_removed_valid_times 1000 molly20220401_CHSGHKWB19.nwb molly20220401_.nwb_r2_r3_LFP_default_artifact_removed_valid_times lfp band 1000Hz 8298babd-e094-4f55-8706-c8ce43e8eb5cac232662-f0ef-11ed-aeb9-f8f21e671294 Theta 5-11 Hz 1000 tonks20211103_.nwb test interval 100 tonks20211103_PYAC8MUKPS.nwb test interval lfp band 100Hz f6d30142-38a0-497b-8d90-bec13fa9f86bfeffa256-f848-11ed-a2a3-04d9f580fe36 Theta 5-11 Hz 1000 j1620210710_.nwb test_interval_one_session 1000 j1620210710_MO2OQRLMV4.nwb test_interval_one_session lfp band 1000Hz 902ba18a-7066-4202-b2fd-88d492d35401 <p>Total: 7</p> <p>Now create a dictionary of keys that index the theta band data we want to analyze.</p> In\u00a0[3]: Copied! <pre>nwb_file_name = \"j1620210710_.nwb\"\nlfp_key = dict()\nlfp_key[\"nwb_file_name\"] = nwb_file_name\nlfp_key[\"filter_name\"] = \"Theta 5-11 Hz\"\n# Use the same interval list name as in the LFPBand entry of interest; here for simplicity, we will use a test interval\nlfp_key[\"target_interval_list_name\"] = \"test_interval_one_session\"\n\n(\n    LFPBand() &amp; lfp_key\n)  # Make sure that this prints out the entry we want to analyze\n</pre> nwb_file_name = \"j1620210710_.nwb\" lfp_key = dict() lfp_key[\"nwb_file_name\"] = nwb_file_name lfp_key[\"filter_name\"] = \"Theta 5-11 Hz\" # Use the same interval list name as in the LFPBand entry of interest; here for simplicity, we will use a test interval lfp_key[\"target_interval_list_name\"] = \"test_interval_one_session\"  (     LFPBand() &amp; lfp_key )  # Make sure that this prints out the entry we want to analyze Out[3]: <p>lfp_id</p> <p>filter_name</p> descriptive name of this filter <p>filter_sampling_rate</p> sampling rate for this filter <p>nwb_file_name</p> name of the NWB file <p>target_interval_list_name</p> descriptive name of this interval list <p>lfp_band_sampling_rate</p> the sampling rate for this band <p>analysis_file_name</p> name of the file <p>interval_list_name</p> descriptive name of this interval list <p>filtered_data_object_id</p> the NWB object ID for loading this object from the file feffa256-f848-11ed-a2a3-04d9f580fe36 Theta 5-11 Hz 1000 j1620210710_.nwb test_interval_one_session 1000 j1620210710_MO2OQRLMV4.nwb test_interval_one_session lfp band 1000Hz 902ba18a-7066-4202-b2fd-88d492d35401 <p>Total: 1</p> <p>Because we do not need all electrodes for computing theta phase/power, we should define a list of electrodes for theta analyses. Here, we choose the hippocampal reference electrode ids we used during recording because we usually use them to determine theta phase and power.</p> In\u00a0[7]: Copied! <pre>electrode_list = [\n    0,\n    40,\n]\n</pre> electrode_list = [     0,     40, ] <p>Make sure that the electrodes defined in the eletrode_list already exist in the LFPBand data; if not, go to the LFP tutorial to generate them.</p> In\u00a0[8]: Copied! <pre># Get all the eletrode ids in the LFPBand data:\nall_electrodes = (\n    (LFPBand() &amp; lfp_key).fetch_nwb()[0][\"filtered_data\"]\n).electrodes.data[:]\n# Check if electrodes defined in our electrode_list exist in the LFPBand data:\nnp.isin(electrode_list, all_electrodes)\n</pre> # Get all the eletrode ids in the LFPBand data: all_electrodes = (     (LFPBand() &amp; lfp_key).fetch_nwb()[0][\"filtered_data\"] ).electrodes.data[:] # Check if electrodes defined in our electrode_list exist in the LFPBand data: np.isin(electrode_list, all_electrodes) Out[8]: <pre>array([ True,  True])</pre> In\u00a0[9]: Copied! <pre># Compute the theta analytic signal.\ntheta_analytic_signal = (LFPBand() &amp; lfp_key).compute_analytic_signal(\n    electrode_list=electrode_list\n)\n\ntheta_analytic_signal\n</pre> # Compute the theta analytic signal. theta_analytic_signal = (LFPBand() &amp; lfp_key).compute_analytic_signal(     electrode_list=electrode_list )  theta_analytic_signal Out[9]: electrode 0 electrode 40 time 1.625936e+09 -8.00000+0113.958580j -28.000000+127.578749j 1.625936e+09 -12.000000+84.275575j -33.000000+94.369967j 1.625936e+09 -15.000000+86.760325j -37.000000+96.216452j 1.625936e+09 -19.000000+78.099686j -42.000000+85.408355j 1.625936e+09 -23.000000+79.717213j -46.000000+85.895887j ... ... ... 1.625936e+09 49.000000+54.115342j 35.000000+64.803958j 1.625936e+09 48.000000+56.641484j 34.000000+67.018773j 1.625936e+09 48.000000+69.764801j 33.000000+81.095129j 1.625936e+09 47.000000+74.219121j 32.000000+83.744017j 1.625936e+09 45.000000+109.613565j 31.000000+122.770178j <p>99999 rows \u00d7 2 columns</p> <p>In the theta_analytic_signal dataframe above, the index is the timestamps, and the columns are the analytic sinals of theta band (complex numbers) for electrodes that we defined in eletrode_list.</p> In\u00a0[10]: Copied! <pre># Compute the theta phase and power for electrodes in the electrode_list we previously defined.\ntheta_phase = (LFPBand() &amp; lfp_key).compute_signal_phase(\n    electrode_list=electrode_list\n)\ntheta_power = (LFPBand() &amp; lfp_key).compute_signal_power(\n    electrode_list=electrode_list\n)\n</pre> # Compute the theta phase and power for electrodes in the electrode_list we previously defined. theta_phase = (LFPBand() &amp; lfp_key).compute_signal_phase(     electrode_list=electrode_list ) theta_power = (LFPBand() &amp; lfp_key).compute_signal_power(     electrode_list=electrode_list ) In\u00a0[11]: Copied! <pre># Get theta band data that will be jointly plotted with the theta phase results.\ntheta_band = (LFPBand() &amp; lfp_key).fetch_nwb()[0][\"filtered_data\"]\nelectrode_index = np.isin(theta_band.electrodes.data[:], electrode_list)\ntheta_band_selected = theta_band.data[:, electrode_index]\n</pre> # Get theta band data that will be jointly plotted with the theta phase results. theta_band = (LFPBand() &amp; lfp_key).fetch_nwb()[0][\"filtered_data\"] electrode_index = np.isin(theta_band.electrodes.data[:], electrode_list) theta_band_selected = theta_band.data[:, electrode_index] In\u00a0[12]: Copied! <pre>electrode_id = electrode_list[0]  # the electrode for which we want to plot\nplot_start = 0  # the start time of plotting\nplot_end = 5000  # the end time of plotting\nfig, ax1 = plt.subplots(figsize=(20, 6))\nax1.set_xlabel(\"Time (sec)\", fontsize=15)\nax1.set_ylabel(\"Amplitude (AD units)\", fontsize=15)\nax1.plot(\n    theta_phase.index[plot_start:plot_end],\n    theta_band_selected[\n        plot_start:plot_end, np.where(np.array(electrode_list) == 0)[0][0]\n    ],\n    \"k-\",\n    linewidth=1,\n    alpha=0.9,\n)\nax1.tick_params(axis=\"y\", labelcolor=\"k\")\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\nax2.set_ylabel(\"Phase(deg)\", color=\"b\", fontsize=15)\nax2.plot(\n    theta_phase.index[plot_start:plot_end],\n    theta_phase[f\"electrode {electrode_id}\"].iloc[plot_start:plot_end],\n    \"b\",\n)\nax2.tick_params(axis=\"y\", labelcolor=\"b\")\nax2.axhline(y=0, color=\"r\", linestyle=\"-\")\nfig.tight_layout()\nax1.set_title(\n    f\"Theta band amplitude and phase, electode {electrode_id}\",\n    fontsize=20,\n);\n</pre> electrode_id = electrode_list[0]  # the electrode for which we want to plot plot_start = 0  # the start time of plotting plot_end = 5000  # the end time of plotting fig, ax1 = plt.subplots(figsize=(20, 6)) ax1.set_xlabel(\"Time (sec)\", fontsize=15) ax1.set_ylabel(\"Amplitude (AD units)\", fontsize=15) ax1.plot(     theta_phase.index[plot_start:plot_end],     theta_band_selected[         plot_start:plot_end, np.where(np.array(electrode_list) == 0)[0][0]     ],     \"k-\",     linewidth=1,     alpha=0.9, ) ax1.tick_params(axis=\"y\", labelcolor=\"k\") ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis ax2.set_ylabel(\"Phase(deg)\", color=\"b\", fontsize=15) ax2.plot(     theta_phase.index[plot_start:plot_end],     theta_phase[f\"electrode {electrode_id}\"].iloc[plot_start:plot_end],     \"b\", ) ax2.tick_params(axis=\"y\", labelcolor=\"b\") ax2.axhline(y=0, color=\"r\", linestyle=\"-\") fig.tight_layout() ax1.set_title(     f\"Theta band amplitude and phase, electode {electrode_id}\",     fontsize=20, ); <p>We can also plot the theta power if interested.</p> In\u00a0[13]: Copied! <pre>plot_start = 0\nplot_end = 5000\nfig, ax = plt.subplots(figsize=(20, 6))\nax.set_xlabel(\"Time (sec)\", fontsize=15)\nax.set_ylabel(\"Theta power\", fontsize=15)\nax.plot(\n    theta_power.index[plot_start:plot_end],\n    theta_power[f\"electrode {electrode_id}\"].iloc[plot_start:plot_end],\n    \"k-\",\n    linewidth=1,\n)\nax.tick_params(axis=\"y\", labelcolor=\"k\")\nax.set_title(\n    f\"Theta band power, electode {electrode_id}\",\n    fontsize=20,\n);\n</pre> plot_start = 0 plot_end = 5000 fig, ax = plt.subplots(figsize=(20, 6)) ax.set_xlabel(\"Time (sec)\", fontsize=15) ax.set_ylabel(\"Theta power\", fontsize=15) ax.plot(     theta_power.index[plot_start:plot_end],     theta_power[f\"electrode {electrode_id}\"].iloc[plot_start:plot_end],     \"k-\",     linewidth=1, ) ax.tick_params(axis=\"y\", labelcolor=\"k\") ax.set_title(     f\"Theta band power, electode {electrode_id}\",     fontsize=20, ); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/13_Theta_phase_and_power/#theta-phase-detection-and-theta-power-computation", "title": "Theta phase detection and theta power computation\u00b6", "text": "<p>In this tutorial, we demonstrate how to generate analytic signals from the LFP data, as well as how to compute theta phases and power, all of which depend on methods built into the LFPBand table. Let's import the LFPBand table:</p>"}, {"location": "notebooks/13_Theta_phase_and_power/#1-acquire-theta-band-analytic-signal-from-the-lfpband-data-for-electrodes-we-want-to-perform-theta-analyses-with", "title": "1. Acquire theta band analytic signal from the LFPBand data for electrodes we want to perform theta analyses with.\u00b6", "text": ""}, {"location": "notebooks/13_Theta_phase_and_power/#2-using-a-similar-method-we-can-compute-theta-phase-and-power-from-the-lfpband-table", "title": "2. Using a similar method, we can compute theta phase and power from the LFPBand table.\u00b6", "text": ""}, {"location": "notebooks/13_Theta_phase_and_power/#3-plot-the-results-overlay-theta-and-detected-phase-for-one-example-lfp-electrode", "title": "3. Plot the results: overlay theta and detected phase for one example lfp electrode.\u00b6", "text": "<p>Note that the red horizontal line indicates phase 0 (corresponding to the trough of theta).</p>"}, {"location": "notebooks/4_position_info/", "title": "Information", "text": "In\u00a0[\u00a0]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\n</pre> %reload_ext autoreload %autoreload 2 <p>This data pipeline takes the 2D position (in video pixels) of the green and red LEDs tracked by Trodes, and computes:</p> <ul> <li>head position (in cm)</li> <li>head orientation (in radians)</li> <li>head velocity (in cm/s)</li> <li>head speed (in cm/s)</li> </ul> <p>We can then check the quality of the head position and direction by plotting the them on the video along with the oringal green and red LEDs.</p> <p>This notebook will take you through this process for one dataset.</p> In\u00a0[2]: Copied! <pre>import spyglass as nd\n\nnwb_file_name = \"chimi20200216_new.nwb\"\n\nnd.insert_sessions(nwb_file_name)\n</pre> import spyglass as nd  nwb_file_name = \"chimi20200216_new.nwb\"  nd.insert_sessions(nwb_file_name) <pre>[2022-08-04 16:16:09,213][INFO]: Connecting zoldello@lmf-db.cin.ucsf.edu:3306\n[2022-08-04 16:16:09,264][INFO]: Connected zoldello@lmf-db.cin.ucsf.edu:3306\n</pre> <pre>/home/zoldello/anaconda3/envs/spyglass/lib/python3.9/site-packages/position_tools/core.py:3: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n  from scipy.ndimage.filters import gaussian_filter1d\n</pre> <p>After insert, a new nwb file is made that is a copy of the original version without the electrophysiological data. This name is used as a key in the downstream data analysis so we save the name of the copied version for later use.</p> In\u00a0[3]: Copied! <pre>from spyglass.common.nwb_helper_fn import get_nwb_copy_filename\n\nnwb_copy_file_name = get_nwb_copy_filename(nwb_file_name)\nnwb_copy_file_name\n</pre> from spyglass.common.nwb_helper_fn import get_nwb_copy_filename  nwb_copy_file_name = get_nwb_copy_filename(nwb_file_name) nwb_copy_file_name Out[3]: <pre>'chimi20200216_new_.nwb'</pre> In\u00a0[4]: Copied! <pre>from spyglass.common.common_position import PositionInfoParameters\n\nPositionInfoParameters()\n</pre> from spyglass.common.common_position import PositionInfoParameters  PositionInfoParameters() Out[4]: <p>position_info_param_name</p> name for this set of parameters <p>max_separation</p> max distance (in cm) between head LEDs <p>max_speed</p> max speed (in cm / s) of animal <p>position_smoothing_duration</p> size of moving window (in seconds) <p>speed_smoothing_std_dev</p> smoothing standard deviation (in seconds) <p>head_orient_smoothing_std_dev</p> smoothing std deviation (in seconds) <p>led1_is_front</p> first LED is front LED and second is back LED, else first LED is back <p>is_upsampled</p> upsample the position to higher sampling rate <p>upsampling_sampling_rate</p> The rate to be upsampled to <p>upsampling_interpolation_method</p> see pandas.DataFrame.interpolation for list of methods default 9.0 300.0 0.125 0.1 0.001 1 0 nan lineardefault_decoding 9.0 300.0 0.125 0.1 0.001 1 1 500.0 lineardefault_lfp 9.0 300.0 0.125 0.1 0.001 1 1 1000.0 linear <p>Total: 3</p> In\u00a0[5]: Copied! <pre>PositionInfoParameters.insert1(\n    {\"position_info_param_name\": \"default\"}, skip_duplicates=True\n)\nPositionInfoParameters()\n</pre> PositionInfoParameters.insert1(     {\"position_info_param_name\": \"default\"}, skip_duplicates=True ) PositionInfoParameters() Out[5]: <p>position_info_param_name</p> name for this set of parameters <p>max_separation</p> max distance (in cm) between head LEDs <p>max_speed</p> max speed (in cm / s) of animal <p>position_smoothing_duration</p> size of moving window (in seconds) <p>speed_smoothing_std_dev</p> smoothing standard deviation (in seconds) <p>head_orient_smoothing_std_dev</p> smoothing std deviation (in seconds) <p>led1_is_front</p> first LED is front LED and second is back LED, else first LED is back <p>is_upsampled</p> upsample the position to higher sampling rate <p>upsampling_sampling_rate</p> The rate to be upsampled to <p>upsampling_interpolation_method</p> see pandas.DataFrame.interpolation for list of methods default 9.0 300.0 0.125 0.1 0.001 1 0 nan lineardefault_decoding 9.0 300.0 0.125 0.1 0.001 1 1 500.0 lineardefault_lfp 9.0 300.0 0.125 0.1 0.001 1 1 1000.0 linear <p>Total: 3</p> <p>Now that you have set the parameters, you need to associate those parameters with a partiuclar nwb file and interval using the <code>IntervalPositionInfoSelection</code> table in order to run the position pipeline.</p> <p>Let's look at the intervals for <code>chimi20200216_new.nwb</code> first. We care about the intervals that start with <code>pos</code> and ending with <code>valid times</code>.</p> In\u00a0[6]: Copied! <pre>import pandas as pd\n\npd.DataFrame(nd.common.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name})\n</pre> import pandas as pd  pd.DataFrame(nd.common.IntervalList &amp; {\"nwb_file_name\": nwb_copy_file_name}) Out[6]: nwb_file_name interval_list_name valid_times 0 chimi20200216_new_.nwb 01_s1 [[1581884110.9124804, 1581886102.599356]] 1 chimi20200216_new_.nwb 02_r1 [[1581886847.561317, 1581888227.5987928]] 2 chimi20200216_new_.nwb 02_r1 noPrePostTrialTimes [[1581886896.4185014, 1581888227.5987928]] 3 chimi20200216_new_.nwb 03_s2 [[1581888339.9145422, 1581890273.3987277]] 4 chimi20200216_new_.nwb 04_r2 [[1581890568.8618634, 1581891934.598601]] ... ... ... ... 169 chimi20200216_new_.nwb R_fabb5ef8_ampl_2000_prop_75_artifact_removed_... [[1581886896.4185073, 1581888227.5987911]] 170 chimi20200216_new_.nwb sleeps [[1581884110.9124804, 1581886102.599356], [158... 171 chimi20200216_new_.nwb sleeps_runs_noPrePostTrialTimes [[1581884110.9124804, 1581886102.599356], [158... 172 chimi20200216_new_.nwb sleeps_runs_noPrePostTrialTimes raw data valid... [[1581884110.9124804, 1581886102.599356], [158... 173 chimi20200216_new_.nwb sleeps_runs_noPrePostTrialTimes raw data valid... [[1581884110.9134343, 1581886102.599356], [158... <p>174 rows \u00d7 3 columns</p> <p>Let's choose the interval corresponding to <code>pos 1 valid times</code> in <code>chimi20200216_new_.nwb</code>.</p> <p>We first look at the \"raw\" position data now to see the input into the pipeline. The raw position is in the <code>RawPosition</code> table and corresponds to the inferred position of the red and green LEDs from the video (using an algorithm in Trodes). It is in units of pixels. The number of time points corresponds to when the position tracking was turned on and off (and so may not have the same number of time points as the video itself).</p> <p>We can retrieve the data in the <code>RawPosition</code> table for a given interval using a special method called <code>fetch1_dataframe</code>. It returns the position of the red and green LEDs as a pandas dataframe where time is the index. The columns of the dataframe are:</p> <ul> <li><code>xloc</code>, <code>yloc</code> are the x- and y-position of one of the LEDs</li> <li><code>xloc2</code>, <code>yloc2</code> are the x- and y-position of the other LEDs.</li> </ul> In\u00a0[7]: Copied! <pre>from spyglass.common.common_behav import RawPosition\n\nraw_position_df = (\n    RawPosition()\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n    }\n).fetch1_dataframe()\nraw_position_df\n</pre> from spyglass.common.common_behav import RawPosition  raw_position_df = (     RawPosition()     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",     } ).fetch1_dataframe() raw_position_df <pre>[2022-08-04 16:16:12,541][WARNING]: Skipped checksum for file with hash: 050179d6-42a0-9236-45ee-6069346d0196, and path: /stelmo/nwb/raw/chimi20200216_new_.nwb\n</pre> <pre>/stelmo/nwb/raw/chimi20200216_new_.nwb\n</pre> Out[7]: xloc yloc xloc2 yloc2 time 1.581887e+09 264 638 285 635 1.581887e+09 264 672 269 635 1.581887e+09 264 638 280 642 1.581887e+09 268 635 284 636 1.581887e+09 268 674 283 636 ... ... ... ... ... 1.581888e+09 556 598 543 616 1.581888e+09 555 598 543 616 1.581888e+09 556 598 543 616 1.581888e+09 555 598 543 616 1.581888e+09 556 598 542 615 <p>39340 rows \u00d7 4 columns</p> <p>Let's just quickly plot the two LEDs to get a sense of the inputs to the pipeline:</p> In\u00a0[8]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(raw_position_df.xloc, raw_position_df.yloc, color=\"green\")\nax.plot(raw_position_df.xloc2, raw_position_df.yloc2, color=\"red\")\nax.set_xlabel(\"x-position [pixels]\", fontsize=18)\nax.set_ylabel(\"y-position [pixels]\", fontsize=18)\nax.set_title(\"Raw Position\", fontsize=28)\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(raw_position_df.xloc, raw_position_df.yloc, color=\"green\") ax.plot(raw_position_df.xloc2, raw_position_df.yloc2, color=\"red\") ax.set_xlabel(\"x-position [pixels]\", fontsize=18) ax.set_ylabel(\"y-position [pixels]\", fontsize=18) ax.set_title(\"Raw Position\", fontsize=28) Out[8]: <pre>Text(0.5, 1.0, 'Raw Position')</pre> <p>Okay, now that we understand what the inputs to the pipeline are, let's associate a set of parameters with a given interval.</p> <p>To associate parameters with a given interval, we insert them into the <code>IntervalPositionInfoSelection</code> table.</p> <p>Here we associate the with the <code>default</code> position info parameters with the interval <code>pos 1 valid times</code>:</p> In\u00a0[9]: Copied! <pre>from spyglass.common.common_position import IntervalPositionInfoSelection\n\nIntervalPositionInfoSelection.insert1(\n    {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"position_info_param_name\": \"default\",\n    },\n    skip_duplicates=True,\n)\n</pre> from spyglass.common.common_position import IntervalPositionInfoSelection  IntervalPositionInfoSelection.insert1(     {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"position_info_param_name\": \"default\",     },     skip_duplicates=True, ) <p>Now let's check to see if we've inserted the parameters correctly:</p> In\u00a0[10]: Copied! <pre>IntervalPositionInfoSelection()\n</pre> IntervalPositionInfoSelection() Out[10]: <p>position_info_param_name</p> name for this set of parameters <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list default_decoding CH6120211203_.nwb pos 0 valid timesdefault_decoding CH6520211201_.nwb pos 0 valid timesdefault chimi20200216_new_.nwb pos 1 valid timesdefault_decoding chimi20200216_new_.nwb pos 1 valid timesdefault_lfp chimi20200216_new_.nwb pos 1 valid timesdefault fern20211007_.nwb pos 0 valid timesdefault_decoding fern20211007_.nwb pos 0 valid timesdefault fern20211007_.nwb pos 1 valid timesdefault_decoding fern20211007_.nwb pos 1 valid timesdefault fern20211007_.nwb pos 10 valid timesdefault_decoding fern20211007_.nwb pos 10 valid timesdefault fern20211007_.nwb pos 11 valid times <p>...</p> <p>Total: 1814</p> In\u00a0[11]: Copied! <pre>from spyglass.common.common_position import IntervalPositionInfo\n\nIntervalPositionInfo.populate()\n</pre> from spyglass.common.common_position import IntervalPositionInfo  IntervalPositionInfo.populate() <p>We can see that each NWB file, interval, and parameter set is now associated with a newly created analysis NWB file and object IDs that correspond to our newly computed data. This isn't as useful to work with so we will use another method below to actually retrieve the data for a given interval.</p> In\u00a0[12]: Copied! <pre>IntervalPositionInfo()\n</pre> IntervalPositionInfo() Out[12]: <p>position_info_param_name</p> name for this set of parameters <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list <p>analysis_file_name</p> name of the file <p>head_position_object_id</p> <p>head_orientation_object_id</p> <p>head_velocity_object_id</p> default chimi20200216_new_.nwb pos 1 valid times chimi20200216_new_ZF24K2JHC1.nwb 92a087ca-3df6-49f9-b544-b304dc4a6f26 65199939-b878-457e-836a-98da1ffbf244 a876e869-4eb5-4c4b-9ff5-a50914ff1b45default fern20211007_.nwb pos 0 valid times fern20211007_0V93CW9TIT.nwb f4a20aa2-20a0-4a73-b42d-5a96fff32a13 e835e832-86b0-41a3-9d8e-bf87b5c55e9d d1e0ae3c-4c05-47e2-89fb-3951d70eece9default fern20211007_.nwb pos 1 valid times fern20211007_GF77PRAGTB.nwb ae3317fa-bbd3-4a3a-9279-af969647972f eedee7f8-290b-4a22-92bd-564f83fde9b6 86db909f-fb0d-4db5-883b-b1e20c41a72ddefault fern20211007_.nwb pos 10 valid times fern20211007_I01IPA5QPF.nwb 8b7722cb-fa73-4d34-9617-86a193218bfd 74d5e5a8-d736-4fd0-ad73-fcdf644b5fdf 7e67fbee-f766-4658-be2b-e8da9a377796default fern20211007_.nwb pos 11 valid times fern20211007_504348DO5F.nwb c294ea11-10a8-46a7-8e03-b10859783239 a703e06b-b4f9-42e6-8e19-b0366e606de3 228da90d-3442-40e0-8311-bdd08b3aa647default fern20211007_.nwb pos 12 valid times fern20211007_YJ7FEZ5V4M.nwb e8b7017f-745e-4131-923b-3afaf5f62a9b db509824-cd50-4f4a-9a0c-3b7e766daac1 85064b3e-2cb7-4723-80bd-9c0e36141d6adefault fern20211007_.nwb pos 13 valid times fern20211007_0HXO9LW0QP.nwb 62c1dbc6-8d03-4d02-9e7a-185edb796182 11734fa1-27c8-41d3-9970-b0db9239b5a8 919e565b-7937-488c-a484-c745fe9f4a52default fern20211007_.nwb pos 14 valid times fern20211007_E8IV6HVDZL.nwb 93e1a36b-bd8a-45ec-a4c5-0d42c411c63a f5796260-9bb1-41a5-b272-ac51c645e781 3bac5d8d-d103-4aa8-925e-bb968558b9f3default fern20211007_.nwb pos 15 valid times fern20211007_BN0LH7DV4K.nwb c0ead339-9380-4201-b0ab-d694e440425f 36c65b2e-a2db-414e-bbe6-9a0d851a79b2 d122d225-a1ba-494f-9ca7-9d8cbb0411f6default fern20211007_.nwb pos 16 valid times fern20211007_WRBSHYU23X.nwb 71d010f3-9e46-400c-92e5-04d48829cb47 2c3542ee-18fb-4331-8aff-f514258530e0 842bbba1-6e57-47ed-afcb-7ba84d4033afdefault fern20211007_.nwb pos 17 valid times fern20211007_JJHMRY754E.nwb d82418cf-98ca-43e1-ac3c-c08bd28fa70f 6e19cd2b-f134-4f73-9dae-12d7296c58f5 e100713e-5cdb-42c5-bc9f-60df76aa26b0default fern20211007_.nwb pos 18 valid times fern20211007_6CBPQXBDFJ.nwb edb09afb-62e1-4948-b62c-e94254275845 6c488035-0709-482d-99ef-c4c698d0e0cd 9a0d60f8-60d4-436d-940a-5c5fa83d9d76 <p>...</p> <p>Total: 1814</p> <p>In order to retrieve the results of the computation, we use a special method called <code>fetch1_dataframe</code> from the <code>IntervalPositionInfo</code> table that will retrieve the position pipeline results as a pandas DataFrame. Time is set as the index of the dataframe.</p> <p>This will only work for a single interval so we need to specify the NWB file and the interval.</p> <p>This dataframe has the following columns:</p> <ul> <li><code>head_position_x</code>, <code>head_position_y</code>: the x,y position of the head position (in cm).</li> <li><code>head_orientation</code>: The direction of the head relative to the bottom left corner (in radians)</li> <li><code>head_velocity_x</code>, <code>head_velocity_y</code>: the directional change in head position over time (in cm/s)</li> <li><code>head_speed</code>: the magnitude of the change in head position over time (in cm/s)</li> </ul> In\u00a0[13]: Copied! <pre>position_info = (\n    IntervalPositionInfo()\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"position_info_param_name\": \"default\",\n    }\n).fetch1_dataframe()\nposition_info\n</pre> position_info = (     IntervalPositionInfo()     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"position_info_param_name\": \"default\",     } ).fetch1_dataframe() position_info <pre>/stelmo/nwb/analysis/chimi20200216_new_ZF24K2JHC1.nwb\n</pre> Out[13]: head_position_x head_position_y head_orientation head_velocity_x head_velocity_y head_speed time 1.581887e+09 91.051650 211.127050 2.999696 1.387074 2.848838 3.168573 1.581887e+09 90.844337 211.417287 3.078386 3.123201 3.411111 4.624939 1.581887e+09 90.637025 211.707525 -3.114572 5.431643 4.089597 6.799085 1.581887e+09 90.802875 211.596958 -3.033109 8.097753 4.979262 9.506138 1.581887e+09 91.288579 211.482443 -3.062550 10.840482 6.071373 12.424880 ... ... ... ... ... ... ... 1.581888e+09 182.158583 201.452467 -0.986926 0.348276 0.218575 0.411182 1.581888e+09 182.158583 201.397183 -0.978610 0.279135 -0.058413 0.285182 1.581888e+09 182.213867 201.341900 -0.957589 0.193798 -0.283200 0.343162 1.581888e+09 182.158583 201.341900 -0.970083 0.110838 -0.417380 0.431846 1.581888e+09 182.158583 201.286617 -0.936414 0.045190 -0.453966 0.456209 <p>39340 rows \u00d7 6 columns</p> <p>If you are not familiar with pandas, the time variable is set as the index. It can be accessed using <code>.index</code> on the dataframe.</p> In\u00a0[14]: Copied! <pre>position_info.index\n</pre> position_info.index Out[14]: <pre>Float64Index([1581886916.3153033, 1581886916.3486283, 1581886916.3819742,\n              1581886916.4152992,  1581886916.448645, 1581886916.4819698,\n              1581886916.5152948, 1581886916.5486405, 1581886916.5819652,\n              1581886916.6152902,\n              ...\n              1581888227.3021932,  1581888227.335518,  1581888227.368864,\n              1581888227.4021888,  1581888227.435535, 1581888227.4688597,\n              1581888227.5021844, 1581888227.5355306, 1581888227.5688553,\n                1581888227.60218],\n             dtype='float64', name='time', length=39340)</pre> In\u00a0[15]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(position_info.head_position_x, position_info.head_position_y)\nax.set_xlabel(\"x-position [cm]\", fontsize=18)\nax.set_ylabel(\"y-position [cm]\", fontsize=18)\nax.set_title(\"Head Position\", fontsize=28)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(position_info.head_position_x, position_info.head_position_y) ax.set_xlabel(\"x-position [cm]\", fontsize=18) ax.set_ylabel(\"y-position [cm]\", fontsize=18) ax.set_title(\"Head Position\", fontsize=28) Out[15]: <pre>Text(0.5, 1.0, 'Head Position')</pre> In\u00a0[16]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nax.plot(position_info.head_velocity_x, position_info.head_velocity_y)\nax.set_xlabel(\"x-velocity [cm/s]\", fontsize=18)\nax.set_ylabel(\"y-velocity [cm/s]\", fontsize=18)\nax.set_title(\"Head Velocity\", fontsize=28)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.plot(position_info.head_velocity_x, position_info.head_velocity_y) ax.set_xlabel(\"x-velocity [cm/s]\", fontsize=18) ax.set_ylabel(\"y-velocity [cm/s]\", fontsize=18) ax.set_title(\"Head Velocity\", fontsize=28) Out[16]: <pre>Text(0.5, 1.0, 'Head Velocity')</pre> In\u00a0[17]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(25, 3))\nax.plot(position_info.index, position_info.head_speed)\nax.set_xlabel(\"Time\", fontsize=18)\nax.set_ylabel(\"Speed [cm/s]\", fontsize=18)\nax.set_title(\"Head Speed\", fontsize=28)\nax.set_xlim((position_info.index.min(), position_info.index.max()))\n</pre> fig, ax = plt.subplots(1, 1, figsize=(25, 3)) ax.plot(position_info.index, position_info.head_speed) ax.set_xlabel(\"Time\", fontsize=18) ax.set_ylabel(\"Speed [cm/s]\", fontsize=18) ax.set_title(\"Head Speed\", fontsize=28) ax.set_xlim((position_info.index.min(), position_info.index.max())) Out[17]: <pre>(1581886916.3153033, 1581888227.60218)</pre> In\u00a0[24]: Copied! <pre>from spyglass.common.common_position import PositionVideo\n\nPositionVideo().make(\n    {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"position_info_param_name\": \"default\",\n    }\n)\n</pre> from spyglass.common.common_position import PositionVideo  PositionVideo().make(     {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"position_info_param_name\": \"default\",     } ) <pre>[2022-08-04 15:05:53,003][WARNING]: Skipped checksum for file with hash: 050179d6-42a0-9236-45ee-6069346d0196, and path: /stelmo/nwb/raw/chimi20200216_new_.nwb\n</pre> <pre>Loading position data...\nLoading video data...\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nInput In [24], in &lt;cell line: 3&gt;()\n      1 from spyglass.common.common_position import PositionVideo\n----&gt; 3 PositionVideo().make({'nwb_file_name': nwb_copy_file_name,\n      4 'interval_list_name': 'pos 1 valid times',\n      5 'position_info_param_name': 'default'\n      6 })\n\nFile ~/Documents/code/spyglass/src/spyglass/common/common_position.py:631, in PositionVideo.make(self, key)\n    628 io = pynwb.NWBHDF5IO('/stelmo/nwb/raw/' +\n    629                      video_info['nwb_file_name'], 'r')\n    630 nwb_file = io.read()\n--&gt; 631 nwb_video = nwb_file.objects[video_info['video_file_object_id']]\n    632 video_filename = nwb_video.external_file.value[0]\n    634 nwb_base_filename = key['nwb_file_name'].replace('.nwb', '')\n\nFile ~/anaconda3/envs/spyglass/lib/python3.9/site-packages/hdmf/utils.py:932, in LabelledDict.__getitem__(self, args)\n    930         return super().__getitem__(val)\n    931 else:\n--&gt; 932     return super().__getitem__(key)\n\nKeyError: '3234c1e5-c992-4fb4-9c82-d1603d90bce7'</pre> In\u00a0[25]: Copied! <pre>PositionInfoParameters.insert1(\n    {\n        \"position_info_param_name\": \"default_decoding\",\n        \"is_upsampled\": 1,\n        \"upsampling_sampling_rate\": 500,\n    },\n    skip_duplicates=True,\n)\n\nPositionInfoParameters()\n</pre> PositionInfoParameters.insert1(     {         \"position_info_param_name\": \"default_decoding\",         \"is_upsampled\": 1,         \"upsampling_sampling_rate\": 500,     },     skip_duplicates=True, )  PositionInfoParameters() Out[25]: <p>position_info_param_name</p> name for this set of parameters <p>max_separation</p> max distance (in cm) between head LEDs <p>max_speed</p> max speed (in cm / s) of animal <p>position_smoothing_duration</p> size of moving window (in seconds) <p>speed_smoothing_std_dev</p> smoothing standard deviation (in seconds) <p>head_orient_smoothing_std_dev</p> smoothing std deviation (in seconds) <p>led1_is_front</p> first LED is front LED and second is back LED, else first LED is back <p>is_upsampled</p> upsample the position to higher sampling rate <p>upsampling_sampling_rate</p> The rate to be upsampled to <p>upsampling_interpolation_method</p> see pandas.DataFrame.interpolation for list of methods default 9.0 300.0 0.125 0.1 0.001 1 0 nan lineardefault_decoding 9.0 300.0 0.125 0.1 0.001 1 1 500.0 lineardefault_lfp 9.0 300.0 0.125 0.1 0.001 1 1 1000.0 linear <p>Total: 3</p> In\u00a0[26]: Copied! <pre>IntervalPositionInfoSelection.insert1(\n    {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"position_info_param_name\": \"default_decoding\",\n    },\n    skip_duplicates=True,\n)\n\nIntervalPositionInfoSelection()\n</pre> IntervalPositionInfoSelection.insert1(     {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"position_info_param_name\": \"default_decoding\",     },     skip_duplicates=True, )  IntervalPositionInfoSelection() Out[26]: <p>position_info_param_name</p> name for this set of parameters <p>nwb_file_name</p> name of the NWB file <p>interval_list_name</p> descriptive name of this interval list default_decoding CH6120211203_.nwb pos 0 valid timesdefault_decoding CH6520211125_.nwb pos 0 valid timesdefault_decoding CH6520211201_.nwb pos 0 valid timesdefault chimi20200216_new_.nwb pos 1 valid timesdefault_decoding chimi20200216_new_.nwb pos 1 valid timesdefault_lfp chimi20200216_new_.nwb pos 1 valid timesdefault fern20211007_.nwb pos 0 valid timesdefault_decoding fern20211007_.nwb pos 0 valid timesdefault fern20211007_.nwb pos 1 valid timesdefault_decoding fern20211007_.nwb pos 1 valid timesdefault fern20211007_.nwb pos 10 valid timesdefault_decoding fern20211007_.nwb pos 10 valid times <p>...</p> <p>Total: 1815</p> In\u00a0[27]: Copied! <pre>IntervalPositionInfo.populate()\n</pre> IntervalPositionInfo.populate() In\u00a0[28]: Copied! <pre>upsampled_position_info = (\n    IntervalPositionInfo()\n    &amp; {\n        \"nwb_file_name\": nwb_copy_file_name,\n        \"interval_list_name\": \"pos 1 valid times\",\n        \"position_info_param_name\": \"default_decoding\",\n    }\n).fetch1_dataframe()\n\nupsampled_position_info\n</pre> upsampled_position_info = (     IntervalPositionInfo()     &amp; {         \"nwb_file_name\": nwb_copy_file_name,         \"interval_list_name\": \"pos 1 valid times\",         \"position_info_param_name\": \"default_decoding\",     } ).fetch1_dataframe()  upsampled_position_info <pre>/stelmo/nwb/analysis/chimi20200216_new_6YC9LPAR7S.nwb\n</pre> Out[28]: head_position_x head_position_y head_orientation head_velocity_x head_velocity_y head_speed time 1.581887e+09 91.051650 211.127050 2.680048 1.741550 2.301478 2.886139 1.581887e+09 91.039455 211.144123 3.003241 1.827555 2.333931 2.964320 1.581887e+09 91.027260 211.161196 3.008398 1.915800 2.366668 3.044898 1.581887e+09 91.015065 211.178268 3.012802 2.006286 2.399705 3.127901 1.581887e+09 91.002871 211.195341 3.017242 2.099012 2.433059 3.213352 ... ... ... ... ... ... ... 1.581888e+09 182.158583 201.299625 -0.944304 0.057520 -0.356012 0.360629 1.581888e+09 182.158583 201.296373 -0.942329 0.053954 -0.356343 0.360404 1.581888e+09 182.158583 201.293121 -0.940357 0.050477 -0.356407 0.359964 1.581888e+09 182.158583 201.289869 -0.953059 0.047091 -0.356212 0.359312 1.581888e+09 182.158583 201.286617 -0.588081 0.043796 -0.355764 0.358450 <p>655645 rows \u00d7 6 columns</p> In\u00a0[24]: Copied! <pre>fig, axes = plt.subplots(\n    1, 2, figsize=(20, 10), sharex=True, sharey=True, constrained_layout=True\n)\naxes[0].plot(position_info.head_position_x, position_info.head_position_y)\naxes[0].set_xlabel(\"x-position [cm]\", fontsize=18)\naxes[0].set_ylabel(\"y-position [cm]\", fontsize=18)\naxes[0].set_title(\"Head Position\", fontsize=28)\n\naxes[1].plot(\n    upsampled_position_info.head_position_x,\n    upsampled_position_info.head_position_y,\n)\naxes[1].set_xlabel(\"x-position [cm]\", fontsize=18)\naxes[1].set_ylabel(\"y-position [cm]\", fontsize=18)\naxes[1].set_title(\"Upsampled Head Position\", fontsize=28)\n</pre> fig, axes = plt.subplots(     1, 2, figsize=(20, 10), sharex=True, sharey=True, constrained_layout=True ) axes[0].plot(position_info.head_position_x, position_info.head_position_y) axes[0].set_xlabel(\"x-position [cm]\", fontsize=18) axes[0].set_ylabel(\"y-position [cm]\", fontsize=18) axes[0].set_title(\"Head Position\", fontsize=28)  axes[1].plot(     upsampled_position_info.head_position_x,     upsampled_position_info.head_position_y, ) axes[1].set_xlabel(\"x-position [cm]\", fontsize=18) axes[1].set_ylabel(\"y-position [cm]\", fontsize=18) axes[1].set_title(\"Upsampled Head Position\", fontsize=28) Out[24]: <pre>Text(0.5, 1.0, 'Upsampled Head Position')</pre> In\u00a0[25]: Copied! <pre>fig, axes = plt.subplots(\n    2, 1, figsize=(25, 6), sharex=True, sharey=True, constrained_layout=True\n)\naxes[0].plot(position_info.index, position_info.head_speed)\naxes[0].set_xlabel(\"Time\", fontsize=18)\naxes[0].set_ylabel(\"Speed [cm/s]\", fontsize=18)\naxes[0].set_title(\"Head Speed\", fontsize=28)\naxes[0].set_xlim((position_info.index.min(), position_info.index.max()))\n\naxes[1].plot(upsampled_position_info.index, upsampled_position_info.head_speed)\naxes[1].set_xlabel(\"Time\", fontsize=18)\naxes[1].set_ylabel(\"Speed [cm/s]\", fontsize=18)\naxes[1].set_title(\"Upsampled Head Speed\", fontsize=28)\n</pre> fig, axes = plt.subplots(     2, 1, figsize=(25, 6), sharex=True, sharey=True, constrained_layout=True ) axes[0].plot(position_info.index, position_info.head_speed) axes[0].set_xlabel(\"Time\", fontsize=18) axes[0].set_ylabel(\"Speed [cm/s]\", fontsize=18) axes[0].set_title(\"Head Speed\", fontsize=28) axes[0].set_xlim((position_info.index.min(), position_info.index.max()))  axes[1].plot(upsampled_position_info.index, upsampled_position_info.head_speed) axes[1].set_xlabel(\"Time\", fontsize=18) axes[1].set_ylabel(\"Speed [cm/s]\", fontsize=18) axes[1].set_title(\"Upsampled Head Speed\", fontsize=28) Out[25]: <pre>Text(0.5, 1.0, 'Upsampled Head Speed')</pre> In\u00a0[26]: Copied! <pre>fig, axes = plt.subplots(\n    1, 2, figsize=(20, 10), sharex=True, sharey=True, constrained_layout=True\n)\naxes[0].plot(position_info.head_velocity_x, position_info.head_velocity_y)\naxes[0].set_xlabel(\"x-velocity [cm/s]\", fontsize=18)\naxes[0].set_ylabel(\"y-velocity [cm/s]\", fontsize=18)\naxes[0].set_title(\"Head Velocity\", fontsize=28)\n\naxes[1].plot(\n    upsampled_position_info.head_velocity_x,\n    upsampled_position_info.head_velocity_y,\n)\naxes[1].set_xlabel(\"x-velocity [cm/s]\", fontsize=18)\naxes[1].set_ylabel(\"y-velocity [cm/s]\", fontsize=18)\naxes[1].set_title(\"Upsampled Head Velocity\", fontsize=28)\n</pre> fig, axes = plt.subplots(     1, 2, figsize=(20, 10), sharex=True, sharey=True, constrained_layout=True ) axes[0].plot(position_info.head_velocity_x, position_info.head_velocity_y) axes[0].set_xlabel(\"x-velocity [cm/s]\", fontsize=18) axes[0].set_ylabel(\"y-velocity [cm/s]\", fontsize=18) axes[0].set_title(\"Head Velocity\", fontsize=28)  axes[1].plot(     upsampled_position_info.head_velocity_x,     upsampled_position_info.head_velocity_y, ) axes[1].set_xlabel(\"x-velocity [cm/s]\", fontsize=18) axes[1].set_ylabel(\"y-velocity [cm/s]\", fontsize=18) axes[1].set_title(\"Upsampled Head Velocity\", fontsize=28) Out[26]: <pre>Text(0.5, 1.0, 'Upsampled Head Velocity')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "notebooks/4_position_info/#position-pipeline", "title": "Position Pipeline\u00b6", "text": ""}, {"location": "notebooks/4_position_info/#1-loading-the-session-data", "title": "1. Loading the session data\u00b6", "text": "<p>First let us make sure that the session we want to analyze is inserted into the database:</p>"}, {"location": "notebooks/4_position_info/#2-setting-the-parameters-for-running-the-position-pipeline", "title": "2. Setting the parameters for running the position pipeline\u00b6", "text": "<p>The parameters for the position pipeline are set by the <code>PositionInfoParameters</code> table. <code>default</code> is the name of the standard set of parameters. As usual, if you want to change the parameters, you can insert your own into the table.</p> <p>The parameters are as follows:</p> <ul> <li><code>max_separation</code> is the maxmium acceptable distance (in cm) between the red and green LEDs. When the distance between the LEDs becomes greater than this number, the times are marked as NaNs and inferred by interpolation. This is useful parameter when the inferred red or green LED position tracks a reflection instead of the true red or green LED position. It is set to 9.0 cm by default.</li> <li><code>max_speed</code> is the maximum plausible speed (in cm/s) the animal can move at. Times when the speed is greater than this threshold are marked as NaNs and inferred by interpolation. This can be useful in preventing big, sudden jumps in position. It is set to 300.0 cm/s by default.</li> <li><code>position_smoothing_duration</code> controls how much the red and green LEDs are smoothed before computing the average of their position to get the head position. It is in units of seconds.</li> <li><code>speed_smoothing_std_dev</code> controls how much the head speed is smoothed. It corresponds to the standard deviation of the Gaussian kernel used to smooth the speed and is in units of seconds. It is set to 0.100 seconds by default.</li> <li><code>front_led1</code> is either 1 or 0 indicating True or False. It controls which LED is treated as the front LED and the back LED, which is important for calculating the head direction.<ul> <li>1 indicates that the LED corresponding to <code>xloc</code>, <code>yloc</code> in the <code>RawPosition</code> table as the front LED and the LED corresponding to <code>xloc2</code>, <code>yloc2</code> as the back LED.</li> <li>0 indicates that <code>xloc</code>, <code>yloc</code> are treated as the back LED and <code>xloc2</code>, <code>yloc2</code> are treated as the front LED.</li> </ul> </li> </ul>"}, {"location": "notebooks/4_position_info/#3-running-the-position-pipeline-and-retrieving-the-results", "title": "3. Running the position pipeline and retrieving the results\u00b6", "text": "<p>Now that we have associated the parameters with the interval we want to run, we can finally run the pipeline for that interval.</p> <p>We run the pipeline using the <code>populate</code> method on the <code>IntervalPositionInfo</code> table.</p>"}, {"location": "notebooks/4_position_info/#4-examining-the-results", "title": "4. Examining the results\u00b6", "text": "<p>We should always spot check our results to verify that the pipeline worked correctly.</p>"}, {"location": "notebooks/4_position_info/#plots", "title": "Plots\u00b6", "text": "<p>Let's plot some of the variables first:</p>"}, {"location": "notebooks/4_position_info/#video", "title": "Video\u00b6", "text": "<p>These look reasonable but it is better to evaluate these variables by plotting the results on the video where we can see how they correspond.</p> <p>The video will appear in the current working directory when it is done.</p>"}, {"location": "notebooks/4_position_info/#4-upsampling-position-data", "title": "4. Upsampling position data\u00b6", "text": "<p>Sometimes you need the position data to be in a different rate than it is sampled in, such as when decoding in 2 ms time bins. You can use the upsampling parameters to get this data:</p> <ul> <li><code>is_upsampled</code> controls whether upsampling happens. If it is 1 then there is upsampling, and if it is 0 then upsampling does not happen.</li> <li><code>upsampling_sampling_rate</code> is the rate you want to upsample to. For example position is typically recorded at 33 frames per seconds and you may want to upsample up to 500 frames per second.</li> <li><code>upsampling_interpolation_method</code> is the interpolation method used for upsampling. It is set to linear by default. See the methods available for pandas.DataFrame.interpolate to get a list of the methods.</li> </ul>"}, {"location": "notebooks/Spyglass_kachery_setup/", "title": "Spyglass Kachery Setup", "text": "In\u00a0[\u00a0]: Copied! <pre>import spyglass as sg\nfrom spyglass.common import Lab, AnalysisNwbfile\nfrom spyglass.sharing import (KacheryZone, AnalysisNwbfileKachery, AnalysisNwbfileKacherySelection)\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre>   import spyglass as sg from spyglass.common import Lab, AnalysisNwbfile from spyglass.sharing import (KacheryZone, AnalysisNwbfileKachery, AnalysisNwbfileKacherySelection) import warnings warnings.filterwarnings('ignore')  <p>We start by inserting into the KacheryZone table, which will be empty if you're just starting out. change the entries in zone_key below to match your zone.</p> <p>Note that the lab_name is a reference to the Lab() table, so get the name from there to match a lab that is defined already.</p> In\u00a0[\u00a0]: Copied! <pre>KacheryZone()\nAnalysisNwbfileKachery()\n</pre> KacheryZone() AnalysisNwbfileKachery() In\u00a0[\u00a0]: Copied! <pre># zone_key = {'kachery_zone_name': 'franklab.collaborators', \n#             'description'      : 'franklab collaborator zone',\n#             'kachery_cloud_dir': '/stelmo/nwb/.kachery_cloud',\n#             'kachery_proxy'    : 'https://kachery-resource-proxy.herokuapp.com',\n#             'lab_name'         : 'Loren Frank'}\n# KacheryZone().insert1(zone_key)\n</pre> # zone_key = {'kachery_zone_name': 'franklab.collaborators',  #             'description'      : 'franklab collaborator zone', #             'kachery_cloud_dir': '/stelmo/nwb/.kachery_cloud', #             'kachery_proxy'    : 'https://kachery-resource-proxy.herokuapp.com', #             'lab_name'         : 'Loren Frank'} # KacheryZone().insert1(zone_key) <p>Now that our zone exists, all we have to do is add the AnalysisNWB files that we want to share. To do so, add entries to the AnalysisNwbfileKacherySelectionTable as sketched out below</p> In\u00a0[\u00a0]: Copied! <pre>#note that this would share all AnalysisFiles associated with a given nwb_file\nnwb_file_name = \"wilbur20210331_.nwb\"\n\nanalysis_file_list = (AnalysisNwbfile() &amp; {'nwb_file_name': nwb_file_name}).fetch('analysis_file_name')\n\nanks_key = {'kachery_zone_name': 'franklab.collaborators', 'analysis_file_name': ''}\nfor file in analysis_file_list:\n    anks_key['analysis_file_name'] = file\n    AnalysisNwbfileKacherySelection.insert1(anks_key)\n</pre> #note that this would share all AnalysisFiles associated with a given nwb_file nwb_file_name = \"wilbur20210331_.nwb\"  analysis_file_list = (AnalysisNwbfile() &amp; {'nwb_file_name': nwb_file_name}).fetch('analysis_file_name')  anks_key = {'kachery_zone_name': 'franklab.collaborators', 'analysis_file_name': ''} for file in analysis_file_list:     anks_key['analysis_file_name'] = file     AnalysisNwbfileKacherySelection.insert1(anks_key)  <p>Now that we have those files in the selection table, we can add them as links to the kachery zone by populating the AnalysisNwbfileKachery table:</p> In\u00a0[\u00a0]: Copied! <pre>AnalysisNwbfileKachery.populate()\n</pre> AnalysisNwbfileKachery.populate() <p>If all of that worked,</p> <ol> <li>go to https://kachery-gateway.figurl.org/admin?zone=your_zone (changing your_zone to the name of your zone)</li> <li>Go to the Admin/Authorization Settings tab</li> <li>Add the github login names and permissions for the users you want to share with.</li> </ol> <p>If those users can connect to your database, they should now be able to use the .fetch_nwb() method to download any AnalysisNwbfiles that have been shared through Kachery.</p> <p>for example:</p> <p>nwb_file_name = \"wilbur20210331_.nwb\" from spyglass.spikesorting import CuratedSpikeSorting</p> <p>test_sort = (CuratedSpikeSorting &amp; {'nwb_file_name' : nwb_file_name}).fetch()[0] sort = (CuratedSpikeSorting &amp; test_sort).fetch_nwb()</p>"}, {"location": "notebooks/Spyglass_kachery_setup/#this-notebook-contains-instructions-for-setting-up-spyglass-data-sharing-through-kachery-cloud", "title": "This notebook contains instructions for setting up Spyglass data sharing through Kachery cloud.\u00b6", "text": ""}, {"location": "notebooks/Spyglass_kachery_setup/#overview-of-functionality", "title": "Overview of functionality\u00b6", "text": "<p>Spyglass uses Kachery Cloud (https://github.com/flatironinstitute/kachery-cloud) to make it possible to share analysis results (stored in NWB files) with others who do not have access to the filesystem where the original files are stored.</p> <p>When a user tries to access an AnalysisNWB file, Spyglass does the following:</p> <ol> <li>Try to load it from the local store.</li> <li>If it is not available, check if the file is in the relevant sharing table (NwbKachery or AnalysisNWBKachery).</li> <li>If so, attempt to download from the associated Kachery Resource.</li> </ol> <p>Note that large file downloads may take a long time, so we currently do not support sharing of the main NWB files in this way. We suggest direct transfer of these files with globus or a similar service.</p>"}, {"location": "notebooks/Spyglass_kachery_setup/#overview-of-setup", "title": "Overview of setup\u00b6", "text": "<p>The Frank laboratory has three separate Kachery zones:</p> <ol> <li>franklab.default Used for all internal file sharing, including figurls</li> <li>franklab.collaborator Used for files that should be shared with collaborating labs.</li> <li>franklab.public Used for files that are publicly accessible (not yet active)</li> </ol> <p>You may or may not want to set up a similar list on your systems depending on your sharing needs, and as each zone is setup in the same way, you can choose how many zones to establish or add new zones later relatively easily.</p>"}, {"location": "notebooks/Spyglass_kachery_setup/#setting-up-kachery-zones", "title": "Setting up Kachery zones.\u00b6", "text": "<p>The instructions for setting up each zone can be found at https://github.com/flatironinstitute/kachery-cloud/blob/main/doc/create_kachery_zone.md  These instructions walk you through the process of creating a cloud bucket and registering it with the Kachery team. Note that the bucket names cannot include periods, so we substite a dash, as in \"franklab-default\".</p> <ol> <li>Follow the instructions for the first zone you want to create.</li> <li>Follow the same instructions for the subsequent zones, but note that you can skip the Click \"Create API Token\" step</li> </ol>"}, {"location": "notebooks/Spyglass_kachery_setup/#setting-up-kachery-resources", "title": "Setting up Kachery resources.\u00b6", "text": "<p>If you want to share files on demand from a zone, you need to set up a resource for that zone, following the instructions at https://github.com/scratchrealm/kachery-resource/blob/main/README.md</p> <p>We suggest making the name of the zone and the resource identical to make things easy.</p> <p>Note that for each zone, you need to run the local daemon that listens for requests from that zone. An example of the bash script we use is</p> <pre><code>#!/bin/bash\nexport KACHERY_ZONE=franklab.collaborators\nexport KACHERY_CLOUD_DIR=/stelmo/nwb/.kachery_cloud\ncd /stelmo/nwb/franklab_collaborators_resource\nnpx kachery-resource@latest share\n</code></pre>"}, {"location": "notebooks/Spyglass_kachery_setup/#add-zonesresources-to-your-spyglass-database", "title": "Add zones/resources to your Spyglass database\u00b6", "text": "<p>Once the zones and resources exist, you need to add them to the database</p> <p>First we import spyglass, etc.</p>"}, {"location": "notebooks/Spyglass_kachery_setup/#if-we-wanted-to-insert-a-new-zone-we-could-use-the-cell-below-currently-commented-out-to-avoid-problems", "title": "If we wanted to insert a new zone, we could use the cell below (currently commented out to avoid problems)\u00b6", "text": ""}, {"location": "notebooks/docker_mysql_tutorial/", "title": "Docker", "text": "In\u00a0[12]: Copied! <pre>import datajoint as dj\nimport os\nfrom pathlib import Path\n\n# set dirs\nbase_dir = Path(\"/home/kyu/dj\")  # change this to your desired directory\nif (base_dir).exists() is False:\n    os.mkdir(base_dir)\nraw_dir = base_dir / \"raw\"\nif (raw_dir).exists() is False:\n    os.mkdir(raw_dir)\nanalysis_dir = base_dir / \"analysis\"\nif (analysis_dir).exists() is False:\n    os.mkdir(analysis_dir)\nkachery_storage_dir = base_dir / \"kachery-storage\"\nif (kachery_storage_dir).exists() is False:\n    os.mkdir(kachery_storage_dir)\nrecording_dir = base_dir / \"recording\"\nif (recording_dir).exists() is False:\n    os.mkdir(recording_dir)\nsorting_dir = base_dir / \"sorting\"\nif (sorting_dir).exists() is False:\n    os.mkdir(sorting_dir)\nwaveforms_dir = base_dir / \"waveforms\"\nif (waveforms_dir).exists() is False:\n    os.mkdir(waveforms_dir)\ntmp_dir = base_dir / \"tmp\"\nif (tmp_dir).exists() is False:\n    os.mkdir(tmp_dir)\n\n# set dj config\ndj.config[\"database.host\"] = \"localhost\"\ndj.config[\"database.user\"] = \"root\"\ndj.config[\"database.password\"] = \"tutorial\"\ndj.config[\"database.port\"] = 3306\ndj.config[\"stores\"] = {\n    \"raw\": {\"protocol\": \"file\", \"location\": str(raw_dir), \"stage\": str(raw_dir)},\n    \"analysis\": {\n        \"protocol\": \"file\",\n        \"location\": str(analysis_dir),\n        \"stage\": str(analysis_dir),\n    },\n}\n\n# set env vars\nos.environ[\"SPYGLASS_BASE_DIR\"] = str(base_dir)\nos.environ[\"SPYGLASS_RECORDING_DIR\"] = str(recording_dir)\nos.environ[\"SPYGLASS_SORTING_DIR\"] = str(sorting_dir)\nos.environ[\"SPYGLASS_WAVEFORMS_DIR\"] = str(waveforms_dir)\nos.environ[\"KACHERY_STORAGE_DIR\"] = str(kachery_storage_dir)\nos.environ[\"KACHERY_TEMP_DIR\"] = str(tmp_dir)\nos.environ[\"FIGURL_CHANNEL\"] = \"franklab2\"\nos.environ[\"DJ_SUPPORT_FILEPATH_MANAGEMENT\"] = \"TRUE\"\n</pre> import datajoint as dj import os from pathlib import Path  # set dirs base_dir = Path(\"/home/kyu/dj\")  # change this to your desired directory if (base_dir).exists() is False:     os.mkdir(base_dir) raw_dir = base_dir / \"raw\" if (raw_dir).exists() is False:     os.mkdir(raw_dir) analysis_dir = base_dir / \"analysis\" if (analysis_dir).exists() is False:     os.mkdir(analysis_dir) kachery_storage_dir = base_dir / \"kachery-storage\" if (kachery_storage_dir).exists() is False:     os.mkdir(kachery_storage_dir) recording_dir = base_dir / \"recording\" if (recording_dir).exists() is False:     os.mkdir(recording_dir) sorting_dir = base_dir / \"sorting\" if (sorting_dir).exists() is False:     os.mkdir(sorting_dir) waveforms_dir = base_dir / \"waveforms\" if (waveforms_dir).exists() is False:     os.mkdir(waveforms_dir) tmp_dir = base_dir / \"tmp\" if (tmp_dir).exists() is False:     os.mkdir(tmp_dir)  # set dj config dj.config[\"database.host\"] = \"localhost\" dj.config[\"database.user\"] = \"root\" dj.config[\"database.password\"] = \"tutorial\" dj.config[\"database.port\"] = 3306 dj.config[\"stores\"] = {     \"raw\": {\"protocol\": \"file\", \"location\": str(raw_dir), \"stage\": str(raw_dir)},     \"analysis\": {         \"protocol\": \"file\",         \"location\": str(analysis_dir),         \"stage\": str(analysis_dir),     }, }  # set env vars os.environ[\"SPYGLASS_BASE_DIR\"] = str(base_dir) os.environ[\"SPYGLASS_RECORDING_DIR\"] = str(recording_dir) os.environ[\"SPYGLASS_SORTING_DIR\"] = str(sorting_dir) os.environ[\"SPYGLASS_WAVEFORMS_DIR\"] = str(waveforms_dir) os.environ[\"KACHERY_STORAGE_DIR\"] = str(kachery_storage_dir) os.environ[\"KACHERY_TEMP_DIR\"] = str(tmp_dir) os.environ[\"FIGURL_CHANNEL\"] = \"franklab2\" os.environ[\"DJ_SUPPORT_FILEPATH_MANAGEMENT\"] = \"TRUE\"  <pre>/home/kyu/miniconda3/envs/spyglass/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n</pre> <p>Now you should be able to connect to the datajoint server running in Docker.</p> In\u00a0[2]: Copied! <pre>import spyglass as nd\n</pre> import spyglass as nd  <pre>Connecting root@localhost:3306\n</pre> <p>Here is an example of a table that you can inspect. It should be empty since there is nothing in our databse.</p> In\u00a0[13]: Copied! <pre>nd.common.SpikeSorting()\n</pre> nd.common.SpikeSorting()  Out[13]: Table for holding spike sorting runs <p>nwb_file_name</p> name of the NWB file <p>sort_group_id</p> identifier for a group of electrodes <p>sorter_name</p> the name of the spike sorting algorithm <p>parameter_set_name</p> label for this set of parameters <p>sort_interval_name</p> name for this interval <p>analysis_file_name</p> name of the file <p>units_object_id</p> Object ID for the units in NWB file <p>time_of_sort</p> This is when the sort was done <p>Total: 0</p>"}, {"location": "notebooks/docker_mysql_tutorial/#starting-your-own-datajoint-mysql-database-with-docker", "title": "Starting your own <code>datajoint</code> (mysql) database with Docker\u00b6", "text": "<p>First, install Docker. Add yourself to the <code>docker</code> group so that you don't have to be sudo to run docker. Then download the docker image for datajoint/mysql:</p> <pre>docker pull datajoint/mysql\n</pre> <p>Start the server in a Docker container with the mysql root password, which we set to <code>tutorial</code>. Give the container a name you like. As for the mapping between ports, we use 3306 by default.</p> <pre>docker run --name &lt;name&gt; -p 3306:3306 -e MYSQL_ROOT_PASSWORD=tutorial datajoint/mysql\n</pre> <p>If you want to make the data persist after terminating the container, you may want to attach a volume to your Docker container as part of the command to running it:</p> <pre>docker volume create dj-vol\ndocker run --name &lt;name&gt; -v dj-vol:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=tutorial datajoint/mysql\n</pre> <p>Next, set the following configurations by running the cell below</p>"}]}